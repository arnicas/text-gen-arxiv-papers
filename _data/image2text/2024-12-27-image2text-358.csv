title,pubdate,id,authors,categories,search,abstract,displaydate
Is Your Text-to-Image Model Robust to Caption Noise?,2024-12-27 08:53:37+00:00,http://arxiv.org/abs/2412.19531v1,"Weichen Yu, Ziyan Yang, Shanchuan Lin, Qi Zhao, Jianyi Wang, Liangke Gui, Matt Fredrikson, Lu Jiang","cs.CV, cs.AI",image2text,"In text-to-image (T2I) generation, a prevalent training technique involves
utilizing Vision Language Models (VLMs) for image re-captioning. Even though
VLMs are known to exhibit hallucination, generating descriptive content that
deviates from the visual reality, the ramifications of such caption
hallucinations on T2I generation performance remain under-explored. Through our
empirical investigation, we first establish a comprehensive dataset comprising
VLM-generated captions, and then systematically analyze how caption
hallucination influences generation outcomes. Our findings reveal that (1) the
disparities in caption quality persistently impact model outputs during
fine-tuning. (2) VLMs confidence scores serve as reliable indicators for
detecting and characterizing noise-related patterns in the data distribution.
(3) even subtle variations in caption fidelity have significant effects on the
quality of learned representations. These findings collectively emphasize the
profound impact of caption quality on model performance and highlight the need
for more sophisticated robust training algorithm in T2I. In response to these
observations, we propose a approach leveraging VLM confidence score to mitigate
caption noise, thereby enhancing the robustness of T2I models against
hallucination in caption.",2024-12-27
"RDPM: Solve Diffusion Probabilistic Models via Recurrent Token
  Prediction",2024-12-24 12:28:19+00:00,http://arxiv.org/abs/2412.18390v2,"Xiaoping Wu, Jie Hu, Xiaoming Wei","cs.CV, cs.AI, cs.LG, cs.MM",image2text,"Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach
for high-fidelity image synthesis, operating diffusion processes on continuous
VAE latent, which significantly differ from the text generation methods
employed by Large Language Models (LLMs). In this paper, we introduce a novel
generative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which
enhances the diffusion process through a recurrent token prediction mechanism,
thereby pioneering the field of Discrete Diffusion. By progressively
introducing Gaussian noise into the latent representations of images and
encoding them into vector-quantized tokens in a recurrent manner, RDPM
facilitates a unique diffusion process on discrete-value domains. This process
iteratively predicts the token codes for subsequent timesteps, transforming the
initial standard Gaussian noise into the source data distribution, aligning
with GPT-style models in terms of the loss function. RDPM demonstrates superior
performance while benefiting from the speed advantage of requiring only a few
inference steps. This model not only leverages the diffusion process to ensure
high-quality generation but also converts continuous signals into a series of
high-fidelity discrete tokens, thereby maintaining a unified optimization
strategy with other discrete tokens, such as text. We anticipate that this work
will contribute to the development of a unified model for multimodal
generation, specifically by integrating continuous signal domains such as
images, videos, and audio with text. We will release the code and model weights
to the open-source community.",2024-12-24
"GenAI Content Detection Task 2: AI vs. Human -- Academic Essay
  Authenticity Challenge",2024-12-24 08:33:44+00:00,http://arxiv.org/abs/2412.18274v1,"Shammur Absar Chowdhury, Hind Almerekhi, Mucahid Kutlu, Kaan Efe Keles, Fatema Ahmad, Tasnim Mohiuddin, George Mikros, Firoj Alam","cs.CL, cs.AI, 68T50, F.2.2; I.2.7",image2text,"This paper presents a comprehensive overview of the first edition of the
Academic Essay Authenticity Challenge, organized as part of the GenAI Content
Detection shared tasks collocated with COLING 2025. This challenge focuses on
detecting machine-generated vs. human-authored essays for academic purposes.
The task is defined as follows: ""Given an essay, identify whether it is
generated by a machine or authored by a human.'' The challenge involves two
languages: English and Arabic. During the evaluation phase, 25 teams submitted
systems for English and 21 teams for Arabic, reflecting substantial interest in
the task. Finally, seven teams submitted system description papers. The
majority of submissions utilized fine-tuned transformer-based models, with one
team employing Large Language Models (LLMs) such as Llama 2 and Llama 3. This
paper outlines the task formulation, details the dataset construction process,
and explains the evaluation framework. Additionally, we present a summary of
the approaches adopted by participating teams. Nearly all submitted systems
outperformed the n-gram-based baseline, with the top-performing systems
achieving F1 scores exceeding 0.98 for both languages, indicating significant
progress in the detection of machine-generated text.",2024-12-24
"PromptDresser: Improving the Quality and Controllability of Virtual
  Try-On via Generative Textual Prompt and Prompt-aware Mask",2024-12-22 11:38:04+00:00,http://arxiv.org/abs/2412.16978v1,"Jeongho Kim, Hoiyeong Jin, Sunghyun Park, Jaegul Choo","cs.CV, cs.AI",image2text,"Recent virtual try-on approaches have advanced by fine-tuning the pre-trained
text-to-image diffusion models to leverage their powerful generative ability.
However, the use of text prompts in virtual try-on is still underexplored. This
paper tackles a text-editable virtual try-on task that changes the clothing
item based on the provided clothing image while editing the wearing style
(e.g., tucking style, fit) according to the text descriptions. In the
text-editable virtual try-on, three key aspects exist: (i) designing rich text
descriptions for paired person-clothing data to train the model, (ii)
addressing the conflicts where textual information of the existing person's
clothing interferes the generation of the new clothing, and (iii) adaptively
adjust the inpainting mask aligned with the text descriptions, ensuring proper
editing areas while preserving the original person's appearance irrelevant to
the new clothing. To address these aspects, we propose PromptDresser, a
text-editable virtual try-on model that leverages large multimodal model (LMM)
assistance to enable high-quality and versatile manipulation based on
generative text prompts. Our approach utilizes LMMs via in-context learning to
generate detailed text descriptions for person and clothing images
independently, including pose details and editing attributes using minimal
human cost. Moreover, to ensure the editing areas, we adjust the inpainting
mask depending on the text prompts adaptively. We found that our approach,
utilizing detailed text prompts, not only enhances text editability but also
effectively conveys clothing details that are difficult to capture through
images alone, thereby enhancing image quality. Our code is available at
https://github.com/rlawjdghek/PromptDresser.",2024-12-22
"Movie2Story: A framework for understanding videos and telling stories in
  the form of novel text",2024-12-19 15:44:04+00:00,http://arxiv.org/abs/2412.14965v1,"Kangning Li, Zheyang Jia, Anyu Ying","cs.CV, cs.AI, cs.CL",image2text,"Multimodal video-to-text models have made considerable progress, primarily in
generating brief descriptions of video content. However, there is still a
deficiency in generating rich long-form text descriptions that integrate both
video and audio. In this paper, we introduce a framework called M2S, designed
to generate novel-length text by combining audio, video, and character
recognition. M2S includes modules for video long-form text description and
comprehension, audio-based analysis of emotion, speech rate, and character
alignment, and visual-based character recognition alignment. By integrating
multimodal information using the large language model GPT4o, M2S stands out in
the field of multimodal text generation. We demonstrate the effectiveness and
accuracy of M2S through comparative experiments and human evaluation.
Additionally, the model framework has good scalability and significant
potential for future research.",2024-12-19
"HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic
  Evaluation Using a Vision Language Model",2024-12-19 08:03:16+00:00,http://arxiv.org/abs/2412.14613v1,"Masanari Ohi, Masahiro Kaneko, Naoaki Okazaki, Nakamasa Inoue","cs.CL, cs.AI, cs.CV",image2text,"Vision-language models (VLMs) have shown impressive abilities in text and
image understanding. However, existing metrics for evaluating the text
generated by VLMs focus exclusively on overall quality, leading to two
limitations: 1) it is challenging to identify which aspects of the text need
improvement from the overall score; 2) metrics may overlook specific evaluation
criteria when predicting an overall score. To address these limitations, we
propose HarmonicEval, a reference-free evaluation metric that aggregates
criterion-wise scores to produce the overall score in a bottom-up manner.
Furthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE)
dataset, which comprises 18,000 expert human judgments across four
vision-language tasks. Our experiments demonstrate that HarmonicEval achieves
higher correlations with human judgments than conventional metrics while
providing numerical scores for each criterion.",2024-12-19
"Typhoon 2: A Family of Open Text and Multimodal Thai Large Language
  Models",2024-12-18 10:45:24+00:00,http://arxiv.org/abs/2412.13702v2,"Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai","cs.CL, cs.AI",image2text,"This paper introduces Typhoon 2, a series of text and multimodal large
language models optimized for the Thai language. The series includes models for
text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models,
such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture
of English and Thai data. We employ post-training techniques to enhance Thai
language performance while preserving the base models' original capabilities.
We release text models across a range of sizes, from 1 to 70 billion
parameters, available in both base and instruction-tuned variants. To guardrail
text generation, we release Typhoon2-Safety, a classifier enhanced for Thai
cultures and language. Typhoon2-Vision improves Thai document understanding
while retaining general visual capabilities, such as image captioning.
Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture
capable of processing audio, speech, and text inputs and generating both text
and speech outputs.",2024-12-18
"LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse
  Input Contexts",2024-12-16 17:29:51+00:00,http://arxiv.org/abs/2412.12001v1,"Zhuhao Wang, Yihua Sun, Zihan Li, Xuan Yang, Fang Chen, Hongen Liao","cs.CL, cs.CV",image2text,"Drafting radiology reports is a complex task requiring flexibility, where
radiologists tail content to available information and particular clinical
demands. However, most current radiology report generation (RRG) models are
constrained to a fixed task paradigm, such as predicting the full ``finding''
section from a single image, inherently involving a mismatch between inputs and
outputs. The trained models lack the flexibility for diverse inputs and could
generate harmful, input-agnostic hallucinations. To bridge the gap between
current RRG models and the clinical demands in practice, we first develop a
data generation pipeline to create a new MIMIC-RG4 dataset, which considers
four common radiology report drafting scenarios and has perfectly corresponded
input and output. Secondly, we propose a novel large language model (LLM) based
RRG framework, namely LLM-RG4, which utilizes LLM's flexible
instruction-following capabilities and extensive general knowledge. We further
develop an adaptive token fusion module that offers flexibility to handle
diverse scenarios with different input combinations, while minimizing the
additional computational burden associated with increased input volumes.
Besides, we propose a token-level loss weighting strategy to direct the model's
attention towards positive and uncertain descriptions. Experimental results
demonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical
efficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR
datasets. We quantitatively demonstrate that our model has minimal
input-agnostic hallucinations, whereas current open-source models commonly
suffer from this problem.",2024-12-16
"VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video
  Prompting",2024-12-16 10:08:38+00:00,http://arxiv.org/abs/2412.11621v1,"Muhammet Furkan Ilaslan, Ali Koksal, Kevin Qinhong Lin, Burak Satar, Mike Zheng Shou, Qianli Xu","cs.CV, cs.MM",image2text,"Large Language Model (LLM)-based agents have shown promise in procedural
tasks, but the potential of multimodal instructions augmented by texts and
videos to assist users remains under-explored. To address this gap, we propose
the Visually Grounded Text-Video Prompting (VG-TVP) method which is a novel
LLM-empowered Multimodal Procedural Planning (MPP) framework. It generates
cohesive text and video procedural plans given a specified high-level
objective. The main challenges are achieving textual and visual
informativeness, temporal coherence, and accuracy in procedural plans. VG-TVP
leverages the zero-shot reasoning capability of LLMs, the video-to-text
generation ability of the video captioning models, and the text-to-video
generation ability of diffusion models. VG-TVP improves the interaction between
modalities by proposing a novel Fusion of Captioning (FoC) method and using
Text-to-Video Bridge (T2V-B) and Video-to-Text Bridge (V2T-B). They allow LLMs
to guide the generation of visually-grounded text plans and textual-grounded
video plans. To address the scarcity of datasets suitable for MPP, we have
curated a new dataset called Daily-Life Task Procedural Plans (Daily-PP). We
conduct comprehensive experiments and benchmarks to evaluate human preferences
(regarding textual and visual informativeness, temporal coherence, and plan
accuracy). Our VG-TVP method outperforms unimodal baselines on the Daily-PP
dataset.",2024-12-16
Homeostasis and Sparsity in Transformer,2024-11-30 15:03:41+00:00,http://arxiv.org/abs/2412.00503v3,"Leonid Kotyuzanskiy, Artem Klimov","cs.LG, cs.AI",image2text,"The transformer architecture has become an integral part of the field of
modern neural networks, playing a crucial role in a variety of tasks, such as
text generation, machine translation, image and audio processing, among others.
There is also an alternative approach to building intelligent systems, proposed
by Jeff Hawkins and inspired by the processes occurring in the neocortex. In
our article we want to combine some of these ideas and to propose the use of
homeostasis mechanisms, such as RFB-kWTA and ""Smart"" Inhibition, in the
attention mechanism of the transformer and at the output of the transformer
block, as well as conducting an experiment involving the introduction of sparse
distributed representations of the transformer at various points. RFB-kWTA
utilizes statistics of layer activations across time to adjust the entire
layer, enhancing the values of rare activations while reducing those of
frequent ones. ""Smart"" Inhibition also uses activation statistics to sample
sparsity masks, with rarer activation times are more likely to be activated.
Our proposed mechanisms significantly outperform the classical transformer
0.2768 BLEU and a model that only makes use of dropout in the attention
mechanism and output of the transformer block 0.3007 BLEU, achieving a score of
0.3062 on the Multi30K dataset.",2024-11-30
"LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology
  Report Generation",2024-11-25 16:10:05+00:00,http://arxiv.org/abs/2411.16523v1,"Steven Song, Anirudh Subramanyam, Irene Madejski, Robert L. Grossman","cs.CV, cs.CL",image2text,"In the current paradigm of image captioning, deep learning models are trained
to generate text from image embeddings of latent features. We challenge the
assumption that these latent features ought to be high-dimensional vectors
which require model fine tuning to handle. Here we propose Label Boosted
Retrieval Augmented Generation (LaB-RAG), a text-based approach to image
captioning that leverages image descriptors in the form of categorical labels
to boost standard retrieval augmented generation (RAG) with pretrained large
language models (LLMs). We study our method in the context of radiology report
generation (RRG), where the task is to generate a clinician's report detailing
their observations from a set of radiological images, such as X-rays. We argue
that simple linear classifiers over extracted image embeddings can effectively
transform X-rays into text-space as radiology-specific labels. In combination
with standard RAG, we show that these derived text labels can be used with
general-domain LLMs to generate radiology reports. Without ever training our
generative language model or image feature encoder models, and without ever
directly ""showing"" the LLM an X-ray, we demonstrate that LaB-RAG achieves
better results across natural language and radiology language metrics compared
with other retrieval-based RRG methods, while attaining competitive results
compared to other fine-tuned vision-language RRG models. We further present
results of our experiments with various components of LaB-RAG to better
understand our method. Finally, we critique the use of a popular RRG metric,
arguing it is possible to artificially inflate its results without true
data-leakage.",2024-11-25
"MolMetaLM: a Physicochemical Knowledge-Guided Molecular Meta Language
  Model",2024-11-23 09:27:38+00:00,http://arxiv.org/abs/2411.15500v1,"Yifan Wu, Min Zeng, Yang Li, Yang Zhang, Min Li","cs.ET, cs.CL",image2text,"Most current molecular language models transfer the masked language model or
image-text generation model from natural language processing to molecular
field. However, molecules are not solely characterized by atom/bond symbols;
they encapsulate important physical/chemical properties. Moreover, normal
language models bring grammar rules that are irrelevant for understanding
molecules. In this study, we propose a novel physicochemical knowledge-guided
molecular meta language framework MolMetaLM. We design a molecule-specialized
meta language paradigm, formatted as multiple <S,P,O> (subject, predicate,
object) knowledge triples sharing the same S (i.e., molecule) to enhance
learning the semantic relationships between physicochemical knowledge and
molecules. By introducing different molecular knowledge and noises, the meta
language paradigm generates tens of thousands of pretraining tasks. By
recovering the token/sequence/order-level noises, MolMetaLM exhibits
proficiency in large-scale benchmark evaluations involving property prediction,
molecule generation, conformation inference, and molecular optimization.
Through MolMetaLM, we offer a new insight for designing language models.",2024-11-23
"Improving Factuality of 3D Brain MRI Report Generation with Paired
  Image-domain Retrieval and Text-domain Augmentation",2024-11-23 08:18:55+00:00,http://arxiv.org/abs/2411.15490v1,"Junhyeok Lee, Yujin Oh, Dahyoun Lee, Hyon Keun Joh, Chul-Ho Sohn, Sung Hyun Baik, Cheol Kyu Jung, Jung Hyun Park, Kyu Sung Choi, Byung-Hoon Kim, Jong Chul Ye","cs.CV, cs.LG, eess.IV",image2text,"Acute ischemic stroke (AIS) requires time-critical management, with hours of
delayed intervention leading to an irreversible disability of the patient.
Since diffusion weighted imaging (DWI) using the magnetic resonance image (MRI)
plays a crucial role in the detection of AIS, automated prediction of AIS from
DWI has been a research topic of clinical importance. While text radiology
reports contain the most relevant clinical information from the image findings,
the difficulty of mapping across different modalities has limited the
factuality of conventional direct DWI-to-report generation methods. Here, we
propose paired image-domain retrieval and text-domain augmentation (PIRTA), a
cross-modal retrieval-augmented generation (RAG) framework for providing
clinician-interpretative AIS radiology reports with improved factuality. PIRTA
mitigates the need for learning cross-modal mapping, which poses difficulty in
image-to-text generation, by casting the cross-modal mapping problem as an
in-domain retrieval of similar DWI images that have paired ground-truth text
radiology reports. By exploiting the retrieved radiology reports to augment the
report generation process of the query image, we show by experiments with
extensive in-house and public datasets that PIRTA can accurately retrieve
relevant reports from 3D DWI images. This approach enables the generation of
radiology reports with significantly higher accuracy compared to direct
image-to-text generation using state-of-the-art multimodal language models.",2024-11-23
"Benchmarking Multimodal Models for Ukrainian Language Understanding
  Across Academic and Cultural Domains",2024-11-22 00:37:49+00:00,http://arxiv.org/abs/2411.14647v1,"Yurii Paniv, Artur Kiulian, Dmytro Chaplynskyi, Mykola Khandoga, Anton Polishko, Tetiana Bas, Guillermo Gabrielli",cs.CL,image2text,"While the evaluation of multimodal English-centric models is an active area
of research with numerous benchmarks, there is a profound lack of benchmarks or
evaluation suites for low- and mid-resource languages. We introduce ZNO-Vision,
a comprehensive multimodal Ukrainian-centric benchmark derived from
standardized university entrance examination (ZNO). The benchmark consists of
over 4,300 expert-crafted questions spanning 12 academic disciplines, including
mathematics, physics, chemistry, and humanities. We evaluated the performance
of both open-source models and API providers, finding that only a handful of
models performed above baseline. Alongside the new benchmark, we performed the
first evaluation study of multimodal text generation for the Ukrainian
language: we measured caption generation quality on the Multi30K-UK dataset,
translated the VQA benchmark into Ukrainian, and measured performance
degradation relative to original English versions. Lastly, we tested a few
models from a cultural perspective on knowledge of national cuisine. We believe
our work will advance multimodal generation capabilities for the Ukrainian
language and our approach could be useful for other low-resource languages.",2024-11-22
"From Text to Pose to Image: Improving Diffusion Model Control and
  Quality",2024-11-19 21:34:50+00:00,http://arxiv.org/abs/2411.12872v2,"Clément Bonnet, Ariel N. Lee, Franck Wertel, Antoine Tamano, Tanguy Cizain, Pablo Ducru","cs.CV, cs.AI, cs.LG",image2text,"In the last two years, text-to-image diffusion models have become extremely
popular. As their quality and usage increase, a major concern has been the need
for better output control. In addition to prompt engineering, one effective
method to improve the controllability of diffusion models has been to condition
them on additional modalities such as image style, depth map, or keypoints.
This forms the basis of ControlNets or Adapters. When attempting to apply these
methods to control human poses in outputs of text-to-image diffusion models,
two main challenges have arisen. The first challenge is generating poses
following a wide range of semantic text descriptions, for which previous
methods involved searching for a pose within a dataset of (caption, pose)
pairs. The second challenge is conditioning image generation on a specified
pose while keeping both high aesthetic and high pose fidelity. In this article,
we fix these two main issues by introducing a text-to-pose (T2P) generative
model alongside a new sampling algorithm, and a new pose adapter that
incorporates more pose keypoints for higher pose fidelity. Together, these two
new state-of-the-art models enable, for the first time, a generative
text-to-pose-to-image framework for higher pose control in diffusion models. We
release all models and the code used for the experiments at
https://github.com/clement-bonnet/text-to-pose.",2024-11-19
"Debias your Large Multi-Modal Model at Test-Time with Non-Contrastive
  Visual Attribute Steering",2024-11-15 20:06:09+00:00,http://arxiv.org/abs/2411.12590v1,"Neale Ratzlaff, Matthew Lyle Olson, Musashi Hinck, Estelle Aflalo, Shao-Yen Tseng, Vasudev Lal, Phillip Howard","cs.CV, cs.LG",image2text,"Large Multi-Modal Models (LMMs) have demonstrated impressive capabilities as
general-purpose chatbots that can engage in conversations about a provided
input, such as an image. However, their responses are influenced by societal
biases present in their training datasets, leading to undesirable differences
in how the model responds when presented with images depicting people of
different demographics. In this work, we propose a novel debiasing framework
for LMMs that directly removes biased representations during text generation to
decrease outputs related to protected attributes, or even representing them
internally. Our proposed method is training-free; given a single image and a
list of target attributes, we can ablate the corresponding representations with
just one step of gradient descent on the image itself. Our experiments show
that not only can we can minimize the propensity of LMMs to generate text
related to protected attributes, but we can improve sentiment and even simply
use synthetic data to inform the ablation while retaining language modeling
capabilities on real data such as COCO or FACET. Furthermore, we find the
resulting generations from a debiased LMM exhibit similar accuracy as a
baseline biased model, showing that debiasing effects can be achieved without
sacrificing model performance.",2024-11-15
"Bridging the Visual Gap: Fine-Tuning Multimodal Models with
  Knowledge-Adapted Captions",2024-11-13 20:50:04+00:00,http://arxiv.org/abs/2411.09018v1,"Moran Yanuka, Assaf Ben Kish, Yonatan Bitton, Idan Szpektor, Raja Giryes","cs.CV, cs.CL, cs.LG",image2text,"Recent research increasingly focuses on training vision-language models
(VLMs) with long, detailed image captions. However, small-scale VLMs often
struggle to balance the richness of these captions with the risk of
hallucinating content during fine-tuning. In this paper, we explore how well
VLMs adapt to such captions. To quantify caption quality, we propose Decomposed
NLI (DNLI), an evaluation framework that breaks down generated captions into
individual propositions, assessing each in isolation. This fine-grained
analysis reveals a critical balance between capturing descriptive details and
preventing hallucinations. Our findings show that simply reducing caption
complexity or employing standard data curation techniques does not effectively
resolve this issue. To tackle this challenge, we introduce Knowledge Adapted
(KnowAda) fine-tuning, a data-centric approach that automatically adapts
training data with the model's existing knowledge and visual understanding.
KnowAda minimizes hallucinations while preserving high descriptiveness. We
validate this approach across several small-scale VLMs (up to 7B parameters)
and dense caption datasets, demonstrating that KnowAda effectively balances
hallucination reduction and descriptiveness. Our results show that KnowAda
outperforms various baselines in both automatic metrics and human evaluations.
We will release our code and models.",2024-11-13
"Decoding Report Generators: A Cyclic Vision-Language Adapter for
  Counterfactual Explanations",2024-11-08 01:46:11+00:00,http://arxiv.org/abs/2411.05261v1,"Yingying Fang, Zihao Jin, Shaojie Guo, Jinda Liu, Yijian Gao, Junzhi Ning, Zhiling Yue, Zhi Li, Simon LF Walsh, Guang Yang","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Despite significant advancements in report generation methods, a critical
limitation remains: the lack of interpretability in the generated text. This
paper introduces an innovative approach to enhance the explainability of text
generated by report generation models. Our method employs cyclic text
manipulation and visual comparison to identify and elucidate the features in
the original content that influence the generated text. By manipulating the
generated reports and producing corresponding images, we create a comparative
framework that highlights key attributes and their impact on the text
generation process. This approach not only identifies the image features
aligned to the generated text but also improves transparency but also provides
deeper insights into the decision-making mechanisms of the report generation
models. Our findings demonstrate the potential of this method to significantly
enhance the interpretability and transparency of AI-generated reports.",2024-11-08
"PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology
  Report Generation",2024-11-07 19:06:17+00:00,http://arxiv.org/abs/2411.05085v1,"Daniel C. Castro, Aurelia Bustos, Shruthi Bannur, Stephanie L. Hyland, Kenza Bouzid, Maria Teodora Wetscherek, Maria Dolores Sánchez-Valverde, Lara Jaques-Pérez, Lourdes Pérez-Rodríguez, Kenji Takeda, José María Salinas, Javier Alvarez-Valle, Joaquín Galant Herrero, Antonio Pertusa","cs.AI, cs.CL, cs.CV",image2text,"Radiology report generation (RRG) aims to create free-text radiology reports
from clinical imaging. Grounded radiology report generation (GRRG) extends RRG
by including the localisation of individual findings on the image. Currently,
there are no manually annotated chest X-ray (CXR) datasets to train GRRG
models. In this work, we present a dataset called PadChest-GR
(Grounded-Reporting) derived from PadChest aimed at training GRRG models for
CXR images. We curate a public bi-lingual dataset of 4,555 CXR studies with
grounded reports (3,099 abnormal and 1,456 normal), each containing complete
lists of sentences describing individual present (positive) and absent
(negative) findings in English and Spanish. In total, PadChest-GR contains
7,037 positive and 3,422 negative finding sentences. Every positive finding
sentence is associated with up to two independent sets of bounding boxes
labelled by different readers and has categorical labels for finding type,
locations, and progression. To the best of our knowledge, PadChest-GR is the
first manually curated dataset designed to train GRRG models for understanding
and interpreting radiological images and generated text. By including detailed
localization and comprehensive annotations of all clinically relevant findings,
it provides a valuable resource for developing and evaluating GRRG models from
CXR images. PadChest-GR can be downloaded under request from
https://bimcv.cipf.es/bimcv-projects/padchest-gr/",2024-11-07
"RS-MoE: Mixture of Experts for Remote Sensing Image Captioning and
  Visual Question Answering",2024-11-03 15:05:49+00:00,http://arxiv.org/abs/2411.01595v1,"Hui Lin, Danfeng Hong, Shuhang Ge, Chuyao Luo, Kai Jiang, Hao Jin, Congcong Wen","cs.CV, cs.AI",image2text,"Remote Sensing Image Captioning (RSIC) presents unique challenges and plays a
critical role in applications. Traditional RSIC methods often struggle to
produce rich and diverse descriptions. Recently, with advancements in VLMs,
efforts have emerged to integrate these models into the remote sensing domain
and to introduce descriptive datasets specifically designed to enhance VLM
training. This paper proposes RS-MoE, a first Mixture of Expert based VLM
specifically customized for remote sensing domain. Unlike traditional MoE
models, the core of RS-MoE is the MoE Block, which incorporates a novel
Instruction Router and multiple lightweight Large Language Models (LLMs) as
expert models. The Instruction Router is designed to generate specific prompts
tailored for each corresponding LLM, guiding them to focus on distinct aspects
of the RSIC task. This design not only allows each expert LLM to concentrate on
a specific subset of the task, thereby enhancing the specificity and accuracy
of the generated captions, but also improves the scalability of the model by
facilitating parallel processing of sub-tasks. Additionally, we present a
two-stage training strategy for tuning our RS-MoE model to prevent performance
degradation due to sparsity. We fine-tuned our model on the RSICap dataset
using our proposed training strategy. Experimental results on the RSICap
dataset, along with evaluations on other traditional datasets where no
additional fine-tuning was applied, demonstrate that our model achieves
state-of-the-art performance in generating precise and contextually relevant
captions. Notably, our RS-MoE-1B variant achieves performance comparable to 13B
VLMs, demonstrating the efficiency of our model design. Moreover, our model
demonstrates promising generalization capabilities by consistently achieving
state-of-the-art performance on the Remote Sensing Visual Question Answering
(RSVQA) task.",2024-11-03
TypeScore: A Text Fidelity Metric for Text-to-Image Generative Models,2024-11-02 07:56:54+00:00,http://arxiv.org/abs/2411.02437v1,"Georgia Gabriela Sampaio, Ruixiang Zhang, Shuangfei Zhai, Jiatao Gu, Josh Susskind, Navdeep Jaitly, Yizhe Zhang","cs.CV, cs.AI",image2text,"Evaluating text-to-image generative models remains a challenge, despite the
remarkable progress being made in their overall performances. While existing
metrics like CLIPScore work for coarse evaluations, they lack the sensitivity
to distinguish finer differences as model performance rapidly improves. In this
work, we focus on the text rendering aspect of these models, which provides a
lens for evaluating a generative model's fine-grained instruction-following
capabilities. To this end, we introduce a new evaluation framework called
TypeScore to sensitively assess a model's ability to generate images with
high-fidelity embedded text by following precise instructions. We argue that
this text generation capability serves as a proxy for general
instruction-following ability in image synthesis. TypeScore uses an additional
image description model and leverages an ensemble dissimilarity measure between
the original and extracted text to evaluate the fidelity of the rendered text.
Our proposed metric demonstrates greater resolution than CLIPScore to
differentiate popular image generation models across a range of instructions
with diverse text styles. Our study also evaluates how well these
vision-language models (VLMs) adhere to stylistic instructions, disentangling
style evaluation from embedded-text fidelity. Through human evaluation studies,
we quantitatively meta-evaluate the effectiveness of the metric. Comprehensive
analysis is conducted to explore factors such as text length, captioning
models, and current progress towards human parity on this task. The framework
provides insights into remaining gaps in instruction-following for image
generation with embedded text.",2024-11-02
"Using Multimodal Deep Neural Networks to Disentangle Language from
  Visual Aesthetics",2024-10-31 03:37:21+00:00,http://arxiv.org/abs/2410.23603v1,"Colin Conwell, Christopher Hamblin, Chelsea Boccagno, David Mayo, Jesse Cummings, Leyla Isik, Andrei Barbu","cs.CV, cs.CL",image2text,"When we experience a visual stimulus as beautiful, how much of that
experience derives from perceptual computations we cannot describe versus
conceptual knowledge we can readily translate into natural language?
Disentangling perception from language in visually-evoked affective and
aesthetic experiences through behavioral paradigms or neuroimaging is often
empirically intractable. Here, we circumnavigate this challenge by using linear
decoding over the learned representations of unimodal vision, unimodal
language, and multimodal (language-aligned) deep neural network (DNN) models to
predict human beauty ratings of naturalistic images. We show that unimodal
vision models (e.g. SimCLR) account for the vast majority of explainable
variance in these ratings. Language-aligned vision models (e.g. SLIP) yield
small gains relative to unimodal vision. Unimodal language models (e.g. GPT2)
conditioned on visual embeddings to generate captions (via CLIPCap) yield no
further gains. Caption embeddings alone yield less accurate predictions than
image and caption embeddings combined (concatenated). Taken together, these
results suggest that whatever words we may eventually find to describe our
experience of beauty, the ineffable computations of feedforward perception may
provide sufficient foundation for that experience.",2024-10-31
Private Synthetic Text Generation with Diffusion Models,2024-10-30 12:38:49+00:00,http://arxiv.org/abs/2410.22971v1,"Sebastian Ochs, Ivan Habernal",cs.CL,image2text,"How capable are diffusion models of generating synthetics texts? Recent
research shows their strengths, with performance reaching that of
auto-regressive LLMs. But are they also good in generating synthetic data if
the training was under differential privacy? Here the evidence is missing, yet
the promises from private image generation look strong. In this paper we
address this open question by extensive experiments. At the same time, we
critically assess (and reimplement) previous works on synthetic private text
generation with LLMs and reveal some unmet assumptions that might have led to
violating the differential privacy guarantees. Our results partly contradict
previous non-private findings and show that fully open-source LLMs outperform
diffusion models in the privacy regime. Our complete source codes, datasets,
and experimental setup is publicly available to foster future research.",2024-10-30
"Dreaming Out Loud: A Self-Synthesis Approach For Training
  Vision-Language Models With Developmentally Plausible Data",2024-10-29 10:50:03+00:00,http://arxiv.org/abs/2411.00828v1,"Badr AlKhamissi, Yingtian Tang, Abdülkadir Gökce, Johannes Mehrer, Martin Schrimpf","cs.CV, cs.LG",image2text,"While today's large language models exhibit impressive abilities in
generating human-like text, they require massive amounts of data during
training. We here take inspiration from human cognitive development to train
models in limited data conditions. Specifically we present a self-synthesis
approach that iterates through four phases: Phase 1 sets up fundamental
language abilities, training the model from scratch on a small corpus. Language
is then associated with the visual environment in phase 2, integrating the
model with a vision encoder to generate descriptive captions from labeled
images. In the ""self-synthesis"" phase 3, the model generates captions for
unlabeled images, that it then uses to further train its language component
with a mix of synthetic, and previous real-world text. This phase is meant to
expand the model's linguistic repertoire, similar to humans self-annotating new
experiences. Finally, phase 4 develops advanced cognitive skills, by training
the model on specific tasks such as visual question answering and reasoning.
Our approach offers a proof of concept for training a multimodal model using a
developmentally plausible amount of data.",2024-10-29
Towards Visual Text Design Transfer Across Languages,2024-10-24 15:15:01+00:00,http://arxiv.org/abs/2410.18823v2,"Yejin Choi, Jiwan Chung, Sumin Shim, Giyeong Oh, Youngjae Yu","cs.CV, cs.AI",image2text,"Visual text design plays a critical role in conveying themes, emotions, and
atmospheres in multimodal formats such as film posters and album covers.
Translating these visual and textual elements across languages extends the
concept of translation beyond mere text, requiring the adaptation of aesthetic
and stylistic features. To address this, we introduce a novel task of
Multimodal Style Translation (MuST-Bench), a benchmark designed to evaluate the
ability of visual text generation models to perform translation across
different writing systems while preserving design intent. Our initial
experiments on MuST-Bench reveal that existing visual text generation models
struggle with the proposed task due to the inadequacy of textual descriptions
in conveying visual design. In response, we introduce SIGIL, a framework for
multimodal style translation that eliminates the need for style descriptions.
SIGIL enhances image generation models through three innovations: glyph latent
for multilingual settings, pretrained VAEs for stable style guidance, and an
OCR model with reinforcement learning feedback for optimizing readable
character generation. SIGIL outperforms existing baselines by achieving
superior style consistency and legibility while maintaining visual fidelity,
setting itself apart from traditional description-based approaches. We release
MuST-Bench publicly for broader use and exploration
https://huggingface.co/datasets/yejinc/MuST-Bench.",2024-10-24
"IFCap: Image-like Retrieval and Frequency-based Entity Filtering for
  Zero-shot Captioning",2024-09-26 16:47:32+00:00,http://arxiv.org/abs/2409.18046v1,"Soeun Lee, Si-Woo Kim, Taewhan Kim, Dong-Jin Kim","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Recent advancements in image captioning have explored text-only training
methods to overcome the limitations of paired image-text data. However,
existing text-only training methods often overlook the modality gap between
using text data during training and employing images during inference. To
address this issue, we propose a novel approach called Image-like Retrieval,
which aligns text features with visually relevant features to mitigate the
modality gap. Our method further enhances the accuracy of generated captions by
designing a Fusion Module that integrates retrieved captions with input
features. Additionally, we introduce a Frequency-based Entity Filtering
technique that significantly improves caption quality. We integrate these
methods into a unified framework, which we refer to as IFCap
($\textbf{I}$mage-like Retrieval and $\textbf{F}$requency-based Entity
Filtering for Zero-shot $\textbf{Cap}$tioning). Through extensive
experimentation, our straightforward yet powerful approach has demonstrated its
efficacy, outperforming the state-of-the-art methods by a significant margin in
both image captioning and video captioning compared to zero-shot captioning
based on text-only training.",2024-09-26
MIO: A Foundation Model on Multimodal Tokens,2024-09-26 09:57:16+00:00,http://arxiv.org/abs/2409.17692v1,"Zekun Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, Wenhao Huang","cs.CL, cs.AI, cs.LG",image2text,"In this paper, we introduce MIO, a novel foundation model built on multimodal
tokens, capable of understanding and generating speech, text, images, and
videos in an end-to-end, autoregressive manner. While the emergence of large
language models (LLMs) and multimodal large language models (MM-LLMs) propels
advancements in artificial general intelligence through their versatile
capabilities, they still lack true any-to-any understanding and generation.
Recently, the release of GPT-4o has showcased the remarkable potential of
any-to-any LLMs for complex real-world tasks, enabling omnidirectional input
and output across images, speech, and text. However, it is closed-source and
does not support the generation of multimodal interleaved sequences. To address
this gap, we present MIO, which is trained on a mixture of discrete tokens
across four modalities using causal multimodal modeling. MIO undergoes a
four-stage training process: (1) alignment pre-training, (2) interleaved
pre-training, (3) speech-enhanced pre-training, and (4) comprehensive
supervised fine-tuning on diverse textual, visual, and speech tasks. Our
experimental results indicate that MIO exhibits competitive, and in some cases
superior, performance compared to previous dual-modal baselines, any-to-any
model baselines, and even modality-specific baselines. Moreover, MIO
demonstrates advanced capabilities inherent to its any-to-any feature, such as
interleaved video-text generation, chain-of-visual-thought reasoning, visual
guideline generation, instructional image editing, etc.",2024-09-26
"Copying style, Extracting value: Illustrators' Perception of AI Style
  Transfer and its Impact on Creative Labor",2024-09-25 22:45:08+00:00,http://arxiv.org/abs/2409.17410v1,"Julien Porquet, Sitong Wang, Lydia B. Chilton",cs.HC,image2text,"Generative text-to-image models are disrupting the lives of creative
professionals. Specifically, illustrators are threatened by models that claim
to extract and reproduce their style. Yet, research on style transfer has
rarely focused on their perspectives. We provided four illustrators with a
model fine-tuned to their style and conducted semi-structured interviews about
the model's successes, limitations, and potential uses. Evaluating their
output, artists reported that style transfer successfully copies aesthetic
fragments but is limited by content-style disentanglement and lacks the crucial
emergent quality of their style. They also deemed the others' copies more
successful. Understanding the results of style transfer as ""boundary objects,""
we analyze how they can simultaneously be considered unsuccessful by artists
and poised to replace their work by others. We connect our findings to critical
HCI frameworks, demonstrating that style transfer, rather than merely a
Creativity Support Tool, should also be understood as a supply chain
optimization one.",2024-09-25
"Brotherhood at WMT 2024: Leveraging LLM-Generated Contextual
  Conversations for Cross-Lingual Image Captioning",2024-09-23 14:29:46+00:00,http://arxiv.org/abs/2409.15052v1,"Siddharth Betala, Ishan Chokshi","cs.CL, cs.AI",image2text,"In this paper, we describe our system under the team name Brotherhood for the
English-to-Lowres Multi-Modal Translation Task. We participate in the
multi-modal translation tasks for English-Hindi, English-Hausa,
English-Bengali, and English-Malayalam language pairs. We present a method
leveraging multi-modal Large Language Models (LLMs), specifically GPT-4o and
Claude 3.5 Sonnet, to enhance cross-lingual image captioning without
traditional training or fine-tuning. Our approach utilizes instruction-tuned
prompting to generate rich, contextual conversations about cropped images,
using their English captions as additional context. These synthetic
conversations are then translated into the target languages. Finally, we employ
a weighted prompting strategy, balancing the original English caption with the
translated conversation to generate captions in the target language. This
method achieved competitive results, scoring 37.90 BLEU on the English-Hindi
Challenge Set and ranking first and second for English-Hausa on the Challenge
and Evaluation Leaderboards, respectively. We conduct additional experiments on
a subset of 250 images, exploring the trade-offs between BLEU scores and
semantic similarity across various weighting schemes.",2024-09-23
Recommendation with Generative Models,2024-09-18 18:29:15+00:00,http://arxiv.org/abs/2409.15173v1,"Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau Ramisa, Rene Vidal, Maheswaran Sathiamoorthy, Atoosa Kasrizadeh, Silvia Milano, Francesco Ricci",cs.IR,image2text,"Generative models are a class of AI models capable of creating new instances
of data by learning and sampling from their statistical distributions. In
recent years, these models have gained prominence in machine learning due to
the development of approaches such as generative adversarial networks (GANs),
variational autoencoders (VAEs), and transformer-based architectures such as
GPT. These models have applications across various domains, such as image
generation, text synthesis, and music composition. In recommender systems,
generative models, referred to as Gen-RecSys, improve the accuracy and
diversity of recommendations by generating structured outputs, text-based
interactions, and multimedia content. By leveraging these capabilities,
Gen-RecSys can produce more personalized, engaging, and dynamic user
experiences, expanding the role of AI in eCommerce, media, and beyond.
  Our book goes beyond existing literature by offering a comprehensive
understanding of generative models and their applications, with a special focus
on deep generative models (DGMs) and their classification. We introduce a
taxonomy that categorizes DGMs into three types: ID-driven models, large
language models (LLMs), and multimodal models. Each category addresses unique
technical and architectural advancements within its respective research area.
This taxonomy allows researchers to easily navigate developments in Gen-RecSys
across domains such as conversational AI and multimodal content generation.
Additionally, we examine the impact and potential risks of generative models,
emphasizing the importance of robust evaluation frameworks.",2024-09-18
"Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large
  Language Models",2024-09-16 19:52:24+00:00,http://arxiv.org/abs/2409.10695v1,"Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, Daiqing Li","cs.CV, cs.AI, cs.GR",image2text,"We introduce Playground v3 (PGv3), our latest text-to-image model that
achieves state-of-the-art (SoTA) performance across multiple testing
benchmarks, excels in graphic design abilities and introduces new capabilities.
Unlike traditional text-to-image generative models that rely on pre-trained
language models like T5 or CLIP text encoders, our approach fully integrates
Large Language Models (LLMs) with a novel structure that leverages text
conditions exclusively from a decoder-only LLM. Additionally, to enhance image
captioning quality-we developed an in-house captioner, capable of generating
captions with varying levels of detail, enriching the diversity of text
structures. We also introduce a new benchmark CapsBench to evaluate detailed
image captioning performance. Experimental results demonstrate that PGv3 excels
in text prompt adherence, complex reasoning, and accurate text rendering. User
preference studies indicate the super-human graphic design ability of our model
for common design applications, such as stickers, posters, and logo designs.
Furthermore, PGv3 introduces new capabilities, including precise RGB color
control and robust multilingual understanding.",2024-09-16
Spatio-Temporal Context Prompting for Zero-Shot Action Detection,2024-08-28 17:59:05+00:00,http://arxiv.org/abs/2408.15996v2,"Wei-Jhe Huang, Min-Hung Chen, Shang-Hong Lai","cs.CV, cs.AI",image2text,"Spatio-temporal action detection encompasses the tasks of localizing and
classifying individual actions within a video. Recent works aim to enhance this
process by incorporating interaction modeling, which captures the relationship
between people and their surrounding context. However, these approaches have
primarily focused on fully-supervised learning, and the current limitation lies
in the lack of generalization capability to recognize unseen action categories.
In this paper, we aim to adapt the pretrained image-language models to detect
unseen actions. To this end, we propose a method which can effectively leverage
the rich knowledge of visual-language models to perform Person-Context
Interaction. Meanwhile, our Context Prompting module will utilize contextual
information to prompt labels, thereby enhancing the generation of more
representative text features. Moreover, to address the challenge of recognizing
distinct actions by multiple people at the same timestamp, we design the
Interest Token Spotting mechanism which employs pretrained visual knowledge to
find each person's interest context tokens, and then these tokens will be used
for prompting to generate text features tailored to each individual. To
evaluate the ability to detect unseen actions, we propose a comprehensive
benchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our
method achieves superior results compared to previous approaches and can be
further extended to multi-action videos, bringing it closer to real-world
applications. The code and data can be found in
https://webber2933.github.io/ST-CLIP-project-page.",2024-08-28
DIAGen: Diverse Image Augmentation with Generative Models,2024-08-26 19:09:13+00:00,http://arxiv.org/abs/2408.14584v1,"Tobias Lingenberg, Markus Reuter, Gopika Sudhakaran, Dominik Gojny, Stefan Roth, Simone Schaub-Meyer","cs.CV, cs.AI",image2text,"Simple data augmentation techniques, such as rotations and flips, are widely
used to enhance the generalization power of computer vision models. However,
these techniques often fail to modify high-level semantic attributes of a
class. To address this limitation, researchers have explored generative
augmentation methods like the recently proposed DA-Fusion. Despite some
progress, the variations are still largely limited to textural changes, thus
falling short on aspects like varied viewpoints, environment, weather
conditions, or even class-level semantic attributes (eg, variations in a dog's
breed). To overcome this challenge, we propose DIAGen, building upon DA-Fusion.
First, we apply Gaussian noise to the embeddings of an object learned with
Textual Inversion to diversify generations using a pre-trained diffusion
model's knowledge. Second, we exploit the general knowledge of a text-to-text
generative model to guide the image generation of the diffusion model with
varied class-specific prompts. Finally, we introduce a weighting mechanism to
mitigate the impact of poorly generated samples. Experimental results across
various datasets show that DIAGen not only enhances semantic diversity but also
improves the performance of subsequent classifiers. The advantages of DIAGen
over standard augmentations and the DA-Fusion baseline are particularly
pronounced with out-of-distribution samples.",2024-08-26
"Revisiting Image Captioning Training Paradigm via Direct CLIP-based
  Optimization",2024-08-26 18:00:33+00:00,http://arxiv.org/abs/2408.14547v1,"Nicholas Moratelli, Davide Caffagni, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara","cs.CV, cs.AI, cs.CL, cs.MM",image2text,"The conventional training approach for image captioning involves pre-training
a network using teacher forcing and subsequent fine-tuning with Self-Critical
Sequence Training to maximize hand-crafted captioning metrics. However, when
attempting to optimize modern and higher-quality metrics like CLIP-Score and
PAC-Score, this training method often encounters instability and fails to
acquire the genuine descriptive capabilities needed to produce fluent and
informative captions. In this paper, we propose a new training paradigm termed
Direct CLIP-Based Optimization (DiCO). Our approach jointly learns and
optimizes a reward model that is distilled from a learnable captioning
evaluator with high human correlation. This is done by solving a weighted
classification problem directly inside the captioner. At the same time, DiCO
prevents divergence from the original model, ensuring that fluency is
maintained. DiCO not only exhibits improved stability and enhanced quality in
the generated captions but also aligns more closely with human preferences
compared to existing methods, especially in modern metrics. Additionally, it
maintains competitive performance in traditional metrics. Our source code and
trained models are publicly available at https://github.com/aimagelab/DiCO.",2024-08-26
Cap2Sum: Learning to Summarize Videos by Generating Captions,2024-08-23 02:28:54+00:00,http://arxiv.org/abs/2408.12800v1,"Cairong Zhao, Chutian Wang, Zifan Song, Guosheng Hu, Haonan Chen, Xiaofan Zhai",cs.MM,image2text,"With the rapid growth of video data on the internet, video summarization is
becoming a very important AI technology. However, due to the high labelling
cost of video summarization, existing studies have to be conducted on
small-scale datasets, leading to limited performance and generalization
capacity. In this work, we introduce the use of dense video captions as a
supervision signal to train video summarization models. Motivated by this, we
propose Cap2Sum, a model that learns to summarize videos by generating
captions, to exploit dense video caption annotations. This weakly-supervised
approach allows us to train the models on large-scale dense video caption
datasets to achieve better performance and generalization capacity. To further
improve the generalization capacity, we introduce a CLIP (a strong
vision-language model) Prior mechanism to enhance the learning of important
objects that captions may ignore in the videos. In practice, Cap2Sum can
perform zero-shot video summarization or be fine-tuned by the ground-truth
summary or video caption of the target dataset. To examine the performance of
Cap2Sum after weakly-supervised fine-tuning by the video captions, we propose
two new datasets, TVSum-Caption and SumMe-Caption, which are derived from two
common video summarization datasets and will be publicly released. We conduct
extensive experiments and the results demonstrate that our method achieves
significant improvements in performance and generalization capacity compared
with previous methods.",2024-08-23
"UniFashion: A Unified Vision-Language Model for Multimodal Fashion
  Retrieval and Generation",2024-08-21 03:17:20+00:00,http://arxiv.org/abs/2408.11305v1,"Xiangyu Zhao, Yuehan Zhang, Wenlong Zhang, Xiao-Ming Wu","cs.CV, cs.AI",image2text,"The fashion domain encompasses a variety of real-world multimodal tasks,
including multimodal retrieval and multimodal generation. The rapid
advancements in artificial intelligence generated content, particularly in
technologies like large language models for text generation and diffusion
models for visual generation, have sparked widespread research interest in
applying these multimodal models in the fashion domain. However, tasks
involving embeddings, such as image-to-text or text-to-image retrieval, have
been largely overlooked from this perspective due to the diverse nature of the
multimodal fashion domain. And current research on multi-task single models
lack focus on image generation. In this work, we present UniFashion, a unified
framework that simultaneously tackles the challenges of multimodal generation
and retrieval tasks within the fashion domain, integrating image generation
with retrieval tasks and text generation tasks. UniFashion unifies embedding
and generative tasks by integrating a diffusion model and LLM, enabling
controllable and high-fidelity generation. Our model significantly outperforms
previous single-task state-of-the-art models across diverse fashion tasks, and
can be readily adapted to manage complex vision-language tasks. This work
demonstrates the potential learning synergy between multimodal generation and
retrieval, offering a promising direction for future research in the fashion
domain. The source code is available at
https://github.com/xiangyu-mm/UniFashion.",2024-08-21
"SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From
  Pre-Trained Foundation Models",2024-08-19 17:32:15+00:00,http://arxiv.org/abs/2408.10174v2,"Anke Tang, Li Shen, Yong Luo, Shuai Xie, Han Hu, Lefei Zhang, Bo Du, Dacheng Tao","cs.LG, cs.AI",image2text,"Deep model training on extensive datasets is increasingly becoming
cost-prohibitive, prompting the widespread adoption of deep model fusion
techniques to leverage knowledge from pre-existing models. From simple weight
averaging to more sophisticated methods like AdaMerging, model fusion
effectively improves model performance and accelerates the development of new
models. However, potential interference between parameters of individual models
and the lack of interpretability in the fusion progress remain significant
challenges. Existing methods often try to resolve the parameter interference
issue by evaluating attributes of parameters, such as their magnitude or sign,
or by parameter pruning. In this study, we begin by examining the fine-tuning
of linear layers through the lens of subspace analysis and explicitly define
parameter interference as an optimization problem to shed light on this
subject. Subsequently, we introduce an innovative approach to model fusion
called zero-shot Sparse MIxture of Low-rank Experts (SMILE) construction, which
allows for the upscaling of source models into an MoE model without extra data
or further training. Our approach relies on the observation that fine-tuning
mostly keeps the important parts from the pre-training, but it uses less
significant or unused areas to adapt to new tasks. Also, the issue of parameter
interference, which is intrinsically intractable in the original parameter
space, can be managed by expanding the dimensions. We conduct extensive
experiments across diverse scenarios, such as image classification and text
generation tasks, using full fine-tuning and LoRA fine-tuning, and we apply our
method to large language models (CLIP models, Flan-T5 models, and Mistral-7B
models), highlighting the adaptability and scalability of SMILE. Code is
available at https://github.com/tanganke/fusion_bench",2024-08-19
"LLaVA-VSD: Large Language-and-Vision Assistant for Visual Spatial
  Description",2024-08-09 09:22:40+00:00,http://arxiv.org/abs/2408.04957v3,"Yizhang Jin, Jian Li, Jiangning Zhang, Jianlong Hu, Zhenye Gan, Xin Tan, Yong Liu, Yabiao Wang, Chengjie Wang, Lizhuang Ma","cs.CV, cs.AI",image2text,"Visual Spatial Description (VSD) aims to generate texts that describe the
spatial relationships between objects within images. Traditional visual spatial
relationship classification (VSRC) methods typically output the spatial
relationship between two objects in an image, often neglecting world knowledge
and lacking general language capabilities. In this paper, we propose a Large
Language-and-Vision Assistant for Visual Spatial Description, named LLaVA-VSD,
which is designed for the classification, description, and open-ended
description of visual spatial relationships. Specifically, the model first
constructs a VSD instruction-following dataset using given figure-caption pairs
for the three tasks. It then employs LoRA to fine-tune a Large Language and
Vision Assistant for VSD, which has 13 billion parameters and supports
high-resolution images. Finally, a large language model (Qwen-2) is used to
refine the generated sentences, enhancing their diversity and accuracy.
LLaVA-VSD demonstrates excellent multimodal conversational capabilities and can
follow open-ended instructions to assist with inquiries about object
relationships in images.",2024-08-09
Wolf: Captioning Everything with a World Summarization Framework,2024-07-26 17:59:09+00:00,http://arxiv.org/abs/2407.18908v1,"Boyi Li, Ligeng Zhu, Ran Tian, Shuhan Tan, Yuxiao Chen, Yao Lu, Yin Cui, Sushant Veer, Max Ehrlich, Jonah Philion, Xinshuo Weng, Fuzhao Xue, Andrew Tao, Ming-Yu Liu, Sanja Fidler, Boris Ivanovic, Trevor Darrell, Jitendra Malik, Song Han, Marco Pavone","cs.LG, cs.CL, cs.CV",image2text,"We propose Wolf, a WOrLd summarization Framework for accurate video
captioning. Wolf is an automated captioning framework that adopts a
mixture-of-experts approach, leveraging complementary strengths of Vision
Language Models (VLMs). By utilizing both image and video models, our framework
captures different levels of information and summarizes them efficiently. Our
approach can be applied to enhance video understanding, auto-labeling, and
captioning. To evaluate caption quality, we introduce CapScore, an LLM-based
metric to assess the similarity and quality of generated captions compared to
the ground truth captions. We further build four human-annotated datasets in
three domains: autonomous driving, general scenes, and robotics, to facilitate
comprehensive comparisons. We show that Wolf achieves superior captioning
performance compared to state-of-the-art approaches from the research community
(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For
instance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise
by 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,
we establish a benchmark for video captioning and introduce a leaderboard,
aiming to accelerate advancements in video understanding, captioning, and data
alignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.",2024-07-26
Guided Latent Slot Diffusion for Object-Centric Learning,2024-07-25 10:38:32+00:00,http://arxiv.org/abs/2407.17929v1,"Krishnakant Singh, Simone Schaub-Meyer, Stefan Roth","cs.CV, cs.LG",image2text,"Slot attention aims to decompose an input image into a set of meaningful
object files (slots). These latent object representations enable various
downstream tasks. Yet, these slots often bind to object parts, not objects
themselves, especially for real-world datasets. To address this, we introduce
Guided Latent Slot Diffusion - GLASS, an object-centric model that uses
generated captions as a guiding signal to better align slots with objects. Our
key insight is to learn the slot-attention module in the space of generated
images. This allows us to repurpose the pre-trained diffusion decoder model,
which reconstructs the images from the slots, as a semantic mask generator
based on the generated captions. GLASS learns an object-level representation
suitable for multiple tasks simultaneously, e.g., segmentation, image
generation, and property prediction, outperforming previous methods. For object
discovery, GLASS achieves approx. a +35% and +10% relative improvement for mIoU
over the previous state-of-the-art (SOTA) method on the VOC and COCO datasets,
respectively, and establishes a new SOTA FID score for conditional image
generation amongst slot-attention-based methods. For the segmentation task,
GLASS surpasses SOTA weakly-supervised and language-based segmentation models,
which were specifically designed for the task.",2024-07-25
"Generative artificial intelligence in dentistry: Current approaches and
  future challenges",2024-07-24 03:33:47+00:00,http://arxiv.org/abs/2407.17532v1,"Fabián Villena, Claudia Véliz, Rosario García-Huidobro, Sebastián Aguayo",cs.CL,image2text,"Artificial intelligence (AI) has become a commodity for people because of the
advent of generative AI (GenAI) models that bridge the usability gap of AI by
providing a natural language interface to interact with complex models. These
GenAI models range from text generation - such as two-way chat systems - to the
generation of image or video from textual descriptions input by a user. These
advancements in AI have impacted Dentistry in multiple aspects. In dental
education, the student now has the opportunity to solve a plethora of questions
by only prompting a GenAI model and have the answer in a matter of seconds.
GenAI models can help us deliver better patient healthcare by helping
practitioners gather knowledge quickly and efficiently. Finally, GenAI can also
be used in dental research, where the applications range from new drug
discovery to assistance in academic writing. In this review, we first define
GenAI models and describe their multiple generation modalities; then, we
explain and discuss their current and potential applications in Dentistry; and
finally, we describe the challenges these new technologies impose in our area.",2024-07-24
"When Do Universal Image Jailbreaks Transfer Between Vision-Language
  Models?",2024-07-21 16:27:24+00:00,http://arxiv.org/abs/2407.15211v1,"Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Cristóbal Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, John Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, Ethan Perez","cs.CL, cs.AI, cs.CR, cs.CV, cs.LG",image2text,"The integration of new modalities into frontier AI systems offers exciting
capabilities, but also increases the possibility such systems can be
adversarially manipulated in undesirable ways. In this work, we focus on a
popular class of vision-language models (VLMs) that generate text outputs
conditioned on visual and textual inputs. We conducted a large-scale empirical
study to assess the transferability of gradient-based universal image
""jailbreaks"" using a diverse set of over 40 open-parameter VLMs, including 18
new VLMs that we publicly release. Overall, we find that transferable
gradient-based image jailbreaks are extremely difficult to obtain. When an
image jailbreak is optimized against a single VLM or against an ensemble of
VLMs, the jailbreak successfully jailbreaks the attacked VLM(s), but exhibits
little-to-no transfer to any other VLMs; transfer is not affected by whether
the attacked and target VLMs possess matching vision backbones or language
models, whether the language model underwent instruction-following and/or
safety-alignment training, or many other factors. Only two settings display
partially successful transfer: between identically-pretrained and
identically-initialized VLMs with slightly different VLM training data, and
between different training checkpoints of a single VLM. Leveraging these
results, we then demonstrate that transfer can be significantly improved
against a specific target VLM by attacking larger ensembles of ""highly-similar""
VLMs. These results stand in stark contrast to existing evidence of universal
and transferable text jailbreaks against language models and transferable
adversarial attacks against image classifiers, suggesting that VLMs may be more
robust to gradient-based transfer attacks.",2024-07-21
"DOPRA: Decoding Over-accumulation Penalization and Re-allocation in
  Specific Weighting Layer",2024-07-21 11:54:49+00:00,http://arxiv.org/abs/2407.15130v2,"Jinfeng Wei, Xiaofeng Zhang","cs.CL, cs.AI",image2text,"In this work, we introduce DOPRA, a novel approach designed to mitigate
hallucinations in multi-modal large language models (MLLMs). Unlike existing
solutions that typically involve costly supplementary training data or the
integration of external knowledge sources, DOPRA innovatively addresses
hallucinations by decoding specific weighted layer penalties and
redistribution, offering an economical and effective solution without
additional resources. DOPRA is grounded in unique insights into the intrinsic
mechanisms controlling hallucinations within MLLMs, especially the models'
tendency to over-rely on a subset of summary tokens in the self-attention
matrix, neglecting critical image-related information. This phenomenon is
particularly pronounced in certain strata. To counteract this over-reliance,
DOPRA employs a strategy of weighted overlay penalties and redistribution in
specific layers, such as the 12th layer, during the decoding process.
Furthermore, DOPRA includes a retrospective allocation process that re-examines
the sequence of generated tokens, allowing the algorithm to reallocate token
selection to better align with the actual image content, thereby reducing the
incidence of hallucinatory descriptions in auto-generated captions. Overall,
DOPRA represents a significant step forward in improving the output quality of
MLLMs by systematically reducing hallucinations through targeted adjustments
during the decoding process.",2024-07-21
"KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with
  Large Language Models",2024-07-19 12:13:08+00:00,http://arxiv.org/abs/2407.14239v1,"Kemou Jiang, Xuan Cai, Zhiyong Cui, Aoyong Li, Yilong Ren, Haiyang Yu, Hao Yang, Daocheng Fu, Licheng Wen, Pinlong Cai",cs.AI,image2text,"Large language models (LLMs) as autonomous agents offer a novel avenue for
tackling real-world challenges through a knowledge-driven manner. These
LLM-enhanced methodologies excel in generalization and interpretability.
However, the complexity of driving tasks often necessitates the collaboration
of multiple, heterogeneous agents, underscoring the need for such LLM-driven
agents to engage in cooperative knowledge sharing and cognitive synergy.
Despite the promise of LLMs, current applications predominantly center around
single agent scenarios. To broaden the horizons of knowledge-driven strategies
and bolster the generalization capabilities of autonomous agents, we propose
the KoMA framework consisting of multi-agent interaction, multi-step planning,
shared-memory, and ranking-based reflection modules to enhance multi-agents'
decision-making in complex driving scenarios. Based on the framework's
generated text descriptions of driving scenarios, the multi-agent interaction
module enables LLM agents to analyze and infer the intentions of surrounding
vehicles, akin to human cognition. The multi-step planning module enables LLM
agents to analyze and obtain final action decisions layer by layer to ensure
consistent goals for short-term action decisions. The shared memory module can
accumulate collective experience to make superior decisions, and the
ranking-based reflection module can evaluate and improve agent behavior with
the aim of enhancing driving safety and efficiency. The KoMA framework not only
enhances the robustness and adaptability of autonomous driving agents but also
significantly elevates their generalization capabilities across diverse
scenarios. Empirical results demonstrate the superiority of our approach over
traditional methods, particularly in its ability to handle complex,
unpredictable driving environments without extensive retraining.",2024-07-19
"Turning Generative Models Degenerate: The Power of Data Poisoning
  Attacks",2024-07-17 03:02:15+00:00,http://arxiv.org/abs/2407.12281v2,"Shuli Jiang, Swanand Ravindra Kadhe, Yi Zhou, Farhan Ahmed, Ling Cai, Nathalie Baracaldo","cs.CR, cs.AI",image2text,"The increasing use of large language models (LLMs) trained by third parties
raises significant security concerns. In particular, malicious actors can
introduce backdoors through poisoning attacks to generate undesirable outputs.
While such attacks have been extensively studied in image domains and
classification tasks, they remain underexplored for natural language generation
(NLG) tasks. To address this gap, we conduct an investigation of various
poisoning techniques targeting the LLM's fine-tuning phase via prefix-tuning, a
Parameter Efficient Fine-Tuning (PEFT) method. We assess their effectiveness
across two generative tasks: text summarization and text completion; and we
also introduce new metrics to quantify the success and stealthiness of such NLG
poisoning attacks. Through our experiments, we find that the prefix-tuning
hyperparameters and trigger designs are the most crucial factors to influence
attack success and stealthiness. Moreover, we demonstrate that existing popular
defenses are ineffective against our poisoning attacks. Our study presents the
first systematic approach to understanding poisoning attacks targeting NLG
tasks during fine-tuning via PEFT across a wide range of triggers and attack
settings. We hope our findings will aid the AI security community in developing
effective defenses against such threats.",2024-07-17
"Towards Dataset-scale and Feature-oriented Evaluation of Text
  Summarization in Large Language Model Prompts",2024-07-16 21:36:43+00:00,http://arxiv.org/abs/2407.12192v2,"Sam Yu-Te Lee, Aryaman Bahukhandi, Dongyu Liu, Kwan-Liu Ma",cs.HC,image2text,"Recent advancements in Large Language Models (LLMs) and Prompt Engineering
have made chatbot customization more accessible, significantly reducing
barriers to tasks that previously required programming skills. However, prompt
evaluation, especially at the dataset scale, remains complex due to the need to
assess prompts across thousands of test instances within a dataset. Our study,
based on a comprehensive literature review and pilot study, summarized five
critical challenges in prompt evaluation. In response, we introduce a
feature-oriented workflow for systematic prompt evaluation. In the context of
text summarization, our workflow advocates evaluation with summary
characteristics (feature metrics) such as complexity, formality, or
naturalness, instead of using traditional quality metrics like ROUGE. This
design choice enables a more user-friendly evaluation of prompts, as it guides
users in sorting through the ambiguity inherent in natural language. To support
this workflow, we introduce Awesum, a visual analytics system that facilitates
identifying optimal prompt refinements for text summarization through
interactive visualizations, featuring a novel Prompt Comparator design that
employs a BubbleSet-inspired design enhanced by dimensionality reduction
techniques. We evaluate the effectiveness and general applicability of the
system with practitioners from various domains and found that (1) our design
helps overcome the learning curve for non-technical people to conduct a
systematic evaluation of summarization prompts, and (2) our feature-oriented
workflow has the potential to generalize to other NLG and image-generation
tasks. For future works, we advocate moving towards feature-oriented evaluation
of LLM prompts and discuss unsolved challenges in terms of human-agent
interaction.",2024-07-16
"CIC-BART-SSA: Controllable Image Captioning with Structured Semantic
  Augmentation",2024-07-16 05:26:12+00:00,http://arxiv.org/abs/2407.11393v2,"Kalliopi Basioti, Mohamed A. Abdelsalam, Federico Fancellu, Vladimir Pavlovic, Afsaneh Fazly","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Controllable Image Captioning (CIC) aims at generating natural language
descriptions for an image, conditioned on information provided by end users,
e.g., regions, entities or events of interest. However, available
image-language datasets mainly contain captions that describe the entirety of
an image, making them ineffective for training CIC models that can potentially
attend to any subset of regions or relationships. To tackle this challenge, we
propose a novel, fully automatic method to sample additional focused and
visually grounded captions using a unified structured semantic representation
built on top of the existing set of captions associated with an image. We
leverage Abstract Meaning Representation (AMR), a cross-lingual graph-based
semantic formalism, to encode all possible spatio-semantic relations between
entities, beyond the typical spatial-relations-only focus of current methods.
We use this Structured Semantic Augmentation (SSA) framework to augment
existing image-caption datasets with the grounded controlled captions,
increasing their spatial and semantic diversity and focal coverage. We then
develop a new model, CIC-BART-SSA, specifically tailored for the CIC task, that
sources its control signals from SSA-diversified datasets. We empirically show
that, compared to SOTA CIC models, CIC-BART-SSA generates captions that are
superior in diversity and text quality, are competitive in controllability,
and, importantly, minimize the gap between broad and highly focused controlled
captioning performance by efficiently generalizing to the challenging highly
focused scenarios. Code is available at
https://github.com/SamsungLabs/CIC-BART-SSA.",2024-07-16
"Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer
  from Text to Image via CLIP Inversion",2024-07-15 19:53:02+00:00,http://arxiv.org/abs/2407.11211v2,"Philipp Allgeuer, Kyra Ahrens, Stefan Wermter","cs.CV, cs.AI, cs.CL",image2text,"We introduce NOVIC, an innovative uNconstrained Open Vocabulary Image
Classifier that uses an autoregressive transformer to generatively output
classification labels as language. Leveraging the extensive knowledge of CLIP
models, NOVIC harnesses the embedding space to enable zero-shot transfer from
pure text to images. Traditional CLIP models, despite their ability for open
vocabulary classification, require an exhaustive prompt of potential class
labels, restricting their application to images of known content or context. To
address this, we propose an ""object decoder"" model that is trained on a
large-scale 92M-target dataset of templated object noun sets and LLM-generated
captions to always output the object noun in question. This effectively inverts
the CLIP text encoder and allows textual object labels to be generated directly
from image-derived embedding vectors, without requiring any a priori knowledge
of the potential content of an image. The trained decoders are tested on a mix
of manually and web-curated datasets, as well as standard image classification
benchmarks, and achieve fine-grained prompt-free prediction scores of up to
87.5%, a strong result considering the model must work for any conceivable
image and without any contextual clues.",2024-07-15
"Pseudo-RIS: Distinctive Pseudo-supervision Generation for Referring
  Image Segmentation",2024-07-10 07:14:48+00:00,http://arxiv.org/abs/2407.07412v3,"Seonghoon Yu, Paul Hongsuck Seo, Jeany Son","cs.CV, cs.AI",image2text,"We propose a new framework that automatically generates high-quality
segmentation masks with their referring expressions as pseudo supervisions for
referring image segmentation (RIS). These pseudo supervisions allow the
training of any supervised RIS methods without the cost of manual labeling. To
achieve this, we incorporate existing segmentation and image captioning
foundation models, leveraging their broad generalization capabilities. However,
the naive incorporation of these models may generate non-distinctive
expressions that do not distinctively refer to the target masks. To address
this challenge, we propose two-fold strategies that generate distinctive
captions: 1) 'distinctive caption sampling', a new decoding method for the
captioning model, to generate multiple expression candidates with detailed
words focusing on the target. 2) 'distinctiveness-based text filtering' to
further validate the candidates and filter out those with a low level of
distinctiveness. These two strategies ensure that the generated text
supervisions can distinguish the target from other objects, making them
appropriate for the RIS annotations. Our method significantly outperforms both
weakly and zero-shot SoTA methods on the RIS benchmark datasets. It also
surpasses fully supervised methods in unseen domains, proving its capability to
tackle the open-world challenge within RIS. Furthermore, integrating our method
with human annotations yields further improvements, highlighting its potential
in semi-supervised learning applications.",2024-07-10
"MetaToken: Detecting Hallucination in Image Descriptions by Meta
  Classification",2024-05-29 15:28:42+00:00,http://arxiv.org/abs/2405.19186v1,"Laura Fieback, Jakob Spiegelberg, Hanno Gottschalk","cs.CV, cs.CL, cs.LG, I.4",image2text,"Large Vision Language Models (LVLMs) have shown remarkable capabilities in
multimodal tasks like visual question answering or image captioning. However,
inconsistencies between the visual information and the generated text, a
phenomenon referred to as hallucinations, remain an unsolved problem with
regard to the trustworthiness of LVLMs. To address this problem, recent works
proposed to incorporate computationally costly Large (Vision) Language Models
in order to detect hallucinations on a sentence- or subsentence-level. In this
work, we introduce MetaToken, a lightweight binary classifier to detect
hallucinations on the token-level at negligible cost. Based on a statistical
analysis, we reveal key factors of hallucinations in LVLMs which have been
overseen in previous works. MetaToken can be applied to any open-source LVLM
without any knowledge about ground truth data providing a reliable detection of
hallucinations. We evaluate our method on four state-of-the-art LVLMs
demonstrating the effectiveness of our approach.",2024-05-29
Alt4Blind: A User Interface to Simplify Charts Alt-Text Creation,2024-05-29 14:19:57+00:00,http://arxiv.org/abs/2405.19111v1,"Omar Moured, Shahid Ali Farooqui, Karin Muller, Sharifeh Fadaeijouybari, Thorsten Schwarz, Mohammed Javed, Rainer Stiefelhagen","cs.CV, cs.HC",image2text,"Alternative Texts (Alt-Text) for chart images are essential for making
graphics accessible to people with blindness and visual impairments.
Traditionally, Alt-Text is manually written by authors but often encounters
issues such as oversimplification or complication. Recent trends have seen the
use of AI for Alt-Text generation. However, existing models are susceptible to
producing inaccurate or misleading information. We address this challenge by
retrieving high-quality alt-texts from similar chart images, serving as a
reference for the user when creating alt-texts. Our three contributions are as
follows: (1) we introduce a new benchmark comprising 5,000 real images with
semantically labeled high-quality Alt-Texts, collected from Human Computer
Interaction venues. (2) We developed a deep learning-based model to rank and
retrieve similar chart images that share the same visual and textual semantics.
(3) We designed a user interface (UI) to facilitate the alt-text creation
process. Our preliminary interviews and investigations highlight the usability
of our UI. For the dataset and further details, please refer to our project
page: https://moured.github.io/alt4blind/.",2024-05-29
"Automatic detection of cognitive impairment in elderly people using an
  entertainment chatbot with Natural Language Processing capabilities",2024-05-28 19:17:48+00:00,http://arxiv.org/abs/2405.18542v1,"Francisco de Arriba-Pérez, Silvia García-Méndez, Francisco J. González-Castaño, Enrique Costa-Montenegro","cs.AI, cs.CL, cs.HC, cs.LG",image2text,"Previous researchers have proposed intelligent systems for therapeutic
monitoring of cognitive impairments. However, most existing practical
approaches for this purpose are based on manual tests. This raises issues such
as excessive caretaking effort and the white-coat effect. To avoid these
issues, we present an intelligent conversational system for entertaining
elderly people with news of their interest that monitors cognitive impairment
transparently. Automatic chatbot dialogue stages allow assessing content
description skills and detecting cognitive impairment with Machine Learning
algorithms. We create these dialogue flows automatically from updated news
items using Natural Language Generation techniques. The system also infers the
gold standard of the answers to the questions, so it can assess cognitive
capabilities automatically by comparing these answers with the user responses.
It employs a similarity metric with values in [0, 1], in increasing level of
similarity. To evaluate the performance and usability of our approach, we have
conducted field tests with a test group of 30 elderly people in the earliest
stages of dementia, under the supervision of gerontologists. In the
experiments, we have analysed the effect of stress and concentration in these
users. Those without cognitive impairment performed up to five times better. In
particular, the similarity metric varied between 0.03, for stressed and
unfocused participants, and 0.36, for relaxed and focused users. Finally, we
developed a Machine Learning algorithm based on textual analysis features for
automatic cognitive impairment detection, which attained accuracy, F-measure
and recall levels above 80%. We have thus validated the automatic approach to
detect cognitive impairment in elderly people based on entertainment content.",2024-05-28
"ECG Semantic Integrator (ESI): A Foundation ECG Model Pretrained with
  LLM-Enhanced Cardiological Text",2024-05-26 06:45:39+00:00,http://arxiv.org/abs/2405.19366v1,"Han Yu, Peikun Guo, Akane Sano","eess.SP, cs.AI",image2text,"The utilization of deep learning on electrocardiogram (ECG) analysis has
brought the advanced accuracy and efficiency of cardiac healthcare diagnostics.
By leveraging the capabilities of deep learning in semantic understanding,
especially in feature extraction and representation learning, this study
introduces a new multimodal contrastive pretaining framework that aims to
improve the quality and robustness of learned representations of 12-lead ECG
signals. Our framework comprises two key components, including Cardio Query
Assistant (CQA) and ECG Semantics Integrator(ESI). CQA integrates a
retrieval-augmented generation (RAG) pipeline to leverage large language models
(LLMs) and external medical knowledge to generate detailed textual descriptions
of ECGs. The generated text is enriched with information about demographics and
waveform patterns. ESI integrates both contrastive and captioning loss to
pretrain ECG encoders for enhanced representations. We validate our approach
through various downstream tasks, including arrhythmia detection and ECG-based
subject identification. Our experimental results demonstrate substantial
improvements over strong baselines in these tasks. These baselines encompass
supervised and self-supervised learning methods, as well as prior multimodal
pretraining approaches.",2024-05-26
"VDGD: Mitigating LVLM Hallucinations in Cognitive Prompts by Bridging
  the Visual Perception Gap",2024-05-24 16:21:59+00:00,http://arxiv.org/abs/2405.15683v1,"Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Utkarsh Tyagi, Oriol Nieto, Zeyu Jin, Dinesh Manocha","cs.CV, cs.AI, cs.CL",image2text,"Recent interest in Large Vision-Language Models (LVLMs) for practical
applications is moderated by the significant challenge of hallucination or the
inconsistency between the factual information and the generated text. In this
paper, we first perform an in-depth analysis of hallucinations and discover
several novel insights about how and when LVLMs hallucinate. From our analysis,
we show that: (1) The community's efforts have been primarily targeted towards
reducing hallucinations related to visual recognition (VR) prompts (e.g.,
prompts that only require describing the image), thereby ignoring
hallucinations for cognitive prompts (e.g., prompts that require additional
skills like reasoning on contents of the image). (2) LVLMs lack visual
perception, i.e., they can see but not necessarily understand or perceive the
input image. We analyze responses to cognitive prompts and show that LVLMs
hallucinate due to a perception gap: although LVLMs accurately recognize visual
elements in the input image and possess sufficient cognitive skills, they
struggle to respond accurately and hallucinate. To overcome this shortcoming,
we propose Visual Description Grounded Decoding (VDGD), a simple, robust, and
training-free method for alleviating hallucinations. Specifically, we first
describe the image and add it as a prefix to the instruction. Next, during
auto-regressive decoding, we sample from the plausible candidates according to
their KL-Divergence (KLD) to the description, where lower KLD is given higher
preference. Experimental results on several benchmarks and LVLMs show that VDGD
improves significantly over other baselines in reducing hallucinations. We also
propose VaLLu, a benchmark for the comprehensive evaluation of the cognitive
capabilities of LVLMs.",2024-05-24
"A Misleading Gallery of Fluid Motion by Generative Artificial
  Intelligence",2024-05-24 10:17:15+00:00,http://arxiv.org/abs/2405.15406v1,Ali Kashefi,"physics.flu-dyn, cs.LG",image2text,"In this technical report, we extensively investigate the accuracy of outputs
from well-known generative artificial intelligence (AI) applications in
response to prompts describing common fluid motion phenomena familiar to the
fluid mechanics community. We examine a range of applications, including
Midjourney, Dall-E, Runway ML, Microsoft Designer, Gemini, Meta AI, and
Leonardo AI, introduced by prominent companies such as Google, OpenAI, Meta,
and Microsoft. Our text prompts for generating images or videos include
examples such as ""Von Karman vortex street"", ""flow past an airfoil"",
""Kelvin-Helmholtz instability"", ""shock waves on a sharp-nosed supersonic body"",
etc. We compare the images generated by these applications with real images
from laboratory experiments and numerical software. Our findings indicate that
these generative AI models are not adequately trained in fluid dynamics
imagery, leading to potentially misleading outputs. Beyond text-to-image/video
generation, we further explore the transition from image/video to text
generation using these AI tools, aiming to investigate the accuracy of their
descriptions of fluid motion phenomena. This report serves as a cautionary note
for educators in academic institutions, highlighting the potential for these
tools to mislead students. It also aims to inform researchers at these renowned
companies, encouraging them to address this issue. We conjecture that a primary
reason for this shortcoming is the limited access to copyright-protected fluid
motion images from scientific journals.",2024-05-24
Calibrated Self-Rewarding Vision Language Models,2024-05-23 14:30:33+00:00,http://arxiv.org/abs/2405.14622v2,"Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, Huaxiu Yao","cs.LG, cs.CL, cs.CV",image2text,"Large Vision-Language Models (LVLMs) have made substantial progress by
integrating pre-trained large language models (LLMs) and vision models through
instruction tuning. Despite these advancements, LVLMs often exhibit the
hallucination phenomenon, where generated text responses appear linguistically
plausible but contradict the input image, indicating a misalignment between
image and text pairs. This misalignment arises because the model tends to
prioritize textual information over visual input, even when both the language
model and visual representations are of high quality. Existing methods leverage
additional models or human annotations to curate preference data and enhance
modality alignment through preference optimization. These approaches may not
effectively reflect the target LVLM's preferences, making the curated
preferences easily distinguishable. Our work addresses these challenges by
proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model
to self-improve by iteratively generating candidate responses, evaluating the
reward for each response, and curating preference data for fine-tuning. In the
reward modeling, we employ a step-wise strategy and incorporate visual
constraints into the self-rewarding process to place greater emphasis on visual
input. Empirical results demonstrate that CSR enhances performance and reduces
hallucinations across ten benchmarks and tasks, achieving substantial
improvements over existing methods by 7.62%. Our empirical results are further
supported by rigorous theoretical analysis, under mild assumptions, verifying
the effectiveness of introducing visual constraints into the self-rewarding
paradigm. Additionally, CSR shows compatibility with different vision-language
models and the ability to incrementally improve performance through iterative
fine-tuning. Our data and code are available at
https://github.com/YiyangZhou/CSR.",2024-05-23
"Visual Delta Generator with Large Multi-modal Models for Semi-supervised
  Composed Image Retrieval",2024-04-23 21:00:22+00:00,http://arxiv.org/abs/2404.15516v1,"Young Kyun Jang, Donghyun Kim, Zihang Meng, Dat Huynh, Ser-Nam Lim","cs.CV, cs.AI",image2text,"Composed Image Retrieval (CIR) is a task that retrieves images similar to a
query, based on a provided textual modification. Current techniques rely on
supervised learning for CIR models using labeled triplets of the reference
image, text, target image. These specific triplets are not as commonly
available as simple image-text pairs, limiting the widespread use of CIR and
its scalability. On the other hand, zero-shot CIR can be relatively easily
trained with image-caption pairs without considering the image-to-image
relation, but this approach tends to yield lower accuracy. We propose a new
semi-supervised CIR approach where we search for a reference and its related
target images in auxiliary data and learn our large language model-based Visual
Delta Generator (VDG) to generate text describing the visual difference (i.e.,
visual delta) between the two. VDG, equipped with fluent language knowledge and
being model agnostic, can generate pseudo triplets to boost the performance of
CIR models. Our approach significantly improves the existing supervised
learning approaches and achieves state-of-the-art results on the CIR
benchmarks.",2024-04-23
"Parameter Efficient Fine Tuning: A Comprehensive Analysis Across
  Applications",2024-04-21 02:26:15+00:00,http://arxiv.org/abs/2404.13506v2,"Charith Chandra Sai Balne, Sreyoshi Bhaduri, Tamoghna Roy, Vinija Jain, Aman Chadha","cs.LG, cs.AI, cs.CL",image2text,"The rise of deep learning has marked significant progress in fields such as
computer vision, natural language processing, and medical imaging, primarily
through the adaptation of pre-trained models for specific tasks. Traditional
fine-tuning methods, involving adjustments to all parameters, face challenges
due to high computational and memory demands. This has led to the development
of Parameter Efficient Fine-Tuning (PEFT) techniques, which selectively update
parameters to balance computational efficiency with performance. This review
examines PEFT approaches, offering a detailed comparison of various strategies
highlighting applications across different domains, including text generation,
medical imaging, protein modeling, and speech synthesis. By assessing the
effectiveness of PEFT methods in reducing computational load, speeding up
training, and lowering memory usage, this paper contributes to making deep
learning more accessible and adaptable, facilitating its wider application and
encouraging innovation in model optimization. Ultimately, the paper aims to
contribute towards insights into PEFT's evolving landscape, guiding researchers
and practitioners in overcoming the limitations of conventional fine-tuning
approaches.",2024-04-21
Data Alignment for Zero-Shot Concept Generation in Dermatology AI,2024-04-19 17:57:29+00:00,http://arxiv.org/abs/2404.13043v1,"Soham Gadgil, Mahtab Bigverdi","cs.CV, cs.CL, cs.LG",image2text,"AI in dermatology is evolving at a rapid pace but the major limitation to
training trustworthy classifiers is the scarcity of data with ground-truth
concept level labels, which are meta-labels semantically meaningful to humans.
Foundation models like CLIP providing zero-shot capabilities can help alleviate
this challenge by leveraging vast amounts of image-caption pairs available on
the internet. CLIP can be fine-tuned using domain specific image-caption pairs
to improve classification performance. However, CLIP's pre-training data is not
well-aligned with the medical jargon that clinicians use to perform diagnoses.
The development of large language models (LLMs) in recent years has led to the
possibility of leveraging the expressive nature of these models to generate
rich text. Our goal is to use these models to generate caption text that aligns
well with both the clinical lexicon and with the natural human language used in
CLIP's pre-training data. Starting with captions used for images in PubMed
articles, we extend them by passing the raw captions through an LLM fine-tuned
on the field's several textbooks. We find that using captions generated by an
expressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept
classification performance.",2024-04-19
"Incubating Text Classifiers Following User Instruction with Nothing but
  LLM",2024-04-16 19:53:35+00:00,http://arxiv.org/abs/2404.10877v1,"Letian Peng, Jingbo Shang",cs.CL,image2text,"In this paper, we aim to generate text classification data given arbitrary
class definitions (i.e., user instruction), so one can train a small text
classifier without any human annotation or raw corpus. Compared with pioneer
attempts, our proposed Incubator is the first framework that can handle
complicated and even mutually dependent classes (e.g., ""TED Talk given by
Educator"" and ""Other""). Specifically, Incubator is an LLM firstly tuned on the
instruction-to-data mappings that we obtained from classification datasets and
descriptions on HuggingFace together with in-context augmentation by GPT-4. We
then refine Incubator by learning on the cluster centers of semantic textual
embeddings to emphasize the uniformity and semantic diversity in generations.
We compare Incubator on various classification tasks with strong baselines such
as direct LLM-based inference and training data generation by prompt
engineering. Experiments show Incubator is able to (1) perform well on
traditional benchmarks, (2) take label dependency and user preference into
consideration, and (3) enable logical text mining by incubating multiple
classifiers.",2024-04-16
"LaDiC: Are Diffusion Models Really Inferior to Autoregressive
  Counterparts for Image-to-Text Generation?",2024-04-16 17:47:16+00:00,http://arxiv.org/abs/2404.10763v1,"Yuchi Wang, Shuhuai Ren, Rundong Gao, Linli Yao, Qingyan Guo, Kaikai An, Jianhong Bai, Xu Sun","cs.AI, cs.CL, cs.CV",image2text,"Diffusion models have exhibited remarkable capabilities in text-to-image
generation. However, their performance in image-to-text generation,
specifically image captioning, has lagged behind Auto-Regressive (AR) models,
casting doubt on their applicability for such tasks. In this work, we revisit
diffusion models, highlighting their capacity for holistic context modeling and
parallel decoding. With these benefits, diffusion models can alleviate the
inherent limitations of AR methods, including their slow inference speed, error
propagation, and unidirectional constraints. Furthermore, we identify the prior
underperformance of diffusion models stemming from the absence of an effective
latent space for image-text alignment, and the discrepancy between continuous
diffusion processes and discrete textual data. In response, we introduce a
novel architecture, LaDiC, which utilizes a split BERT to create a dedicated
latent space for captions and integrates a regularization module to manage
varying text lengths. Our framework also includes a diffuser for semantic
image-to-text conversion and a Back&Refine technique to enhance token
interactivity during inference. LaDiC achieves state-of-the-art performance for
diffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2
CIDEr, demonstrating exceptional performance without pre-training or ancillary
modules. This indicates strong competitiveness with AR models, revealing the
previously untapped potential of diffusion models in image-to-text generation.",2024-04-16
Contextual Chart Generation for Cyber Deception,2024-04-07 07:56:14+00:00,http://arxiv.org/abs/2404.04854v1,"David D. Nguyen, David Liebowitz, Surya Nepal, Salil S. Kanhere, Sharif Abuadbba","cs.LG, cs.AI, cs.CR",image2text,"Honeyfiles are security assets designed to attract and detect intruders on
compromised systems. Honeyfiles are a type of honeypot that mimic real,
sensitive documents, creating the illusion of the presence of valuable data.
Interaction with a honeyfile reveals the presence of an intruder, and can
provide insights into their goals and intentions. Their practical use, however,
is limited by the time, cost and effort associated with manually creating
realistic content. The introduction of large language models has made
high-quality text generation accessible, but honeyfiles contain a variety of
content including charts, tables and images. This content needs to be plausible
and realistic, as well as semantically consistent both within honeyfiles and
with the real documents they mimic, to successfully deceive an intruder.
  In this paper, we focus on an important component of the honeyfile content
generation problem: document charts. Charts are ubiquitous in corporate
documents and are commonly used to communicate quantitative and scientific
data. Existing image generation models, such as DALL-E, are rather prone to
generating charts with incomprehensible text and unconvincing data. We take a
multi-modal approach to this problem by combining two purpose-built generative
models: a multitask Transformer and a specialized multi-head autoencoder. The
Transformer generates realistic captions and plot text, while the autoencoder
generates the underlying tabular data for the plot.
  To advance the field of automated honeyplot generation, we also release a new
document-chart dataset and propose a novel metric Keyword Semantic Matching
(KSM). This metric measures the semantic consistency between keywords of a
corpus and a smaller bag of words. Extensive experiments demonstrate excellent
performance against multiple large language models, including ChatGPT and GPT4.",2024-04-07
Semi-Supervised Image Captioning Considering Wasserstein Graph Matching,2024-03-26 14:47:05+00:00,http://arxiv.org/abs/2403.17995v1,Yang Yang,"cs.CV, cs.AI, cs.LG",image2text,"Image captioning can automatically generate captions for the given images,
and the key challenge is to learn a mapping function from visual features to
natural language features. Existing approaches are mostly supervised ones,
i.e., each image has a corresponding sentence in the training set. However,
considering that describing images always requires a huge of manpower, we
usually have limited amount of described images (i.e., image-text pairs) and a
large number of undescribed images in real-world applications. Thereby, a
dilemma is the ""Semi-Supervised Image Captioning"". To solve this problem, we
propose a novel Semi-Supervised Image Captioning method considering Wasserstein
Graph Matching (SSIC-WGM), which turns to adopt the raw image inputs to
supervise the generated sentences. Different from traditional single modal
semi-supervised methods, the difficulty of semi-supervised cross-modal learning
lies in constructing intermediately comparable information among heterogeneous
modalities. In this paper, SSIC-WGM adopts the successful scene graphs as
intermediate information, and constrains the generated sentences from two
aspects: 1) inter-modal consistency. SSIC-WGM constructs the scene graphs of
the raw image and generated sentence respectively, then employs the wasserstein
distance to better measure the similarity between region embeddings of
different graphs. 2) intra-modal consistency. SSIC-WGM takes the data
augmentation techniques for the raw images, then constrains the consistency
among augmented images and generated sentences. Consequently, SSIC-WGM combines
the cross-modal pseudo supervision and structure invariant measure for
efficiently using the undescribed images, and learns more reasonable mapping
function.",2024-03-26
"The Solution for the ICCV 2023 1st Scientific Figure Captioning
  Challenge",2024-03-26 03:03:50+00:00,http://arxiv.org/abs/2403.17342v1,"Dian Chao, Xin Song, Shupeng Zhong, Boyuan Wang, Xiangyu Wu, Chen Zhu, Yang Yang","cs.CV, cs.AI",image2text,"In this paper, we propose a solution for improving the quality of captions
generated for figures in papers. We adopt the approach of summarizing the
textual content in the paper to generate image captions. Throughout our study,
we encounter discrepancies in the OCR information provided in the official
dataset. To rectify this, we employ the PaddleOCR toolkit to extract OCR
information from all images. Moreover, we observe that certain textual content
in the official paper pertains to images that are not relevant for captioning,
thereby introducing noise during caption generation. To mitigate this issue, we
leverage LLaMA to extract image-specific information by querying the textual
content based on image mentions, effectively filtering out extraneous
information. Additionally, we recognize a discrepancy between the primary use
of maximum likelihood estimation during text generation and the evaluation
metrics such as ROUGE employed to assess the quality of generated captions. To
bridge this gap, we integrate the BRIO model framework, enabling a more
coherent alignment between the generation and evaluation processes. Our
approach ranked first in the final test with a score of 4.49.",2024-03-26
"UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation
  Model for Urban Indicator Prediction",2024-03-25 14:57:18+00:00,http://arxiv.org/abs/2403.16831v1,"Xixuan Hao, Wei Chen, Yibo Yan, Siru Zhong, Kun Wang, Qingsong Wen, Yuxuan Liang","cs.CV, cs.AI",image2text,"Urban indicator prediction aims to infer socio-economic metrics in diverse
urban landscapes using data-driven methods. However, prevalent pre-trained
models, particularly those reliant on satellite imagery, face dual challenges.
Firstly, concentrating solely on macro-level patterns from satellite data may
introduce bias, lacking nuanced details at micro levels, such as architectural
details at a place. Secondly, the lack of interpretability in pre-trained
models limits their utility in providing transparent evidence for urban
planning. In response to these issues, we devise a novel Vision-Language
Pre-Trained Model (UrbanVLP) in this paper. Our UrbanVLP seamlessly integrates
multi-granularity information from both macro (satellite) and micro
(street-view) levels, overcoming the limitations of prior pre-trained models.
Moreover, it introduces automatic text generation and calibration, elevating
interpretability in downstream applications by producing high-quality text
descriptions of urban imagery. Rigorous experiments conducted across six
socio-economic tasks underscore UrbanVLP's superior performance. We also deploy
a web platform to verify its practicality.",2024-03-25
"Grammatical vs Spelling Error Correction: An Investigation into the
  Responsiveness of Transformer-based Language Models using BART and MarianMT",2024-03-25 11:45:21+00:00,http://arxiv.org/abs/2403.16655v1,"Rohit Raju, Peeta Basa Pati, SA Gandheesh, Gayatri Sanjana Sannala, Suriya KS",cs.CL,image2text,"Text continues to remain a relevant form of representation for information.
Text documents are created either in digital native platforms or through the
conversion of other media files such as images and speech. While the digital
native text is invariably obtained through physical or virtual keyboards,
technologies such as OCR and speech recognition are utilized to transform the
images and speech signals into text content. All these variety of mechanisms of
text generation also introduce errors into the captured text.
  This project aims at analyzing different kinds of error that occurs in text
documents. The work employs two of the advanced deep neural network-based
language models, namely, BART and MarianMT, to rectify the anomalies present in
the text. Transfer learning of these models with available dataset is performed
to finetune their capacity for error correction. A comparative study is
conducted to investigate the effectiveness of these models in handling each of
the defined error categories. It is observed that while both models can bring
down the erroneous sentences by 20+%, BART can handle spelling errors far
better (24.6%) than grammatical errors (8.8%).",2024-03-25
"Visually Guided Generative Text-Layout Pre-training for Document
  Intelligence",2024-03-25 08:00:43+00:00,http://arxiv.org/abs/2403.16516v2,"Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu, Kam-Fai Wong","cs.CL, cs.CV",image2text,"Prior study shows that pre-training techniques can boost the performance of
visual document understanding (VDU), which typically requires models to gain
abilities to perceive and reason both document texts and layouts (e.g.,
locations of texts and table-cells). To this end, we propose visually guided
generative text-layout pre-training, named ViTLP. Given a document image, the
model optimizes hierarchical language and layout modeling objectives to
generate the interleaved text and layout sequence. In addition, to address the
limitation of processing long documents by Transformers, we introduce a
straightforward yet effective multi-segment generative pre-training scheme,
facilitating ViTLP to process word-intensive documents of any length. ViTLP can
function as a native OCR model to localize and recognize texts of document
images. Besides, ViTLP can be effectively applied to various downstream VDU
tasks. Extensive experiments show that ViTLP achieves competitive performance
over existing baselines on benchmark VDU tasks, including information
extraction, document classification, and document question answering.",2024-03-25
"Refining Text-to-Image Generation: Towards Accurate Training-Free
  Glyph-Enhanced Image Generation",2024-03-25 04:54:49+00:00,http://arxiv.org/abs/2403.16422v1,"Sanyam Lakhanpal, Shivang Chopra, Vinija Jain, Aman Chadha, Man Luo","cs.CV, cs.AI",image2text,"Over the past few years, Text-to-Image (T2I) generation approaches based on
diffusion models have gained significant attention. However, vanilla diffusion
models often suffer from spelling inaccuracies in the text displayed within the
generated images. The capability to generate visual text is crucial, offering
both academic interest and a wide range of practical applications. To produce
accurate visual text images, state-of-the-art techniques adopt a
glyph-controlled image generation approach, consisting of a text layout
generator followed by an image generator that is conditioned on the generated
text layout. Nevertheless, our study reveals that these models still face three
primary challenges, prompting us to develop a testbed to facilitate future
research. We introduce a benchmark, LenCom-Eval, specifically designed for
testing models' capability in generating images with Lengthy and Complex visual
text. Subsequently, we introduce a training-free framework to enhance the
two-stage generation approaches. We examine the effectiveness of our approach
on both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable
improvements across a range of evaluation metrics, including CLIPScore, OCR
precision, recall, F1 score, accuracy, and edit distance scores. For instance,
our proposed framework improves the backbone model, TextDiffuser, by more than
23\% and 13.5\% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval,
respectively. Our work makes a unique contribution to the field by focusing on
generating images with long and rare text sequences, a niche previously
unexplored by existing literature",2024-03-25
Dia-LLaMA: Towards Large Language Model-driven CT Report Generation,2024-03-25 03:02:51+00:00,http://arxiv.org/abs/2403.16386v1,"Zhixuan Chen, Luyang Luo, Yequan Bie, Hao Chen","cs.CV, cs.AI",image2text,"Medical report generation has achieved remarkable advancements yet has still
been faced with several challenges. First, the inherent imbalance in the
distribution of normal and abnormal cases may lead models to exhibit a biased
focus on normal samples, resulting in unreliable diagnoses. Second, the
frequent occurrence of common template sentences in the reports may overwhelm
the critical abnormal information. Moreover, existing works focus on 2D chest
X-rays, leaving CT report generation underexplored due to the high-dimensional
nature of CT images and the limited availability of CT-report pairs. Recently,
LLM has shown a great ability to generate reliable answers with appropriate
prompts, which shed light on addressing the aforementioned challenges. In this
paper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report
generation by incorporating diagnostic information as guidance prompts.
Considering the high dimension of CT, we leverage a pre-trained ViT3D with
perceiver to extract the visual information. To tailor the LLM for report
generation and emphasize abnormality, we extract additional diagnostic
information by referring to a disease prototype memory bank, which is updated
during training to capture common disease representations. Furthermore, we
introduce disease-aware attention to enable the model to adjust attention for
different diseases. Experiments on the chest CT dataset demonstrated that our
proposed method outperformed previous methods and achieved state-of-the-art on
both clinical efficacy performance and natural language generation metrics. The
code will be made publically available.",2024-03-25
"Exploiting Semantic Reconstruction to Mitigate Hallucinations in
  Vision-Language Models",2024-03-24 14:21:06+00:00,http://arxiv.org/abs/2403.16167v2,"Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru Chang","cs.CV, cs.CL",image2text,"Hallucinations in vision-language models pose a significant challenge to
their reliability, particularly in the generation of long captions. Current
methods fall short of accurately identifying and mitigating these
hallucinations. To address this issue, we introduce ESREAL, a novel
unsupervised learning framework designed to suppress the generation of
hallucinations through accurate localization and penalization of hallucinated
tokens. Initially, ESREAL creates a reconstructed image based on the generated
caption and aligns its corresponding regions with those of the original image.
This semantic reconstruction aids in identifying both the presence and type of
token-level hallucinations within the generated caption. Subsequently, ESREAL
computes token-level hallucination scores by assessing the semantic similarity
of aligned regions based on the type of hallucination. Finally, ESREAL employs
a proximal policy optimization algorithm, where it selectively penalizes
hallucinated tokens according to their token-level hallucination scores. Our
framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2
by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved
solely through signals derived from the image itself, without the need for any
image-text pairs.",2024-03-24
"Cognitive resilience: Unraveling the proficiency of image-captioning
  models to interpret masked visual content",2024-03-23 15:53:00+00:00,http://arxiv.org/abs/2403.15876v1,"Zhicheng Du, Zhaotian Xie, Huazhang Ying, Likun Zhang, Peiwu Qin","cs.CV, cs.AI",image2text,"This study explores the ability of Image Captioning (IC) models to decode
masked visual content sourced from diverse datasets. Our findings reveal the IC
model's capability to generate captions from masked images, closely resembling
the original content. Notably, even in the presence of masks, the model adeptly
crafts descriptive textual information that goes beyond what is observable in
the original image-generated captions. While the decoding performance of the IC
model experiences a decline with an increase in the masked region's area, the
model still performs well when important regions of the image are not masked at
high coverage.",2024-03-23
"InstaSynth: Opportunities and Challenges in Generating Synthetic
  Instagram Data with ChatGPT for Sponsored Content Detection",2024-03-22 13:58:42+00:00,http://arxiv.org/abs/2403.15214v1,"Thales Bertaglia, Lily Heisig, Rishabh Kaushal, Adriana Iamnitchi","cs.CY, cs.CL, cs.SI",image2text,"Large Language Models (LLMs) raise concerns about lowering the cost of
generating texts that could be used for unethical or illegal purposes,
especially on social media. This paper investigates the promise of such models
to help enforce legal requirements related to the disclosure of sponsored
content online. We investigate the use of LLMs for generating synthetic
Instagram captions with two objectives: The first objective (fidelity) is to
produce realistic synthetic datasets. For this, we implement content-level and
network-level metrics to assess whether synthetic captions are realistic. The
second objective (utility) is to create synthetic data that is useful for
sponsored content detection. For this, we evaluate the effectiveness of the
generated synthetic data for training classifiers to identify undisclosed
advertisements on Instagram. Our investigations show that the objectives of
fidelity and utility may conflict and that prompt engineering is a useful but
insufficient strategy. Additionally, we find that while individual synthetic
posts may appear realistic, collectively they lack diversity, topic
connectivity, and realistic user interaction patterns.",2024-03-22
"SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and
  Related Observable Overgeneration Mistakes",2024-03-12 15:06:22+00:00,http://arxiv.org/abs/2403.07726v1,"Timothee Mickus, Elaine Zosa, Raúl Vázquez, Teemu Vahtola, Jörg Tiedemann, Vincent Segonne, Alessandro Raganato, Marianna Apidianaki",cs.CL,image2text,"This paper presents the results of the SHROOM, a shared task focused on
detecting hallucinations: outputs from natural language generation (NLG)
systems that are fluent, yet inaccurate. Such cases of overgeneration put in
jeopardy many NLG applications, where correctness is often mission-critical.
The shared task was conducted with a newly constructed dataset of 4000 model
outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine
translation, paraphrase generation and definition modeling.
  The shared task was tackled by a total of 58 different users grouped in 42
teams, out of which 27 elected to write a system description paper;
collectively, they submitted over 300 prediction sets on both tracks of the
shared task. We observe a number of key trends in how this approach was tackled
-- many participants rely on a handful of model, and often rely either on
synthetic data for fine-tuning or zero-shot prompting strategies. While a
majority of the teams did outperform our proposed baseline system, the
performances of top-scoring systems are still consistent with a random handling
of the more challenging items.",2024-03-12
"Premonition: Using Generative Models to Preempt Future Data Changes in
  Continual Learning",2024-03-12 06:29:54+00:00,http://arxiv.org/abs/2403.07356v1,"Mark D. McDonnell, Dong Gong, Ehsan Abbasnejad, Anton van den Hengel","cs.CV, cs.LG",image2text,"Continual learning requires a model to adapt to ongoing changes in the data
distribution, and often to the set of tasks to be performed. It is rare,
however, that the data and task changes are completely unpredictable. Given a
description of an overarching goal or data theme, which we call a realm, humans
can often guess what concepts are associated with it. We show here that the
combination of a large language model and an image generation model can
similarly provide useful premonitions as to how a continual learning challenge
might develop over time. We use the large language model to generate text
descriptions of semantically related classes that might potentially appear in
the data stream in future. These descriptions are then rendered using Stable
Diffusion to generate new labelled image samples. The resulting synthetic
dataset is employed for supervised pre-training, but is discarded prior to
commencing continual learning, along with the pre-training classification head.
We find that the backbone of our pre-trained networks can learn representations
useful for the downstream continual learning problem, thus becoming a valuable
input to any existing continual learning method. Although there are
complexities arising from the domain gap between real and synthetic images, we
show that pre-training models in this manner improves multiple Class Incremenal
Learning (CIL) methods on fine-grained image classification benchmarks.
Supporting code can be found at https://github.com/cl-premonition/premonition.",2024-03-12
"MAP-Elites with Transverse Assessment for Multimodal Problems in
  Creative Domains",2024-03-11 21:50:22+00:00,http://arxiv.org/abs/2403.07182v1,"Marvin Zammit, Antonios Liapis, Georgios N. Yannakakis",cs.NE,image2text,"The recent advances in language-based generative models have paved the way
for the orchestration of multiple generators of different artefact types (text,
image, audio, etc.) into one system. Presently, many open-source pre-trained
models combine text with other modalities, thus enabling shared vector
embeddings to be compared across different generators. Within this context we
propose a novel approach to handle multimodal creative tasks using Quality
Diversity evolution. Our contribution is a variation of the MAP-Elites
algorithm, MAP-Elites with Transverse Assessment (MEliTA), which is tailored
for multimodal creative tasks and leverages deep learned models that assess
coherence across modalities. MEliTA decouples the artefacts' modalities and
promotes cross-pollination between elites. As a test bed for this algorithm, we
generate text descriptions and cover images for a hypothetical video game and
assign each artefact a unique modality-specific behavioural characteristic.
Results indicate that MEliTA can improve text-to-image mappings within the
solution space, compared to a baseline MAP-Elites algorithm that strictly
treats each image-text pair as one solution. Our approach represents a
significant step forward in multimodal bottom-up orchestration and lays the
groundwork for more complex systems coordinating multimodal creative agents in
the future.",2024-03-11
One Category One Prompt: Dataset Distillation using Diffusion Models,2024-03-11 20:23:59+00:00,http://arxiv.org/abs/2403.07142v1,"Ali Abbasi, Ashkan Shahbazi, Hamed Pirsiavash, Soheil Kolouri","cs.CV, cs.CL, cs.LG",image2text,"The extensive amounts of data required for training deep neural networks pose
significant challenges on storage and transmission fronts. Dataset distillation
has emerged as a promising technique to condense the information of massive
datasets into a much smaller yet representative set of synthetic samples.
However, traditional dataset distillation approaches often struggle to scale
effectively with high-resolution images and more complex architectures due to
the limitations in bi-level optimization. Recently, several works have proposed
exploiting knowledge distillation with decoupled optimization schemes to scale
up dataset distillation. Although these methods effectively address the
scalability issue, they rely on extensive image augmentations requiring the
storage of soft labels for augmented images. In this paper, we introduce
Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for
dataset distillation, leveraging recent advancements in generative
text-to-image foundation models. Our approach utilizes textual inversion, a
technique for fine-tuning text-to-image generative models, to create concise
and informative representations for large datasets. By employing these learned
text prompts, we can efficiently store and infer new samples for introducing
data variability within a fixed memory budget. We show the effectiveness of our
method through extensive experiments across various computer vision benchmark
datasets with different memory budgets.",2024-03-11
Narrating Causal Graphs with Large Language Models,2024-03-11 19:19:59+00:00,http://arxiv.org/abs/2403.07118v1,"Atharva Phatak, Vijay K. Mago, Ameeta Agrawal, Aravind Inbasekaran, Philippe J. Giabbanelli",cs.CL,image2text,"The use of generative AI to create text descriptions from graphs has mostly
focused on knowledge graphs, which connect concepts using facts. In this work
we explore the capability of large pretrained language models to generate text
from causal graphs, where salient concepts are represented as nodes and
causality is represented via directed, typed edges. The causal reasoning
encoded in these graphs can support applications as diverse as healthcare or
marketing. Using two publicly available causal graph datasets, we empirically
investigate the performance of four GPT-3 models under various settings. Our
results indicate that while causal text descriptions improve with training
data, compared to fact-based graphs, they are harder to generate under
zero-shot settings. Results further suggest that users of generative AI can
deploy future applications faster since similar performances are obtained when
training a model with only a few examples as compared to fine-tuning via a
large curated dataset.",2024-03-11
"Enhancing Image Caption Generation Using Reinforcement Learning with
  Human Feedback",2024-03-11 13:57:05+00:00,http://arxiv.org/abs/2403.06735v1,"Adarsh N L, Arun P V, Aravindh N L","cs.CV, cs.AI",image2text,"Research on generative models to produce human-aligned / human-preferred
outputs has seen significant recent contributions. Between text and
image-generative models, we narrowed our focus to text-based generative models,
particularly to produce captions for images that align with human preferences.
In this research, we explored a potential method to amplify the performance of
the Deep Neural Network Model to generate captions that are preferred by
humans. This was achieved by integrating Supervised Learning and Reinforcement
Learning with Human Feedback (RLHF) using the Flickr8k dataset. Also, a novel
loss function that is capable of optimizing the model based on human feedback
is introduced. In this paper, we provide a concise sketch of our approach and
results, hoping to contribute to the ongoing advances in the field of
human-aligned generative AI models.",2024-03-11
"Defending Against Unforeseen Failure Modes with Latent Adversarial
  Training",2024-03-08 04:22:48+00:00,http://arxiv.org/abs/2403.05030v1,"Stephen Casper, Lennart Schulze, Oam Patel, Dylan Hadfield-Menell","cs.CR, cs.AI, cs.LG",image2text,"AI systems sometimes exhibit harmful unintended behaviors post-deployment.
This is often despite extensive diagnostics and debugging by developers.
Minimizing risks from models is challenging because the attack surface is so
large. It is not tractable to exhaustively search for inputs that may cause a
model to fail. Red-teaming and adversarial training (AT) are commonly used to
make AI systems more robust. However, they have not been sufficient to avoid
many real-world failure modes that differ from the ones adversarially trained
on. In this work, we utilize latent adversarial training (LAT) to defend
against vulnerabilities without generating inputs that elicit them. LAT
leverages the compressed, abstract, and structured latent representations of
concepts that the network actually uses for prediction. We use LAT to remove
trojans and defend against held-out classes of adversarial attacks. We show in
image classification, text classification, and text generation tasks that LAT
usually improves both robustness and performance on clean data relative to AT.
This suggests that LAT can be a promising tool for defending against failure
modes that are not explicitly identified by developers.",2024-03-08
Enhancing Court View Generation with Knowledge Injection and Guidance,2024-03-07 09:51:11+00:00,http://arxiv.org/abs/2403.04366v1,"Ang Li, Yiquan Wu, Yifei Liu, Fei Wu, Ming Cai, Kun Kuang",cs.AI,image2text,"Court View Generation (CVG) is a challenging task in the field of Legal
Artificial Intelligence (LegalAI), which aims to generate court views based on
the plaintiff claims and the fact descriptions. While Pretrained Language
Models (PLMs) have showcased their prowess in natural language generation,
their application to the complex, knowledge-intensive domain of CVG often
reveals inherent limitations. In this paper, we present a novel approach, named
Knowledge Injection and Guidance (KIG), designed to bolster CVG using PLMs. To
efficiently incorporate domain knowledge during the training stage, we
introduce a knowledge-injected prompt encoder for prompt tuning, thereby
reducing computational overhead. Moreover, to further enhance the model's
ability to utilize domain knowledge, we employ a generating navigator, which
dynamically guides the text generation process in the inference stage without
altering the model's architecture, making it readily transferable.
Comprehensive experiments on real-world data demonstrate the effectiveness of
our approach compared to several established baselines, especially in the
responsivity of claims, where it outperforms the best baseline by 11.87%.",2024-03-07
"Neural Image Compression with Text-guided Encoding for both Pixel-level
  and Perceptual Fidelity",2024-03-05 13:15:01+00:00,http://arxiv.org/abs/2403.02944v1,"Hagyeong Lee, Minkyu Kim, Jun-Hyuk Kim, Seungeon Kim, Dokwan Oh, Jaeho Lee","cs.CV, cs.LG",image2text,"Recent advances in text-guided image compression have shown great potential
to enhance the perceptual quality of reconstructed images. These methods,
however, tend to have significantly degraded pixel-wise fidelity, limiting
their practicality. To fill this gap, we develop a new text-guided image
compression algorithm that achieves both high perceptual and pixel-wise
fidelity. In particular, we propose a compression framework that leverages text
information mainly by text-adaptive encoding and training with joint image-text
loss. By doing so, we avoid decoding based on text-guided generative models --
known for high generative diversity -- and effectively utilize the semantic
information of text at a global level. Experimental results on various datasets
show that our method can achieve high pixel-level and perceptual quality, with
either human- or machine-generated captions. In particular, our method
outperforms all baselines in terms of LPIPS, with some room for even more
improvements when we use more carefully generated captions.",2024-03-05
"Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow
  Models",2024-03-02 22:27:44+00:00,http://arxiv.org/abs/2403.01329v1,"Neta Shaul, Uriel Singer, Ricky T. Q. Chen, Matthew Le, Ali Thabet, Albert Pumarola, Yaron Lipman","cs.LG, cs.AI, cs.CV",image2text,"This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver
distillation approach to improve sample efficiency of Diffusion and Flow
models. BNS solvers are based on a family of non-stationary solvers that
provably subsumes existing numerical ODE solvers and consequently demonstrate
considerable improvement in sample approximation (PSNR) over these baselines.
Compared to model distillation, BNS solvers benefit from a tiny parameter space
($<$200 parameters), fast optimization (two orders of magnitude faster),
maintain diversity of samples, and in contrast to previous solver distillation
approaches nearly close the gap from standard distillation methods such as
Progressive Distillation in the low-medium NFE regime. For example, BNS solver
achieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We
experimented with BNS solvers for conditional image generation, text-to-image
generation, and text-2-audio generation showing significant improvement in
sample approximation (PSNR) in all.",2024-03-02
"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic
  Compositional Reasoning via Counterfactual Examples",2024-02-20 18:59:55+00:00,http://arxiv.org/abs/2402.13254v2,"Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"We propose CounterCurate, a framework to comprehensively improve the
visio-linguistic compositional reasoning capability for both contrastive and
generative multimodal models. In particular, we identify two critical
under-explored problems: the neglect of the physically grounded reasoning
(counting and position understanding) and the potential of using highly capable
text and image generation models for semantic counterfactual fine-tuning. Our
work pioneers an approach that addresses these gaps. We first spotlight the
near-chance performance of multimodal models like CLIP and LLaVA in physically
grounded compositional reasoning. We then apply simple data augmentation using
grounded image generation model GLIGEN to generate fine-tuning data, resulting
in significant performance improvements: +33% and +37% for CLIP and LLaVA,
respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we
exploit the capabilities of high-performing text generation and image
generation models, specifically GPT-4V and DALLE-3, to curate challenging
semantic counterfactuals, thereby further enhancing compositional reasoning
capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms
GPT-4V.",2024-02-20
"Towards Explainable Harmful Meme Detection through Multimodal Debate
  between Large Language Models",2024-01-24 08:37:16+00:00,http://arxiv.org/abs/2401.13298v1,"Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma, Bo Wang, Ruichao Yang","cs.CL, cs.AI",image2text,"The age of social media is flooded with Internet memes, necessitating a clear
grasp and effective identification of harmful ones. This task presents a
significant challenge due to the implicit meaning embedded in memes, which is
not explicitly conveyed through the surface text and image. However, existing
harmful meme detection methods do not present readable explanations that unveil
such implicit meaning to support their detection decisions. In this paper, we
propose an explainable approach to detect harmful memes, achieved through
reasoning over conflicting rationales from both harmless and harmful positions.
Specifically, inspired by the powerful capacity of Large Language Models (LLMs)
on text generation and reasoning, we first elicit multimodal debate between
LLMs to generate the explanations derived from the contradictory arguments.
Then we propose to fine-tune a small language model as the debate judge for
harmfulness inference, to facilitate multimodal fusion between the harmfulness
rationales and the intrinsic multimodal information within memes. In this way,
our model is empowered to perform dialectical reasoning over intricate and
implicit harm-indicative patterns, utilizing multimodal explanations
originating from both harmless and harmful arguments. Extensive experiments on
three public meme datasets demonstrate that our harmful meme detection approach
achieves much better performance than state-of-the-art methods and exhibits a
superior capacity for explaining the meme harmfulness of the model predictions.",2024-01-24
"Large Language Models for Scientific Information Extraction: An
  Empirical Study for Virology",2024-01-18 15:04:55+00:00,http://arxiv.org/abs/2401.10040v1,"Mahsa Shamsabadi, Jennifer D'Souza, Sören Auer","cs.CL, cs.AI, cs.DL, cs.IT, math.IT",image2text,"In this paper, we champion the use of structured and semantic content
representation of discourse-based scholarly communication, inspired by tools
like Wikipedia infoboxes or structured Amazon product descriptions. These
representations provide users with a concise overview, aiding scientists in
navigating the dense academic landscape. Our novel automated approach leverages
the robust text generation capabilities of LLMs to produce structured scholarly
contribution summaries, offering both a practical solution and insights into
LLMs' emergent abilities.
  For LLMs, the prime focus is on improving their general intelligence as
conversational agents. We argue that these models can also be applied
effectively in information extraction (IE), specifically in complex IE tasks
within terse domains like Science. This paradigm shift replaces the traditional
modular, pipelined machine learning approach with a simpler objective expressed
through instructions. Our results show that finetuned FLAN-T5 with 1000x fewer
parameters than the state-of-the-art GPT-davinci is competitive for the task.",2024-01-18
Textual Summarisation of Large Sets: Towards a General Approach,2024-01-17 08:16:05+00:00,http://arxiv.org/abs/2401.09041v1,"Kittipitch Kuptavanich, Ehud Reiter, Kees Van Deemter, Advaith Siddharthan",cs.CL,image2text,"We are developing techniques to generate summary descriptions of sets of
objects. In this paper, we present and evaluate a rule-based NLG technique for
summarising sets of bibliographical references in academic papers. This extends
our previous work on summarising sets of consumer products and shows how our
model generalises across these two very different domains.",2024-01-17
Jewelry Recognition via Encoder-Decoder Models,2024-01-15 23:10:50+00:00,http://arxiv.org/abs/2401.08003v1,"José M. Alcalde-Llergo, Enrique Yeguas-Bolívar, Andrea Zingoni, Alejandro Fuerte-Jurado","cs.CV, cs.AI",image2text,"Jewelry recognition is a complex task due to the different styles and designs
of accessories. Precise descriptions of the various accessories is something
that today can only be achieved by experts in the field of jewelry. In this
work, we propose an approach for jewelry recognition using computer vision
techniques and image captioning, trying to simulate this expert human behavior
of analyzing accessories. The proposed methodology consist on using different
image captioning models to detect the jewels from an image and generate a
natural language description of the accessory. Then, this description is also
utilized to classify the accessories at different levels of detail. The
generated caption includes details such as the type of jewel, color, material,
and design. To demonstrate the effectiveness of the proposed method in
accurately recognizing different types of jewels, a dataset consisting of
images of accessories belonging to jewelry stores in C\'ordoba (Spain) has been
created. After testing the different image captioning architectures designed,
the final model achieves a captioning accuracy of 95\%. The proposed
methodology has the potential to be used in various applications such as
jewelry e-commerce, inventory management or automatic jewels recognition to
analyze people's tastes and social status.",2024-01-15
DRLC: Reinforcement Learning with Dense Rewards from LLM Critic,2024-01-14 22:05:11+00:00,http://arxiv.org/abs/2401.07382v1,"Meng Cao, Lei Shu, Lei Yu, Yun Zhu, Nevan Wichers, Yinxiao Liu, Lei Meng","cs.CL, cs.AI",image2text,"Reinforcement learning (RL) can align language models with non-differentiable
reward signals, such as human preferences. However, a major challenge arises
from the sparsity of these reward signals - typically, there is only one reward
for the entire generation. This sparsity of rewards can lead to inefficient and
unstable learning. In this paper, we introduce a novel framework leveraging the
critique ability of LLMs to produce dense rewards throughout the learning
process. Our approach incorporates a critic language model alongside the policy
model. This critic is prompted with the task description, question, policy
model's output, and environment's reward signal as input, and provides token or
span-level dense rewards that reflect the quality of each segment of the
output. We assess our approach on three text generation tasks: sentiment
control, language model detoxification, and summarization. Experimental results
show that incorporating artificial dense rewards in training yields consistent
performance gains over the PPO baseline with holistic rewards. Furthermore, in
a setting where the same model serves as both policy and critic, we demonstrate
that ""self-critique"" rewards also boost learning efficiency.",2024-01-14
"PizzaCommonSense: Learning to Model Commonsense Reasoning about
  Intermediate Steps in Cooking Recipes",2024-01-12 23:33:01+00:00,http://arxiv.org/abs/2401.06930v1,"Aissatou Diallo, Antonis Bikakis, Luke Dickens, Anthony Hunter, Rob Miller",cs.CL,image2text,"Decoding the core of procedural texts, exemplified by cooking recipes, is
crucial for intelligent reasoning and instruction automation. Procedural texts
can be comprehensively defined as a sequential chain of steps to accomplish a
task employing resources. From a cooking perspective, these instructions can be
interpreted as a series of modifications to a food preparation, which initially
comprises a set of ingredients. These changes involve transformations of
comestible resources. For a model to effectively reason about cooking recipes,
it must accurately discern and understand the inputs and outputs of
intermediate steps within the recipe. Aiming to address this, we present a new
corpus of cooking recipes enriched with descriptions of intermediate steps of
the recipes that explicate the input and output for each step. We discuss the
data collection process, investigate and provide baseline models based on T5
and GPT-3.5. This work presents a challenging task and insight into commonsense
reasoning and procedural text generation.",2024-01-12
"Zur Darstellung eines mehrstufigen Prototypbegriffs in der
  multilingualen automatischen Sprachgenerierung: vom Korpus über word
  embeddings bis hin zum automatischen Wörterbuch",2023-12-26 19:39:25+00:00,http://arxiv.org/abs/2312.16311v1,María José Domínguez Vázquez,cs.CL,image2text,"The multilingual dictionary of noun valency Portlex is considered to be the
trigger for the creation of the automatic language generators Xera and
Combinatoria, whose development and use is presented in this paper. Both
prototypes are used for the automatic generation of nominal phrases with their
mono- and bi-argumental valence slots, which could be used, among others, as
dictionary examples or as integrated components of future autonomous
E-Learning-Tools. As samples for new types of automatic valency dictionaries
including user interaction, we consider the language generators as we know them
today. In the specific methodological procedure for the development of the
language generators, the syntactic-semantic description of the noun slots turns
out to be the main focus from a syntagmatic and paradigmatic point of view.
Along with factors such as representativeness, grammatical correctness,
semantic coherence, frequency and the variety of lexical candidates, as well as
semantic classes and argument structures, which are fixed components of both
resources, a concept of a multi-sided prototype stands out. The combined
application of this prototype concept as well as of word embeddings together
with techniques from the field of automatic natural language processing and
generation (NLP and NLG) opens up a new way for the future development of
automatically generated plurilingual valency dictionaries. All things
considered, the paper depicts the language generators both from the point of
view of their development as well as from that of the users. The focus lies on
the role of the prototype concept within the development of the resources.",2023-12-26
"Diffusion-EXR: Controllable Review Generation for Explainable
  Recommendation via Diffusion Models",2023-12-24 14:23:15+00:00,http://arxiv.org/abs/2312.15490v1,"Ling Li, Shaohua Li, Winda Marantika, Alex C. Kot, Huijing Zhan","cs.IR, cs.AI",image2text,"Denoising Diffusion Probabilistic Model (DDPM) has shown great competence in
image and audio generation tasks. However, there exist few attempts to employ
DDPM in the text generation, especially review generation under recommendation
systems. Fueled by the predicted reviews explainability that justifies
recommendations could assist users better understand the recommended items and
increase the transparency of recommendation system, we propose a Diffusion
Model-based Review Generation towards EXplainable Recommendation named
Diffusion-EXR. Diffusion-EXR corrupts the sequence of review embeddings by
incrementally introducing varied levels of Gaussian noise to the sequence of
word embeddings and learns to reconstruct the original word representations in
the reverse process. The nature of DDPM enables our lightweight Transformer
backbone to perform excellently in the recommendation review generation task.
Extensive experimental results have demonstrated that Diffusion-EXR can achieve
state-of-the-art review generation for recommendation on two publicly available
benchmark datasets.",2023-12-24
Continuous Diffusion for Mixed-Type Tabular Data,2023-12-16 12:21:03+00:00,http://arxiv.org/abs/2312.10431v1,"Markus Mueller, Kathrin Gruber, Dennis Fok","cs.LG, stat.ML",image2text,"Score-based generative models (or diffusion models for short) have proven
successful across many domains in generating text and image data. However, the
consideration of mixed-type tabular data with this model family has fallen
short so far. Existing research mainly combines different diffusion processes
without explicitly accounting for the feature heterogeneity inherent to tabular
data. In this paper, we combine score matching and score interpolation to
ensure a common type of continuous noise distribution that affects both
continuous and categorical features alike. Further, we investigate the impact
of distinct noise schedules per feature or per data type. We allow for
adaptive, learnable noise schedules to ensure optimally allocated model
capacity and balanced generative capability. Results show that our model
consistently outperforms state-of-the-art benchmark models and that accounting
for heterogeneity within the noise schedule design boosts the sample quality.",2023-12-16
"Do LVLMs Understand Charts? Analyzing and Correcting Factual Errors in
  Chart Captioning",2023-12-15 19:16:21+00:00,http://arxiv.org/abs/2312.10160v1,"Kung-Hsiang Huang, Mingyang Zhou, Hou Pong Chan, Yi R. Fung, Zhenhailong Wang, Lingyu Zhang, Shih-Fu Chang, Heng Ji",cs.CL,image2text,"Recent advancements in large vision-language models (LVLMs) have led to
significant progress in generating natural language descriptions for visual
content and thus enhancing various applications. One issue with these powerful
models is that they sometimes produce texts that are factually inconsistent
with the visual input. While there has been some effort to mitigate such
inconsistencies in natural image captioning, the factuality of generated
captions for structured document images, such as charts, has not received as
much scrutiny, posing a potential threat to information reliability in critical
applications. This work delves into the factuality aspect by introducing a
comprehensive typology of factual errors in generated chart captions. A
large-scale human annotation effort provides insight into the error patterns
and frequencies in captions crafted by various chart captioning models,
ultimately forming the foundation of a novel dataset, CHOCOLATE. Our analysis
reveals that even state-of-the-art models, including GPT-4V, frequently produce
captions laced with factual inaccuracies. In response to this challenge, we
establish the new task of Chart Caption Factual Error Correction and introduce
CHARTVE, a model for visual entailment that outperforms proprietary and
open-source LVLMs in evaluating factual consistency. Furthermore, we propose
C2TFEC, an interpretable two-stage framework that excels at correcting factual
errors. This work inaugurates a new domain in factual error correction for
chart captions, presenting a novel evaluation mechanism, and demonstrating an
effective approach to ensuring the factuality of generated chart captions.",2023-12-15
Fast Sampling via De-randomization for Discrete Diffusion Models,2023-12-14 18:14:11+00:00,http://arxiv.org/abs/2312.09193v1,"Zixiang Chen, Huizhuo Yuan, Yongqian Li, Yiwen Kou, Junkai Zhang, Quanquan Gu","cs.LG, cs.AI, stat.ML",image2text,"Diffusion models have emerged as powerful tools for high-quality data
generation, such as image generation. Despite its success in continuous spaces,
discrete diffusion models, which apply to domains such as texts and natural
languages, remain under-studied and often suffer from slow generation speed. In
this paper, we propose a novel de-randomized diffusion process, which leads to
an accelerated algorithm for discrete diffusion models. Our technique
significantly reduces the number of function evaluations (i.e., calls to the
neural network), making the sampling process much faster. Furthermore, we
introduce a continuous-time (i.e., infinite-step) sampling algorithm that can
provide even better sample qualities than its discrete-time (finite-step)
counterpart. Extensive experiments on natural language generation and machine
translation tasks demonstrate the superior performance of our method in terms
of both generation speed and sample quality over existing methods for discrete
diffusion models.",2023-12-14
ToViLaG: Your Visual-Language Generative Model is Also An Evildoer,2023-12-13 08:25:07+00:00,http://arxiv.org/abs/2312.11523v1,"Xinpeng Wang, Xiaoyuan Yi, Han Jiang, Shanlin Zhou, Zhihua Wei, Xing Xie","cs.CL, cs.AI",image2text,"Warning: this paper includes model outputs showing offensive content. Recent
large-scale Visual-Language Generative Models (VLGMs) have achieved
unprecedented improvement in multimodal image/text generation. However, these
models might also generate toxic content, e.g., offensive text and pornography
images, raising significant ethical risks. Despite exhaustive studies on toxic
degeneration of language models, this problem remains largely unexplored within
the context of visual-language generation. This work delves into the propensity
for toxicity generation and susceptibility to toxic data across various VLGMs.
For this purpose, we built ToViLaG, a dataset comprising 32K
co-toxic/mono-toxic text-image pairs and 1K innocuous but evocative text that
tends to stimulate toxicity. Furthermore, we propose WInToRe, a novel toxicity
metric tailored to visual-language generation, which theoretically reflects
different aspects of toxicity considering both input and output. On such a
basis, we benchmarked the toxicity of a diverse spectrum of VLGMs and
discovered that some models do more evil than expected while some are more
vulnerable to infection, underscoring the necessity of VLGMs detoxification.
Therefore, we develop an innovative bottleneck-based detoxification method. Our
method could reduce toxicity while maintaining comparable generation quality,
providing a promising initial solution to this line of research.",2023-12-13
Multimodal Sentiment Analysis: Perceived vs Induced Sentiments,2023-12-12 07:24:02+00:00,http://arxiv.org/abs/2312.07627v1,"Aditi Aggarwal, Deepika Varshney, Saurabh Patel","cs.CV, cs.LG, cs.SI",image2text,"Social media has created a global network where people can easily access and
exchange vast information. This information gives rise to a variety of
opinions, reflecting both positive and negative viewpoints. GIFs stand out as a
multimedia format offering a visually engaging way for users to communicate. In
this research, we propose a multimodal framework that integrates visual and
textual features to predict the GIF sentiment. It also incorporates attributes
including face emotion detection and OCR generated captions to capture the
semantic aspects of the GIF. The developed classifier achieves an accuracy of
82.7% on Twitter GIFs, which is an improvement over state-of-the-art models.
Moreover, we have based our research on the ReactionGIF dataset, analysing the
variance in sentiment perceived by the author and sentiment induced in the
reader",2023-12-12
Adaptive Compression of the Latent Space in Variational Autoencoders,2023-12-11 10:35:31+00:00,http://arxiv.org/abs/2312.06280v1,"Gabriela Sejnova, Michal Vavrecka, Karla Stepanova","cs.LG, cs.AI",image2text,"Variational Autoencoders (VAEs) are powerful generative models that have been
widely used in various fields, including image and text generation. However,
one of the known challenges in using VAEs is the model's sensitivity to its
hyperparameters, such as the latent space size. This paper presents a simple
extension of VAEs for automatically determining the optimal latent space size
during the training process by gradually decreasing the latent size through
neuron removal and observing the model performance. The proposed method is
compared to traditional hyperparameter grid search and is shown to be
significantly faster while still achieving the best optimal dimensionality on
four image datasets. Furthermore, we show that the final performance of our
method is comparable to training on the optimal latent size from scratch, and
might thus serve as a convenient substitute.",2023-12-11
"Identifying and Mitigating Model Failures through Few-shot CLIP-aided
  Diffusion Generation",2023-12-09 04:43:49+00:00,http://arxiv.org/abs/2312.05464v1,"Atoosa Chegini, Soheil Feizi","cs.CV, cs.LG",image2text,"Deep learning models can encounter unexpected failures, especially when
dealing with challenging sub-populations. One common reason for these failures
is the occurrence of objects in backgrounds that are rarely seen during
training. To gain a better understanding of these failure modes,
human-interpretable descriptions are crucial for further analysis and
improvement which is expensive. In this study, we propose an end-to-end
framework that utilizes the capabilities of large language models (ChatGPT) and
vision-language deep models (CLIP) to generate text descriptions of failure
modes associated with spurious correlations (e.g. rarely seen backgrounds)
without human-in-the-loop intervention. These descriptions can be used to
generate synthetic data using generative models, such as diffusion models. The
model can now use this generated data to learn from its weaknesses and enhance
its performance on backgrounds that are uncommon for each class of data. Our
approach serves as a broad solution, promising progress in comprehending model
failure modes and strengthening deep learning models across a wide range of
failure scenarios (e.g. bacckgrounds, colors) automatically in a few-shot
manner. Our experiments have shown remarkable \textbf{improvements in accuracy
($\sim \textbf{21%}$)} on hard sub-populations (particularly for wrong
background association) across $40$ different models, such as ResNets,
EfficientNets, DenseNets, Vision Transformer (ViT), SwAVs, MoCos, DINOs, and
CLIPs on various datasets such as ImageNet-1000, CIFAR-10, and CIFAR-100.",2023-12-09
"Forcing Generative Models to Degenerate Ones: The Power of Data
  Poisoning Attacks",2023-12-07 23:26:06+00:00,http://arxiv.org/abs/2312.04748v1,"Shuli Jiang, Swanand Ravindra Kadhe, Yi Zhou, Ling Cai, Nathalie Baracaldo","cs.CR, cs.AI, cs.CL",image2text,"Growing applications of large language models (LLMs) trained by a third party
raise serious concerns on the security vulnerability of LLMs.It has been
demonstrated that malicious actors can covertly exploit these vulnerabilities
in LLMs through poisoning attacks aimed at generating undesirable outputs.
While poisoning attacks have received significant attention in the image domain
(e.g., object detection), and classification tasks, their implications for
generative models, particularly in the realm of natural language generation
(NLG) tasks, remain poorly understood. To bridge this gap, we perform a
comprehensive exploration of various poisoning techniques to assess their
effectiveness across a range of generative tasks. Furthermore, we introduce a
range of metrics designed to quantify the success and stealthiness of poisoning
attacks specifically tailored to NLG tasks. Through extensive experiments on
multiple NLG tasks, LLMs and datasets, we show that it is possible to
successfully poison an LLM during the fine-tuning stage using as little as 1\%
of the total tuning data samples. Our paper presents the first systematic
approach to comprehend poisoning attacks targeting NLG tasks considering a wide
range of triggers and attack settings. We hope our findings will assist the AI
security community in devising appropriate defenses against such threats.",2023-12-07
"Think While You Write: Hypothesis Verification Promotes Faithful
  Knowledge-to-Text Generation",2023-11-16 00:13:19+00:00,http://arxiv.org/abs/2311.09467v1,"Yifu Qiu, Varun Embar, Shay B. Cohen, Benjamin Han","cs.CL, cs.AI",image2text,"Neural knowledge-to-text generation models often struggle to faithfully
generate descriptions for the input facts: they may produce hallucinations that
contradict the given facts, or describe facts not present in the input. To
reduce hallucinations, we propose a novel decoding method, TWEAK (Think While
Effectively Articulating Knowledge). TWEAK treats the generated sequences at
each decoding step and its future sequences as hypotheses, and ranks each
generation candidate based on how well their corresponding hypotheses support
the input facts using a Hypothesis Verification Model (HVM). We first
demonstrate the effectiveness of TWEAK by using a Natural Language Inference
(NLI) model as the HVM and report improved faithfulness with minimal impact on
the quality. We then replace the NLI model with our task-specific HVM trained
with a first-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which
pairs input facts with their faithful and hallucinated descriptions with the
hallucinated spans marked. The new HVM improves the faithfulness and the
quality further and runs faster. Overall the best TWEAK variants improve on
average 2.22/7.17 points on faithfulness measured by FactKB over WebNLG and
TekGen/GenWiki, respectively, with only 0.14/0.32 points degradation on quality
measured by BERTScore over the same datasets. Since TWEAK is a decoding-only
approach, it can be integrated with any neural generative model without
retraining.",2023-11-16
GRIM: GRaph-based Interactive narrative visualization for gaMes,2023-11-15 18:55:45+00:00,http://arxiv.org/abs/2311.09213v1,"Jorge Leandro, Sudha Rao, Michael Xu, Weijia Xu, Nebosja Jojic, Chris Brockett, Bill Dolan",cs.CL,image2text,"Dialogue-based Role Playing Games (RPGs) require powerful storytelling. The
narratives of these may take years to write and typically involve a large
creative team. In this work, we demonstrate the potential of large generative
text models to assist this process. \textbf{GRIM}, a prototype
\textbf{GR}aph-based \textbf{I}nteractive narrative visualization system for
ga\textbf{M}es, generates a rich narrative graph with branching storylines that
match a high-level narrative description and constraints provided by the
designer. Game designers can interactively edit the graph by automatically
generating new sub-graphs that fit the edits within the original narrative and
constraints. We illustrate the use of \textbf{GRIM} in conjunction with GPT-4,
generating branching narratives for four well-known stories with different
contextual constraints.",2023-11-15
"Zero-shot audio captioning with audio-language model guidance and audio
  context keywords",2023-11-14 18:55:48+00:00,http://arxiv.org/abs/2311.08396v1,"Leonard Salewski, Stefan Fauth, A. Sophia Koepke, Zeynep Akata","eess.AS, cs.AI, cs.CL, cs.SD",image2text,"Zero-shot audio captioning aims at automatically generating descriptive
textual captions for audio content without prior training for this task.
Different from speech recognition which translates audio content that contains
spoken language into text, audio captioning is commonly concerned with ambient
sounds, or sounds produced by a human performing an action. Inspired by
zero-shot image captioning methods, we propose ZerAuCap, a novel framework for
summarising such general audio signals in a text caption without requiring
task-specific training. In particular, our framework exploits a pre-trained
large language model (LLM) for generating the text which is guided by a
pre-trained audio-language model to produce captions that describe the audio
content. Additionally, we use audio context keywords that prompt the language
model to generate text that is broadly relevant to sounds. Our proposed
framework achieves state-of-the-art results in zero-shot audio captioning on
the AudioCaps and Clotho datasets. Our code is available at
https://github.com/ExplainableML/ZerAuCap.",2023-11-14
"Multitask Multimodal Prompted Training for Interactive Embodied Task
  Completion",2023-11-07 15:27:52+00:00,http://arxiv.org/abs/2311.04067v1,"Georgios Pantazopoulos, Malvina Nikandrou, Amit Parekh, Bhathiya Hemanthage, Arash Eshghi, Ioannis Konstas, Verena Rieser, Oliver Lemon, Alessandro Suglia","cs.LG, cs.AI, cs.CV",image2text,"Interactive and embodied tasks pose at least two fundamental challenges to
existing Vision & Language (VL) models, including 1) grounding language in
trajectories of actions and observations, and 2) referential disambiguation. To
tackle these challenges, we propose an Embodied MultiModal Agent (EMMA): a
unified encoder-decoder model that reasons over images and trajectories, and
casts action prediction as multimodal text generation. By unifying all tasks as
text generation, EMMA learns a language of actions which facilitates transfer
across tasks. Different to previous modular approaches with independently
trained components, we use a single multitask model where each task contributes
to goal completion. EMMA performs on par with similar models on several VL
benchmarks and sets a new state-of-the-art performance (36.81% success rate) on
the Dialog-guided Task Completion (DTC), a benchmark to evaluate dialog-guided
agents in the Alexa Arena",2023-11-07
Grounded Intuition of GPT-Vision's Abilities with Scientific Images,2023-11-03 17:53:43+00:00,http://arxiv.org/abs/2311.02069v1,"Alyssa Hwang, Andrew Head, Chris Callison-Burch",cs.CL,image2text,"GPT-Vision has impressed us on a range of vision-language tasks, but it comes
with the familiar new challenge: we have little idea of its capabilities and
limitations. In our study, we formalize a process that many have instinctively
been trying already to develop ""grounded intuition"" of this new model. Inspired
by the recent movement away from benchmarking in favor of example-driven
qualitative evaluation, we draw upon grounded theory and thematic analysis in
social science and human-computer interaction to establish a rigorous framework
for qualitative evaluation in natural language processing. We use our technique
to examine alt text generation for scientific figures, finding that GPT-Vision
is particularly sensitive to prompting, counterfactual text in images, and
relative spatial relationships. Our method and analysis aim to help researchers
ramp up their own grounded intuitions of new models while exposing how
GPT-Vision can be applied to make information more accessible.",2023-11-03
"Multimodal Foundation Models for Zero-shot Animal Species Recognition in
  Camera Trap Images",2023-11-02 08:32:00+00:00,http://arxiv.org/abs/2311.01064v1,"Zalan Fabian, Zhongqi Miao, Chunyuan Li, Yuanhan Zhang, Ziwei Liu, Andrés Hernández, Andrés Montes-Rojas, Rafael Escucha, Laura Siabatto, Andrés Link, Pablo Arbeláez, Rahul Dodhia, Juan Lavista Ferres","cs.CV, cs.LG",image2text,"Due to deteriorating environmental conditions and increasing human activity,
conservation efforts directed towards wildlife is crucial. Motion-activated
camera traps constitute an efficient tool for tracking and monitoring wildlife
populations across the globe. Supervised learning techniques have been
successfully deployed to analyze such imagery, however training such techniques
requires annotations from experts. Reducing the reliance on costly labelled
data therefore has immense potential in developing large-scale wildlife
tracking solutions with markedly less human labor. In this work we propose
WildMatch, a novel zero-shot species classification framework that leverages
multimodal foundation models. In particular, we instruction tune
vision-language models to generate detailed visual descriptions of camera trap
images using similar terminology to experts. Then, we match the generated
caption to an external knowledge base of descriptions in order to determine the
species in a zero-shot manner. We investigate techniques to build instruction
tuning datasets for detailed animal description generation and propose a novel
knowledge augmentation technique to enhance caption quality. We demonstrate the
performance of WildMatch on a new camera trap dataset collected in the
Magdalena Medio region of Colombia.",2023-11-02
"Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning
  for Medical Image Captioning",2023-11-02 05:44:13+00:00,http://arxiv.org/abs/2311.01004v1,"Gaoang Wang, Zhenyu Zhang, Benlu Wang, Weijie Liang, Yizhi Li, Xuechen Guo, Guanhong Wang, Shiyan Li","cs.CV, cs.AI",image2text,"With the development of multimodality and large language models, the deep
learning-based technique for medical image captioning holds the potential to
offer valuable diagnostic recommendations. However, current generic text and
image pre-trained models do not yield satisfactory results when it comes to
describing intricate details within medical images. In this paper, we present a
novel medical image captioning method guided by the segment anything model
(SAM) to enable enhanced encoding with both general and detailed feature
extraction. In addition, our approach employs a distinctive pre-training
strategy with mixed semantic learning to simultaneously capture both the
overall information and finer details within medical images. We demonstrate the
effectiveness of this approach, as it outperforms the pre-trained BLIP2 model
on various evaluation metrics for generating descriptions of medical images.",2023-11-02
"Form follows Function: Text-to-Text Conditional Graph Generation based
  on Functional Requirements",2023-11-01 11:12:02+00:00,http://arxiv.org/abs/2311.00444v1,"Peter A. Zachares, Vahan Hovhannisyan, Alan Mosca, Yarin Gal",cs.LG,image2text,"This work focuses on the novel problem setting of generating graphs
conditioned on a description of the graph's functional requirements in a
downstream task. We pose the problem as a text-to-text generation problem and
focus on the approach of fine-tuning a pretrained large language model (LLM) to
generate graphs. We propose an inductive bias which incorporates information
about the structure of the graph into the LLM's generation process by
incorporating message passing layers into an LLM's architecture. To evaluate
our proposed method, we design a novel set of experiments using publicly
available and widely studied molecule and knowledge graph data sets. Results
suggest our proposed approach generates graphs which more closely meet the
requested functional requirements, outperforming baselines developed on similar
tasks by a statistically significant margin.",2023-11-01
"Woodpecker: Hallucination Correction for Multimodal Large Language
  Models",2023-10-24 17:58:07+00:00,http://arxiv.org/abs/2310.16045v1,"Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, Enhong Chen","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Hallucination is a big shadow hanging over the rapidly evolving Multimodal
Large Language Models (MLLMs), referring to the phenomenon that the generated
text is inconsistent with the image content. In order to mitigate
hallucinations, existing studies mainly resort to an instruction-tuning manner
that requires retraining the models with specific data. In this paper, we pave
a different way, introducing a training-free method named Woodpecker. Like a
woodpecker heals trees, it picks out and corrects hallucinations from the
generated text. Concretely, Woodpecker consists of five stages: key concept
extraction, question formulation, visual knowledge validation, visual claim
generation, and hallucination correction. Implemented in a post-remedy manner,
Woodpecker can easily serve different MLLMs, while being interpretable by
accessing intermediate outputs of the five stages. We evaluate Woodpecker both
quantitatively and qualitatively and show the huge potential of this new
paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement
in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released
at https://github.com/BradyFU/Woodpecker.",2023-10-24
GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions,2023-10-23 23:24:57+00:00,http://arxiv.org/abs/2310.15405v1,"Ting-Yao Hsu, Chieh-Yang Huang, Ryan Rossi, Sungchul Kim, C. Lee Giles, Ting-Hao K. Huang",cs.CL,image2text,"There is growing interest in systems that generate captions for scientific
figures. However, assessing these systems output poses a significant challenge.
Human evaluation requires academic expertise and is costly, while automatic
evaluation depends on often low-quality author-written captions. This paper
investigates using large language models (LLMs) as a cost-effective,
reference-free method for evaluating figure captions. We first constructed
SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600
scientific figure captions, both original and machine-made, for 600 arXiv
figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption
based on its potential to aid reader understanding, given relevant context such
as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot
evaluator, outperformed all other models and even surpassed assessments made by
Computer Science and Informatics undergraduates, achieving a Kendall
correlation score of 0.401 with Ph.D. students rankings",2023-10-23
"HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online
  Posts using Large Language Models",2023-10-21 12:18:29+00:00,http://arxiv.org/abs/2310.13985v1,"Vibhor Agarwal, Yu Chen, Nishanth Sastry",cs.CL,image2text,"Hate speech has become pervasive in today's digital age. Although there has
been considerable research to detect hate speech or generate counter speech to
combat hateful views, these approaches still cannot completely eliminate the
potential harmful societal consequences of hate speech -- hate speech, even
when detected, can often not be taken down or is often not taken down enough;
and hate speech unfortunately spreads quickly, often much faster than any
generated counter speech.
  This paper investigates a relatively new yet simple and effective approach of
suggesting a rephrasing of potential hate speech content even before the post
is made. We show that Large Language Models (LLMs) perform well on this task,
outperforming state-of-the-art baselines such as BART-Detox. We develop 4
different prompts based on task description, hate definition, few-shot
demonstrations and chain-of-thoughts for comprehensive experiments and conduct
experiments on open-source LLMs such as LLaMA-1, LLaMA-2 chat, Vicuna as well
as OpenAI's GPT-3.5. We propose various evaluation metrics to measure the
efficacy of the generated text and ensure the generated text has reduced hate
intensity without drastically changing the semantic meaning of the original
text.
  We find that LLMs with a few-shot demonstrations prompt work the best in
generating acceptable hate-rephrased text with semantic meaning similar to the
original text. Overall, we find that GPT-3.5 outperforms the baseline and
open-source models for all the different kinds of prompts. We also perform
human evaluations and interestingly, find that the rephrasings generated by
GPT-3.5 outperform even the human-generated ground-truth rephrasings in the
dataset. We also conduct detailed ablation studies to investigate why LLMs work
satisfactorily on this task and conduct a failure analysis to understand the
gaps.",2023-10-21
"RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question
  Answering",2023-10-19 19:32:27+00:00,http://arxiv.org/abs/2310.13120v1,"Yuduo Wang, Pedram Ghamisi","cs.CV, cs.LG",image2text,"In recent years, with the rapid advancement of transformer models,
transformer-based multimodal architectures have found wide application in
various downstream tasks, including but not limited to Image Captioning, Visual
Question Answering (VQA), and Image-Text Generation. However, contemporary
approaches to Remote Sensing (RS) VQA often involve resource-intensive
techniques, such as full fine-tuning of large models or the extraction of
image-text features from pre-trained multimodal models, followed by modality
fusion using decoders. These approaches demand significant computational
resources and time, and a considerable number of trainable parameters are
introduced. To address these challenges, we introduce a novel method known as
RSAdapter, which prioritizes runtime and parameter efficiency. RSAdapter
comprises two key components: the Parallel Adapter and an additional linear
transformation layer inserted after each fully connected (FC) layer within the
Adapter. This approach not only improves adaptation to pre-trained multimodal
models but also allows the parameters of the linear transformation layer to be
integrated into the preceding FC layers during inference, reducing inference
costs. To demonstrate the effectiveness of RSAdapter, we conduct an extensive
series of experiments using three distinct RS-VQA datasets and achieve
state-of-the-art results on all three datasets. The code for RSAdapter will be
available online at https://github.com/Y-D-Wang/RSAdapter.",2023-10-19
"MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and
  Uni-Modal Adapter",2023-10-19 14:52:58+00:00,http://arxiv.org/abs/2310.12798v1,"Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, Tat-Seng Chua","cs.CL, cs.MM",image2text,"Language Models (LMs) have demonstrated impressive molecule understanding
ability on various 1D text-related tasks. However, they inherently lack 2D
graph perception - a critical ability of human professionals in comprehending
molecules' topological structures. To bridge this gap, we propose MolCA:
Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal
Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and
graph-based molecular contents via the cross-modal projector. Specifically, the
cross-modal projector is implemented as a Q-Former to connect a graph encoder's
representation space and an LM's text space. Further, MolCA employs a uni-modal
adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks.
Unlike previous studies that couple an LM with a graph encoder via cross-modal
contrastive learning, MolCA retains the LM's ability of open-ended text
generation and augments it with 2D graph information. To showcase its
effectiveness, we extensively benchmark MolCA on tasks of molecule captioning,
IUPAC name prediction, and molecule-text retrieval, on which MolCA
significantly outperforms the baselines. Our codes and checkpoints can be found
at https://github.com/acharkq/MolCA.",2023-10-19
"Motion2Language, Unsupervised learning of synchronized semantic motion
  segmentation",2023-10-16 17:16:32+00:00,http://arxiv.org/abs/2310.10594v1,"Karim Radouane, Andon Tchechmedjiev, Sylvie Ranwez, Julien Lagarde","cs.CV, cs.CL",image2text,"In this paper, we investigate building a sequence to sequence architecture
for motion to language translation and synchronization. The aim is to translate
motion capture inputs into English natural-language descriptions, such that the
descriptions are generated synchronously with the actions performed, enabling
semantic segmentation as a byproduct, but without requiring synchronized
training data. We propose a new recurrent formulation of local attention that
is suited for synchronous/live text generation, as well as an improved motion
encoder architecture better suited to smaller data and for synchronous
generation. We evaluate both contributions in individual experiments, using the
standard BLEU4 metric, as well as a simple semantic equivalence measure, on the
KIT motion language dataset. In a follow-up experiment, we assess the quality
of the synchronization of generated text in our proposed approaches through
multiple evaluation metrics. We find that both contributions to the attention
mechanism and the encoder architecture additively improve the quality of
generated text (BLEU and semantic equivalence), but also of synchronization.
Our code will be made available at
\url{https://github.com/rd20karim/M2T-Segmentation/tree/main}",2023-10-16
"BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools
  for Video-based Texts Generation",2023-10-16 17:05:56+00:00,http://arxiv.org/abs/2310.10586v1,"Ji Qi, Kaixuan Ji, Jifan Yu, Duokang Wang, Bin Xu, Lei Hou, Juanzi Li","cs.CV, cs.CL",image2text,"Building models that generate textual responses to user instructions for
videos is a practical and challenging topic, as it requires both vision
understanding and knowledge reasoning. Compared to language and image
modalities, training efficiency remains a serious problem as existing studies
train models on massive sparse videos aligned with brief descriptions. In this
paper, we introduce BiLL-VTG, a fast adaptive framework that leverages large
language models (LLMs) to reasoning on videos based on essential lightweight
visual tools. Specifically, we reveal the key to response specific instructions
is the concentration on relevant video events, and utilize two visual tools of
structured scene graph generation and descriptive image caption generation to
gather and represent the events information. Thus, a LLM equipped with world
knowledge is adopted as the reasoning agent to achieve the response by
performing multiple reasoning steps on specified video events.To address the
difficulty of specifying events from agent, we further propose an
Instruction-oriented Video Events Recognition (InsOVER) algorithm based on the
efficient Hungarian matching to localize corresponding video events using
linguistic instructions, enabling LLMs to interact with long videos. Extensive
experiments on two typical video-based texts generations tasks show that our
tuning-free framework outperforms the pre-trained models including
Flamingo-80B, to achieve the state-of-the-art performance.",2023-10-16
"Prompting for Discovery: Flexible Sense-Making for AI Art-Making with
  Dreamsheets",2023-10-15 23:54:20+00:00,http://arxiv.org/abs/2310.09985v1,"Shm Garanganao Almeda, J. D. Zamfirescu-Pereira, Kyu Won Kim, Pradeep Mani Rathnam, Bjoern Hartmann",cs.HC,image2text,"Design space exploration (DSE) for Text-to-Image (TTI) models entails
navigating a vast, opaque space of possible image outputs, through a
commensurately vast input space of hyperparameters and prompt text. Minor
adjustments to prompt input can surface unexpectedly disparate images. How can
interfaces support end-users in reliably steering prompt-space explorations
towards interesting results? Our design probe, DreamSheets, supports
exploration strategies with LLM-based functions for assisted prompt
construction and simultaneous display of generated results, hosted in a
spreadsheet interface. The flexible layout and novel generative functions
enable experimentation with user-defined workflows. Two studies, a preliminary
lab study and a longitudinal study with five expert artists, revealed a set of
strategies participants use to tackle the challenges of TTI design space
exploration, and the interface features required to support them - like using
text-generation to define local ""axes"" of exploration. We distill these
insights into a UI mockup to guide future interfaces.",2023-10-15
VLIS: Unimodal Language Models Guide Multimodal Language Generation,2023-10-15 07:58:52+00:00,http://arxiv.org/abs/2310.09767v1,"Jiwan Chung, Youngjae Yu","cs.CL, cs.AI",image2text,"Multimodal language generation, which leverages the synergy of language and
vision, is a rapidly expanding field. However, existing vision-language models
face challenges in tasks that require complex linguistic understanding. To
address this issue, we introduce Visual-Language models as Importance Sampling
weights (VLIS), a novel framework that combines the visual conditioning
capability of vision-language models with the language understanding of
unimodal text-only language models without further training. It extracts
pointwise mutual information of each image and text from a visual-language
model and uses the value as an importance sampling weight to adjust the token
likelihood from a text-only model. VLIS improves vision-language models on
diverse tasks, including commonsense understanding (WHOOPS, OK-VQA, and
ScienceQA) and complex text generation (Concadia, Image Paragraph Captioning,
and ROCStories). Our results suggest that VLIS represents a promising new
direction for multimodal language generation.",2023-10-15
"GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language
  Models",2023-10-12 16:46:58+00:00,http://arxiv.org/abs/2310.08487v1,"Yuanchun Shen, Ruotong Liao, Zhen Han, Yunpu Ma, Volker Tresp",cs.CL,image2text,"While multi-modal models have successfully integrated information from image,
video, and audio modalities, integrating graph modality into large language
models (LLMs) remains unexplored. This discrepancy largely stems from the
inherent divergence between structured graph data and unstructured text data.
Incorporating graph knowledge provides a reliable source of information,
enabling potential solutions to address issues in text generation, e.g.,
hallucination, and lack of domain knowledge. To evaluate the integration of
graph knowledge into language models, a dedicated dataset is needed. However,
there is currently no benchmark dataset specifically designed for multimodal
graph-language models. To address this gap, we propose GraphextQA, a question
answering dataset with paired subgraphs, retrieved from Wikidata, to facilitate
the evaluation and future development of graph-language models. Additionally,
we introduce a baseline model called CrossGNN, which conditions answer
generation on the paired graphs by cross-attending question-aware graph
features at decoding. The proposed dataset is designed to evaluate
graph-language models' ability to understand graphs and make use of it for
answer generation. We perform experiments with language-only models and the
proposed graph-language model to validate the usefulness of the paired graphs
and to demonstrate the difficulty of the task.",2023-10-12
"CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large
  Language Models",2023-10-12 12:31:23+00:00,http://arxiv.org/abs/2310.08279v1,"Rui Yang, Li Fang, Yi Zhou","cs.CL, cs.AI",image2text,"Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce
and infer missing connections within knowledge graphs. Text-based approaches,
like SimKGC, have outperformed graph embedding methods, showcasing the promise
of inductive KGC. However, the efficacy of text-based methods hinges on the
quality of entity textual descriptions. In this paper, we identify the key
issue of whether large language models (LLMs) can generate effective text. To
mitigate hallucination in LLM-generated text in this paper, we introduce a
constraint-based prompt that utilizes the entity and its textual description as
contextual constraints to enhance data quality. Our Constrained-Prompt
Knowledge Graph Completion (CP-KGC) method demonstrates effective inference
under low resource computing conditions and surpasses prior results on the
WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC
tasks and provides new directions for future research.",2023-10-12
"Ziya-VL: Bilingual Large Vision-Language Model via Multi-Task
  Instruction Tuning",2023-10-12 09:39:17+00:00,http://arxiv.org/abs/2310.08166v1,"Junyu Lu, Dixiang Zhang, Xiaojun Wu, Xinyu Gao, Ruyi Gan, Jiaxing Zhang, Yan Song, Pingjian Zhang",cs.CL,image2text,"Recent advancements enlarge the capabilities of large language models (LLMs)
in zero-shot image-to-text generation and understanding by integrating
multi-modal inputs. However, such success is typically limited to English
scenarios due to the lack of large-scale and high-quality non-English
multi-modal resources, making it extremely difficult to establish competitive
counterparts in other languages. In this paper, we introduce the Ziya-VL
series, a set of bilingual large-scale vision-language models (LVLMs) designed
to incorporate visual semantics into LLM for multi-modal dialogue. Composed of
Ziya-VL-Base and Ziya-VL-Chat, our models adopt the Querying Transformer from
BLIP-2, further exploring the assistance of optimization schemes such as
instruction tuning, multi-stage training and low-rank adaptation module for
visual-language alignment. In addition, we stimulate the understanding ability
of GPT-4 in multi-modal scenarios, translating our gathered English image-text
datasets into Chinese and generating instruction-response through the
in-context learning method. The experiment results demonstrate that compared to
the existing LVLMs, Ziya-VL achieves competitive performance across a wide
range of English-only tasks including zero-shot image-text retrieval, image
captioning, and visual question answering. The evaluation leaderboard accessed
by GPT-4 also indicates that our models possess satisfactory image-text
understanding and generation capabilities in Chinese multi-modal scenario
dialogues. Code, demo and models are available at
~\url{https://huggingface.co/IDEA-CCNL/Ziya-BLIP2-14B-Visual-v1}.",2023-10-12
Multimodal Graph Learning for Generative Tasks,2023-10-11 13:25:03+00:00,http://arxiv.org/abs/2310.07478v2,"Minji Yoon, Jing Yu Koh, Bryan Hooi, Ruslan Salakhutdinov",cs.AI,image2text,"Multimodal learning combines multiple data modalities, broadening the types
and complexity of data our models can utilize: for example, from plain text to
image-caption pairs. Most multimodal learning algorithms focus on modeling
simple one-to-one pairs of data from two modalities, such as image-caption
pairs, or audio-text pairs. However, in most real-world settings, entities of
different modalities interact with each other in more complex and multifaceted
ways, going beyond one-to-one mappings. We propose to represent these complex
relationships as graphs, allowing us to capture data with any number of
modalities, and with complex relationships between modalities that can flexibly
vary from one sample to another. Toward this goal, we propose Multimodal Graph
Learning (MMGL), a general and systematic framework for capturing information
from multiple multimodal neighbors with relational structures among them. In
particular, we focus on MMGL for generative tasks, building upon pretrained
Language Models (LMs), aiming to augment their text generation with multimodal
neighbor contexts. We study three research questions raised by MMGL: (1) how
can we infuse multiple neighbor information into the pretrained LMs, while
avoiding scalability issues? (2) how can we infuse the graph structure
information among multimodal neighbors into the LMs? and (3) how can we
finetune the pretrained LMs to learn from the neighbor context in a
parameter-efficient manner? We conduct extensive experiments to answer these
three questions on MMGL and analyze the empirical results to pave the way for
future MMGL research.",2023-10-11
Video-CSR: Complex Video Digest Creation for Visual-Language Models,2023-10-08 08:02:43+00:00,http://arxiv.org/abs/2310.05060v1,"Tingkai Liu, Yunzhe Tao, Haogeng Liu, Qihang Fan, Ding Zhou, Huaibo Huang, Ran He, Hongxia Yang","cs.CV, cs.AI",image2text,"We present a novel task and human annotated dataset for evaluating the
ability for visual-language models to generate captions and summaries for
real-world video clips, which we call Video-CSR (Captioning, Summarization and
Retrieval). The dataset contains 4.8K YouTube video clips of 20-60 seconds in
duration and covers a wide range of topics and interests. Each video clip
corresponds to 5 independently annotated captions (1 sentence) and summaries
(3-10 sentences). Given any video selected from the dataset and its
corresponding ASR information, we evaluate visual-language models on either
caption or summary generation that is grounded in both the visual and auditory
content of the video. Additionally, models are also evaluated on caption- and
summary-based retrieval tasks, where the summary-based retrieval task requires
the identification of a target video given excerpts of a corresponding summary.
Given the novel nature of the paragraph-length video summarization task, we
perform extensive comparative analyses of different existing evaluation metrics
and their alignment with human preferences. Finally, we propose a foundation
model with competitive generation and retrieval capabilities that serves as a
baseline for the Video-CSR task. We aim for Video-CSR to serve as a useful
evaluation set in the age of large language models and complex multi-modal
tasks.",2023-10-08
"InstructProtein: Aligning Human and Protein Language via Knowledge
  Instruction",2023-10-05 02:45:39+00:00,http://arxiv.org/abs/2310.03269v1,"Zeyuan Wang, Qiang Zhang, Keyan Ding, Ming Qin, Xiang Zhuang, Xiaotong Li, Huajun Chen","q-bio.BM, cs.CL",image2text,"Large Language Models (LLMs) have revolutionized the field of natural
language processing, but they fall short in comprehending biological sequences
such as proteins. To address this challenge, we propose InstructProtein, an
innovative LLM that possesses bidirectional generation capabilities in both
human and protein languages: (i) taking a protein sequence as input to predict
its textual function description and (ii) using natural language to prompt
protein sequence generation. To achieve this, we first pre-train an LLM on both
protein and natural language corpora, enabling it to comprehend individual
languages. Then supervised instruction tuning is employed to facilitate the
alignment of these two distinct languages. Herein, we introduce a knowledge
graph-based instruction generation framework to construct a high-quality
instruction dataset, addressing annotation imbalance and instruction deficits
in existing protein-text corpus. In particular, the instructions inherit the
structural relations between proteins and function annotations in knowledge
graphs, which empowers our model to engage in the causal modeling of protein
functions, akin to the chain-of-thought processes in natural languages.
Extensive experiments on bidirectional protein-text generation tasks show that
InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,
InstructProtein serves as a pioneering step towards text-based protein function
prediction and sequence design, effectively bridging the gap between protein
and human language understanding.",2023-10-05
"Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image
  Captioning",2023-09-10 08:55:24+00:00,http://arxiv.org/abs/2309.04965v1,"Guisheng Liu, Yi Li, Zhengcong Fei, Haiyan Fu, Xiangyang Luo, Yanqing Guo","cs.CV, cs.AI, cs.CL",image2text,"While impressive performance has been achieved in image captioning, the
limited diversity of the generated captions and the large parameter scale
remain major barriers to the real-word application of these systems. In this
work, we propose a lightweight image captioning network in combination with
continuous diffusion, called Prefix-diffusion. To achieve diversity, we design
an efficient method that injects prefix image embeddings into the denoising
process of the diffusion model. In order to reduce trainable parameters, we
employ a pre-trained model to extract image features and further design an
extra mapping network. Prefix-diffusion is able to generate diverse captions
with relatively less parameters, while maintaining the fluency and relevance of
the captions benefiting from the generative capabilities of the diffusion
model. Our work paves the way for scaling up diffusion models for image
captioning, and achieves promising performance compared with recent approaches.",2023-09-10
Zero-Shot Audio Captioning via Audibility Guidance,2023-09-07 17:45:58+00:00,http://arxiv.org/abs/2309.03884v1,"Tal Shaharabany, Ariel Shaulov, Lior Wolf","cs.SD, cs.CL, eess.AS",image2text,"The task of audio captioning is similar in essence to tasks such as image and
video captioning. However, it has received much less attention. We propose
three desiderata for captioning audio -- (i) fluency of the generated text,
(ii) faithfulness of the generated text to the input audio, and the somewhat
related (iii) audibility, which is the quality of being able to be perceived
based only on audio. Our method is a zero-shot method, i.e., we do not learn to
perform captioning. Instead, captioning occurs as an inference process that
involves three networks that correspond to the three desired qualities: (i) A
Large Language Model, in our case, for reasons of convenience, GPT-2, (ii) A
model that provides a matching score between an audio file and a text, for
which we use a multimodal matching network called ImageBind, and (iii) A text
classifier, trained using a dataset we collected automatically by instructing
GPT-4 with prompts designed to direct the generation of both audible and
inaudible sentences. We present our results on the AudioCap dataset,
demonstrating that audibility guidance significantly enhances performance
compared to the baseline, which lacks this objective.",2023-09-07
"Parameter Efficient Audio Captioning With Faithful Guidance Using
  Audio-text Shared Latent Representation",2023-09-06 19:42:52+00:00,http://arxiv.org/abs/2309.03340v1,"Arvind Krishna Sridhar, Yinyi Guo, Erik Visser, Rehana Mahfuz","cs.CL, cs.MM, cs.SD",image2text,"There has been significant research on developing pretrained transformer
architectures for multimodal-to-text generation tasks. Albeit performance
improvements, such models are frequently overparameterized, hence suffer from
hallucination and large memory footprint making them challenging to deploy on
edge devices. In this paper, we address both these issues for the application
of automated audio captioning. First, we propose a data augmentation technique
for generating hallucinated audio captions and show that similarity based on an
audio-text shared latent space is suitable for detecting hallucination. Then,
we propose a parameter efficient inference time faithful decoding algorithm
that enables smaller audio captioning models with performance equivalent to
larger models trained with more data. During the beam decoding step, the
smaller model utilizes an audio-text shared latent representation to
semantically align the generated text with corresponding input audio. Faithful
guidance is introduced into the beam probability by incorporating the cosine
similarity between latent representation projections of greedy rolled out
intermediate beams and audio clip. We show the efficacy of our algorithm on
benchmark datasets and evaluate the proposed scheme against baselines using
conventional audio captioning and semantic similarity metrics while
illustrating tradeoffs between performance and complexity.",2023-09-06
"Generative AI-aided Joint Training-free Secure Semantic Communications
  via Multi-modal Prompts",2023-09-05 23:24:56+00:00,http://arxiv.org/abs/2309.02616v1,"Hongyang Du, Guangyuan Liu, Dusit Niyato, Jiayi Zhang, Jiawen Kang, Zehui Xiong, Bo Ai, Dong In Kim","eess.IV, cs.LG, cs.NI",image2text,"Semantic communication (SemCom) holds promise for reducing network resource
consumption while achieving the communications goal. However, the computational
overheads in jointly training semantic encoders and decoders-and the subsequent
deployment in network devices-are overlooked. Recent advances in Generative
artificial intelligence (GAI) offer a potential solution. The robust learning
abilities of GAI models indicate that semantic decoders can reconstruct source
messages using a limited amount of semantic information, e.g., prompts, without
joint training with the semantic encoder. A notable challenge, however, is the
instability introduced by GAI's diverse generation ability. This instability,
evident in outputs like text-generated images, limits the direct application of
GAI in scenarios demanding accurate message recovery, such as face image
transmission. To solve the above problems, this paper proposes a GAI-aided
SemCom system with multi-model prompts for accurate content decoding. Moreover,
in response to security concerns, we introduce the application of covert
communications aided by a friendly jammer. The system jointly optimizes the
diffusion step, jamming, and transmitting power with the aid of the generative
diffusion models, enabling successful and secure transmission of the source
messages.",2023-09-05
"Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction
  Tuning",2023-09-05 21:27:27+00:00,http://arxiv.org/abs/2309.02591v1,"Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, Armen Aghajanyan","cs.LG, cs.CL, cs.CV",image2text,"We present CM3Leon (pronounced ""Chameleon""), a retrieval-augmented,
token-based, decoder-only multi-modal language model capable of generating and
infilling both text and images. CM3Leon uses the CM3 multi-modal architecture
but additionally shows the extreme benefits of scaling up and tuning on more
diverse instruction-style data. It is the first multi-modal model trained with
a recipe adapted from text-only language models, including a large-scale
retrieval-augmented pre-training stage and a second multi-task supervised
fine-tuning (SFT) stage. It is also a general-purpose model that can do both
text-to-image and image-to-text generation, allowing us to introduce
self-contained contrastive decoding methods that produce high-quality outputs.
Extensive experiments demonstrate that this recipe is highly effective for
multi-modal models. CM3Leon achieves state-of-the-art performance in
text-to-image generation with 5x less training compute than comparable methods
(zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon can also demonstrate
unprecedented levels of controllability in tasks ranging from language-guided
image editing to image-controlled generation and segmentation.",2023-09-05
"Breaking Barriers to Creative Expression: Co-Designing and Implementing
  an Accessible Text-to-Image Interface",2023-09-05 17:31:10+00:00,http://arxiv.org/abs/2309.02402v1,"Atieh Taheri, Mohammad Izadi, Gururaj Shriram, Negar Rostamzadeh, Shaun Kane","cs.HC, J.5; J.6; I.2.7",image2text,"Text-to-image generation models have grown in popularity due to their ability
to produce high-quality images from a text prompt. One use for this technology
is to enable the creation of more accessible art creation software. In this
paper, we document the development of an alternative user interface that
reduces the typing effort needed to enter image prompts by providing
suggestions from a large language model, developed through iterative design and
testing within the project team. The results of this testing demonstrate how
generative text models can support the accessibility of text-to-image models,
enabling users with a range of abilities to create visual art.",2023-09-05
PromptTTS 2: Describing and Generating Voices with Text Prompt,2023-09-05 14:45:27+00:00,http://arxiv.org/abs/2309.02285v1,"Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao, Tao Qin, Jiang Bian","eess.AS, cs.CL, cs.LG, cs.SD",image2text,"Speech conveys more information than just text, as the same word can be
uttered in various voices to convey diverse information. Compared to
traditional text-to-speech (TTS) methods relying on speech prompts (reference
speech) for voice variability, using text prompts (descriptions) is more
user-friendly since speech prompts can be hard to find or may not exist at all.
TTS approaches based on the text prompt face two challenges: 1) the one-to-many
problem, where not all details about voice variability can be described in the
text prompt, and 2) the limited availability of text prompt datasets, where
vendors and large cost of data labeling are required to write text prompt for
speech. In this work, we introduce PromptTTS 2 to address these challenges with
a variation network to provide variability information of voice not captured by
text prompts, and a prompt generation pipeline to utilize the large language
models (LLM) to compose high quality text prompts. Specifically, the variation
network predicts the representation extracted from the reference speech (which
contains full information about voice) based on the text prompt representation.
For the prompt generation pipeline, it generates text prompts for speech with a
speech understanding model to recognize voice attributes (e.g., gender, speed)
from speech and a large language model to formulate text prompt based on the
recognition results. Experiments on a large-scale (44K hours) speech dataset
demonstrate that compared to the previous works, PromptTTS 2 generates voices
more consistent with text prompts and supports the sampling of diverse voice
variability, thereby offering users more choices on voice generation.
Additionally, the prompt generation pipeline produces high-quality prompts,
eliminating the large labeling cost. The demo page of PromptTTS 2 is available
online\footnote{https://speechresearch.github.io/prompttts2}.",2023-09-05
"Towards Vision-Language Mechanistic Interpretability: A Causal Tracing
  Tool for BLIP",2023-08-27 18:46:47+00:00,http://arxiv.org/abs/2308.14179v1,"Vedant Palit, Rohan Pandey, Aryaman Arora, Paul Pu Liang","cs.CL, cs.AI, cs.CV",image2text,"Mechanistic interpretability seeks to understand the neural mechanisms that
enable specific behaviors in Large Language Models (LLMs) by leveraging
causality-based methods. While these approaches have identified neural circuits
that copy spans of text, capture factual knowledge, and more, they remain
unusable for multimodal models since adapting these tools to the
vision-language domain requires considerable architectural changes. In this
work, we adapt a unimodal causal tracing tool to BLIP to enable the study of
the neural mechanisms underlying image-conditioned text generation. We
demonstrate our approach on a visual question answering dataset, highlighting
the causal relevance of later layer representations for all tokens.
Furthermore, we release our BLIP causal tracing tool as open source to enable
further experimentation in vision-language mechanistic interpretability by the
community. Our code is available at
https://github.com/vedantpalit/Towards-Vision-Language-Mechanistic-Interpretability.",2023-08-27
"GeoExplainer: A Visual Analytics Framework for Spatial Modeling
  Contextualization and Report Generation",2023-08-25 16:55:33+00:00,http://arxiv.org/abs/2308.13588v1,"Fan Lei, Yuxin Ma, Stewart Fotheringham, Elizabeth Mack, Ziqi Li, Mehak Sachdeva, Sarah Bardin, Ross Maciejewski","cs.HC, cs.LG",image2text,"Geographic regression models of various descriptions are often applied to
identify patterns and anomalies in the determinants of spatially distributed
observations. These types of analyses focus on answering why questions about
underlying spatial phenomena, e.g., why is crime higher in this locale, why do
children in one school district outperform those in another, etc.? Answers to
these questions require explanations of the model structure, the choice of
parameters, and contextualization of the findings with respect to their
geographic context. This is particularly true for local forms of regression
models which are focused on the role of locational context in determining human
behavior. In this paper, we present GeoExplainer, a visual analytics framework
designed to support analysts in creating explanative documentation that
summarizes and contextualizes their spatial analyses. As analysts create their
spatial models, our framework flags potential issues with model parameter
selections, utilizes template-based text generation to summarize model outputs,
and links with external knowledge repositories to provide annotations that help
to explain the model results. As analysts explore the model results, all
visualizations and annotations can be captured in an interactive report
generation widget. We demonstrate our framework using a case study modeling the
determinants of voting in the 2016 US Presidential Election.",2023-08-25
Manipulating Embeddings of Stable Diffusion Prompts,2023-08-23 10:59:41+00:00,http://arxiv.org/abs/2308.12059v1,"Niklas Deckers, Julia Peters, Martin Potthast","cs.CV, cs.LG",image2text,"Generative text-to-image models such as Stable Diffusion allow users to
generate images based on a textual description, the prompt. Changing the prompt
is still the primary means for the user to change a generated image as desired.
However, changing the image by reformulating the prompt remains a difficult
process of trial and error, which has led to the emergence of prompt
engineering as a new field of research. We propose and analyze methods to
change the embedding of a prompt directly instead of the prompt text. It allows
for more fine-grained and targeted control that takes into account user
intentions. Our approach treats the generative text-to-image model as a
continuous function and passes gradients between the image space and the prompt
embedding space. By addressing different user interaction problems, we can
apply this idea in three scenarios: (1) Optimization of a metric defined in
image space that could measure, for example, image style. (2) Assistance of
users in creative tasks by enabling them to navigate the image space along a
selection of directions of ""near"" prompt embeddings. (3) Changing the embedding
of the prompt to include information that the user has seen in a particular
seed but finds difficult to describe in the prompt. Our experiments demonstrate
the feasibility of the described methods.",2023-08-23
CgT-GAN: CLIP-guided Text GAN for Image Captioning,2023-08-23 10:25:37+00:00,http://arxiv.org/abs/2308.12045v1,"Jiarui Yu, Haoran Li, Yanbin Hao, Bin Zhu, Tong Xu, Xiangnan He","cs.CV, cs.AI, cs.CL, cs.MM",image2text,"The large-scale visual-language pre-trained model, Contrastive Language-Image
Pre-training (CLIP), has significantly improved image captioning for scenarios
without human-annotated image-caption pairs. Recent advanced CLIP-based image
captioning without human annotations follows a text-only training paradigm,
i.e., reconstructing text from shared embedding space. Nevertheless, these
approaches are limited by the training/inference gap or huge storage
requirements for text embeddings. Given that it is trivial to obtain images in
the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates
images into the training process to enable the model to ""see"" real visual
modality. Particularly, we use adversarial training to teach CgT-GAN to mimic
the phrases of an external text corpus and CLIP-based reward to provide
semantic guidance. The caption generator is jointly rewarded based on the
caption naturalness to human language calculated from the GAN's discriminator
and the semantic guidance reward computed by the CLIP-based reward module. In
addition to the cosine similarity as the semantic guidance reward (i.e.,
CLIP-cos), we further introduce a novel semantic guidance reward called
CLIP-agg, which aligns the generated caption with a weighted text embedding by
attentively aggregating the entire corpus. Experimental results on three
subtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms
state-of-the-art methods significantly across all metrics. Code is available at
https://github.com/Lihr747/CgtGAN.",2023-08-23
"Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal
  Embeddings",2023-08-22 21:57:22+00:00,http://arxiv.org/abs/2308.11804v1,"Eugene Bagdasaryan, Vitaly Shmatikov","cs.CR, cs.AI, cs.LG",image2text,"Multi-modal encoders map images, sounds, texts, videos, etc. into a single
embedding space, aligning representations across modalities (e.g., associate an
image of a dog with a barking sound). We show that multi-modal embeddings can
be vulnerable to an attack we call ""adversarial illusions."" Given an input in
any modality, an adversary can perturb it so as to make its embedding close to
that of an arbitrary, adversary-chosen input in another modality. Illusions
thus enable the adversary to align any image with any text, any text with any
sound, etc.
  Adversarial illusions exploit proximity in the embedding space and are thus
agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how
adversarially aligned inputs, generated without knowledge of specific
downstream tasks, mislead image generation, text generation, and zero-shot
classification.",2023-08-22
"Music Understanding LLaMA: Advancing Text-to-Music Generation with
  Question Answering and Captioning",2023-08-22 08:43:33+00:00,http://arxiv.org/abs/2308.11276v1,"Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, Ying Shan","cs.SD, cs.AI, cs.CL, cs.MM, eess.AS",image2text,"Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity
of large-scale publicly available music datasets with natural language
captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA),
capable of answering music-related questions and generating captions for music
files. Our model utilizes audio representations from a pretrained MERT model to
extract music features. However, obtaining a suitable dataset for training the
MU-LLaMA model remains challenging, as existing publicly accessible audio
question answering datasets lack the necessary depth for open-ended music
question answering. To fill this gap, we present a methodology for generating
question-answer pairs from existing audio captioning datasets and introduce the
MusicQA Dataset designed for answering open-ended music-related questions. The
experiments demonstrate that the proposed MU-LLaMA model, trained on our
designed MusicQA dataset, achieves outstanding performance in both music
question answering and music caption generation across various metrics,
outperforming current state-of-the-art (SOTA) models in both fields and
offering a promising advancement in the T2M-Gen research field.",2023-08-22
Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection,2023-08-22 01:55:03+00:00,http://arxiv.org/abs/2308.11119v1,Masato Tamura,"cs.CV, cs.LG",image2text,"This paper presents a novel method that leverages a visual-language model,
CLIP, as a data source for zero-shot anomaly detection. Tremendous efforts have
been put towards developing anomaly detectors due to their potential industrial
applications. Considering the difficulty in acquiring various anomalous samples
for training, most existing methods train models with only normal samples and
measure discrepancies from the distribution of normal samples during inference,
which requires training a model for each object category. The problem of this
inefficient training requirement has been tackled by designing a CLIP-based
anomaly detector that applies prompt-guided classification to each part of an
image in a sliding window manner. However, the method still suffers from the
labor of careful prompt ensembling with known object categories. To overcome
the issues above, we propose leveraging CLIP as a data source for training. Our
method generates text embeddings with the text encoder in CLIP with typical
prompts that include words of normal and anomaly. In addition to these words,
we insert several randomly generated words into prompts, which enables the
encoder to generate a diverse set of normal and anomalous samples. Using the
generated embeddings as training data, a feed-forward neural network learns to
extract features of normal and anomaly from CLIP's embeddings, and as a result,
a category-agnostic anomaly detector can be obtained without any training
images. Experimental results demonstrate that our method achieves
state-of-the-art performance without laborious prompt ensembling in zero-shot
setups.",2023-08-22
"VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity
  Control",2023-08-18 20:18:30+00:00,http://arxiv.org/abs/2308.09804v1,"Zi-Yuan Hu, Yanyang Li, Michael R. Lyu, Liwei Wang","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"As the model size of pre-trained language models (PLMs) grows rapidly, full
fine-tuning becomes prohibitively expensive for model training and storage. In
vision-and-language (VL), parameter-efficient tuning (PET) techniques are
proposed to integrate modular modifications (e.g., Adapter and LoRA) into
encoder-decoder PLMs. By tuning a small set of trainable parameters, these
techniques perform on par with full fine-tuning. However, excessive modular
modifications and neglecting the functionality gap between the encoders and
decoders can lead to performance degradation, while existing PET techniques
(e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a
Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose
effective control over modular modifications via a novel granularity-controlled
mechanism. Considering different granularity-controlled matrices generated by
this mechanism, a variety of model-agnostic VL-PET modules can be instantiated
from our framework for better efficiency and effectiveness trade-offs. We
further propose lightweight PET module designs to enhance VL alignment and
modeling for the encoders and maintain text generation for the decoders.
Extensive experiments conducted on four image-text tasks and four video-text
tasks demonstrate the efficiency, effectiveness and transferability of our
VL-PET framework. In particular, our VL-PET-large with lightweight PET module
designs significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37%
(7.03%) with BART-base (T5-base) on image-text tasks. Furthermore, we validate
the enhanced effect of employing our VL-PET designs on existing PET techniques,
enabling them to achieve significant performance improvements. Our code is
available at https://github.com/HenryHZY/VL-PET.",2023-08-18
Can Knowledge Graphs Simplify Text?,2023-08-14 07:20:49+00:00,http://arxiv.org/abs/2308.06975v2,"Anthony Colas, Haodi Ma, Xuanli He, Yang Bai, Daisy Zhe Wang",cs.CL,image2text,"Knowledge Graph (KG)-to-Text Generation has seen recent improvements in
generating fluent and informative sentences which describe a given KG. As KGs
are widespread across multiple domains and contain important entity-relation
information, and as text simplification aims to reduce the complexity of a text
while preserving the meaning of the original text, we propose KGSimple, a novel
approach to unsupervised text simplification which infuses KG-established
techniques in order to construct a simplified KG path and generate a concise
text which preserves the original input's meaning. Through an iterative and
sampling KG-first approach, our model is capable of simplifying text when
starting from a KG by learning to keep important information while harnessing
KG-to-text generation to output fluent and descriptive sentences. We evaluate
various settings of the KGSimple model on currently-available KG-to-text
datasets, demonstrating its effectiveness compared to unsupervised text
simplification models which start with a given complex text. Our code is
available on GitHub.",2023-08-14
Mirror Diffusion Models,2023-08-11 18:31:54+00:00,http://arxiv.org/abs/2308.06342v2,Jaesung Tae,cs.LG,image2text,"Diffusion models have successfully been applied to generative tasks in
various continuous domains. However, applying diffusion to discrete categorical
data remains a non-trivial task. Moreover, generation in continuous domains
often requires clipping in practice, which motivates the need for a theoretical
framework for adapting diffusion to constrained domains. Inspired by the mirror
Langevin algorithm for the constrained sampling problem, in this theoretical
report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the
context of simplex diffusion and propose natural extensions to popular domains
such as image and text generation.",2023-08-11
Generative Forests,2023-08-07 14:58:53+00:00,http://arxiv.org/abs/2308.03648v1,"Richard Nock, Mathieu Guillame-Bert","cs.LG, I.2.6",image2text,"Tabular data represents one of the most prevalent form of data. When it comes
to data generation, many approaches would learn a density for the data
generation process, but would not necessarily end up with a sampler, even less
so being exact with respect to the underlying density. A second issue is on
models: while complex modeling based on neural nets thrives in image or text
generation (etc.), less is known for powerful generative models on tabular
data. A third problem is the visible chasm on tabular data between training
algorithms for supervised learning with remarkable properties (e.g. boosting),
and a comparative lack of guarantees when it comes to data generation. In this
paper, we tackle the three problems, introducing new tree-based generative
models convenient for density modeling and tabular data generation that improve
on modeling capabilities of recent proposals, and a training algorithm which
simplifies the training setting of previous approaches and displays
boosting-compliant convergence. This algorithm has the convenient property to
rely on a supervised training scheme that can be implemented by a few tweaks to
the most popular induction scheme for decision tree induction with two classes.
Experiments are provided on missing data imputation and comparing generated
data to real data, displaying the quality of the results obtained by our
approach, in particular against state of the art.",2023-08-07
FAST: Font-Agnostic Scene Text Editing,2023-08-05 15:54:06+00:00,http://arxiv.org/abs/2308.02905v1,"Alloy Das, Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal, Michael Blumenstein","cs.CV, cs.MM",image2text,"Scene Text Editing (STE) is a challenging research problem, and it aims to
modify existing texts in an image while preserving the background and the font
style of the original text of the image. Due to its various real-life
applications, researchers have explored several approaches toward STE in recent
years. However, most of the existing STE methods show inferior editing
performance because of (1) complex image backgrounds, (2) various font styles,
and (3) varying word lengths within the text. To address such inferior editing
performance issues, in this paper, we propose a novel font-agnostic scene text
editing framework, named FAST, for simultaneously generating text in arbitrary
styles and locations while preserving a natural and realistic appearance
through combined mask generation and style transfer. The proposed approach
differs from the existing methods as they directly modify all image pixels.
Instead, the proposed method has introduced a filtering mechanism to remove
background distractions, allowing the network to focus solely on the text
regions where editing is required. Additionally, a text-style transfer module
has been designed to mitigate the challenges posed by varying word lengths.
Extensive experiments and ablations have been conducted, and the results
demonstrate that the proposed method outperforms the existing methods both
qualitatively and quantitatively.",2023-08-05
Guiding Image Captioning Models Toward More Specific Captions,2023-07-31 14:00:12+00:00,http://arxiv.org/abs/2307.16686v1,"Simon Kornblith, Lala Li, Zirui Wang, Thao Nguyen","cs.CV, cs.LG",image2text,"Image captioning is conventionally formulated as the task of generating
captions for images that match the distribution of reference image-caption
pairs. However, reference captions in standard captioning datasets are short
and may not uniquely identify the images they describe. These problems are
further exacerbated when models are trained directly on image-alt text pairs
collected from the internet. In this work, we show that it is possible to
generate more specific captions with minimal changes to the training process.
We implement classifier-free guidance for an autoregressive captioning model by
fine-tuning it to estimate both conditional and unconditional distributions
over captions. The guidance scale applied at decoding controls a trade-off
between maximizing $p(\mathrm{caption}|\mathrm{image})$ and
$p(\mathrm{image}|\mathrm{caption})$. Compared to standard greedy decoding,
decoding with a guidance scale of 2 substantially improves reference-free
metrics such as CLIPScore (0.808 vs. 0.775) and caption$\to$image retrieval
performance in the CLIP embedding space (recall@1 44.6% vs. 26.5%), but worsens
standard reference-based captioning metrics (e.g., CIDEr 78.6 vs 126.1). We
further explore the use of language models to guide the decoding process,
obtaining small improvements over the Pareto frontier of reference-free vs.
reference-based captioning metrics that arises from classifier-free guidance,
and substantially improving the quality of captions generated from a model
trained only on minimally curated web data.",2023-07-31
"Transferable Decoding with Visual Entities for Zero-Shot Image
  Captioning",2023-07-31 09:47:06+00:00,http://arxiv.org/abs/2307.16525v1,"Junjie Fei, Teng Wang, Jinrui Zhang, Zhenyu He, Chengjie Wang, Feng Zheng","cs.CV, cs.CL",image2text,"Image-to-text generation aims to describe images using natural language.
Recently, zero-shot image captioning based on pre-trained vision-language
models (VLMs) and large language models (LLMs) has made significant progress.
However, we have observed and empirically demonstrated that these methods are
susceptible to modality bias induced by LLMs and tend to generate descriptions
containing objects (entities) that do not actually exist in the image but
frequently appear during training (i.e., object hallucination). In this paper,
we propose ViECap, a transferable decoding model that leverages entity-aware
decoding to generate descriptions in both seen and unseen scenarios. ViECap
incorporates entity-aware hard prompts to guide LLMs' attention toward the
visual entities present in the image, enabling coherent caption generation
across diverse scenes. With entity-aware hard prompts, ViECap is capable of
maintaining performance when transferring from in-domain to out-of-domain
scenarios. Extensive experiments demonstrate that ViECap sets a new
state-of-the-art cross-domain (transferable) captioning and performs
competitively in-domain captioning compared to previous VLMs-based zero-shot
methods. Our code is available at: https://github.com/FeiElysia/ViECap",2023-07-31
"Visual Captioning at Will: Describing Images and Videos Guided by a Few
  Stylized Sentences",2023-07-31 04:26:01+00:00,http://arxiv.org/abs/2307.16399v1,"Dingyi Yang, Hongyu Chen, Xinglin Hou, Tiezheng Ge, Yuning Jiang, Qin Jin","cs.MM, cs.CV",image2text,"Stylized visual captioning aims to generate image or video descriptions with
specific styles, making them more attractive and emotionally appropriate. One
major challenge with this task is the lack of paired stylized captions for
visual content, so most existing works focus on unsupervised methods that do
not rely on parallel datasets. However, these approaches still require training
with sufficient examples that have style labels, and the generated captions are
limited to predefined styles. To address these limitations, we explore the
problem of Few-Shot Stylized Visual Captioning, which aims to generate captions
in any desired style, using only a few examples as guidance during inference,
without requiring further training. We propose a framework called FS-StyleCap
for this task, which utilizes a conditional encoder-decoder language model and
a visual projection module. Our two-step training scheme proceeds as follows:
first, we train a style extractor to generate style representations on an
unlabeled text-only corpus. Then, we freeze the extractor and enable our
decoder to generate stylized descriptions based on the extracted style vector
and projected visual content vectors. During inference, our model can generate
desired stylized captions by deriving the style representation from
user-supplied examples. Our automatic evaluation results for few-shot
sentimental visual captioning outperform state-of-the-art approaches and are
comparable to models that are fully trained on labeled style corpora. Human
evaluations further confirm our model s ability to handle multiple styles.",2023-07-31
"Learning Multi-modal Representations by Watching Hundreds of Surgical
  Video Lectures",2023-07-27 22:38:12+00:00,http://arxiv.org/abs/2307.15220v1,"Kun Yuan, Vinkle Srivastav, Tong Yu, Joel Lavanchy, Pietro Mascagni, Nassir Navab, Nicolas Padoy","cs.CV, cs.AI",image2text,"Recent advancements in surgical computer vision applications have been driven
by fully-supervised methods, primarily using only visual data. These methods
rely on manually annotated surgical videos to predict a fixed set of object
categories, limiting their generalizability to unseen surgical procedures and
downstream tasks. In this work, we put forward the idea that the surgical video
lectures available through open surgical e-learning platforms can provide
effective supervisory signals for multi-modal representation learning without
relying on manual annotations. We address the surgery-specific linguistic
challenges present in surgical video lectures by employing multiple
complementary automatic speech recognition systems to generate text
transcriptions. We then present a novel method, SurgVLP - Surgical Vision
Language Pre-training, for multi-modal representation learning. SurgVLP
constructs a new contrastive learning objective to align video clip embeddings
with the corresponding multiple text embeddings by bringing them together
within a joint latent space. To effectively show the representation capability
of the learned joint latent space, we introduce several vision-and-language
tasks for surgery, such as text-based video retrieval, temporal activity
grounding, and video captioning, as benchmarks for evaluation. We further
demonstrate that without using any labeled ground truth, our approach can be
employed for traditional vision-only surgical downstream tasks, such as
surgical tool, phase, and triplet recognition. The code will be made available
at https://github.com/CAMMA-public/SurgVLP",2023-07-27
"A Transformer-based Approach for Arabic Offline Handwritten Text
  Recognition",2023-07-27 17:51:52+00:00,http://arxiv.org/abs/2307.15045v1,"Saleh Momeni, Bagher BabaAli","cs.CV, cs.LG",image2text,"Handwriting recognition is a challenging and critical problem in the fields
of pattern recognition and machine learning, with applications spanning a wide
range of domains. In this paper, we focus on the specific issue of recognizing
offline Arabic handwritten text. Existing approaches typically utilize a
combination of convolutional neural networks for image feature extraction and
recurrent neural networks for temporal modeling, with connectionist temporal
classification used for text generation. However, these methods suffer from a
lack of parallelization due to the sequential nature of recurrent neural
networks. Furthermore, these models cannot account for linguistic rules,
necessitating the use of an external language model in the post-processing
stage to boost accuracy. To overcome these issues, we introduce two alternative
architectures, namely the Transformer Transducer and the standard
sequence-to-sequence Transformer, and compare their performance in terms of
accuracy and speed. Our approach can model language dependencies and relies
only on the attention mechanism, thereby making it more parallelizable and less
complex. We employ pre-trained Transformers for both image understanding and
language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that
our proposed method outperforms the current state-of-the-art approaches for
recognizing offline Arabic handwritten text.",2023-07-27
Evaluating Generative Models for Graph-to-Text Generation,2023-07-27 09:03:05+00:00,http://arxiv.org/abs/2307.14712v1,"Shuzhou Yuan, Michael Färber","cs.CL, cs.AI",image2text,"Large language models (LLMs) have been widely employed for graph-to-text
generation tasks. However, the process of finetuning LLMs requires significant
training resources and annotation work. In this paper, we explore the
capability of generative models to generate descriptive text from graph data in
a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two
graph-to-text datasets and compare their performance with that of finetuned LLM
models such as T5 and BART. Our results demonstrate that generative models are
capable of generating fluent and coherent text, achieving BLEU scores of 10.57
and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error
analysis reveals that generative models still struggle with understanding the
semantic relations between entities, and they also tend to generate text with
hallucinations or irrelevant information. As a part of error analysis, we
utilize BERT to detect machine-generated text and achieve high macro-F1 scores.
We have made the text generated by generative models publicly available.",2023-07-27
XDLM: Cross-lingual Diffusion Language Model for Machine Translation,2023-07-25 15:08:34+00:00,http://arxiv.org/abs/2307.13560v2,"Linyao Chen, Aosong Feng, Boming Yang, Zihui Li",cs.CL,image2text,"Recently, diffusion models have excelled in image generation tasks and have
also been applied to neural language processing (NLP) for controllable text
generation. However, the application of diffusion models in a cross-lingual
setting is less unexplored. Additionally, while pretraining with diffusion
models has been studied within a single language, the potential of
cross-lingual pretraining remains understudied. To address these gaps, we
propose XDLM, a novel Cross-lingual diffusion model for machine translation,
consisting of pretraining and fine-tuning stages. In the pretraining stage, we
propose TLDM, a new training objective for mastering the mapping between
different languages; in the fine-tuning stage, we build up the translation
system based on the pretrained model. We evaluate the result on several machine
translation benchmarks and outperformed both diffusion and Transformer
baselines.",2023-07-25
Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts,2023-07-21 15:49:59+00:00,http://arxiv.org/abs/2307.11661v1,"Mayug Maniparambil, Chris Vorster, Derek Molloy, Noel Murphy, Kevin McGuinness, Noel E. O'Connor","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have
revolutionized visual representation learning by providing good performance on
downstream datasets. VLMs are 0-shot adapted to a downstream dataset by
designing prompts that are relevant to the dataset. Such prompt engineering
makes use of domain expertise and a validation dataset. Meanwhile, recent
developments in generative pretrained models like GPT-4 mean they can be used
as advanced internet search tools. They can also be manipulated to provide
visual information in any structure. In this work, we show that GPT-4 can be
used to generate text that is visually descriptive and how this can be used to
adapt CLIP to downstream tasks. We show considerable improvements in 0-shot
transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD
(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.
We also design a simple few-shot adapter that learns to choose the best
possible sentences to construct generalizable classifiers that outperform the
recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized
fine-grained datasets. We will release the code, prompts, and auxiliary text
dataset upon acceptance.",2023-07-21
OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?,2023-07-21 14:58:44+00:00,http://arxiv.org/abs/2307.11636v1,"Runjia Li, Shuyang Sun, Mohamed Elhoseiny, Philip Torr","cs.CV, cs.CL",image2text,"This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scale
dataset for humour generation and understanding. Humour is an abstract,
subjective, and context-dependent cognitive construct involving several
cognitive factors, making it a challenging task to generate and interpret.
Hence, humour generation and understanding can serve as a new task for
evaluating the ability of deep-learning methods to process abstract and
subjective information. Due to the scarcity of data, humour-related generation
tasks such as captioning remain under-explored. To address this gap,
OxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores to
train a generalizable humour captioning model. Contrary to existing captioning
datasets, OxfordTVG-HIC features a wide range of emotional and semantic
diversity resulting in out-of-context examples that are particularly conducive
to generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive
content. We also show how OxfordTVG-HIC can be leveraged for evaluating the
humour of a generated text. Through explainability analysis of the trained
models, we identify the visual and linguistic cues influential for evoking
humour prediction (and generation). We observe qualitatively that these cues
are aligned with the benign violation theory of humour in cognitive psychology.",2023-07-21
"Generating Image-Specific Text Improves Fine-grained Image
  Classification",2023-07-21 02:47:18+00:00,http://arxiv.org/abs/2307.11315v1,"Emily Mu, Kathleen M. Lewis, Adrian V. Dalca, John Guttag","cs.CV, cs.CL",image2text,"Recent vision-language models outperform vision-only models on many image
classification tasks. However, because of the absence of paired text/image
descriptions, it remains difficult to fine-tune these models for fine-grained
image classification. In this work, we propose a method, GIST, for generating
image-specific fine-grained text descriptions from image-only datasets, and
show that these text descriptions can be used to improve classification. Key
parts of our method include 1. prompting a pretrained large language model with
domain-specific prompts to generate diverse fine-grained text descriptions for
each class and 2. using a pretrained vision-language model to match each image
to label-preserving text descriptions that capture relevant visual features in
the image. We demonstrate the utility of GIST by fine-tuning vision-language
models on the image-and-generated-text pairs to learn an aligned
vision-language representation space for improved classification. We evaluate
our learned representation space in full-shot and few-shot scenarios across
four diverse fine-grained classification datasets, each from a different
domain. Our method achieves an average improvement of $4.1\%$ in accuracy over
CLIP linear probes and an average of $1.1\%$ improvement in accuracy over the
previous state-of-the-art image-text classification method on the full-shot
datasets. Our method achieves similar improvements across few-shot regimes.
Code is available at https://github.com/emu1729/GIST.",2023-07-21
"FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with
  Human Feedback",2023-07-20 13:40:22+00:00,http://arxiv.org/abs/2307.10867v1,"Ashish Singh, Prateek Agarwal, Zixuan Huang, Arpita Singh, Tong Yu, Sungchul Kim, Victor Bursztyn, Nikos Vlassis, Ryan A. Rossi","cs.CL, cs.CV, cs.LG",image2text,"Captions are crucial for understanding scientific visualizations and
documents. Existing captioning methods for scientific figures rely on
figure-caption pairs extracted from documents for training, many of which fall
short with respect to metrics like helpfulness, explainability, and
visual-descriptiveness [15] leading to generated captions being misaligned with
reader preferences. To enable the generation of high-quality figure captions,
we introduce FigCaps-HF a new framework for figure-caption generation that can
incorporate domain expert feedback in generating captions optimized for reader
preferences. Our framework comprises of 1) an automatic method for evaluating
quality of figure-caption pairs, 2) a novel reinforcement learning with human
feedback (RLHF) method to optimize a generative figure-to-caption model for
reader preferences. We demonstrate the effectiveness of our simple learning
framework by improving performance over standard fine-tuning across different
types of models. In particular, when using BLIP as the base model, our RLHF
framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and
Meteor, respectively. Finally, we release a large-scale benchmark dataset with
human feedback on figure-caption pairs to enable further evaluation and
development of RLHF techniques for this problem.",2023-07-20
Improving Multimodal Datasets with Image Captioning,2023-07-19 17:47:12+00:00,http://arxiv.org/abs/2307.10350v1,"Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, Ludwig Schmidt","cs.LG, cs.CV",image2text,"Massive web datasets play a key role in the success of large vision-language
models like CLIP and Flamingo. However, the raw web data is noisy, and existing
filtering methods to reduce noise often come at the expense of data diversity.
Our work focuses on caption quality as one major source of noise, and studies
how generated captions can increase the utility of web-scraped datapoints with
nondescript text. Through exploring different mixing strategies for raw and
generated captions, we outperform the best filtering method proposed by the
DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a
candidate pool of 128M image-text pairs. Our best approach is also 2x better at
Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an
effective source of text supervision. In experimenting with different image
captioning models, we also demonstrate that the performance of a model on
standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable
indicator of the utility of the captions it generates for multimodal training.
Finally, our experiments with using generated captions at DataComp's large
scale (1.28B image-text pairs) offer insights into the limitations of synthetic
text, as well as the importance of image curation with increasing training data
quantity.",2023-07-19
"PromptMagician: Interactive Prompt Engineering for Text-to-Image
  Creation",2023-07-18 07:46:25+00:00,http://arxiv.org/abs/2307.09036v1,"Yingchaojie Feng, Xingbo Wang, Kam Kwai Wong, Sijia Wang, Yuhong Lu, Minfeng Zhu, Baicheng Wang, Wei Chen","cs.AI, cs.HC",image2text,"Generative text-to-image models have gained great popularity among the public
for their powerful capability to generate high-quality images based on natural
language prompts. However, developing effective prompts for desired images can
be challenging due to the complexity and ambiguity of natural language. This
research proposes PromptMagician, a visual analysis system that helps users
explore the image results and refine the input prompts. The backbone of our
system is a prompt recommendation model that takes user prompts as input,
retrieves similar prompt-image pairs from DiffusionDB, and identifies special
(important and relevant) prompt keywords. To facilitate interactive prompt
refinement, PromptMagician introduces a multi-level visualization for the
cross-modal embedding of the retrieved images and recommended keywords, and
supports users in specifying multiple criteria for personalized exploration.
Two usage scenarios, a user study, and expert interviews demonstrate the
effectiveness and usability of our system, suggesting it facilitates prompt
engineering and improves the creativity support of the generative text-to-image
model.",2023-07-18
Reading Radiology Imaging Like The Radiologist,2023-07-12 05:36:47+00:00,http://arxiv.org/abs/2307.05921v3,Yuhao Wang,"cs.CV, cs.AI",image2text,"Automated radiology report generation aims to generate radiology reports that
contain rich, fine-grained descriptions of radiology imaging. Compared with
image captioning in the natural image domain, medical images are very similar
to each other, with only minor differences in the occurrence of diseases. Given
the importance of these minor differences in the radiology report, it is
crucial to encourage the model to focus more on the subtle regions of disease
occurrence. Secondly, the problem of visual and textual data biases is serious.
Not only do normal cases make up the majority of the dataset, but sentences
describing areas with pathological changes also constitute only a small part of
the paragraph. Lastly, generating medical image reports involves the challenge
of long text generation, which requires more expertise and empirical training
in medical knowledge. As a result, the difficulty of generating such reports is
increased. To address these challenges, we propose a disease-oriented retrieval
framework that utilizes similar reports as prior knowledge references. We
design a factual consistency captioning generator to generate more accurate and
factually consistent disease descriptions. Our framework can find most similar
reports for a given disease from the CXR database by retrieving a
disease-oriented mask consisting of the position and morphological
characteristics. By referencing the disease-oriented similar report and the
visual features, the factual consistency model can generate a more accurate
radiology report.",2023-07-12
"Empirical Analysis of a Segmentation Foundation Model in Prostate
  Imaging",2023-07-06 20:00:52+00:00,http://arxiv.org/abs/2307.03266v1,"Heejong Kim, Victor Ion Butoi, Adrian V. Dalca, Mert R. Sabuncu","eess.IV, cs.CV, cs.LG",image2text,"Most state-of-the-art techniques for medical image segmentation rely on
deep-learning models. These models, however, are often trained on
narrowly-defined tasks in a supervised fashion, which requires expensive
labeled datasets. Recent advances in several machine learning domains, such as
natural language generation have demonstrated the feasibility and utility of
building foundation models that can be customized for various downstream tasks
with little to no labeled data. This likely represents a paradigm shift for
medical imaging, where we expect that foundation models may shape the future of
the field. In this paper, we consider a recently developed foundation model for
medical image segmentation, UniverSeg. We conduct an empirical evaluation study
in the context of prostate imaging and compare it against the conventional
approach of training a task-specific segmentation model. Our results and
discussion highlight several important factors that will likely be important in
the development and adoption of foundation models for medical image
segmentation.",2023-07-06
Vision Language Transformers: A Survey,2023-07-06 19:08:56+00:00,http://arxiv.org/abs/2307.03254v1,"Clayton Fields, Casey Kennington","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Vision language tasks, such as answering questions about or generating
captions that describe an image, are difficult tasks for computers to perform.
A relatively recent body of research has adapted the pretrained transformer
architecture introduced in \citet{vaswani2017attention} to vision language
modeling. Transformer models have greatly improved performance and versatility
over previous vision language models. They do so by pretraining models on a
large generic datasets and transferring their learning to new tasks with minor
changes in architecture and parameter values. This type of transfer learning
has become the standard modeling practice in both natural language processing
and computer vision. Vision language transformers offer the promise of
producing similar advancements in tasks which require both vision and language.
In this paper, we provide a broad synthesis of the currently available research
on vision language transformer models and offer some analysis of their
strengths, limitations and some open questions that remain.",2023-07-06
Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment,2023-07-05 23:01:26+00:00,http://arxiv.org/abs/2307.02682v2,"Yongrae Jo, Seongyun Lee, Aiden SJ Lee, Hyunji Lee, Hanseok Oh, Minjoon Seo","cs.CV, cs.CL",image2text,"Dense video captioning, a task of localizing meaningful moments and
generating relevant captions for videos, often requires a large, expensive
corpus of annotated video segments paired with text. In an effort to minimize
the annotation cost, we propose ZeroTA, a novel method for dense video
captioning in a zero-shot manner. Our method does not require any videos or
annotations for training; instead, it localizes and describes events within
each input video at test time by optimizing solely on the input. This is
accomplished by introducing a soft moment mask that represents a temporal
segment in the video and jointly optimizing it with the prefix parameters of a
language model. This joint optimization aligns a frozen language generation
model (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,
CLIP) by maximizing the matching score between the generated text and a moment
within the video. We also introduce a pairwise temporal IoU loss to let a set
of soft moment masks capture multiple distinct events within the video. Our
method effectively discovers diverse significant events within the video, with
the resulting captions appropriately describing these events. The empirical
results demonstrate that ZeroTA surpasses zero-shot baselines and even
outperforms the state-of-the-art few-shot method on the widely-used benchmark
ActivityNet Captions. Moreover, our method shows greater robustness compared to
supervised methods when evaluated in out-of-domain scenarios. This research
provides insight into the potential of aligning widely-used models, such as
language generation models and vision-language models, to unlock a new
capability: understanding temporal aspects of videos.",2023-07-05
"A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image
  Diagnosis",2023-07-05 01:45:19+00:00,http://arxiv.org/abs/2307.01981v1,"Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Xiaotang Gai, Yang Feng, Zuozhu Liu","eess.IV, cs.CV, cs.LG",image2text,"Zero-shot medical image classification is a critical process in real-world
scenarios where we have limited access to all possible diseases or large-scale
annotated data. It involves computing similarity scores between a query medical
image and possible disease categories to determine the diagnostic result.
Recent advances in pretrained vision-language models (VLMs) such as CLIP have
shown great performance for zero-shot natural image recognition and exhibit
benefits in medical applications. However, an explainable zero-shot medical
image recognition framework with promising performance is yet under
development. In this paper, we propose a novel CLIP-based zero-shot medical
image classification framework supplemented with ChatGPT for explainable
diagnosis, mimicking the diagnostic process performed by human experts. The key
idea is to query large language models (LLMs) with category names to
automatically generate additional cues and knowledge, such as disease symptoms
or descriptions other than a single category name, to help provide more
accurate and explainable diagnosis in CLIP. We further design specific prompts
to enhance the quality of generated texts by ChatGPT that describe visual
medical features. Extensive results on one private dataset and four public
datasets along with detailed analysis demonstrate the effectiveness and
explainability of our training-free zero-shot diagnosis pipeline, corroborating
the great potential of VLMs and LLMs for medical applications.",2023-07-05
"More for Less: Compact Convolutional Transformers Enable Robust Medical
  Image Classification with Limited Data",2023-07-01 03:31:30+00:00,http://arxiv.org/abs/2307.00213v1,Andrew Kean Gao,"cs.CV, cs.LG, I.4.9, I.2.10",image2text,"Transformers are very powerful tools for a variety of tasks across domains,
from text generation to image captioning. However, transformers require
substantial amounts of training data, which is often a challenge in biomedical
settings, where high quality labeled data can be challenging or expensive to
obtain. This study investigates the efficacy of Compact Convolutional
Transformers (CCT) for robust medical image classification with limited data,
addressing a key issue faced by conventional Vision Transformers - their
requirement for large datasets. A hybrid of transformers and convolutional
layers, CCTs demonstrate high accuracy on modestly sized datasets. We employed
a benchmark dataset of peripheral blood cell images of eight distinct cell
types, each represented by approximately 2,000 low-resolution (28x28x3 pixel)
samples. Despite the dataset size being smaller than those typically used with
Vision Transformers, we achieved a commendable classification accuracy of
92.49% and a micro-average ROC AUC of 0.9935. The CCT also learned quickly,
exceeding 80% validation accuracy after five epochs. Analysis of per-class
precision, recall, F1, and ROC showed that performance was strong across cell
types. Our findings underscore the robustness of CCTs, indicating their
potential as a solution to data scarcity issues prevalent in biomedical
imaging. We substantiate the applicability of CCTs in data-constrained areas
and encourage further work on CCTs.",2023-07-01
Concept-Oriented Deep Learning with Large Language Models,2023-06-29 16:47:11+00:00,http://arxiv.org/abs/2306.17089v1,Daniel T. Chang,"cs.LG, cs.CL",image2text,"Large Language Models (LLMs) have been successfully used in many
natural-language tasks and applications including text generation and AI
chatbots. They also are a promising new technology for concept-oriented deep
learning (CODL). However, the prerequisite is that LLMs understand concepts and
ensure conceptual consistency. We discuss these in this paper, as well as major
uses of LLMs for CODL including concept extraction from text, concept graph
extraction from text, and concept learning. Human knowledge consists of both
symbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only
LLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal
LLMs, on the other hand, are capable of representing the full range (conceptual
and sensory) of human knowledge. We discuss conceptual understanding in
visual-language LLMs, the most important multimodal LLMs, and major uses of
them for CODL including concept extraction from image, concept graph extraction
from image, and concept learning. While uses of LLMs for CODL are valuable
standalone, they are particularly valuable as part of LLM applications such as
AI chatbots.",2023-06-29
Joint Level Generation and Translation Using Gameplay Videos,2023-06-29 03:46:44+00:00,http://arxiv.org/abs/2306.16662v1,"Negar Mirgati, Matthew Guzdial","cs.CV, cs.LG",image2text,"Procedural Content Generation via Machine Learning (PCGML) faces a
significant hurdle that sets it apart from other fields, such as image or text
generation, which is limited annotated data. Many existing methods for
procedural level generation via machine learning require a secondary
representation besides level images. However, the current methods for obtaining
such representations are laborious and time-consuming, which contributes to
this problem. In this work, we aim to address this problem by utilizing
gameplay videos of two human-annotated games to develop a novel multi-tail
framework that learns to perform simultaneous level translation and generation.
The translation tail of our framework can convert gameplay video frames to an
equivalent secondary representation, while its generation tail can produce
novel level segments. Evaluation results and comparisons between our framework
and baselines suggest that combining the level generation and translation tasks
can lead to an overall improved performance regarding both tasks. This
represents a possible solution to limited annotated level data, and we
demonstrate the potential for future versions to generalize to unseen games.",2023-06-29
"ZeroGen: Zero-shot Multimodal Controllable Text Generation with Multiple
  Oracles",2023-06-29 03:22:43+00:00,http://arxiv.org/abs/2306.16649v1,"Haoqin Tu, Bowen Yang, Xianfeng Zhao",cs.CL,image2text,"Automatically generating textual content with desired attributes is an
ambitious task that people have pursued long. Existing works have made a series
of progress in incorporating unimodal controls into language models (LMs),
whereas how to generate controllable sentences with multimodal signals and high
efficiency remains an open question. To tackle the puzzle, we propose a new
paradigm of zero-shot controllable text generation with multimodal signals
(\textsc{ZeroGen}). Specifically, \textsc{ZeroGen} leverages controls of text
and image successively from token-level to sentence-level and maps them into a
unified probability space at decoding, which customizes the LM outputs by
weighted addition without extra training. To achieve better inter-modal
trade-offs, we further introduce an effective dynamic weighting mechanism to
regulate all control weights. Moreover, we conduct substantial experiments to
probe the relationship of being in-depth or in-width between signals from
distinct modalities. Encouraging empirical results on three downstream tasks
show that \textsc{ZeroGen} not only outperforms its counterparts on captioning
tasks by a large margin but also shows great potential in multimodal news
generation with a higher degree of control. Our code will be released at
https://github.com/ImKeTT/ZeroGen.",2023-06-29
"You Can Generate It Again: Data-to-text Generation with Verification and
  Correction Prompting",2023-06-28 05:34:25+00:00,http://arxiv.org/abs/2306.15933v1,"Xuan Ren, Lingqiao Liu","cs.CL, cs.AI, cs.LG",image2text,"Despite significant advancements in existing models, generating text
descriptions from structured data input, known as data-to-text generation,
remains a challenging task. In this paper, we propose a novel approach that
goes beyond traditional one-shot generation methods by introducing a multi-step
process consisting of generation, verification, and correction stages. Our
approach, VCP(Verification and Correction Prompting), begins with the model
generating an initial output. We then proceed to verify the correctness of
different aspects of the generated text. The observations from the verification
step are converted into a specialized error-indication prompt, which instructs
the model to regenerate the output while considering the identified errors. To
enhance the model's correction ability, we have developed a carefully designed
training procedure. This procedure enables the model to incorporate feedback
from the error-indication prompt, resulting in improved output generation.
Through experimental results, we demonstrate that our approach effectively
reduces slot error rates while maintaining the overall quality of the generated
text.",2023-06-28
FunQA: Towards Surprising Video Comprehension,2023-06-26 17:59:55+00:00,http://arxiv.org/abs/2306.14899v1,"Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, Ziwei Liu","cs.CV, cs.AI, cs.CL, cs.MM",image2text,"Surprising videos, e.g., funny clips, creative performances, or visual
illusions, attract significant attention. Enjoyment of these videos is not
simply a response to visual stimuli; rather, it hinges on the human capacity to
understand (and appreciate) commonsense violations depicted in these videos. We
introduce FunQA, a challenging video question answering (QA) dataset
specifically designed to evaluate and enhance the depth of video reasoning
based on counter-intuitive and fun videos. Unlike most video QA benchmarks
which focus on less surprising contexts, e.g., cooking or instructional videos,
FunQA covers three previously unexplored types of surprising videos: 1)
HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous
QA tasks designed to assess the model's capability in counter-intuitive
timestamp localization, detailed video description, and reasoning around
counter-intuitiveness. We also pose higher-level tasks, such as attributing a
fitting and vivid title to the video, and scoring the video creativity. In
total, the FunQA benchmark consists of 312K free-text QA pairs derived from
4.3K video clips, spanning a total of 24 video hours. Extensive experiments
with existing VideoQA models reveal significant performance gaps for the FunQA
videos across spatial-temporal reasoning, visual-centered reasoning, and
free-text generation.",2023-06-26
"Learning Descriptive Image Captioning via Semipermeable Maximum
  Likelihood Estimation",2023-06-23 12:03:07+00:00,http://arxiv.org/abs/2306.13460v1,"Zihao Yue, Anwen Hu, Liang Zhang, Qin Jin",cs.CL,image2text,"Image captioning aims to describe visual content in natural language. As 'a
picture is worth a thousand words', there could be various correct descriptions
for an image. However, with maximum likelihood estimation as the training
objective, the captioning model is penalized whenever its prediction mismatches
with the label. For instance, when the model predicts a word expressing richer
semantics than the label, it will be penalized and optimized to prefer more
concise expressions, referred to as conciseness optimization. In contrast,
predictions that are more concise than labels lead to richness optimization.
Such conflicting optimization directions could eventually result in the model
generating general descriptions. In this work, we introduce Semipermeable
MaxImum Likelihood Estimation (SMILE), which allows richness optimization while
blocking conciseness optimization, thus encouraging the model to generate
longer captions with more details. Extensive experiments on two mainstream
image captioning datasets MSCOCO and Flickr30K demonstrate that SMILE
significantly enhances the descriptiveness of generated captions. We further
provide in-depth investigations to facilitate a better understanding of how
SMILE works.",2023-06-23
"Improving Image Captioning Descriptiveness by Ranking and LLM-based
  Fusion",2023-06-20 15:13:02+00:00,http://arxiv.org/abs/2306.11593v1,"Simone Bianco, Luigi Celona, Marco Donzella, Paolo Napoletano","cs.CV, cs.AI, cs.CL, cs.DB, cs.LG",image2text,"State-of-The-Art (SoTA) image captioning models often rely on the Microsoft
COCO (MS-COCO) dataset for training. This dataset contains annotations provided
by human annotators, who typically produce captions averaging around ten
tokens. However, this constraint presents a challenge in effectively capturing
complex scenes and conveying detailed information. Furthermore, captioning
models tend to exhibit bias towards the ``average'' caption, which captures
only the more general aspects. What would happen if we were able to
automatically generate longer captions, thereby making them more detailed?
Would these captions, evaluated by humans, be more or less representative of
the image content compared to the original MS-COCO captions? In this paper, we
present a novel approach to address previous challenges by showcasing how
captions generated from different SoTA models can be effectively fused,
resulting in richer captions. Our proposed method leverages existing models
from the literature, eliminating the need for additional training. Instead, it
utilizes an image-text based metric to rank the captions generated by SoTA
models for a given image. Subsequently, the top two captions are fused using a
Large Language Model (LLM). Experimental results demonstrate the effectiveness
of our approach, as the captions generated by our model exhibit higher
consistency with human judgment when evaluated on the MS-COCO test set. By
combining the strengths of various SoTA models, our method enhances the quality
and appeal of image captions, bridging the gap between automated systems and
the rich, informative nature of human-generated descriptions. This advance
opens up new possibilities for generating captions that are more suitable for
the training of both vision-language and captioning models.",2023-06-20
"Energy-Based Cross Attention for Bayesian Context Update in
  Text-to-Image Diffusion Models",2023-06-16 14:30:41+00:00,http://arxiv.org/abs/2306.09869v2,"Geon Yeong Park, Jeongsol Kim, Beomsu Kim, Sang Wan Lee, Jong Chul Ye","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Despite the remarkable performance of text-to-image diffusion models in image
generation tasks, recent studies have raised the issue that generated images
sometimes cannot capture the intended semantic contents of the text prompts,
which phenomenon is often called semantic misalignment. To address this, here
we present a novel energy-based model (EBM) framework. Specifically, we first
formulate EBMs of latent image representations and text embeddings in each
cross-attention layer of the denoising autoencoder. Then, we obtain the
gradient of the log posterior of context vectors, which can be updated and
transferred to the subsequent cross-attention layer, thereby implicitly
minimizing a nested hierarchy of energy functions. Our latent EBMs further
allow zero-shot compositional generation as a linear combination of
cross-attention outputs from different contexts. Using extensive experiments,
we demonstrate that the proposed method is highly effective in handling various
image generation tasks, including multi-concept generation, text-guided image
inpainting, and real and synthetic image editing.",2023-06-16
"Human Preference Score v2: A Solid Benchmark for Evaluating Human
  Preferences of Text-to-Image Synthesis",2023-06-15 17:59:31+00:00,http://arxiv.org/abs/2306.09341v1,"Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, Hongsheng Li","cs.CV, cs.AI, cs.DB",image2text,"Recent text-to-image generative models can generate high-fidelity images from
text inputs, but the quality of these generated images cannot be accurately
evaluated by existing evaluation metrics. To address this issue, we introduce
Human Preference Dataset v2 (HPD v2), a large-scale dataset that captures human
preferences on images from a wide range of sources. HPD v2 comprises 798,090
human preference choices on 430,060 pairs of images, making it the largest
dataset of its kind. The text prompts and images are deliberately collected to
eliminate potential bias, which is a common issue in previous datasets. By
fine-tuning CLIP on HPD v2, we obtain Human Preference Score v2 (HPS v2), a
scoring model that can more accurately predict text-generated images' human
preferences. Our experiments demonstrate that HPS v2 generalizes better than
previous metrics across various image distributions and is responsive to
algorithmic improvements of text-to-image generative models, making it a
preferable evaluation metric for these models. We also investigate the design
of the evaluation prompts for text-to-image generative models, to make the
evaluation stable, fair and easy-to-use. Finally, we establish a benchmark for
text-to-image generative models using HPS v2, which includes a set of recent
text-to-image models from the academia, community and industry. The code and
dataset is / will be available at https://github.com/tgxs002/HPSv2.",2023-06-15
GBSD: Generative Bokeh with Stage Diffusion,2023-06-14 05:34:02+00:00,http://arxiv.org/abs/2306.08251v1,"Jieren Deng, Xin Zhou, Hao Tian, Zhihong Pan, Derek Aguiar","cs.CV, cs.AI",image2text,"The bokeh effect is an artistic technique that blurs out-of-focus areas in a
photograph and has gained interest due to recent developments in text-to-image
synthesis and the ubiquity of smart-phone cameras and photo-sharing apps. Prior
work on rendering bokeh effects have focused on post hoc image manipulation to
produce similar blurring effects in existing photographs using classical
computer graphics or neural rendering techniques, but have either depth
discontinuity artifacts or are restricted to reproducing bokeh effects that are
present in the training data. More recent diffusion based models can synthesize
images with an artistic style, but either require the generation of
high-dimensional masks, expensive fine-tuning, or affect global image
characteristics. In this paper, we present GBSD, the first generative
text-to-image model that synthesizes photorealistic images with a bokeh style.
Motivated by how image synthesis occurs progressively in diffusion models, our
approach combines latent diffusion models with a 2-stage conditioning algorithm
to render bokeh effects on semantically defined objects. Since we can focus the
effect on objects, this semantic bokeh effect is more versatile than classical
rendering techniques. We evaluate GBSD both quantitatively and qualitatively
and demonstrate its ability to be applied in both text-to-image and
image-to-image settings.",2023-06-14
I See Dead People: Gray-Box Adversarial Attack on Image-To-Text Models,2023-06-13 07:35:28+00:00,http://arxiv.org/abs/2306.07591v1,"Raz Lapid, Moshe Sipper","cs.CV, cs.NE",image2text,"Modern image-to-text systems typically adopt the encoder-decoder framework,
which comprises two main components: an image encoder, responsible for
extracting image features, and a transformer-based decoder, used for generating
captions. Taking inspiration from the analysis of neural networks' robustness
against adversarial perturbations, we propose a novel gray-box algorithm for
creating adversarial examples in image-to-text models. Unlike image
classification tasks that have a finite set of class labels, finding visually
similar adversarial examples in an image-to-text task poses greater challenges
because the captioning system allows for a virtually infinite space of possible
captions. In this paper, we present a gray-box adversarial attack on
image-to-text, both untargeted and targeted. We formulate the process of
discovering adversarial perturbations as an optimization problem that uses only
the image-encoder component, meaning the proposed attack is language-model
agnostic. Through experiments conducted on the ViT-GPT2 model, which is the
most-used image-to-text model in Hugging Face, and the Flickr30k dataset, we
demonstrate that our proposed attack successfully generates visually similar
adversarial examples, both with untargeted and targeted captions. Notably, our
attack operates in a gray-box manner, requiring no knowledge about the decoder
module. We also show that our attacks fool the popular open-source platform
Hugging Face.",2023-06-13
"Generative Text-Guided 3D Vision-Language Pretraining for Unified
  Medical Image Segmentation",2023-06-07 22:20:51+00:00,http://arxiv.org/abs/2306.04811v1,"Yinda Chen, Che Liu, Wei Huang, Sibo Cheng, Rossella Arcucci, Zhiwei Xiong","cs.CV, cs.AI",image2text,"Vision-Language Pretraining (VLP) has demonstrated remarkable capabilities in
learning visual representations from textual descriptions of images without
annotations. Yet, effective VLP demands large-scale image-text pairs, a
resource that suffers scarcity in the medical domain. Moreover, conventional
VLP is limited to 2D images while medical images encompass diverse modalities,
often in 3D, making the learning process more challenging. To address these
challenges, we present Generative Text-Guided 3D Vision-Language Pretraining
for Unified Medical Image Segmentation (GTGM), a framework that extends of VLP
to 3D medical images without relying on paired textual descriptions.
Specifically, GTGM utilizes large language models (LLM) to generate
medical-style text from 3D medical images. This synthetic text is then used to
supervise 3D visual representation learning. Furthermore, a negative-free
contrastive learning objective strategy is introduced to cultivate consistent
visual representations between augmented 3D medical image patches, which
effectively mitigates the biases associated with strict positive-negative
sample pairings. We evaluate GTGM on three imaging modalities - Computed
Tomography (CT), Magnetic Resonance Imaging (MRI), and electron microscopy (EM)
over 13 datasets. GTGM's superior performance across various medical image
segmentation tasks underscores its effectiveness and versatility, by enabling
VLP extension into 3D medical imagery while bypassing the need for paired text.",2023-06-07
On the Difference of BERT-style and CLIP-style Text Encoders,2023-06-06 13:41:09+00:00,http://arxiv.org/abs/2306.03678v1,"Zhihong Chen, Guiming Hardy Chen, Shizhe Diao, Xiang Wan, Benyou Wang",cs.CL,image2text,"Masked language modeling (MLM) has been one of the most popular pretraining
recipes in natural language processing, e.g., BERT, one of the representative
models. Recently, contrastive language-image pretraining (CLIP) has also
attracted attention, especially its vision models that achieve excellent
performance on a broad range of vision tasks. However, few studies are
dedicated to studying the text encoders learned by CLIP. In this paper, we
analyze the difference between BERT-style and CLIP-style text encoders from
three experiments: (i) general text understanding, (ii) vision-centric text
understanding, and (iii) text-to-image generation. Experimental analyses show
that although CLIP-style text encoders underperform BERT-style ones for general
text understanding tasks, they are equipped with a unique ability, i.e.,
synesthesia, for the cross-modal association, which is more similar to the
senses of humans.",2023-06-06
Putting Humans in the Image Captioning Loop,2023-06-06 07:50:46+00:00,http://arxiv.org/abs/2306.03476v1,"Aliki Anagnostopoulou, Mareike Hartmann, Daniel Sonntag","cs.CL, cs.CV",image2text,"Image Captioning (IC) models can highly benefit from human feedback in the
training process, especially in cases where data is limited. We present
work-in-progress on adapting an IC system to integrate human feedback, with the
goal to make it easily adaptable to user-specific data. Our approach builds on
a base IC model pre-trained on the MS COCO dataset, which generates captions
for unseen images. The user will then be able to offer feedback on the image
and the generated/predicted caption, which will be augmented to create
additional training instances for the adaptation of the model. The additional
instances are integrated into the model using step-wise updates, and a sparse
memory replay component is used to avoid catastrophic forgetting. We hope that
this approach, while leading to improved results, will also result in
customizable IC models.",2023-06-06
"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video
  Understanding",2023-06-05 13:17:27+00:00,http://arxiv.org/abs/2306.02858v2,"Hang Zhang, Xin Li, Lidong Bing","cs.CL, cs.CV, cs.SD, eess.AS",image2text,"We present Video-LLaMA, a multi-modal framework that empowers Large Language
Models (LLMs) with the capability of understanding both visual and auditory
content in the video. Video-LLaMA bootstraps cross-modal training from the
frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous
vision-LLMs that focus on static image comprehensions such as MiniGPT-4 and
LLaVA, Video-LLaMA mainly tackles two challenges in video understanding: (1)
capturing the temporal changes in visual scenes, (2) integrating audio-visual
signals. To counter the first challenge, we propose a Video Q-former to
assemble the pre-trained image encoder into our video encoder and introduce a
video-to-text generation task to learn video-language correspondence. For the
second challenge, we leverage ImageBind, a universal embedding model aligning
multiple modalities as the pre-trained audio encoder, and introduce an Audio
Q-former on top of ImageBind to learn reasonable auditory query embeddings for
the LLM module. To align the output of both visual & audio encoders with LLM's
embedding space, we train Video-LLaMA on massive video/image-caption pairs as
well as visual-instruction-tuning datasets of moderate amount but higher
quality. We found Video-LLaMA showcases the ability to perceive and comprehend
video content, generating meaningful responses that are grounded in the visual
and auditory information presented in the videos. This highlights the potential
of Video-LLaMA as a promising prototype for audio-visual AI assistants.",2023-06-05
"Identifying the style by a qualified reader on a short fragment of
  generated poetry",2023-06-05 10:55:15+00:00,http://arxiv.org/abs/2306.02771v1,Boris Orekhov,"cs.CL, cs.AI, cs.LG",image2text,"Style is an important concept in today's challenges in natural language
generating. After the success in the field of image style transfer, the task of
text style transfer became actual and attractive. Researchers are also
interested in the tasks of style reproducing in generation of the poetic text.
Evaluation of style reproducing in natural poetry generation remains a problem.
I used 3 character-based LSTM-models to work with style reproducing assessment.
All three models were trained on the corpus of texts by famous Russian-speaking
poets. Samples were shown to the assessors and 4 answer options were offered,
the style of which poet this sample reproduces. In addition, the assessors were
asked how well they were familiar with the work of the poet they had named.
Students studying history of literature were the assessors, 94 answers were
received. It has appeared that accuracy of definition of style increases if the
assessor can quote the poet by heart. Each model showed at least 0.7
macro-average accuracy. The experiment showed that it is better to involve a
professional rather than a naive reader in the evaluation of style in the tasks
of poetry generation, while lstm models are good at reproducing the style of
Russian poets even on a limited training corpus.",2023-06-05
Multilingual Conceptual Coverage in Text-to-Image Models,2023-06-02 17:59:09+00:00,http://arxiv.org/abs/2306.01735v1,"Michael Saxon, William Yang Wang","cs.CL, cs.AI, cs.CV, eess.IV",image2text,"We propose ""Conceptual Coverage Across Languages"" (CoCo-CroLa), a technique
for benchmarking the degree to which any generative text-to-image system
provides multilingual parity to its training language in terms of tangible
nouns. For each model we can assess ""conceptual coverage"" of a given target
language relative to a source language by comparing the population of images
generated for a series of tangible nouns in the source language to the
population of images generated for each noun under translation in the target
language. This technique allows us to estimate how well-suited a model is to a
target language as well as identify model-specific weaknesses, spurious
correlations, and biases without a-priori assumptions. We demonstrate how it
can be used to benchmark T2I models in terms of multilinguality, and how
despite its simplicity it is a good proxy for impressive generalization.",2023-06-02
"FlexRound: Learnable Rounding based on Element-wise Division for
  Post-Training Quantization",2023-06-01 03:31:12+00:00,http://arxiv.org/abs/2306.00317v1,"Jung Hyun Lee, Jeonghoon Kim, Se Jung Kwon, Dongsoo Lee","cs.LG, cs.AI",image2text,"Post-training quantization (PTQ) has been gaining popularity for the
deployment of deep neural networks on resource-limited devices since unlike
quantization-aware training, neither a full training dataset nor end-to-end
training is required at all. As PTQ schemes based on reconstructing each layer
or block output turn out to be effective to enhance quantized model
performance, recent works have developed algorithms to devise and learn a new
weight-rounding scheme so as to better reconstruct each layer or block output.
In this work, we propose a simple yet effective new weight-rounding mechanism
for PTQ, coined FlexRound, based on element-wise division instead of typical
element-wise addition such that FlexRound enables jointly learning a common
quantization grid size as well as a different scale for each pre-trained
weight. Thanks to the reciprocal rule of derivatives induced by element-wise
division, FlexRound is inherently able to exploit pre-trained weights when
updating their corresponding scales, and thus, flexibly quantize pre-trained
weights depending on their magnitudes. We empirically validate the efficacy of
FlexRound on a wide range of models and tasks. To the best of our knowledge,
our work is the first to carry out comprehensive experiments on not only image
classification and natural language understanding but also natural language
generation, assuming a per-tensor uniform PTQ setting. Moreover, we
demonstrate, for the first time, that large language models can be efficiently
quantized, with only a negligible impact on performance compared to
half-precision baselines, achieved by reconstructing the output in a
block-by-block manner.",2023-06-01
"CapText: Large Language Model-based Caption Generation From Image
  Context and Description",2023-06-01 02:40:44+00:00,http://arxiv.org/abs/2306.00301v2,"Shinjini Ghosh, Sagnik Anupam","cs.LG, cs.CL",image2text,"While deep-learning models have been shown to perform well on image-to-text
datasets, it is difficult to use them in practice for captioning images. This
is because captions traditionally tend to be context-dependent and offer
complementary information about an image, while models tend to produce
descriptions that describe the visual features of the image. Prior research in
caption generation has explored the use of models that generate captions when
provided with the images alongside their respective descriptions or contexts.
We propose and evaluate a new approach, which leverages existing large language
models to generate captions from textual descriptions and context alone,
without ever processing the image directly. We demonstrate that after
fine-tuning, our approach outperforms current state-of-the-art image-text
alignment models like OSCAR-VinVL on this task on the CIDEr metric.",2023-06-01
"LMCap: Few-shot Multilingual Image Captioning by Retrieval Augmented
  Language Model Prompting",2023-05-31 13:03:17+00:00,http://arxiv.org/abs/2305.19821v1,"Rita Ramos, Bruno Martins, Desmond Elliott","cs.CL, cs.CV",image2text,"Multilingual image captioning has recently been tackled by training with
large-scale machine translated data, which is an expensive, noisy, and
time-consuming process. Without requiring any multilingual caption data, we
propose LMCap, an image-blind few-shot multilingual captioning model that works
by prompting a language model with retrieved captions. Specifically, instead of
following the standard encoder-decoder paradigm, given an image, LMCap first
retrieves the captions of similar images using a multilingual CLIP encoder.
These captions are then combined into a prompt for an XGLM decoder, in order to
generate captions in the desired language. In other words, the generation model
does not directly process the image, instead processing retrieved captions.
Experiments on the XM3600 dataset of geographically diverse images show that
our model is competitive with fully-supervised multilingual captioning models,
without requiring any supervised training on any captioning data.",2023-05-31
"Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic
  Rewards",2023-05-31 06:59:21+00:00,http://arxiv.org/abs/2305.19599v2,"Guian Fang, Zutao Jiang, Jianhua Han, Guansong Lu, Hang Xu, Xiaodan Liang","cs.CV, cs.AI",image2text,"Recent advances in text-to-image diffusion models have achieved remarkable
success in generating high-quality, realistic images from given text prompts.
However, previous methods fail to perform accurate modality alignment between
text concepts and generated images due to the lack of fine-level semantic
guidance that successfully diagnoses the modality discrepancy. In this paper,
we propose FineRewards to improve the alignment between text and images in
text-to-image diffusion models by introducing two new fine-grained semantic
rewards: the caption reward and the Semantic Segment Anything (SAM) reward.
From the global semantic view, the caption reward generates a corresponding
detailed caption that depicts all important contents in the synthetic image via
a BLIP-2 model and then calculates the reward score by measuring the similarity
between the generated caption and the given prompt. From the local semantic
view, the SAM reward segments the generated images into local parts with
category labels, and scores the segmented parts by measuring the likelihood of
each category appearing in the prompted scene via a large language model, i.e.,
Vicuna-7B. Additionally, we adopt an assemble reward-ranked learning strategy
to enable the integration of multiple reward functions to jointly guide the
model training. Adapting results of text-to-image models on the MS-COCO
benchmark show that the proposed semantic reward outperforms other baseline
reward functions with a considerable margin on both visual quality and semantic
similarity with the input prompt. Moreover, by adopting the assemble
reward-ranked learning strategy, we further demonstrate that model performance
is further improved when adapting under the unifying of the proposed semantic
reward with the current image rewards.",2023-05-31
Fine-grained Text Style Transfer with Diffusion-Based Language Models,2023-05-31 02:51:26+00:00,http://arxiv.org/abs/2305.19512v1,"Yiwei Lyu, Tiange Luo, Jiacheng Shi, Todd C. Hollon, Honglak Lee","cs.CL, cs.AI, cs.LG",image2text,"Diffusion probabilistic models have shown great success in generating
high-quality images controllably, and researchers have tried to utilize this
controllability into text generation domain. Previous works on diffusion-based
language models have shown that they can be trained without external knowledge
(such as pre-trained weights) and still achieve stable performance and
controllability. In this paper, we trained a diffusion-based model on StylePTB
dataset, the standard benchmark for fine-grained text style transfers. The
tasks in StylePTB requires much more refined control over the output text
compared to tasks evaluated in previous works, and our model was able to
achieve state-of-the-art performance on StylePTB on both individual and
compositional transfers. Moreover, our model, trained on limited data from
StylePTB without external knowledge, outperforms previous works that utilized
pretrained weights, embeddings, and external grammar parsers, and this may
indicate that diffusion-based language models have great potential under
low-resource settings.",2023-05-31
Learning to Imagine: Visually-Augmented Natural Language Generation,2023-05-26 13:59:45+00:00,http://arxiv.org/abs/2305.16944v2,"Tianyi Tang, Yushuo Chen, Yifan Du, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen",cs.CL,image2text,"People often imagine relevant scenes to aid in the writing process. In this
work, we aim to utilize visual information for composition in the same manner
as humans. We propose a method, LIVE, that makes pre-trained language models
(PLMs) Learn to Imagine for Visuallyaugmented natural language gEneration.
First, we imagine the scene based on the text: we use a diffusion model to
synthesize high-quality images conditioned on the input texts. Second, we use
CLIP to determine whether the text can evoke the imagination in a posterior
way. Finally, our imagination is dynamic, and we conduct synthesis for each
sentence rather than generate only one image for an entire paragraph.
Technically, we propose a novel plug-and-play fusion layer to obtain
visually-augmented representations for each text. Our vision-text fusion layer
is compatible with Transformerbased architecture. We have conducted extensive
experiments on four generation tasks using BART and T5, and the automatic
results and human evaluation demonstrate the effectiveness of our proposed
method. We will release the code, model, and data at the link:
https://github.com/RUCAIBox/LIVE.",2023-05-26
"Not All Metrics Are Guilty: Improving NLG Evaluation with LLM
  Paraphrasing",2023-05-24 11:53:29+00:00,http://arxiv.org/abs/2305.15067v1,"Tianyi Tang, Hongyuan Lu, Yuchen Eleanor Jiang, Haoyang Huang, Dongdong Zhang, Wayne Xin Zhao, Furu Wei",cs.CL,image2text,"Most research about natural language generation (NLG) relies on evaluation
benchmarks with limited references for a sample, which may result in poor
correlations with human judgements. The underlying reason is that one semantic
meaning can actually be expressed in different forms, and the evaluation with a
single or few references may not accurately reflect the quality of the model's
hypotheses. To address this issue, this paper presents a novel method, named
Para-Ref, to enhance existing evaluation benchmarks by enriching the number of
references. We leverage large language models (LLMs) to paraphrase a single
reference into multiple high-quality ones in diverse expressions. Experimental
results on representative NLG tasks of machine translation, text summarization,
and image caption demonstrate that our method can effectively improve the
correlation with human evaluation for sixteen automatic evaluation metrics by
+7.82% in ratio. We release the code and data at
https://github.com/RUCAIBox/Para-Ref.",2023-05-24
"I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create
  Visual Metaphors",2023-05-24 05:01:10+00:00,http://arxiv.org/abs/2305.14724v1,"Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, Smaranda Muresan","cs.CL, cs.AI, cs.CV, cs.HC",image2text,"Visual metaphors are powerful rhetorical devices used to persuade or
communicate creative ideas through images. Similar to linguistic metaphors,
they convey meaning implicitly through symbolism and juxtaposition of the
symbols. We propose a new task of generating visual metaphors from linguistic
metaphors. This is a challenging task for diffusion-based text-to-image models,
such as DALL$\cdot$E 2, since it requires the ability to model implicit meaning
and compositionality. We propose to solve the task through the collaboration
between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3
(davinci-002) with Chain-of-Thought prompting generates text that represents a
visual elaboration of the linguistic metaphor containing the implicit meaning
and relevant objects, which is then used as input to the diffusion-based
text-to-image models.Using a human-AI collaboration framework, where humans
interact both with the LLM and the top-performing diffusion model, we create a
high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic
metaphors and their associated visual elaborations. Evaluation by professional
illustrators shows the promise of LLM-Diffusion Model collaboration for this
task.To evaluate the utility of our Human-AI collaboration framework and the
quality of our dataset, we perform both an intrinsic human-based evaluation and
an extrinsic evaluation using visual entailment as a downstream task.",2023-05-24
"Gender Biases in Automatic Evaluation Metrics: A Case Study on Image
  Captioning",2023-05-24 04:27:40+00:00,http://arxiv.org/abs/2305.14711v1,"Haoyi Qiu, Zi-Yi Dou, Tianlu Wang, Asli Celikyilmaz, Nanyun Peng",cs.CL,image2text,"Pretrained model-based evaluation metrics have demonstrated strong
performance with high correlations with human judgments in various natural
language generation tasks such as image captioning. Despite the impressive
results, their impact on fairness is under-explored -- it is widely
acknowledged that pretrained models can encode societal biases, and utilizing
them for evaluation purposes may inadvertently manifest and potentially amplify
biases. In this paper, we conduct a systematic study in gender biases of
model-based evaluation metrics with a focus on image captioning tasks.
Specifically, we first identify and quantify gender biases in different
evaluation metrics regarding profession, activity, and object concepts. Then,
we demonstrate the negative consequences of using these biased metrics, such as
favoring biased generation models in deployment and propagating the biases to
generation models through reinforcement learning. We also present a simple but
effective alternative to reduce gender biases by combining n-gram
matching-based and pretrained model-based evaluation metrics.",2023-05-24
"Process-To-Text: A Framework for the Quantitative Description of
  Processes in Natural Language",2023-05-23 13:14:34+00:00,http://arxiv.org/abs/2305.14044v1,"Yago Fontenla-Seco, Alberto Bugarín-Diz, Manuel Lama",cs.CL,image2text,"In this paper we present the Process-To-Text (P2T) framework for the
automatic generation of textual descriptive explanations of processes. P2T
integrates three AI paradigms: process mining for extracting temporal and
structural information from a process, fuzzy linguistic protoforms for
modelling uncertain terms, and natural language generation for building the
explanations. A real use-case in the cardiology domain is presented, showing
the potential of P2T for providing natural language explanations addressed to
specialists.",2023-05-23
"VNHSGE: VietNamese High School Graduation Examination Dataset for Large
  Language Models",2023-05-20 14:13:08+00:00,http://arxiv.org/abs/2305.12199v1,"Dao Xuan-Quy, Le Ngoc-Bich, Vo The-Duy, Phan Xuan-Dung, Ngo Bac-Bien, Nguyen Van-Tien, Nguyen Thi-My-Thanh, Nguyen Hong-Phuoc",cs.CL,image2text,"The VNHSGE (VietNamese High School Graduation Examination) dataset, developed
exclusively for evaluating large language models (LLMs), is introduced in this
article. The dataset, which covers nine subjects, was generated from the
Vietnamese National High School Graduation Examination and comparable tests.
300 literary essays have been included, and there are over 19,000
multiple-choice questions on a range of topics. The dataset assesses LLMs in
multitasking situations such as question answering, text generation, reading
comprehension, visual question answering, and more by including both textual
data and accompanying images. Using ChatGPT and BingChat, we evaluated LLMs on
the VNHSGE dataset and contrasted their performance with that of Vietnamese
students to see how well they performed. The results show that ChatGPT and
BingChat both perform at a human level in a number of areas, including
literature, English, history, geography, and civics education. They still have
space to grow, though, especially in the areas of mathematics, physics,
chemistry, and biology. The VNHSGE dataset seeks to provide an adequate
benchmark for assessing the abilities of LLMs with its wide-ranging coverage
and variety of activities. We intend to promote future developments in the
creation of LLMs by making this dataset available to the scientific community,
especially in resolving LLMs' limits in disciplines involving mathematics and
the natural sciences.",2023-05-20
STOAT: Structured Data to Analytical Text With Controls,2023-05-19 17:03:09+00:00,http://arxiv.org/abs/2305.11826v1,"Deepanway Ghosal, Preksha Nema, Aravindan Raghuveer","cs.CL, cs.AI",image2text,"Recent language models have made tremendous progress in the structured data
to text generation task. However, these models still give sub-optimal
performance where logical inference is required to generate the descriptions.
In this work, we specifically focus on analytical text generation from
structured data such as tables. Building on the taxonomy proposed in (Gupta et
al., 2020) we focus on controllable table to text generation for the following
reasoning categories: numerical reasoning, commonsense reasoning, temporal
reasoning, table knowledge, and entity knowledge. We propose STOAT model, which
is table and reasoning aware, with vector-quantization to infuse the given
reasoning categories in the output. We observe that our model provides 10.19%,
1.13% improvement on the PARENT metric in iToTTo and Infotabs for the
analytical sentence task. We also found that our model generates 15.3% more
faithful and analytical descriptions as compared to the baseline models in
human evaluation. We curate and release two reasoning category annotated
table-to-interesting text generation datasets based on the ToTTo (Parikh et
al., 2020) and InfoTabs datasets (Gupta et al.,2020).",2023-05-19
"Generating Visual Spatial Description via Holistic 3D Scene
  Understanding",2023-05-19 15:53:56+00:00,http://arxiv.org/abs/2305.11768v1,"Yu Zhao, Hao Fei, Wei Ji, Jianguo Wei, Meishan Zhang, Min Zhang, Tat-Seng Chua","cs.CV, cs.CL",image2text,"Visual spatial description (VSD) aims to generate texts that describe the
spatial relations of the given objects within images. Existing VSD work merely
models the 2D geometrical vision features, thus inevitably falling prey to the
problem of skewed spatial understanding of target objects. In this work, we
investigate the incorporation of 3D scene features for VSD. With an external 3D
scene extractor, we obtain the 3D objects and scene features for input images,
based on which we construct a target object-centered 3D spatial scene graph
(Go3D-S2G), such that we model the spatial semantics of target objects within
the holistic 3D scenes. Besides, we propose a scene subgraph selecting
mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the
diverse local structure features are navigated to yield spatially-diversified
text generation. Experimental results on two VSD datasets demonstrate that our
framework outperforms the baselines significantly, especially improving on the
cases with complex visual spatial relations. Meanwhile, our method can produce
more spatially-diversified generation. Code is available at
https://github.com/zhaoyucs/VSD.",2023-05-19
Brain Captioning: Decoding human brain activity into images and text,2023-05-19 09:57:19+00:00,http://arxiv.org/abs/2305.11560v1,"Matteo Ferrante, Furkan Ozcelik, Tommaso Boccato, Rufin VanRullen, Nicola Toschi","cs.CV, cs.AI",image2text,"Every day, the human brain processes an immense volume of visual information,
relying on intricate neural mechanisms to perceive and interpret these stimuli.
Recent breakthroughs in functional magnetic resonance imaging (fMRI) have
enabled scientists to extract visual information from human brain activity
patterns. In this study, we present an innovative method for decoding brain
activity into meaningful images and captions, with a specific focus on brain
captioning due to its enhanced flexibility as compared to brain decoding into
images. Our approach takes advantage of cutting-edge image captioning models
and incorporates a unique image reconstruction pipeline that utilizes latent
diffusion models and depth estimation. We utilized the Natural Scenes Dataset,
a comprehensive fMRI dataset from eight subjects who viewed images from the
COCO dataset. We employed the Generative Image-to-text Transformer (GIT) as our
backbone for captioning and propose a new image reconstruction pipeline based
on latent diffusion models. The method involves training regularized linear
regression models between brain activity and extracted features. Additionally,
we incorporated depth maps from the ControlNet model to further guide the
reconstruction process. We evaluate our methods using quantitative metrics for
both generated captions and images. Our brain captioning approach outperforms
existing methods, while our image reconstruction pipeline generates plausible
images with improved spatial relationships. In conclusion, we demonstrate
significant progress in brain decoding, showcasing the enormous potential of
integrating vision and language to better understand human cognition. Our
approach provides a flexible platform for future research, with potential
applications in various fields, including neural art, style transfer, and
portable devices.",2023-05-19
"Collaborative Generative AI: Integrating GPT-k for Efficient Editing in
  Text-to-Image Generation",2023-05-18 21:53:58+00:00,http://arxiv.org/abs/2305.11317v1,"Wanrong Zhu, Xinyi Wang, Yujie Lu, Tsu-Jui Fu, Xin Eric Wang, Miguel Eckstein, William Yang Wang",cs.CL,image2text,"The field of text-to-image (T2I) generation has garnered significant
attention both within the research community and among everyday users. Despite
the advancements of T2I models, a common issue encountered by users is the need
for repetitive editing of input prompts in order to receive a satisfactory
image, which is time-consuming and labor-intensive. Given the demonstrated text
generation power of large-scale language models, such as GPT-k, we investigate
the potential of utilizing such models to improve the prompt editing process
for T2I generation. We conduct a series of experiments to compare the common
edits made by humans and GPT-k, evaluate the performance of GPT-k in prompting
T2I, and examine factors that may influence this process. We found that GPT-k
models focus more on inserting modifiers while humans tend to replace words and
phrases, which includes changes to the subject matter. Experimental results
show that GPT-k are more effective in adjusting modifiers rather than
predicting spontaneous changes in the primary subject matters. Adopting the
edit suggested by GPT-k models may reduce the percentage of remaining edits by
20-30%.",2023-05-18
AIwriting: Relations Between Image Generation and Digital Writing,2023-05-18 09:23:05+00:00,http://arxiv.org/abs/2305.10834v1,"Scott Rettberg, Talan Memmott, Jill Walker Rettberg, Jason Nelson, Patrick Lichty","cs.AI, cs.CL, cs.HC, cs.MM, J.5",image2text,"During 2022, both transformer-based AI text generation sys-tems such as GPT-3
and AI text-to-image generation systems such as DALL-E 2 and Stable Diffusion
made exponential leaps forward and are unquestionably altering the fields of
digital art and electronic literature. In this panel a group of electronic
literature authors and theorists consider new oppor-tunities for human
creativity presented by these systems and present new works have produced
during the past year that specifically address these systems as environments
for literary expressions that are translated through iterative interlocutive
processes into visual representations. The premise that binds these
presentations is that these systems and the works gener-ated must be considered
from a literary perspective, as they originate in human writing. In works
ranging from a visual memoir of the personal experience of a health crisis, to
interac-tive web comics, to architectures based on abstract poetic language, to
political satire, four artists explore the capabili-ties of these writing
environments for new genres of literary artist practice, while a digital
culture theorist considers the origins and effects of the particular training
datasets of human language and images on which these new hybrid forms are
based.",2023-05-18
"ReGen: Zero-Shot Text Classification via Training Data Generation with
  Progressive Dense Retrieval",2023-05-18 04:30:09+00:00,http://arxiv.org/abs/2305.10703v1,"Yue Yu, Yuchen Zhuang, Rongzhi Zhang, Yu Meng, Jiaming Shen, Chao Zhang","cs.CL, cs.IR, cs.LG",image2text,"With the development of large language models (LLMs), zero-shot learning has
attracted much attention for various NLP tasks. Different from prior works that
generate training data with billion-scale natural language generation (NLG)
models, we propose a retrieval-enhanced framework to create training data from
a general-domain unlabeled corpus. To realize this, we first conduct
contrastive pretraining to learn an unsupervised dense retriever for extracting
the most relevant documents using class-descriptive verbalizers. We then
further propose two simple strategies, namely Verbalizer Augmentation with
Demonstrations and Self-consistency Guided Filtering to improve the topic
coverage of the dataset while removing noisy examples. Experiments on nine
datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines
and saves around 70% of the time compared to baselines using large NLG models.
Besides, REGEN can be naturally integrated with recently proposed large
language models to boost performance.",2023-05-18
What You See is What You Read? Improving Text-Image Alignment Evaluation,2023-05-17 17:43:38+00:00,http://arxiv.org/abs/2305.10400v1,"Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee Aharoni, Jonathan Herzig, Oran Lang, Eran Ofek, Idan Szpektor","cs.CL, cs.CV",image2text,"Automatically determining whether a text and a corresponding image are
semantically aligned is a significant challenge for vision-language models,
with applications in generative text-to-image and image-to-text tasks. In this
work, we study methods for automatic text-image alignment evaluation. We first
introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets
from both text-to-image and image-to-text generation tasks, with human
judgements for whether a given text-image pair is semantically aligned. We then
describe two automatic methods to determine alignment: the first involving a
pipeline based on question generation and visual question answering models, and
the second employing an end-to-end classification approach by finetuning
multimodal pretrained models. Both methods surpass prior approaches in various
text-image alignment tasks, with significant improvements in challenging cases
that involve complex composition or unnatural images. Finally, we demonstrate
how our approaches can localize specific misalignments between an image and a
given text, and how they can be used to automatically re-rank candidates in
text-to-image generation.",2023-05-17
Equivariant Few-Shot Learning from Pretrained Models,2023-05-17 02:20:34+00:00,http://arxiv.org/abs/2305.09900v1,"Sourya Basu, Pulkit Katdare, Prasanna Sattigeri, Vijil Chenthamarakshan, Katherine Driggs-Campbell, Payel Das, Lav R. Varshney","cs.LG, cs.AI, cs.CL, cs.CV",image2text,"Efficient transfer learning algorithms are key to the success of foundation
models on diverse downstream tasks even with limited data. Recent works of
\cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging
(\textit{equitune}) and optimization-based methods, respectively, over features
from group-transformed inputs to obtain equivariant outputs from
non-equivariant neural networks. While \cite{kaba2022equivariance} are only
concerned with training from scratch, we find that equitune performs poorly on
equivariant zero-shot tasks despite good finetuning results. We hypothesize
that this is because pretrained models provide better quality features for
certain transformations than others and simply averaging them is deleterious.
Hence, we propose $\lambda$-\textit{equitune} that averages the features using
\textit{importance weights}, $\lambda$s. These weights are learned directly
from the data using a small neural network, leading to excellent zero-shot and
finetuned results that outperform equitune. Further, we prove that
$\lambda$-equitune is equivariant and a universal approximator of equivariant
functions. Additionally, we show that the method of \cite{kaba2022equivariance}
used with appropriate loss functions, which we call \textit{equizero}, also
gives excellent zero-shot and finetuned performance. Both equitune and equizero
are special cases of $\lambda$-equitune. To show the simplicity and generality
of our method, we validate on a wide range of diverse applications and models
such as 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in
natural language generation (NLG), 4) compositional generalization in
languages, and 5) image classification using pretrained CNNs such as Resnet and
Alexnet.",2023-05-17
AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation,2023-05-16 15:10:22+00:00,http://arxiv.org/abs/2305.09515v2,"Tong Wu, Zhihao Fan, Xiao Liu, Yeyun Gong, Yelong Shen, Jian Jiao, Hai-Tao Zheng, Juntao Li, Zhongyu Wei, Jian Guo, Nan Duan, Weizhu Chen",cs.CL,image2text,"Diffusion models have gained significant attention in the realm of image
generation due to their exceptional performance. Their success has been
recently expanded to text generation via generating all tokens within a
sequence concurrently. However, natural language exhibits a far more pronounced
sequential dependency in comparison to images, and the majority of existing
language models are trained with a left-to-right auto-regressive approach. To
account for the inherent sequential characteristic of natural language, we
introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that
the generation of tokens on the right depends on the generated ones on the
left, a mechanism achieved through employing a dynamic number of denoising
steps that vary based on token position. This results in tokens on the left
undergoing fewer denoising steps than those on the right, thereby enabling them
to generate earlier and subsequently influence the generation of tokens on the
right. In a series of experiments on various text generation tasks, including
text summarization, machine translation, and common sense generation,
AR-Diffusion clearly demonstrated its superiority over existing diffusion
language models and that it can be $100\times\sim600\times$ faster when
achieving comparable results. Our code is available at
https://github.com/microsoft/ProphetNet/tree/master/AR-diffusion.",2023-05-16
Generative AI: Implications and Applications for Education,2023-05-12 16:52:38+00:00,http://arxiv.org/abs/2305.07605v2,"Anastasia Olga, Tzirides, Akash Saini, Gabriela Zapata, Duane Searsmith, Bill Cope, Mary Kalantzis, Vania Castro, Theodora Kourkoulou, John Jones, Rodrigo Abrantes da Silva, Jen Whiting, Nikoleta Polyxeni Kastania","cs.CY, cs.AI",image2text,"The launch of ChatGPT in November 2022 precipitated a panic among some
educators while prompting qualified enthusiasm from others. Under the umbrella
term Generative AI, ChatGPT is an example of a range of technologies for the
delivery of computer-generated text, image, and other digitized media. This
paper examines the implications for education of one generative AI technology,
chatbots responding from large language models, or C-LLM. It reports on an
application of a C-LLM to AI review and assessment of complex student work. In
a concluding discussion, the paper explores the intrinsic limits of generative
AI, bound as it is to language corpora and their textual representation through
binary notation. Within these limits, we suggest the range of emerging and
potential applications of Generative AI in education.",2023-05-12
Two-in-One: A Model Hijacking Attack Against Text Generation Models,2023-05-12 12:13:27+00:00,http://arxiv.org/abs/2305.07406v1,"Wai Man Si, Michael Backes, Yang Zhang, Ahmed Salem","cs.CR, cs.CL, cs.LG",image2text,"Machine learning has progressed significantly in various applications ranging
from face recognition to text generation. However, its success has been
accompanied by different attacks. Recently a new attack has been proposed which
raises both accountability and parasitic computing risks, namely the model
hijacking attack. Nevertheless, this attack has only focused on image
classification tasks. In this work, we broaden the scope of this attack to
include text generation and classification models, hence showing its broader
applicability. More concretely, we propose a new model hijacking attack, Ditto,
that can hijack different text classification tasks into multiple generation
ones, e.g., language translation, text summarization, and language modeling. We
use a range of text benchmark datasets such as SST-2, TweetEval, AGnews, QNLI,
and IMDB to evaluate the performance of our attacks. Our results show that by
using Ditto, an adversary can successfully hijack text generation models
without jeopardizing their utility.",2023-05-12
"Vision-Language Models in Remote Sensing: Current Progress and Future
  Trends",2023-05-09 19:17:07+00:00,http://arxiv.org/abs/2305.05726v1,"Congcong Wen, Yuan Hu, Xiang Li, Zhenghang Yuan, Xiao Xiang Zhu","cs.CV, cs.AI",image2text,"The remarkable achievements of ChatGPT and GPT-4 have sparked a wave of
interest and research in the field of large language models for Artificial
General Intelligence (AGI). These models provide us with intelligent solutions
that are more similar to human thinking, enabling us to use general artificial
intelligence to solve problems in various applications. However, in the field
of remote sensing, the scientific literature on the implementation of AGI
remains relatively scant. Existing AI-related research primarily focuses on
visual understanding tasks while neglecting the semantic understanding of the
objects and their relationships. This is where vision-language models excel, as
they enable reasoning about images and their associated textual descriptions,
allowing for a deeper understanding of the underlying semantics.
Vision-language models can go beyond recognizing the objects in an image and
can infer the relationships between them, as well as generate natural language
descriptions of the image. This makes them better suited for tasks that require
both visual and textual understanding, such as image captioning, text-based
image retrieval, and visual question answering. This paper provides a
comprehensive review of the research on vision-language models in remote
sensing, summarizing the latest progress, highlighting the current challenges,
and identifying potential research opportunities. Specifically, we review the
application of vision-language models in several mainstream remote sensing
tasks, including image captioning, text-based image generation, text-based
image retrieval, visual question answering, scene classification, semantic
segmentation, and object detection. For each task, we briefly describe the task
background and review some representative works. Finally, we summarize the
limitations of existing work and provide some possible directions for future
development.",2023-05-09
"UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in
  Vietnamese",2023-05-07 02:48:47+00:00,http://arxiv.org/abs/2305.04166v1,"Doanh C. Bui, Nghia Hieu Nguyen, Khang Nguyen","cs.CV, cs.CL",image2text,"Image Captioning is one of the vision-language tasks that still interest the
research community worldwide in the 2020s. MS-COCO Caption benchmark is
commonly used to evaluate the performance of advanced captioning models,
although it was published in 2015. Recent captioning models trained on the
MS-COCO Caption dataset only have good performance in language patterns of
English; they do not have such good performance in contexts captured in Vietnam
or fluently caption images using Vietnamese. To contribute to the low-resources
research community as in Vietnam, we introduce a novel image captioning dataset
in Vietnamese, the Open-domain Vietnamese Image Captioning dataset
(UIT-OpenViIC). The introduced dataset includes complex scenes captured in
Vietnam and manually annotated by Vietnamese under strict rules and
supervision. In this paper, we present in more detail the dataset creation
process. From preliminary analysis, we show that our dataset is challenging to
recent state-of-the-art (SOTA) Transformer-based baselines, which performed
well on the MS COCO dataset. Then, the modest results prove that UIT-OpenViIC
has room to grow, which can be one of the standard benchmarks in Vietnamese for
the research community to evaluate their captioning models. Furthermore, we
present a CAMO approach that effectively enhances the image representation
ability by a multi-level encoder output fusion mechanism, which helps improve
the quality of generated captions compared to previous captioning models.",2023-05-07
"Simulating H.P. Lovecraft horror literature with the ChatGPT large
  language model",2023-05-05 11:03:03+00:00,http://arxiv.org/abs/2305.03429v1,"Eduardo C. Garrido-Merchán, José Luis Arroyo-Barrigüete, Roberto Gozalo-Brihuela",cs.CL,image2text,"In this paper, we present a novel approach to simulating H.P. Lovecraft's
horror literature using the ChatGPT large language model, specifically the
GPT-4 architecture. Our study aims to generate text that emulates Lovecraft's
unique writing style and themes, while also examining the effectiveness of
prompt engineering techniques in guiding the model's output. To achieve this,
we curated a prompt containing several specialized literature references and
employed advanced prompt engineering methods. We conducted an empirical
evaluation of the generated text by administering a survey to a sample of
undergraduate students. Utilizing statistical hypothesis testing, we assessed
the students ability to distinguish between genuine Lovecraft works and those
generated by our model. Our findings demonstrate that the participants were
unable to reliably differentiate between the two, indicating the effectiveness
of the GPT-4 model and our prompt engineering techniques in emulating
Lovecraft's literary style. In addition to presenting the GPT model's
capabilities, this paper provides a comprehensive description of its underlying
architecture and offers a comparative analysis with related work that simulates
other notable authors and philosophers, such as Dennett. By exploring the
potential of large language models in the context of literary emulation, our
study contributes to the body of research on the applications and limitations
of these models in various creative domains.",2023-05-05
VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation,2023-05-04 23:27:21+00:00,http://arxiv.org/abs/2305.03204v1,"Xilun Chen, Lili Yu, Wenhan Xiong, Barlas Oğuz, Yashar Mehdad, Wen-tau Yih","cs.CV, cs.CL",image2text,"We propose a new two-stage pre-training framework for video-to-text
generation tasks such as video captioning and video question answering: A
generative encoder-decoder model is first jointly pre-trained on massive
image-text data to learn fundamental vision-language concepts, and then adapted
to video data in an intermediate video-text pre-training stage to learn
video-specific skills such as spatio-temporal reasoning. As a result, our
VideoOFA model achieves new state-of-the-art performance on four Video
Captioning benchmarks, beating prior art by an average of 9.7 points in CIDEr
score. It also outperforms existing models on two open-ended Video Question
Answering datasets, showcasing its generalization capability as a universal
video-to-text model.",2023-05-04
Image Captioners Sometimes Tell More Than Images They See,2023-05-04 15:32:41+00:00,http://arxiv.org/abs/2305.02932v1,"Honori Udo, Takafumi Koshinaka","cs.CV, cs.MM",image2text,"Image captioning, a.k.a. ""image-to-text,"" which generates descriptive text
from given images, has been rapidly developing throughout the era of deep
learning. To what extent is the information in the original image preserved in
the descriptive text generated by an image captioner? To answer that question,
we have performed experiments involving the classification of images from
descriptive text alone, without referring to the images at all, and compared
results with those from standard image-based classifiers. We have evaluate
several image captioning models with respect to a disaster image classification
task, CrisisNLP, and show that descriptive text classifiers can sometimes
achieve higher accuracy than standard image-based classifiers. Further, we show
that fusing an image-based classifier with a descriptive text classifier can
provide improvement in accuracy.",2023-05-04
"Governance of the AI, by the AI, and for the AI",2023-05-04 03:29:07+00:00,http://arxiv.org/abs/2305.03719v1,"Andrew W. Torrance, Bill Tomlinson","cs.CY, cs.AI",image2text,"Over the past half century, there have been several false dawns during which
the ""arrival"" of world-changing artificial intelligence (AI) has been heralded.
Tempting fate, the authors believe the age of AI has, indeed, finally arrived.
Powerful image generators, such as DALL-E2 and Midjourney have suddenly allowed
anyone with access the ability easily to create rich and complex art. In a
similar vein, text generators, such as GPT3.5 (including ChatGPT) and BLOOM,
allow users to compose detailed written descriptions of many topics of
interest. And, it is even possible now for a person without extensive expertise
in writing software to use AI to generate code capable of myriad applications.
While AI will continue to evolve and improve, probably at a rapid rate, the
current state of AI is already ushering in profound changes to many different
sectors of society. Every new technology challenges the ability of humanity to
govern it wisely. However, governance is usually viewed as both possible and
necessary due to the disruption new technology often poses to social
structures, industries, the environment, and other important human concerns. In
this article, we offer an analysis of a range of interactions between AI and
governance, with the hope that wise decisions may be made that maximize
benefits and minimize costs. The article addresses two main aspects of this
relationship: the governance of AI by humanity, and the governance of humanity
by AI. The approach we have taken is itself informed by AI, as this article was
written collaboratively by the authors and ChatGPT.",2023-05-04
Controlled Text Generation with Natural Language Instructions,2023-04-27 15:56:34+00:00,http://arxiv.org/abs/2304.14293v1,"Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, Mrinmaya Sachan","cs.CL, cs.AI, cs.LG",image2text,"Large language models generate fluent texts and can follow natural language
instructions to solve a wide range of tasks without task-specific training.
Nevertheless, it is notoriously difficult to control their generation to
satisfy the various constraints required by different applications. In this
work, we present InstructCTG, a controlled text generation framework that
incorporates different constraints by conditioning on natural language
descriptions and demonstrations of the constraints. In particular, we first
extract the underlying constraints of natural texts through a combination of
off-the-shelf NLP tools and simple heuristics. We then verbalize the
constraints into natural language instructions to form weakly supervised
training data. By prepending natural language descriptions of the constraints
and a few demonstrations, we fine-tune a pre-trained language model to
incorporate various types of constraints. Compared to existing search-based or
score-based methods, InstructCTG is more flexible to different constraint types
and has a much smaller impact on the generation quality and speed because it
does not modify the decoding procedure. Additionally, InstructCTG allows the
model to adapt to new constraints without re-training through the use of
few-shot task generalization and in-context learning abilities of
instruction-tuned language models.",2023-04-27
"From Association to Generation: Text-only Captioning by Unsupervised
  Cross-modal Mapping",2023-04-26 04:06:20+00:00,http://arxiv.org/abs/2304.13273v3,"Junyang Wang, Ming Yan, Yi Zhang, Jitao Sang","cs.CV, cs.CL, cs.LG",image2text,"With the development of Vision-Language Pre-training Models (VLPMs)
represented by CLIP and ALIGN, significant breakthroughs have been achieved for
association-based visual tasks such as image classification and image-text
retrieval by the zero-shot capability of CLIP without fine-tuning. However,
CLIP is hard to apply to generation-based tasks. This is due to the lack of
decoder architecture and pre-training tasks for generation. Although previous
works have created generation capacity for CLIP through additional language
models, a modality gap between the CLIP representations of different modalities
and the inability of CLIP to model the offset of this gap, which fails the
concept to transfer across modalities. To solve the problem, we try to map
images/videos to the language modality and generate captions from the language
modality. In this paper, we propose the K-nearest-neighbor Cross-modality
Mapping (Knight), a zero-shot method from association to generation. With
text-only unsupervised training, Knight achieves State-of-the-Art performance
in zero-shot methods for image captioning and video captioning. Our code is
available at https://github.com/junyangwang0410/Knight.",2023-04-26
RenderDiffusion: Text Generation as Image Generation,2023-04-25 02:14:44+00:00,http://arxiv.org/abs/2304.12519v1,"Junyi Li, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen","cs.CL, cs.CV, cs.LG",image2text,"Diffusion models have become a new generative paradigm for text generation.
Considering the discrete categorical nature of text, in this paper, we propose
\textsc{RenderDiffusion}, a novel diffusion approach for text generation via
text-guided image generation. Our key idea is to render the target text as a
\emph{glyph image} containing visual language content. In this way, conditional
text generation can be cast as a glyph image generation task, and it is then
natural to apply continuous diffusion models to discrete texts. Specially, we
utilize a cascaded architecture (\ie a base and a super-resolution diffusion
model) to generate high-fidelity glyph images, conditioned on the input text.
Furthermore, we design a text grounding module to transform and refine the
visual language content from generated glyph images into the final texts. In
experiments over four conditional text generation tasks and two classes of
metrics (\ie quality and diversity), \textsc{RenderDiffusion} can achieve
comparable or even better results than several baselines, including pretrained
language models. Our model also makes significant improvements compared to the
recent diffusion model.",2023-04-25
Token Imbalance Adaptation for Radiology Report Generation,2023-04-18 23:09:36+00:00,http://arxiv.org/abs/2304.09185v1,"Yuexin Wu, I-Chan Huang, Xiaolei Huang","cs.CL, cs.AI",image2text,"Imbalanced token distributions naturally exist in text documents, leading
neural language models to overfit on frequent tokens. The token imbalance may
dampen the robustness of radiology report generators, as complex medical terms
appear less frequently but reflect more medical information. In this study, we
demonstrate how current state-of-the-art models fail to generate infrequent
tokens on two standard benchmark datasets (IU X-RAY and MIMIC-CXR) of radiology
report generation. % However, no prior study has proposed methods to adapt
infrequent tokens for text generators feeding with medical images. To solve the
challenge, we propose the \textbf{T}oken \textbf{Im}balance Adapt\textbf{er}
(\textit{TIMER}), aiming to improve generation robustness on infrequent tokens.
The model automatically leverages token imbalance by an unlikelihood loss and
dynamically optimizes generation processes to augment infrequent tokens. We
compare our approach with multiple state-of-the-art methods on the two
benchmarks. Experiments demonstrate the effectiveness of our approach in
enhancing model robustness overall and infrequent tokens. Our ablation analysis
shows that our reinforcement learning method has a major effect in adapting
token imbalance for radiology report generation.",2023-04-18
"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and
  Dataset",2023-04-17 15:08:15+00:00,http://arxiv.org/abs/2304.08345v1,"Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang, Jing Liu","cs.LG, cs.CL, cs.CV, cs.MM, eess.AS",image2text,"In this paper, we propose a Vision-Audio-Language Omni-peRception pretraining
model (VALOR) for multi-modal understanding and generation. Different from
widely-studied vision-language pretraining models, VALOR jointly models
relationships of vision, audio and language in an end-to-end manner. It
contains three separate encoders for single modality representations, and a
decoder for multimodal conditional text generation. We design two pretext tasks
to pretrain VALOR model, including Multimodal Grouping Alignment (MGA) and
Multimodal Grouping Captioning (MGC). MGA projects vision, language and audio
to the same common space, building vision-language, audio-language and
audiovisual-language alignment simultaneously. MGC learns how to generate text
tokens in conditions of vision, audio or their both. To promote
vision-audio-language pretraining research, we construct a large-scale
high-quality tri-modality dataset named VALOR-1M, which contains 1M audiable
videos with human annotated audiovisual captions. Extensive experiments show
that VALOR can learn strong multimodal correlations and be generalized to
various downstream tasks (e.g., retrieval, captioning and question answering),
with different input modalities (e.g., vision-language, audio-language and
audiovisual-language). VALOR achieves new state-of-the-art performances on
series of public cross-modality benchmarks. Code and data are available at
project page https://casia-iva-group.github.io/projects/VALOR.",2023-04-17
Improving Diffusion Models for Scene Text Editing with Dual Encoders,2023-04-12 02:08:34+00:00,http://arxiv.org/abs/2304.05568v1,"Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian Price, Shiyu Chang","cs.CV, cs.AI",image2text,"Scene text editing is a challenging task that involves modifying or inserting
specified texts in an image while maintaining its natural and realistic
appearance. Most previous approaches to this task rely on style-transfer models
that crop out text regions and feed them into image transfer models, such as
GANs. However, these methods are limited in their ability to change text style
and are unable to insert texts into images. Recent advances in diffusion models
have shown promise in overcoming these limitations with text-conditional image
editing. However, our empirical analysis reveals that state-of-the-art
diffusion models struggle with rendering correct text and controlling text
style. To address these problems, we propose DIFFSTE to improve pre-trained
diffusion models with a dual encoder design, which includes a character encoder
for better text legibility and an instruction encoder for better style control.
An instruction tuning framework is introduced to train our model to learn the
mapping from the text instruction to the corresponding image with either the
specified style or the style of the surrounding texts in the background. Such a
training method further brings our method the zero-shot generalization ability
to the following three scenarios: generating text with unseen font variation,
e.g., italic and bold, mixing different fonts to construct a new font, and
using more relaxed forms of natural language as the instructions to guide the
generation task. We evaluate our approach on five datasets and demonstrate its
superior performance in terms of text correctness, image naturalness, and style
controllability. Our code is publicly available.
https://github.com/UCSB-NLP-Chang/DiffSTE",2023-04-12
"ImageCaptioner$^2$: Image Captioner for Image Captioning Bias
  Amplification Assessment",2023-04-10 21:40:46+00:00,http://arxiv.org/abs/2304.04874v1,"Eslam Mohamed Bakr, Pengzhan Sun, Li Erran Li, Mohamed Elhoseiny","cs.CV, cs.AI, cs.LG",image2text,"Most pre-trained learning systems are known to suffer from bias, which
typically emerges from the data, the model, or both. Measuring and quantifying
bias and its sources is a challenging task and has been extensively studied in
image captioning. Despite the significant effort in this direction, we observed
that existing metrics lack consistency in the inclusion of the visual signal.
In this paper, we introduce a new bias assessment metric, dubbed
$ImageCaptioner^2$, for image captioning. Instead of measuring the absolute
bias in the model or the data, $ImageCaptioner^2$ pay more attention to the
bias introduced by the model w.r.t the data bias, termed bias amplification.
Unlike the existing methods, which only evaluate the image captioning
algorithms based on the generated captions only, $ImageCaptioner^2$
incorporates the image while measuring the bias. In addition, we design a
formulation for measuring the bias of generated captions as prompt-based image
captioning instead of using language classifiers. Finally, we apply our
$ImageCaptioner^2$ metric across 11 different image captioning architectures on
three different datasets, i.e., MS-COCO caption dataset, Artemis V1, and
Artemis V2, and on three different protected attributes, i.e., gender, race,
and emotions. Consequently, we verify the effectiveness of our
$ImageCaptioner^2$ metric by proposing AnonymousBench, which is a novel human
evaluation paradigm for bias metrics. Our metric shows significant superiority
over the recent bias metric; LIC, in terms of human alignment, where the
correlation scores are 80% and 54% for our metric and LIC, respectively. The
code is available at https://eslambakr.github.io/imagecaptioner2.github.io/.",2023-04-10
Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions,2023-04-09 12:46:18+00:00,http://arxiv.org/abs/2304.04227v2,"Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, Mohamed Elhoseiny","cs.CV, cs.AI",image2text,"Video captioning aims to convey dynamic scenes from videos using natural
language, facilitating the understanding of spatiotemporal information within
our environment. Although there have been recent advances, generating detailed
and enriched video descriptions continues to be a substantial challenge. In
this work, we introduce Video ChatCaptioner, an innovative approach for
creating more comprehensive spatiotemporal video descriptions. Our method
employs a ChatGPT model as a controller, specifically designed to select frames
for posing video content-driven questions. Subsequently, a robust algorithm is
utilized to answer these visual queries. This question-answer framework
effectively uncovers intricate video details and shows promise as a method for
enhancing video content. Following multiple conversational rounds, ChatGPT can
summarize enriched video content based on previous conversations. We
qualitatively demonstrate that our Video ChatCaptioner can generate captions
containing more visual details about the videos. The code is publicly available
at https://github.com/Vision-CAIR/ChatCaptioner",2023-04-09
"Opinion Mining from YouTube Captions Using ChatGPT: A Case Study of
  Street Interviews Polling the 2023 Turkish Elections",2023-04-07 01:25:22+00:00,http://arxiv.org/abs/2304.03434v1,"Tuğrulcan Elmas, İlker Gül","cs.SI, cs.CY",image2text,"Opinion mining plays a critical role in understanding public sentiment and
preferences, particularly in the context of political elections. Traditional
polling methods, while useful, can be expensive and less scalable. Social media
offers an alternative source of data for opinion mining but presents challenges
such as noise, biases, and platform limitations in data collection. In this
paper, we propose a novel approach for opinion mining, utilizing YouTube's
auto-generated captions from public interviews as a data source, specifically
focusing on the 2023 Turkish elections as a case study. We introduce an opinion
mining framework using ChatGPT to mass-annotate voting intentions and
motivations that represent the stance and frames prior to the election. We
report that ChatGPT can predict the preferred candidate with 97\% accuracy and
identify the correct voting motivation out of 13 possible choices with 71\%
accuracy based on the data collected from 325 interviews. We conclude by
discussing the robustness of our approach, accounting for factors such as
captions quality, interview length, and channels. This new method will offer a
less noisy and cost-effective alternative for opinion mining using social media
data.",2023-04-07
DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model,2023-04-06 02:27:22+00:00,http://arxiv.org/abs/2304.02827v1,"Hoigi Seo, Hayeon Kim, Gwanghyun Kim, Se Young Chun","cs.CV, cs.AI",image2text,"The increasing demand for high-quality 3D content creation has motivated the
development of automated methods for creating 3D object models from a single
image and/or from a text prompt. However, the reconstructed 3D objects using
state-of-the-art image-to-3D methods still exhibit low correspondence to the
given image and low multi-view consistency. Recent state-of-the-art text-to-3D
methods are also limited, yielding 3D samples with low diversity per prompt
with long synthesis time. To address these challenges, we propose DITTO-NeRF, a
novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a
single image. Our DITTO-NeRF consists of constructing high-quality partial 3D
object for limited in-boundary (IB) angles using the given or text-generated 2D
image from the frontal view and then iteratively reconstructing the remaining
3D NeRF using inpainting latent diffusion model. We propose progressive 3D
object reconstruction schemes in terms of scales (low to high resolution),
angles (IB angles initially to outer-boundary (OB) later), and masks (object to
background boundary) in our DITTO-NeRF so that high-quality information on IB
can be propagated into OB. Our DITTO-NeRF outperforms state-of-the-art methods
in terms of fidelity and diversity qualitatively and quantitatively with much
faster training times than prior arts on image/text-to-3D such as DreamFusion,
and NeuralLift-360.",2023-04-06
"Scalable and Accurate Self-supervised Multimodal Representation Learning
  without Aligned Video and Text Data",2023-04-04 19:11:05+00:00,http://arxiv.org/abs/2304.02080v1,"Vladislav Lialin, Stephen Rawls, David Chan, Shalini Ghosh, Anna Rumshisky, Wael Hamza","cs.CV, cs.CL",image2text,"Scaling up weakly-supervised datasets has shown to be highly effective in the
image-text domain and has contributed to most of the recent state-of-the-art
computer vision and multimodal neural networks. However, existing large-scale
video-text datasets and mining techniques suffer from several limitations, such
as the scarcity of aligned data, the lack of diversity in the data, and the
difficulty of collecting aligned data. Currently popular video-text data mining
approach via automatic speech recognition (ASR) used in HowTo100M provides
low-quality captions that often do not refer to the video content. Other mining
approaches do not provide proper language descriptions (video tags) and are
biased toward short clips (alt text). In this work, we show how recent advances
in image captioning allow us to pre-train high-quality video models without any
parallel video-text data. We pre-train several video captioning models that are
based on an OPT language model and a TimeSformer visual backbone. We fine-tune
these networks on several video captioning datasets. First, we demonstrate that
image captioning pseudolabels work better for pre-training than the existing
HowTo100M ASR captions. Second, we show that pre-training on both images and
videos produces a significantly better network (+4 CIDER on MSR-VTT) than
pre-training on a single modality. Our methods are complementary to the
existing pre-training or data mining approaches and can be used in a variety of
settings. Given the efficacy of the pseudolabeling method, we are planning to
publicly release the generated captions.",2023-04-04
Cross-Domain Image Captioning with Discriminative Finetuning,2023-04-04 09:33:16+00:00,http://arxiv.org/abs/2304.01662v1,"Roberto Dessì, Michele Bevilacqua, Eleonora Gualdoni, Nathanael Carraz Rakotonirina, Francesca Franzon, Marco Baroni","cs.CV, cs.AI, cs.CL",image2text,"Neural captioners are typically trained to mimic human-generated references
without optimizing for any specific communication goal, leading to problems
such as the generation of vague captions. In this paper, we show that
fine-tuning an out-of-the-box neural captioner with a self-supervised
discriminative communication objective helps to recover a plain, visually
descriptive language that is more informative about image contents. Given a
target image, the system must learn to produce a description that enables an
out-of-the-box text-conditioned image retriever to identify such image among a
set of candidates. We experiment with the popular ClipCap captioner, also
replicating the main results with BLIP. In terms of similarity to ground-truth
human descriptions, the captions emerging from discriminative finetuning lag
slightly behind those generated by the non-finetuned model, when the latter is
trained and tested on the same caption dataset. However, when the model is used
without further tuning to generate captions for out-of-domain datasets, our
discriminatively-finetuned captioner generates descriptions that resemble human
references more than those produced by the same captioner without finetuning.
We further show that, on the Conceptual Captions dataset, discriminatively
finetuned captions are more helpful than either vanilla ClipCap captions or
ground-truth captions for human annotators tasked with an image discrimination
task.",2023-04-04
Can AI Put Gamma-Ray Astrophysicists Out of a Job?,2023-03-31 07:29:47+00:00,http://arxiv.org/abs/2303.17853v2,"Samuel T. Spencer, Vikas Joshi, Alison M. W. Mitchell","physics.pop-ph, astro-ph.HE, cs.CL",image2text,"In what will likely be a litany of generative-model-themed arXiv submissions
celebrating April the 1st, we evaluate the capacity of state-of-the-art
transformer models to create a paper detailing the detection of a Pulsar Wind
Nebula with a non-existent Imaging Atmospheric Cherenkov Telescope (IACT)
Array. We do this to evaluate the ability of such models to interpret
astronomical observations and sources based on language information alone, and
to assess potential means by which fraudulently generated scientific papers
could be identified during peer review (given that reliable generative model
watermarking has yet to be deployed for these tools). We conclude that our jobs
as astronomers are safe for the time being. From this point on, prompts given
to ChatGPT and Stable Diffusion are shown in orange, text generated by ChatGPT
is shown in black, whereas analysis by the (human) authors is in blue.",2023-03-31
Prefix tuning for automated audio captioning,2023-03-30 16:01:28+00:00,http://arxiv.org/abs/2303.17489v2,"Minkyu Kim, Kim Sung-Bin, Tae-Hyun Oh","eess.AS, cs.MM, cs.SD",image2text,"Audio captioning aims to generate text descriptions from environmental
sounds. One challenge of audio captioning is the difficulty of the
generalization due to the lack of audio-text paired training data. In this
work, we propose a simple yet effective method of dealing with small-scaled
datasets by leveraging a pre-trained language model. We keep the language model
frozen to maintain the expressivity for text generation, and we only learn to
extract global and temporal features from the input audio. To bridge a modality
gap between the audio features and the language model, we employ mapping
networks that translate audio features to the continuous vectors the language
model can understand, called prefixes. We evaluate our proposed method on the
Clotho and AudioCaps dataset and show our method outperforms prior arts in
diverse experimental settings.",2023-03-30
GPT is becoming a Turing machine: Here are some ways to program it,2023-03-25 00:43:41+00:00,http://arxiv.org/abs/2303.14310v1,"Ana Jojic, Zhen Wang, Nebojsa Jojic",cs.CL,image2text,"We demonstrate that, through appropriate prompting, GPT-3 family of models
can be triggered to perform iterative behaviours necessary to execute (rather
than just write or recall) programs that involve loops, including several
popular algorithms found in computer science curricula or software developer
interviews. We trigger execution and description of Iterations by Regimenting
Self-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong
repetitive structure in an example of an execution path of a target program for
one particular input, 2) Prompting with fragments of execution paths, and 3)
Explicitly forbidding (skipping) self-attention to parts of the generated text.
On a dynamic program execution, IRSA leads to larger accuracy gains than
replacing the model with the much more powerful GPT-4. IRSA has promising
applications in education, as the prompts and responses resemble student
assignments in data structures and algorithms classes. Our findings hold
implications for evaluating LLMs, which typically target the in-context
learning: We show that prompts that may not even cover one full task example
can trigger algorithmic behaviour, allowing solving problems previously thought
of as hard for LLMs, such as logical puzzles. Consequently, prompt design plays
an even more critical role in LLM performance than previously recognized.",2023-03-25
CoBIT: A Contrastive Bi-directional Image-Text Generation Model,2023-03-23 17:24:31+00:00,http://arxiv.org/abs/2303.13455v1,"Haoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, Jiahui Yu","cs.CV, cs.CL",image2text,"The field of vision and language has witnessed a proliferation of pre-trained
foundation models. Most existing methods are independently pre-trained with
contrastive objective like CLIP, image-to-text generative objective like PaLI,
or text-to-image generative objective like Parti. However, the three objectives
can be pre-trained on the same data, image-text pairs, and intuitively they
complement each other as contrasting provides global alignment capacity and
generation grants fine-grained understanding. In this work, we present a
Contrastive Bi-directional Image-Text generation model (CoBIT), which attempts
to unify the three pre-training objectives in one framework. Specifically,
CoBIT employs a novel unicoder-decoder structure, consisting of an image
unicoder, a text unicoder and a cross-modal decoder. The image/text unicoders
can switch between encoding and decoding in different tasks, enabling
flexibility and shared knowledge that benefits both image-to-text and
text-to-image generations. CoBIT achieves superior performance in image
understanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)
and text-based content creation, particularly in zero-shot scenarios. For
instance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in
zero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.",2023-03-23
Open-Vocabulary Object Detection using Pseudo Caption Labels,2023-03-23 05:10:22+00:00,http://arxiv.org/abs/2303.13040v1,"Han-Cheol Cho, Won Young Jhoo, Wooyoung Kang, Byungseok Roh","cs.CV, cs.AI",image2text,"Recent open-vocabulary detection methods aim to detect novel objects by
distilling knowledge from vision-language models (VLMs) trained on a vast
amount of image-text pairs. To improve the effectiveness of these methods,
researchers have utilized datasets with a large vocabulary that contains a
large number of object classes, under the assumption that such data will enable
models to extract comprehensive knowledge on the relationships between various
objects and better generalize to unseen object classes. In this study, we argue
that more fine-grained labels are necessary to extract richer knowledge about
novel objects, including object attributes and relationships, in addition to
their names. To address this challenge, we propose a simple and effective
method named Pseudo Caption Labeling (PCL), which utilizes an image captioning
model to generate captions that describe object instances from diverse
perspectives. The resulting pseudo caption labels offer dense samples for
knowledge distillation. On the LVIS benchmark, our best model trained on the
de-duplicated VisualGenome dataset achieves an AP of 34.5 and an APr of 30.6,
comparable to the state-of-the-art performance. PCL's simplicity and
flexibility are other notable features, as it is a straightforward
pre-processing technique that can be used with any image captioning model
without imposing any restrictions on model architecture or training process.",2023-03-23
HIVE: Harnessing Human Feedback for Instructional Visual Editing,2023-03-16 19:47:41+00:00,http://arxiv.org/abs/2303.09618v1,"Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu","cs.CV, cs.AI, cs.CL, cs.HC, cs.LG",image2text,"Incorporating human feedback has been shown to be crucial to align text
generated by large language models to human preferences. We hypothesize that
state-of-the-art instructional image editing models, where outputs are
generated based on an input image and an editing instruction, could similarly
benefit from human feedback, as their outputs may not adhere to the correct
instructions and preferences of users. In this paper, we present a novel
framework to harness human feedback for instructional visual editing (HIVE).
Specifically, we collect human feedback on the edited images and learn a reward
function to capture the underlying user preferences. We then introduce scalable
diffusion model fine-tuning methods that can incorporate human preferences
based on the estimated reward. Besides, to mitigate the bias brought by the
limitation of data, we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset to boost the
performance of instructional image editing. We conduct extensive empirical
experiments quantitatively and qualitatively, showing that HIVE is favored over
previous state-of-the-art instructional image editing approaches by a large
margin.",2023-03-16
Text-to-image Diffusion Model in Generative AI: A Survey,2023-03-14 13:49:54+00:00,http://arxiv.org/abs/2303.07909v1,"Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, In So Kweon","cs.CV, cs.AI, cs.LG",image2text,"This survey reviews text-to-image diffusion models in the context that
diffusion models have emerged to be popular for a wide range of generative
tasks. As a self-contained work, this survey starts with a brief introduction
of how a basic diffusion model works for image synthesis, followed by how
condition or guidance improves learning. Based on that, we present a review of
state-of-the-art methods on text-conditioned image synthesis, i.e.,
text-to-image. We further summarize applications beyond text-to-image
generation: text-guided creative generation and text-guided image editing.
Beyond the progress made so far, we discuss existing challenges and promising
future directions.",2023-03-14
Diffusion Models in NLP: A Survey,2023-03-14 01:53:49+00:00,http://arxiv.org/abs/2303.07576v1,"Yuansong Zhu, Yu Zhao","cs.CL, cs.AI",image2text,"Diffusion models have become a powerful family of deep generative models,
with record-breaking performance in many applications. This paper first gives
an overview and derivation of the basic theory of diffusion models, then
reviews the research results of diffusion models in the field of natural
language processing, from text generation, text-driven image generation and
other four aspects, and analyzes and summarizes the relevant literature
materials sorted out, and finally records the experience and feelings of this
topic literature review research.",2023-03-14
"ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and
  Multilingual Natural Language Generation",2023-03-11 17:14:33+00:00,http://arxiv.org/abs/2303.06458v1,"Bang Yang, Fenglin Liu, Yuexian Zou, Xian Wu, Yaowei Wang, David A. Clifton","cs.CL, cs.AI, cs.CV",image2text,"Natural Language Generation (NLG) accepts input data in the form of images,
videos, or text and generates corresponding natural language text as output.
Existing NLG methods mainly adopt a supervised approach and rely heavily on
coupled data-to-text pairs. However, for many targeted scenarios and for
non-English languages, sufficient quantities of labeled data are often not
available. To relax the dependency on labeled data of downstream tasks, we
propose an intuitive and effective zero-shot learning framework, ZeroNLG, which
can deal with multiple NLG tasks, including image-to-text (image captioning),
video-to-text (video captioning), and text-to-text (neural machine
translation), across English, Chinese, German, and French within a unified
framework. ZeroNLG does not require any labeled downstream pairs for training.
During training, ZeroNLG (i) projects different domains (across modalities and
languages) to corresponding coordinates in a shared common latent space; (ii)
bridges different domains by aligning their corresponding coordinates in this
space; and (iii) builds an unsupervised multilingual auto-encoder to learn to
generate text by reconstructing the input text given its coordinate in shared
latent space. Consequently, during inference, based on the data-to-text
pipeline, ZeroNLG can generate target sentences across different languages
given the coordinate of input data in the common space. Within this unified
framework, given visual (imaging or video) data as input, ZeroNLG can perform
zero-shot visual captioning; given textual sentences as input, ZeroNLG can
perform zero-shot machine translation. We present the results of extensive
experiments on twelve NLG tasks, showing that, without using any labeled
downstream pairs for training, ZeroNLG generates high-quality and believable
outputs and significantly outperforms existing zero-shot methods.",2023-03-11
"Describe me an Aucklet: Generating Grounded Perceptual Category
  Descriptions",2023-03-07 17:01:25+00:00,http://arxiv.org/abs/2303.04053v2,"Bill Noble, Nikolai Ilinykh",cs.CL,image2text,"Human language users can generate descriptions of perceptual concepts beyond
instance-level representations and also use such descriptions to learn
provisional class-level representations. However, the ability of computational
models to learn and operate with class representations is under-investigated in
the language-and-vision field. In this paper, we train separate neural networks
to generate and interpret class-level descriptions. We then use the zero-shot
classification performance of the interpretation model as a measure of
communicative success and class-level conceptual grounding. We investigate the
performance of prototype- and exemplar-based neural representations grounded
category description. Finally, we show that communicative success reveals
performance issues in the generation model that are not captured by traditional
intrinsic NLG evaluation metrics, and argue that these issues can be traced to
a failure to properly ground language in vision at the class level. We observe
that the interpretation model performs better with descriptions that are low in
diversity on the class level, possibly indicating a strong reliance on
frequently occurring features.",2023-03-07
"DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only
  Training",2023-03-06 11:02:47+00:00,http://arxiv.org/abs/2303.03032v1,"Wei Li, Linchao Zhu, Longyin Wen, Yi Yang","cs.CV, cs.AI, cs.CL",image2text,"Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong
zero-shot transfer capability in many discriminative tasks. Their adaptation to
zero-shot image-conditioned text generation tasks has drawn increasing
interest. Prior arts approach to zero-shot captioning by either utilizing the
existing large language models (e.g., GPT-2) or pre-training the
encoder-decoder network in an end-to-end manner. In this work, we propose a
simple framework, named DeCap, for zero-shot captioning. We introduce a
lightweight visual-aware language decoder. This decoder is both data-efficient
and computation-efficient: 1) it only requires the text data for training,
easing the burden on the collection of paired data. 2) it does not require
end-to-end training. When trained with text-only data, the decoder takes the
text embedding extracted from the off-the-shelf CLIP encoder as a prefix
embedding. The challenge is that the decoder is trained on the text corpus but
at the inference stage, it needs to generate captions based on visual inputs.
The modality gap issue is widely observed in multi-modal contrastive models
that prevents us from directly taking the visual embedding as the prefix
embedding. We propose a training-free mechanism to reduce the modality gap. We
project the visual embedding into the CLIP text embedding space, while the
projected embedding retains the information of the visual input. Taking the
projected embedding as the prefix embedding, the decoder generates high-quality
descriptions that match the visual input. The experiments show that DeCap
outperforms other zero-shot captioning methods and unpaired captioning methods
on the typical image captioning benchmarks, i.e., MSCOCO and NoCaps.",2023-03-06
Interactive Text Generation,2023-03-02 01:57:17+00:00,http://arxiv.org/abs/2303.00908v1,"Felix Faltings, Michel Galley, Baolin Peng, Kianté Brantley, Weixin Cai, Yizhe Zhang, Jianfeng Gao, Bill Dolan",cs.CL,image2text,"Users interact with text, image, code, or other editors on a daily basis.
However, machine learning models are rarely trained in the settings that
reflect the interactivity between users and their editor. This is
understandable as training AI models with real users is not only slow and
costly, but what these models learn may be specific to user interface design
choices. Unfortunately, this means most of the research on text, code, and
image generation has focused on non-interactive settings, whereby the model is
expected to get everything right without accounting for any input from a user
who may be willing to help.
  We introduce a new Interactive Text Generation task that allows training
generation models interactively without the costs of involving real users, by
using user simulators that provide edits that guide the model towards a given
target text. We train our interactive models using Imitation Learning, and our
experiments against competitive non-interactive generation models show that
models trained interactively are superior to their non-interactive
counterparts, even when all models are given the same budget of user inputs or
edits.",2023-03-02
Few-Shot Table-to-Text Generation with Prompt-based Adapter,2023-02-24 05:48:53+00:00,http://arxiv.org/abs/2302.12468v1,"Zhixin Guo, Minyxuan Yan, Jiexing Qi, Jianping Zhou, Ziwei He, Zhouhan Lin, Guanjie Zheng, Xinbing Wang",cs.CL,image2text,"Pre-trained language models (PLMs) have made remarkable progress in
table-to-text generation tasks. However, the topological gap between tabular
data and text and the lack of domain-specific knowledge make it difficult for
PLMs to produce faithful text, especially in real-world applications with
limited resources. In this paper, we mitigate the above challenges by
introducing a novel augmentation method: Prompt-based Adapter (PA), which
targets table-to-text generation under few-shot conditions. The core insight
design of the PA is to inject prompt templates for augmenting domain-specific
knowledge and table-related representations into the model for bridging the
structural gap between tabular data and descriptions through adapters. Such
prompt-based knowledge augmentation method brings at least two benefits: (1)
enables us to fully use the large amounts of unlabelled domain-specific
knowledge, which can alleviate the PLMs' inherent shortcomings of lacking
domain knowledge; (2) allows us to design different types of tasks supporting
the generative challenge. Extensive experiments and analyses are conducted on
three open-domain few-shot NLG datasets: Humans, Books, and Songs. Compared to
previous state-of-the-art approaches, our model achieves superior performance
in terms of both fluency and accuracy as judged by human and automatic
evaluations.",2023-02-24
Improved Training of Mixture-of-Experts Language GANs,2023-02-23 09:25:46+00:00,http://arxiv.org/abs/2302.11875v1,"Yekun Chai, Qiyue Yin, Junge Zhang",cs.CL,image2text,"Despite the dramatic success in image generation, Generative Adversarial
Networks (GANs) still face great challenges in synthesizing sequences of
discrete elements, in particular human language. The difficulty in generator
training arises from the limited representation capacity and uninformative
learning signals obtained from the discriminator. In this work, we (1) first
empirically show that the mixture-of-experts approach is able to enhance the
representation capacity of the generator for language GANs and (2) harness the
Feature Statistics Alignment (FSA) paradigm to render fine-grained learning
signals to advance the generator training. Specifically, FSA forces the mean
statistics of the distribution of fake data to approach that of real samples as
close as possible in the finite-dimensional feature space. Empirical study on
synthetic and real benchmarks shows the superior performance in quantitative
evaluation and demonstrates the effectiveness of our approach to adversarial
text generation.",2023-02-23
Improving User Controlled Table-To-Text Generation Robustness,2023-02-20 07:51:15+00:00,http://arxiv.org/abs/2302.09820v1,"Hanxu Hu, Yunqing Liu, Zhongyi Yu, Laura Perez-Beltrachini",cs.CL,image2text,"In this work we study user controlled table-to-text generation where users
explore the content in a table by selecting cells and reading a natural
language description thereof automatically produce by a natural language
generator. Such generation models usually learn from carefully selected cell
combinations (clean cell selections); however, in practice users may select
unexpected, redundant, or incoherent cell combinations (noisy cell selections).
In experiments, we find that models perform well on test sets coming from the
same distribution as the train data but their performance drops when evaluated
on realistic noisy user inputs. We propose a fine-tuning regime with additional
user-simulated noisy cell selections. Models fine-tuned with the proposed
regime gain 4.85 BLEU points on user noisy test cases and 1.4 on clean test
cases; and achieve comparable state-of-the-art performance on the ToTTo
dataset.",2023-02-20
Large Scale Multi-Lingual Multi-Modal Summarization Dataset,2023-02-13 18:00:23+00:00,http://arxiv.org/abs/2302.06560v1,"Yash Verma, Anubhav Jangra, Raghvendra Kumar, Sriparna Saha","cs.CL, cs.MM",image2text,"Significant developments in techniques such as encoder-decoder models have
enabled us to represent information comprising multiple modalities. This
information can further enhance many downstream tasks in the field of
information retrieval and natural language processing; however, improvements in
multi-modal techniques and their performance evaluation require large-scale
multi-modal data which offers sufficient diversity. Multi-lingual modeling for
a variety of tasks like multi-modal summarization, text generation, and
translation leverages information derived from high-quality multi-lingual
annotated data. In this work, we present the current largest multi-lingual
multi-modal summarization dataset (M3LS), and it consists of over a million
instances of document-image pairs along with a professionally annotated
multi-modal summary for each pair. It is derived from news articles published
by British Broadcasting Corporation(BBC) over a decade and spans 20 languages,
targeting diversity across five language roots, it is also the largest
summarization dataset for 13 languages and consists of cross-lingual
summarization data for 2 languages. We formally define the multi-lingual
multi-modal summarization task utilizing our dataset and report baseline scores
from various state-of-the-art summarization techniques in a multi-lingual
setting. We also compare it with many similar datasets to analyze the
uniqueness and difficulty of M3LS.",2023-02-13
Plan-then-Seam: Towards Efficient Table-to-Text Generation,2023-02-10 09:43:15+00:00,http://arxiv.org/abs/2302.05138v1,"Liang Li, Ruiying Geng, Chengyang Fang, Bing Li, Can Ma, Binhua Li, Yongbin Li",cs.CL,image2text,"Table-to-text generation aims at automatically generating text to help people
conveniently obtain salient information in tables. Recent works explicitly
decompose the generation process into content planning and surface generation
stages, employing two autoregressive networks for them respectively. However,
they are computationally expensive due to the non-parallelizable nature of
autoregressive decoding and the redundant parameters of two networks. In this
paper, we propose the first totally non-autoregressive table-to-text model
(Plan-then-Seam, PTS) that produces its outputs in parallel with one single
network. PTS firstly writes and calibrates one plan of the content to be
generated with a novel rethinking pointer predictor, and then takes the plan as
the context for seaming to decode the description. These two steps share
parameters and perform iteratively to capture token inter-dependency while
keeping parallel decoding. Experiments on two public benchmarks show that PTS
achieves 3.0~5.6 times speedup for inference time, reducing 50% parameters,
while maintaining as least comparable performance against strong two-stage
table-to-text competitors.",2023-02-10
"Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot
  Image Captioning",2023-02-09 18:57:56+00:00,http://arxiv.org/abs/2302.04858v1,"Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhiding Yu, Shiyi Lan, Bo Li, Ming-Yu Liu, Yuke Zhu, Mohammad Shoeybi, Bryan Catanzaro, Chaowei Xiao, Anima Anandkumar","cs.CV, cs.AI, cs.CL, cs.IR, cs.LG",image2text,"Augmenting pretrained language models (LMs) with a vision encoder (e.g.,
Flamingo) has obtained state-of-the-art results in image-to-text generation.
However, these models store all the knowledge within their parameters, thus
often requiring enormous model parameters to model the abundant visual concepts
and very rich textual descriptions. Additionally, they are inefficient in
incorporating new data, requiring a computational-expensive fine-tuning
process. In this work, we introduce a Retrieval-augmented Visual Language
Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant
knowledge from the external database for zero and in-context few-shot
image-to-text generations. By storing certain knowledge explicitly in the
external database, our approach reduces the number of model parameters and can
easily accommodate new data during evaluation by simply updating the database.
We also construct an interleaved image and text data that facilitates
in-context few-shot learning capabilities. We demonstrate that Re-ViLM
significantly boosts performance for image-to-text generation tasks, especially
for zero-shot and few-shot generation in out-of-domain settings with 4 times
less parameters compared with baseline methods.",2023-02-09
"Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge
  Memorization",2023-02-09 03:04:11+00:00,http://arxiv.org/abs/2302.04415v1,"Zhixin Guo, Minyxuan Yan, Jiexing Qi, Jianping Zhou, Ziwei He, Zhouhan Lin, Guanjie Zheng, Xinbing Wang","cs.CL, cs.AI",image2text,"Pre-trained language models (PLM) have achieved remarkable advancement in
table-to-text generation tasks. However, the lack of labeled domain-specific
knowledge and the topology gap between tabular data and text make it difficult
for PLMs to yield faithful text. Low-resource generation likewise faces unique
challenges in this domain. Inspired by how humans descript tabular data with
prior knowledge, we suggest a new framework: PromptMize, which targets
table-to-text generation under few-shot settings. The design of our framework
consists of two aspects: a prompt planner and a knowledge adapter. The prompt
planner aims to generate a prompt signal that provides instance guidance for
PLMs to bridge the topology gap between tabular data and text. Moreover, the
knowledge adapter memorizes domain-specific knowledge from the unlabelled
corpus to supply essential information during generation. Extensive experiments
and analyses are investigated on three open domain few-shot NLG datasets:
human, song, and book. Compared with previous state-of-the-art approaches, our
model achieves remarkable performance in generating quality as judged by human
and automatic evaluations.",2023-02-09
Adversarial Prompting for Black Box Foundation Models,2023-02-08 18:07:31+00:00,http://arxiv.org/abs/2302.04237v1,"Natalie Maus, Patrick Chao, Eric Wong, Jacob Gardner",cs.LG,image2text,"Prompting interfaces allow users to quickly adjust the output of generative
models in both vision and language. However, small changes and design choices
in the prompt can lead to significant differences in the output. In this work,
we develop a black-box framework for generating adversarial prompts for
unstructured image and text generation. These prompts, which can be standalone
or prepended to benign prompts, induce specific behaviors into the generative
process, such as generating images of a particular object or biasing the
frequency of specific letters in the generated text.",2023-02-08
GPTScore: Evaluate as You Desire,2023-02-08 16:17:29+00:00,http://arxiv.org/abs/2302.04166v1,"Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu",cs.CL,image2text,"Generative Artificial Intelligence (AI) has enabled the development of
sophisticated models that are capable of producing high-caliber text, images,
and other outputs through the utilization of large pre-trained models.
Nevertheless, assessing the quality of the generation is an even more arduous
task than the generation itself, and this issue has not been given adequate
consideration recently. This paper proposes a novel evaluation framework,
GPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction)
from generative pre-trained models to score generated texts. Experimental
results on four text generation tasks, 22 evaluation aspects, and corresponding
37 datasets demonstrate that this approach can effectively allow us to achieve
what one desires to evaluate for texts simply by natural language instructions.
This nature helps us overcome several long-standing challenges in text
evaluation--how to achieve customized, multi-faceted evaluation without the
need for annotated samples. We make our code publicly available at
https://github.com/jinlanfu/GPTScore.",2023-02-08
Grounding Language Models to Images for Multimodal Generation,2023-01-31 18:33:44+00:00,http://arxiv.org/abs/2301.13823v1,"Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried","cs.CL, cs.AI, cs.CV, cs.LG",image2text,"We propose an efficient method to ground pretrained text-only language models
to the visual domain, enabling them to process and generate arbitrarily
interleaved image-and-text data. Our method leverages the abilities of language
models learnt from large scale text-only pretraining, such as in-context
learning and free-form text generation. We keep the language model frozen, and
finetune input and output linear layers to enable cross-modality interactions.
This allows our model to process arbitrarily interleaved image-and-text inputs,
and generate free-form text interleaved with retrieved images. We achieve
strong zero-shot performance on grounded tasks such as contextual image
retrieval and multimodal dialogue, and showcase compelling interactive
abilities. Our approach works with any off-the-shelf language model and paves
the way towards an effective, general solution for leveraging pretrained
language models in visually grounded settings.",2023-01-31
Semi-Parametric Video-Grounded Text Generation,2023-01-27 03:00:43+00:00,http://arxiv.org/abs/2301.11507v1,"Sungdong Kim, Jin-Hwa Kim, Jiyoung Lee, Minjoon Seo","cs.CV, cs.CL, cs.LG",image2text,"Efficient video-language modeling should consider the computational cost
because of a large, sometimes intractable, number of video frames. Parametric
approaches such as the attention mechanism may not be ideal since its
computational cost quadratically increases as the video length increases.
Rather, previous studies have relied on offline feature extraction or frame
sampling to represent the video efficiently, focusing on cross-modal modeling
in short video clips. In this paper, we propose a semi-parametric
video-grounded text generation model, SeViT, a novel perspective on scalable
video-language modeling toward long untrimmed videos. Treating a video as an
external data store, SeViT includes a non-parametric frame retriever to select
a few query-relevant frames from the data store for a given query and a
parametric generator to effectively aggregate the frames with the query via
late fusion methods. Experimental results demonstrate our method has a
significant advantage in longer videos and causal video understanding.
Moreover, our model achieves the new state of the art on four video-language
datasets, iVQA (+4.8), Next-QA (+6.9), and Activitynet-QA (+4.8) in accuracy,
and MSRVTT-Caption (+3.6) in CIDEr.",2023-01-27
Explaining Visual Biases as Words by Generating Captions,2023-01-26 13:58:46+00:00,http://arxiv.org/abs/2301.11104v1,"Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, Jinwoo Shin","cs.LG, cs.CV",image2text,"We aim to diagnose the potential biases in image classifiers. To this end,
prior works manually labeled biased attributes or visualized biased features,
which need high annotation costs or are often ambiguous to interpret. Instead,
we leverage two types (generative and discriminative) of pre-trained
vision-language models to describe the visual bias as a word. Specifically, we
propose bias-to-text (B2T), which generates captions of the mispredicted images
using a pre-trained captioning model to extract the common keywords that may
describe visual biases. Then, we categorize the bias type as spurious
correlation or majority bias by checking if it is specific or agnostic to the
class, based on the similarity of class-wise mispredicted images and the
keyword upon a pre-trained vision-language joint embedding space, e.g., CLIP.
We demonstrate that the proposed simple and intuitive scheme can recover
well-known gender and background biases, and discover novel ones in real-world
datasets. Moreover, we utilize B2T to compare the classifiers using different
architectures or training methods. Finally, we show that one can obtain
debiased classifiers using the B2T bias keywords and CLIP, in both zero-shot
and full-shot manners, without using any human annotation on the bias.",2023-01-26
MTTN: Multi-Pair Text to Text Narratives for Prompt Generation,2023-01-21 06:55:44+00:00,http://arxiv.org/abs/2301.10172v1,"Archan Ghosh, Debgandhar Ghosh, Madhurima Maji, Suchinta Chanda, Kalporup Goswami","cs.CL, cs.LG",image2text,"The explosive popularity of diffusion models[ 1][ 2][ 3 ] has provided a huge
stage for further development in generative-text modelling. As prompt based
models are very nuanced, such that a carefully generated prompt can produce
truely breath taking images, on the contrary producing powerful or even
meaningful prompt is a hit or a miss. To lavish on this we have introduced a
large scale derived and synthesized dataset built with on real prompts and
indexed with popular image-text datasets like MS-COCO[4 ], Flickr[ 5], etc. We
have also introduced staging for these sentences that sequentially reduce the
context and increase the complexity, that will further strengthen the output
because of the complex annotations that are being created. MTTN consists of
over 2.4M sentences that are divided over 5 stages creating a combination
amounting to over 12M pairs, along with a vocab size of consisting more than
300 thousands unique words that creates an abundance of variations. The
original 2.4M million pairs are broken down in such a manner that it produces a
true scenario of internet lingo that is used globally thereby heightening the
robustness of the dataset, and any model trained on it.",2023-01-21
Regeneration Learning: A Learning Paradigm for Data Generation,2023-01-21 01:33:34+00:00,http://arxiv.org/abs/2301.08846v1,"Xu Tan, Tao Qin, Jiang Bian, Tie-Yan Liu, Yoshua Bengio","cs.LG, cs.AI, cs.CL, cs.CV, eess.AS",image2text,"Machine learning methods for conditional data generation usually build a
mapping from source conditional data X to target data Y. The target Y (e.g.,
text, speech, music, image, video) is usually high-dimensional and complex, and
contains information that does not exist in source data, which hinders
effective and efficient learning on the source-target mapping. In this paper,
we present a learning paradigm called regeneration learning for data
generation, which first generates Y' (an abstraction/representation of Y) from
X and then generates Y from Y'. During training, Y' is obtained from Y through
either handcrafted rules or self-supervised learning and is used to learn
X-->Y' and Y'-->Y. Regeneration learning extends the concept of representation
learning to data generation tasks, and can be regarded as a counterpart of
traditional representation learning, since 1) regeneration learning handles the
abstraction (Y') of the target data Y for data generation while traditional
representation learning handles the abstraction (X') of source data X for data
understanding; 2) both the processes of Y'-->Y in regeneration learning and
X-->X' in representation learning can be learned in a self-supervised way
(e.g., pre-training); 3) both the mappings from X to Y' in regeneration
learning and from X' to Y in representation learning are simpler than the
direct mapping from X to Y. We show that regeneration learning can be a
widely-used paradigm for data generation (e.g., text generation, speech
recognition, speech synthesis, music composition, image generation, and video
generation) and can provide valuable insights into developing data generation
methods.",2023-01-21
Universal Multimodal Representation for Language Understanding,2023-01-09 13:54:11+00:00,http://arxiv.org/abs/2301.03344v1,"Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao Li, Hai Zhao","cs.CL, cs.AI, cs.CV",image2text,"Representation learning is the foundation of natural language processing
(NLP). This work presents new methods to employ visual information as assistant
signals to general NLP tasks. For each sentence, we first retrieve a flexible
number of images either from a light topic-image lookup table extracted over
the existing sentence-image pairs or a shared cross-modal embedding space that
is pre-trained on out-of-shelf text-image pairs. Then, the text and images are
encoded by a Transformer encoder and convolutional neural network,
respectively. The two sequences of representations are further fused by an
attention layer for the interaction of the two modalities. In this study, the
retrieval process is controllable and flexible. The universal visual
representation overcomes the lack of large-scale bilingual sentence-image
pairs. Our method can be easily applied to text-only tasks without manually
annotated multimodal parallel corpora. We apply the proposed method to a wide
range of natural language generation and understanding tasks, including neural
machine translation, natural language inference, and semantic similarity.
Experimental results show that our method is generally effective for different
tasks and languages. Analysis indicates that the visual signals enrich textual
representations of content words, provide fine-grained grounding information
about the relationship between concepts and events, and potentially conduce to
disambiguation.",2023-01-09
"An Image captioning algorithm based on the Hybrid Deep Learning
  Technique (CNN+GRU)",2023-01-06 10:00:06+00:00,http://arxiv.org/abs/2301.02440v1,"Rana Adnan Ahmad, Muhammad Azhar, Hina Sattar","cs.CV, cs.AI",image2text,"Image captioning by the encoder-decoder framework has shown tremendous
advancement in the last decade where CNN is mainly used as encoder and LSTM is
used as a decoder. Despite such an impressive achievement in terms of accuracy
in simple images, it lacks in terms of time complexity and space complexity
efficiency. In addition to this, in case of complex images with a lot of
information and objects, the performance of this CNN-LSTM pair downgraded
exponentially due to the lack of semantic understanding of the scenes presented
in the images. Thus, to take these issues into consideration, we present
CNN-GRU encoder decode framework for caption-to-image reconstructor to handle
the semantic context into consideration as well as the time complexity. By
taking the hidden states of the decoder into consideration, the input image and
its similar semantic representations is reconstructed and reconstruction scores
from a semantic reconstructor are used in conjunction with likelihood during
model training to assess the quality of the generated caption. As a result, the
decoder receives improved semantic information, enhancing the caption
production process. During model testing, combining the reconstruction score
and the log-likelihood is also feasible to choose the most appropriate caption.
The suggested model outperforms the state-of-the-art LSTM-A5 model for picture
captioning in terms of time complexity and accuracy.",2023-01-06
"Towards Table-to-Text Generation with Pretrained Language Model: A Table
  Structure Understanding and Text Deliberating Approach",2023-01-05 14:03:26+00:00,http://arxiv.org/abs/2301.02071v1,"Miao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Jingbo Zhou, Dejing Dou, Hui Xiong","cs.CL, cs.AI",image2text,"Although remarkable progress on the neural table-to-text methods has been
made, the generalization issues hinder the applicability of these models due to
the limited source tables. Large-scale pretrained language models sound like a
promising solution to tackle such issues. However, how to effectively bridge
the gap between the structured table and the text input by fully leveraging
table information to fuel the pretrained model is still not well explored.
Besides, another challenge of integrating the deliberation mechanism into the
text-to-text pretrained model for solving the table-to-text task remains seldom
studied. In this paper, to implement the table-to-text generation with
pretrained language model, we propose a table structure understanding and text
deliberating approach, namely TASD. Specifically, we devise a three-layered
multi-head attention network to realize the table-structure-aware text
generation model with the help of the pretrained language model. Furthermore, a
multi-pass decoder framework is adopted to enhance the capability of polishing
generated text for table descriptions. The empirical studies, as well as human
evaluation, on two public datasets, validate that our approach can generate
faithful and fluent descriptive texts for different types of tables.",2023-01-05
eVAE: Evolutionary Variational Autoencoder,2023-01-01 23:54:35+00:00,http://arxiv.org/abs/2301.00011v1,"Zhangkai Wu, Longbing Cao, Lei Qi","cs.NE, cs.LG",image2text,"The surrogate loss of variational autoencoders (VAEs) poses various
challenges to their training, inducing the imbalance between task fitting and
representation inference. To avert this, the existing strategies for VAEs focus
on adjusting the tradeoff by introducing hyperparameters, deriving a tighter
bound under some mild assumptions, or decomposing the loss components per
certain neural settings. VAEs still suffer from uncertain tradeoff learning.We
propose a novel evolutionary variational autoencoder (eVAE) building on the
variational information bottleneck (VIB) theory and integrative evolutionary
neural learning. eVAE integrates a variational genetic algorithm into VAE with
variational evolutionary operators including variational mutation, crossover,
and evolution. Its inner-outer-joint training mechanism synergistically and
dynamically generates and updates the uncertain tradeoff learning in the
evidence lower bound (ELBO) without additional constraints. Apart from learning
a lossy compression and representation of data under the VIB assumption, eVAE
presents an evolutionary paradigm to tune critical factors of VAEs and deep
neural networks and addresses the premature convergence and random search
problem by integrating evolutionary optimization into deep learning.
Experiments show that eVAE addresses the KL-vanishing problem for text
generation with low reconstruction loss, generates all disentangled factors
with sharp images, and improves the image generation quality,respectively. eVAE
achieves better reconstruction loss, disentanglement, and generation-inference
balance than its competitors.",2023-01-01
MAUVE Scores for Generative Models: Theory and Practice,2022-12-30 07:37:40+00:00,http://arxiv.org/abs/2212.14578v1,"Krishna Pillutla, Lang Liu, John Thickstun, Sean Welleck, Swabha Swayamdipta, Rowan Zellers, Sewoong Oh, Yejin Choi, Zaid Harchaoui","cs.LG, cs.AI, cs.CL",image2text,"Generative AI has matured to a point where large-scale models can generate
text that seems indistinguishable from human-written text and remarkably
photorealistic images. Automatically measuring how close the distribution of
generated data is to the target real data distribution is a key step in
diagnosing existing models and developing better models. We present MAUVE, a
family of comparison measures between pairs of distributions such as those
encountered in the generative modeling of text or images. These scores are
statistical summaries of divergence frontiers capturing two types of errors in
generative modeling. We explore four approaches to statistically estimate these
scores: vector quantization, non-parametric estimation, classifier-based
estimation, and parametric Gaussian approximations. We provide statistical
bounds for the vector quantization approach. Empirically, we find that the
proposed scores paired with a range of $f$-divergences and statistical
estimation methods can quantify the gaps between the distributions of
human-written text and those of modern neural language models by correlating
with human judgments and identifying known properties of the generated texts.
We conclude the paper by demonstrating its applications to other AI domains and
discussing practical recommendations.",2022-12-30
"Noise-aware Learning from Web-crawled Image-Text Data for Image
  Captioning",2022-12-27 17:33:40+00:00,http://arxiv.org/abs/2212.13563v1,"Wooyoung Kang, Jonghwan Mun, Sungjun Lee, Byungseok Roh","cs.CV, cs.AI",image2text,"Image captioning is one of the straightforward tasks that can take advantage
of large-scale web-crawled data which provides rich knowledge about the visual
world for a captioning model. However, since web-crawled data contains
image-text pairs that are aligned at different levels, the inherent noises
(e.g., misaligned pairs) make it difficult to learn a precise captioning model.
While the filtering strategy can effectively remove noisy data, however, it
leads to a decrease in learnable knowledge and sometimes brings about a new
problem of data deficiency. To take the best of both worlds, we propose a
noise-aware learning framework, which learns rich knowledge from the whole
web-crawled data while being less affected by the noises. This is achieved by
the proposed quality controllable model, which is learned using alignment
levels of the image-text pairs as an additional control signal during training.
The alignment-conditioned training allows the model to generate high-quality
captions of well-aligned by simply setting the control signal to desired
alignment level at inference time. Through in-depth analysis, we show that our
controllable captioning model is effective in handling noise. In addition, with
two tasks of zero-shot captioning and text-to-image retrieval using generated
captions (i.e., self-retrieval), we also demonstrate our model can produce
high-quality captions in terms of descriptiveness and distinctiveness. Code is
available at \url{https://github.com/kakaobrain/noc}.",2022-12-27
"On Realization of Intelligent Decision-Making in the Real World: A
  Foundation Decision Model Perspective",2022-12-24 06:16:45+00:00,http://arxiv.org/abs/2212.12669v1,"Ying Wen, Ziyu Wan, Ming Zhou, Shufang Hou, Zhe Cao, Chenyang Le, Jingxiao Chen, Zheng Tian, Weinan Zhang, Jun Wang","cs.AI, cs.LG",image2text,"Our situated environment is full of uncertainty and highly dynamic, thus
hindering the widespread adoption of machine-led Intelligent Decision-Making
(IDM) in real world scenarios. This means IDM should have the capability of
continuously learning new skills and efficiently generalizing across wider
applications. IDM benefits from any new approaches and theoretical
breakthroughs that exhibit Artificial General Intelligence (AGI) breaking the
barriers between tasks and applications. Recent research has well-examined
neural architecture, Transformer, as a backbone foundation model and its
generalization to various tasks, including computer vision, natural language
processing, and reinforcement learning. We therefore argue that a foundation
decision model (FDM) can be established by formulating various decision-making
tasks as a sequence decoding task using the Transformer architecture; this
would be a promising solution to advance the applications of IDM in more
complex real world tasks. In this paper, we elaborate on how a foundation
decision model improves the efficiency and generalization of IDM. We also
discuss potential applications of a FDM in multi-agent game AI, production
scheduling, and robotics tasks. Finally, through a case study, we demonstrate
our realization of the FDM, DigitalBrain (DB1) with 1.2 billion parameters,
which achieves human-level performance over 453 tasks, including text
generation, images caption, video games playing, robotic control, and traveling
salesman problems. As a foundation decision model, DB1 would be a baby step
towards more autonomous and efficient real world IDM applications.",2022-12-24
Do DALL-E and Flamingo Understand Each Other?,2022-12-23 10:46:56+00:00,http://arxiv.org/abs/2212.12249v1,"Hang Li, Jindong Gu, Rajat Koner, Sahand Sharifzadeh, Volker Tresp","cs.CV, cs.LG",image2text,"A major goal of multimodal research is to improve machine understanding of
images and text. Tasks include image captioning, text-to-image generation, and
vision-language representation learning. So far, research has focused on the
relationships between images and text. For example, captioning models attempt
to understand the semantics of images which are then transformed into text. An
important question is: which annotation reflects best a deep understanding of
image content? Similarly, given a text, what is the best image that can present
the semantics of the text? In this work, we argue that the best text or caption
for a given image is the text which would generate the image which is the most
similar to that image. Likewise, the best image for a given text is the image
that results in the caption which is best aligned with the original text. To
this end, we propose a unified framework that includes both a text-to-image
generative model and an image-to-text generative model. Extensive experiments
validate our approach.",2022-12-23
A survey on text generation using generative adversarial networks,2022-12-20 17:54:08+00:00,http://arxiv.org/abs/2212.11119v1,"Gustavo Henrique de Rosa, João Paulo Papa","cs.CL, cs.AI, cs.LG",image2text,"This work presents a thorough review concerning recent studies and text
generation advancements using Generative Adversarial Networks. The usage of
adversarial learning for text generation is promising as it provides
alternatives to generate the so-called ""natural"" language. Nevertheless,
adversarial text generation is not a simple task as its foremost architecture,
the Generative Adversarial Networks, were designed to cope with continuous
information (image) instead of discrete data (text). Thus, most works are based
on three possible options, i.e., Gumbel-Softmax differentiation, Reinforcement
Learning, and modified training objectives. All alternatives are reviewed in
this survey as they present the most recent approaches for generating text
using adversarial-based techniques. The selected works were taken from renowned
databases, such as Science Direct, IEEEXplore, Springer, Association for
Computing Machinery, and arXiv, whereas each selected work has been critically
analyzed and assessed to present its objective, methodology, and experimental
results.",2022-12-20
SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers,2022-12-20 15:16:24+00:00,http://arxiv.org/abs/2212.10325v1,"Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang",cs.CL,image2text,"Diffusion model, a new generative modelling paradigm, has achieved great
success in image, audio, and video generation. However, considering the
discrete categorical nature of text, it is not trivial to extend continuous
diffusion models to natural language, and text diffusion models are less
studied. Sequence-to-sequence text generation is one of the essential natural
language processing topics. In this work, we apply diffusion models to approach
sequence-to-sequence text generation, and explore whether the superiority
generation performance of diffusion model can transfer to natural language
domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence
generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to
model denoising function. In order to improve generation quality, SeqDiffuSeq
combines the self-conditioning technique and a newly proposed adaptive noise
schedule technique. The adaptive noise schedule has the difficulty of denoising
evenly distributed across time steps, and considers exclusive noise schedules
for tokens at different positional order. Experiment results illustrate the
good performance on sequence-to-sequence generation in terms of text quality
and inference time.",2022-12-20
"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",2022-12-19 18:57:05+00:00,http://arxiv.org/abs/2212.09741v2,"Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu",cs.CL,image2text,"We introduce INSTRUCTOR, a new method for computing text embeddings given
task instructions: every text input is embedded together with instructions
explaining the use case (e.g., task and domain descriptions). Unlike encoders
from prior work that are more specialized, INSTRUCTOR is a single embedder that
can generate text embeddings tailored to different downstream tasks and
domains, without any further training. We first annotate instructions for 330
diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive
loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are
unseen during training), ranging from classification and information retrieval
to semantic textual similarity and text generation evaluation. INSTRUCTOR,
while having an order of magnitude fewer parameters than the previous best
model, achieves state-of-the-art performance, with an average improvement of
3.4% compared to the previous best results on the 70 diverse datasets. Our
analysis suggests that INSTRUCTOR is robust to changes in instructions, and
that instruction finetuning mitigates the challenge of training a single model
on diverse datasets. Our model, code, and data are available at
https://instructor-embedding.github.io.",2022-12-19
"Switching to Discriminative Image Captioning by Relieving a Bottleneck
  of Reinforcement Learning",2022-12-06 18:55:20+00:00,http://arxiv.org/abs/2212.03230v1,"Ukyo Honda, Taro Watanabe, Yuji Matsumoto","cs.CV, cs.CL",image2text,"Discriminativeness is a desirable feature of image captions: captions should
describe the characteristic details of input images. However, recent
high-performing captioning models, which are trained with reinforcement
learning (RL), tend to generate overly generic captions despite their high
performance in various other criteria. First, we investigate the cause of the
unexpectedly low discriminativeness and show that RL has a deeply rooted side
effect of limiting the output words to high-frequency words. The limited
vocabulary is a severe bottleneck for discriminativeness as it is difficult for
a model to describe the details beyond its vocabulary. Then, based on this
identification of the bottleneck, we drastically recast discriminative image
captioning as a much simpler task of encouraging low-frequency word generation.
Hinted by long-tail classification and debiasing methods, we propose methods
that easily switch off-the-shelf RL models to discriminativeness-aware models
with only a single-epoch fine-tuning on the part of the parameters. Extensive
experiments demonstrate that our methods significantly enhance the
discriminativeness of off-the-shelf RL models and even outperform previous
discriminativeness-aware methods with much smaller computational costs.
Detailed analysis and human evaluation also verify that our methods boost the
discriminativeness without sacrificing the overall quality of captions.",2022-12-06
Towards Generating Diverse Audio Captions via Adversarial Training,2022-12-05 05:06:19+00:00,http://arxiv.org/abs/2212.02033v1,"Xinhao Mei, Xubo Liu, Jianyuan Sun, Mark D. Plumbley, Wenwu Wang","eess.AS, cs.AI, cs.MM, cs.SD",image2text,"Automated audio captioning is a cross-modal translation task for describing
the content of audio clips with natural language sentences. This task has
attracted increasing attention and substantial progress has been made in recent
years. Captions generated by existing models are generally faithful to the
content of audio clips, however, these machine-generated captions are often
deterministic (e.g., generating a fixed caption for a given audio clip), simple
(e.g., using common words and simple grammar), and generic (e.g., generating
the same caption for similar audio clips). When people are asked to describe
the content of an audio clip, different people tend to focus on different sound
events and describe an audio clip diversely from various aspects using distinct
words and grammar. We believe that an audio captioning system should have the
ability to generate diverse captions, either for a fixed audio clip, or across
similar audio clips. To this end, we propose an adversarial training framework
based on a conditional generative adversarial network (C-GAN) to improve
diversity of audio captioning systems. A caption generator and two hybrid
discriminators compete and are learned jointly, where the caption generator can
be any standard encoder-decoder captioning model used to generate captions, and
the hybrid discriminators assess the generated captions from different
criteria, such as their naturalness and semantics. We conduct experiments on
the Clotho dataset. The results show that our proposed model can generate
captions with better diversity as compared to state-of-the-art methods.",2022-12-05
Grounded Keys-to-Text Generation: Towards Factual Open-Ended Generation,2022-12-04 23:59:41+00:00,http://arxiv.org/abs/2212.01956v1,"Faeze Brahman, Baolin Peng, Michel Galley, Sudha Rao, Bill Dolan, Snigdha Chaturvedi, Jianfeng Gao",cs.CL,image2text,"Large pre-trained language models have recently enabled open-ended generation
frameworks (e.g., prompt-to-text NLG) to tackle a variety of tasks going beyond
the traditional data-to-text generation. While this framework is more general,
it is under-specified and often leads to a lack of controllability restricting
their real-world usage. We propose a new grounded keys-to-text generation task:
the task is to generate a factual description about an entity given a set of
guiding keys, and grounding passages. To address this task, we introduce a new
dataset, called EntDeGen. Inspired by recent QA-based evaluation measures, we
propose an automatic metric, MAFE, for factual correctness of generated
descriptions. Our EntDescriptor model is equipped with strong rankers to fetch
helpful passages and generate entity descriptions. Experimental result shows a
good correlation (60.14) between our proposed metric and human judgments of
factuality. Our rankers significantly improved the factual correctness of
generated descriptions (15.95% and 34.51% relative gains in recall and
precision). Finally, our ablation study highlights the benefit of combining
keys and groundings.",2022-12-04
"Learning Automata-Based Task Knowledge Representation from Large-Scale
  Generative Language Models",2022-12-04 22:34:16+00:00,http://arxiv.org/abs/2212.01944v1,"Yunhao Yang, Jean-Raphaël Gaglione, Ufuk Topcu","cs.FL, cs.CL",image2text,"Automata-based representations play an important role in control and planning
in sequential decision-making, but obtaining high-level task knowledge for
building automata is often difficult. Although large-scale generative language
models (GLMs) can help automatically distill task knowledge, the textual
outputs from GLMs are not directly utilizable in sequential decision-making. We
resolve this problem by proposing a novel algorithm named GLM2FSA, which
obtains high-level task knowledge, represented in a finite state automaton
(FSA), from a given brief description of the task goal. GLM2FSA sends queries
to a GLM for task knowledge in textual form and then builds a FSA to represent
the textual knowledge. This algorithm fills the gap between text and
automata-based representations, and the constructed FSA can be directly
utilized in sequential decision-making. We provide examples to demonstrate how
GLM2FSA constructs FSAs to represent knowledge encoded in the texts generated
by the large-scale GLMs.",2022-12-04
3D-TOGO: Towards Text-Guided Cross-Category 3D Object Generation,2022-12-02 11:31:49+00:00,http://arxiv.org/abs/2212.01103v1,"Zutao Jiang, Guangsong Lu, Xiaodan Liang, Jihua Zhu, Wei Zhang, Xiaojun Chang, Hang Xu","cs.CV, cs.AI",image2text,"Text-guided 3D object generation aims to generate 3D objects described by
user-defined captions, which paves a flexible way to visualize what we
imagined. Although some works have been devoted to solving this challenging
task, these works either utilize some explicit 3D representations (e.g., mesh),
which lack texture and require post-processing for rendering photo-realistic
views; or require individual time-consuming optimization for every single case.
Here, we make the first attempt to achieve generic text-guided cross-category
3D object generation via a new 3D-TOGO model, which integrates a text-to-views
generation module and a views-to-3D generation module. The text-to-views
generation module is designed to generate different views of the target 3D
object given an input caption. prior-guidance, caption-guidance and view
contrastive learning are proposed for achieving better view-consistency and
caption similarity. Meanwhile, a pixelNeRF model is adopted for the views-to-3D
generation module to obtain the implicit 3D neural representation from the
previously-generated views. Our 3D-TOGO model generates 3D objects in the form
of the neural radiance field with good texture and requires no time-cost
optimization for every single caption. Besides, 3D-TOGO can control the
category, color and shape of generated 3D objects with the input caption.
Extensive experiments on the largest 3D object dataset (i.e., ABO) are
conducted to verify that 3D-TOGO can better generate high-quality 3D objects
according to the input captions across 98 different categories, in terms of
PSNR, SSIM, LPIPS and CLIP-score, compared with text-NeRF and Dreamfields.",2022-12-02
"On the Importance of Image Encoding in Automated Chest X-Ray Report
  Generation",2022-11-24 08:02:52+00:00,http://arxiv.org/abs/2211.13465v1,"Otabek Nazarov, Mohammad Yaqub, Karthik Nandakumar","cs.CV, cs.AI",image2text,"Chest X-ray is one of the most popular medical imaging modalities due to its
accessibility and effectiveness. However, there is a chronic shortage of
well-trained radiologists who can interpret these images and diagnose the
patient's condition. Therefore, automated radiology report generation can be a
very helpful tool in clinical practice. A typical report generation workflow
consists of two main steps: (i) encoding the image into a latent space and (ii)
generating the text of the report based on the latent image embedding. Many
existing report generation techniques use a standard convolutional neural
network (CNN) architecture for image encoding followed by a Transformer-based
decoder for medical text generation. In most cases, CNN and the decoder are
trained jointly in an end-to-end fashion. In this work, we primarily focus on
understanding the relative importance of encoder and decoder components.
Towards this end, we analyze four different image encoding approaches: direct,
fine-grained, CLIP-based, and Cluster-CLIP-based encodings in conjunction with
three different decoders on the large-scale MIMIC-CXR dataset. Among these
encoders, the cluster CLIP visual encoder is a novel approach that aims to
generate more discriminative and explainable representations. CLIP-based
encoders produce comparable results to traditional CNN-based encoders in terms
of NLP metrics, while fine-grained encoding outperforms all other encoders both
in terms of NLP and clinical accuracy metrics, thereby validating the
importance of image encoder to effectively extract semantic information. GitHub
repository: https://github.com/mudabek/encoding-cxr-report-gen",2022-11-24
Retrieval-Augmented Multimodal Language Modeling,2022-11-22 20:26:44+00:00,http://arxiv.org/abs/2211.12561v1,"Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, Wen-tau Yih","cs.CV, cs.CL, cs.LG",image2text,"Recent multimodal models such as DALL-E and CM3 have achieved remarkable
progress in text-to-image and image-to-text generation. However, these models
store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the
model parameters, requiring increasingly larger models and training data to
capture more knowledge. To integrate knowledge in a more scalable and modular
way, we propose a retrieval-augmented multimodal model, which enables a base
multimodal model (generator) to refer to relevant knowledge fetched by a
retriever from external memory (e.g., multimodal documents on the web).
Specifically, we implement a retriever using the pretrained CLIP model and a
generator using the CM3 Transformer architecture, and train this model using
the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3),
is the first multimodal model that can retrieve and generate mixtures of text
and images. We show that RA-CM3 significantly outperforms baseline multimodal
models such as DALL-E and CM3 on both image and caption generation tasks (12
FID and 17 CIDEr improvements on MS-COCO), while requiring much less compute
for training (<30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel
capabilities such as knowledge-intensive image generation and multimodal
in-context learning.",2022-11-22
"Aligning Source Visual and Target Language Domains for Unpaired Video
  Captioning",2022-11-22 10:26:26+00:00,http://arxiv.org/abs/2211.12148v1,"Fenglin Liu, Xian Wu, Chenyu You, Shen Ge, Yuexian Zou, Xu Sun","cs.CV, cs.LG",image2text,"Training supervised video captioning model requires coupled video-caption
pairs. However, for many targeted languages, sufficient paired data are not
available. To this end, we introduce the unpaired video captioning task aiming
to train models without coupled video-caption pairs in target language. To
solve the task, a natural choice is to employ a two-step pipeline system: first
utilizing video-to-pivot captioning model to generate captions in pivot
language and then utilizing pivot-to-target translation model to translate the
pivot captions to the target language. However, in such a pipeline system, 1)
visual information cannot reach the translation model, generating visual
irrelevant target captions; 2) the errors in the generated pivot captions will
be propagated to the translation model, resulting in disfluent target captions.
To address these problems, we propose the Unpaired Video Captioning with Visual
Injection system (UVC-VI). UVC-VI first introduces the Visual Injection Module
(VIM), which aligns source visual and target language domains to inject the
source visual information into the target language domain. Meanwhile, VIM
directly connects the encoder of the video-to-pivot model and the decoder of
the pivot-to-target model, allowing end-to-end inference by completely skipping
the generation of pivot captions. To enhance the cross-modality injection of
the VIM, UVC-VI further introduces a pluggable video encoder, i.e., Multimodal
Collaborative Encoder (MCE). The experiments show that UVC-VI outperforms
pipeline systems and exceeds several supervised systems. Furthermore, equipping
existing supervised systems with our MCE can achieve 4% and 7% relative margins
on the CIDEr scores to current state-of-the-art models on the benchmark MSVD
and MSR-VTT datasets, respectively.",2022-11-22
"How to Describe Images in a More Funny Way? Towards a Modular Approach
  to Cross-Modal Sarcasm Generation",2022-11-20 14:38:24+00:00,http://arxiv.org/abs/2211.10992v1,"Jie Ruan, Yue Wu, Xiaojun Wan, Yuesheng Zhu","cs.CV, cs.CL",image2text,"Sarcasm generation has been investigated in previous studies by considering
it as a text-to-text generation problem, i.e., generating a sarcastic sentence
for an input sentence. In this paper, we study a new problem of cross-modal
sarcasm generation (CMSG), i.e., generating a sarcastic description for a given
image. CMSG is challenging as models need to satisfy the characteristics of
sarcasm, as well as the correlation between different modalities. In addition,
there should be some inconsistency between the two modalities, which requires
imagination. Moreover, high-quality training data is insufficient. To address
these problems, we take a step toward generating sarcastic descriptions from
images without paired training data and propose an
Extraction-Generation-Ranking based Modular method (EGRM) for cross-model
sarcasm generation. Specifically, EGRM first extracts diverse information from
an image at different levels and uses the obtained image tags, sentimental
descriptive caption, and commonsense-based consequence to generate candidate
sarcastic texts. Then, a comprehensive ranking algorithm, which considers
image-text relation, sarcasticness, and grammaticality, is proposed to select a
final text from the candidate texts. Human evaluation at five criteria on a
total of 1200 generated image-text pairs from eight systems and auxiliary
automatic evaluation show the superiority of our method.",2022-11-20
"Feedback is Needed for Retakes: An Explainable Poor Image Notification
  Framework for the Visually Impaired",2022-11-17 09:22:28+00:00,http://arxiv.org/abs/2211.09427v1,"Kazuya Ohata, Shunsuke Kitada, Hitoshi Iyatomi","cs.CV, cs.AI, cs.CL, cs.HC, cs.LG",image2text,"We propose a simple yet effective image captioning framework that can
determine the quality of an image and notify the user of the reasons for any
flaws in the image. Our framework first determines the quality of images and
then generates captions using only those images that are determined to be of
high quality. The user is notified by the flaws feature to retake if image
quality is low, and this cycle is repeated until the input image is deemed to
be of high quality. As a component of the framework, we trained and evaluated a
low-quality image detection model that simultaneously learns difficulty in
recognizing images and individual flaws, and we demonstrated that our proposal
can explain the reasons for flaws with a sufficient score. We also evaluated a
dataset with low-quality images removed by our framework and found improved
values for all four common metrics (e.g., BLEU-4, METEOR, ROUGE-L, CIDEr),
confirming an improvement in general-purpose image captioning capability. Our
framework would assist the visually impaired, who have difficulty judging image
quality.",2022-11-17
PromptCap: Prompt-Guided Task-Aware Image Captioning,2022-11-15 19:07:53+00:00,http://arxiv.org/abs/2211.09699v1,"Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A. Smith, Jiebo Luo","cs.CV, cs.CL",image2text,"Image captioning aims to describe an image with a natural language sentence,
allowing powerful language models to understand images. The framework of
combining image captioning with language models has been successful on various
vision-language tasks. However, an image contains much more information than a
single sentence, leading to underspecification of which visual entities should
be described in the caption sentence. For example, when performing visual
questioning answering (VQA), generic image captions often miss visual details
that are essential for the language model to answer correctly. To address this
challenge, we propose PromptCap, a captioning model that takes a
natural-language prompt to control the contents of the generated caption. The
prompt contains a question that the caption should help to answer, and also
supports taking auxiliary text inputs such as scene text within the image
itself. To finetune a general image caption model for prompt-guided captioning,
we propose a pipeline to synthesize and filter training examples with GPT-3 and
existing VQA datasets. For evaluation, we start with an existing pipeline in
which a language model is prompted with image captions to carry out VQA. With
the same language model, a higher QA accuracy shows that our generated captions
are more relevant to the question prompts. PromptCap outperforms generic
captions by a large margin on a variety of VQA tasks and achieves the
state-of-the-art accuracy of 58.8 % on OK-VQA and 58.0 % on A-OKVQA. Zero-shot
experiments on WebQA show that PromptCap generalizes well to unseen domains.",2022-11-15
"CCPrompt: Counterfactual Contrastive Prompt-Tuning for Many-Class
  Classification",2022-11-11 03:45:59+00:00,http://arxiv.org/abs/2211.05987v1,"Yang Li, Canran Xu, Tao Shen, Jing Jiang, Guodong Long",cs.CL,image2text,"With the success of the prompt-tuning paradigm in Natural Language Processing
(NLP), various prompt templates have been proposed to further stimulate
specific knowledge for serving downstream tasks, e.g., machine translation,
text generation, relation extraction, and so on. Existing prompt templates are
mainly shared among all training samples with the information of task
description. However, training samples are quite diverse. The sharing task
description is unable to stimulate the unique task-related information in each
training sample, especially for tasks with the finite-label space. To exploit
the unique task-related information, we imitate the human decision process
which aims to find the contrastive attributes between the objective factual and
their potential counterfactuals. Thus, we propose the \textbf{C}ounterfactual
\textbf{C}ontrastive \textbf{Prompt}-Tuning (CCPrompt) approach for many-class
classification, e.g., relation classification, topic classification, and entity
typing. Compared with simple classification tasks, these tasks have more
complex finite-label spaces and are more rigorous for prompts. First of all, we
prune the finite label space to construct fact-counterfactual pairs. Then, we
exploit the contrastive attributes by projecting training instances onto every
fact-counterfactual pair. We further set up global prototypes corresponding
with all contrastive attributes for selecting valid contrastive attributes as
additional tokens in the prompt template. Finally, a simple Siamese
representation learning is employed to enhance the robustness of the model. We
conduct experiments on relation classification, topic classification, and
entity typing tasks in both fully supervised setting and few-shot setting. The
results indicate that our model outperforms former baselines.",2022-11-11
Self-conditioned Embedding Diffusion for Text Generation,2022-11-08 13:30:27+00:00,http://arxiv.org/abs/2211.04236v1,"Robin Strudel, Corentin Tallec, Florent Altché, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, Rémi Leblond","cs.CL, cs.LG",image2text,"Can continuous diffusion models bring the same performance breakthrough on
natural language they did for image generation? To circumvent the discrete
nature of text data, we can simply project tokens in a continuous space of
embeddings, as is standard in language modeling. We propose Self-conditioned
Embedding Diffusion, a continuous diffusion mechanism that operates on token
embeddings and allows to learn flexible and scalable diffusion models for both
conditional and unconditional text generation. Through qualitative and
quantitative evaluation, we show that our text diffusion models generate
samples comparable with those produced by standard autoregressive language
models - while being in theory more efficient on accelerator hardware at
inference time. Our work paves the way for scaling up diffusion models for
text, similarly to autoregressive models, and for improving performance with
recent refinements to continuous diffusion.",2022-11-08
Semantic Metadata Extraction from Dense Video Captioning,2022-11-05 22:06:50+00:00,http://arxiv.org/abs/2211.02982v1,"Johannes Scherer, Ansgar Scherp, Deepayan Bhowmik","cs.CV, cs.CL",image2text,"Annotation of multimedia data by humans is time-consuming and costly, while
reliable automatic generation of semantic metadata is a major challenge. We
propose a framework to extract semantic metadata from automatically generated
video captions. As metadata, we consider entities, the entities' properties,
relations between entities, and the video category. We employ two
state-of-the-art dense video captioning models with masked transformer (MT) and
parallel decoding (PVDC) to generate captions for videos of the ActivityNet
Captions dataset. Our experiments show that it is possible to extract entities,
their properties, relations between entities, and the video category from the
generated captions. We observe that the quality of the extracted information is
mainly influenced by the quality of the event localization in the video as well
as the performance of the event caption generation.",2022-11-05
"eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert
  Denoisers",2022-11-02 17:43:04+00:00,http://arxiv.org/abs/2211.01324v3,"Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu","cs.CV, cs.LG",image2text,"Large-scale diffusion-based generative models have led to breakthroughs in
text-conditioned high-resolution image synthesis. Starting from random noise,
such text-to-image diffusion models gradually synthesize images in an iterative
fashion while conditioning on text prompts. We find that their synthesis
behavior qualitatively changes throughout this process: Early in sampling,
generation strongly relies on the text prompt to generate text-aligned content,
while later, the text conditioning is almost entirely ignored. This suggests
that sharing model parameters throughout the entire generation process may not
be ideal. Therefore, in contrast to existing works, we propose to train an
ensemble of text-to-image diffusion models specialized for different synthesis
stages. To maintain training efficiency, we initially train a single model,
which is then split into specialized models that are trained for the specific
stages of the iterative generation process. Our ensemble of diffusion models,
called eDiff-I, results in improved text alignment while maintaining the same
inference computation cost and preserving high visual quality, outperforming
previous large-scale text-to-image diffusion models on the standard benchmark.
In addition, we train our model to exploit a variety of embeddings for
conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We
show that these different embeddings lead to different behaviors. Notably, the
CLIP image embedding allows an intuitive way of transferring the style of a
reference image to the target text-to-image output. Lastly, we show a technique
that enables eDiff-I's ""paint-with-words"" capability. A user can select the
word in the input text and paint it in a canvas to control the output, which is
very handy for crafting the desired image in mind. The project page is
available at https://deepimagination.cc/eDiff-I/",2022-11-02
CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation,2022-11-02 01:40:18+00:00,http://arxiv.org/abs/2211.00818v1,"Yihong Dong, Ge Li","cs.SE, cs.AI",image2text,"General-purpose code generation aims to automatically convert the natural
language (NL) description to code snippets in a general-purpose programming
language (GPL) like Python. Intrinsically, code generation is a special type of
text generation that generates well-formed text, i.e., code. However, existing
sequence-to-sequence (Seq2Seq) approaches generate the GPL code neglecting the
grammar rules. To this end, in this paper, we make the first attempt to
consider grammatical Seq2Seq models for general-purpose code generation and
propose CODEP, a grammatical Seq2Seq code generation framework equipped with a
Pushdown automaton (PDA) module. In the training stage, CODEP additionally
incorporates the state representation and the state prediction task, which
leverages PDA states to help CODEP comprehend the parsing process of the PDA
module. In the inference stage, CODEP generates well-formed code with the PDA
module and the joint prediction of PDA states. Furthermore, the PDA module can
be directly applied to Seq2Seq models without training to ensure the
grammatical correctness of the generated code. To evaluate the effectiveness of
our proposed method, we construct the DPA for the most popular GPL Python and
conduct extensive experiments on four benchmark datasets. The experimental
results demonstrate the superiority of CODEP compared to the state-of-the-art
approaches without pre-training, and the DPA module also achieves significant
improvements on the pre-trained models.",2022-11-02
"SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for
  Text Generation and Modular Control",2022-10-31 16:02:00+00:00,http://arxiv.org/abs/2210.17432v1,"Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov","cs.CL, cs.LG",image2text,"Despite the growing success of diffusion models in continuous-valued domains
(e.g., images), diffusion-based language models on discrete text have yet to
match autoregressive language models on text generation benchmarks. In this
work, we present SSD-LM -- a diffusion language model with two key design
choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of
text, allowing for flexible output length at decoding time while enabling local
bidirectional context updates. Second, it is simplex-based, performing
diffusion on the natural vocabulary space rather than a learned latent space,
allowing us to incorporate classifier guidance and modular control without any
adaptation of off-the-shelf classifiers. We evaluate SSD-LM on unconstrained as
well as controlled text generation benchmarks, and show that it matches or
outperforms strong autoregressive GPT-2 baselines across standard quality and
diversity metrics.",2022-10-31
Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention,2022-10-28 22:45:41+00:00,http://arxiv.org/abs/2210.16428v1,"Xubo Liu, Qiushi Huang, Xinhao Mei, Haohe Liu, Qiuqiang Kong, Jianyuan Sun, Shengchen Li, Tom Ko, Yu Zhang, Lilian H. Tang, Mark D. Plumbley, Volkan Kılıç, Wenwu Wang","eess.AS, cs.AI, cs.MM, cs.SD",image2text,"Audio captioning is the task of generating captions that describe the content
of audio clips. In the real world, many objects produce similar sounds. It is
difficult to identify these auditory ambiguous sound events with access to
audio information only. How to accurately recognize ambiguous sounds is a major
challenge for audio captioning systems. In this work, inspired by the
audio-visual multi-modal perception of human beings, we propose visually-aware
audio captioning, which makes use of visual information to help the recognition
of ambiguous sounding objects. Specifically, we introduce an off-the-shelf
visual encoder to process the video inputs, and incorporate the extracted
visual features into an audio captioning system. Furthermore, to better exploit
complementary contexts from redundant audio-visual streams, we propose an
audio-visual attention mechanism that integrates audio and visual information
adaptively according to their confidence levels. Experimental results on
AudioCaps, the largest publicly available audio captioning dataset, show that
the proposed method achieves significant improvement over a strong baseline
audio captioning system and is on par with the state-of-the-art result.",2022-10-28
"Improving the Factual Correctness of Radiology Report Generation with
  Semantic Rewards",2022-10-21 18:27:45+00:00,http://arxiv.org/abs/2210.12186v1,"Jean-Benoit Delbrouck, Pierre Chambon, Christian Bluethgen, Emily Tsai, Omar Almusa, Curtis P. Langlotz","cs.CL, cs.AI",image2text,"Neural image-to-text radiology report generation systems offer the potential
to improve radiology reporting by reducing the repetitive process of report
drafting and identifying possible medical errors. These systems have achieved
promising performance as measured by widely used NLG metrics such as BLEU and
CIDEr. However, the current systems face important limitations. First, they
present an increased complexity in architecture that offers only marginal
improvements on NLG metrics. Secondly, these systems that achieve high
performance on these metrics are not always factually complete or consistent
due to both inadequate training and evaluation. Recent studies have shown the
systems can be substantially improved by using new methods encouraging 1) the
generation of domain entities consistent with the reference and 2) describing
these entities in inferentially consistent ways. So far, these methods rely on
weakly-supervised approaches (rule-based) and named entity recognition systems
that are not specific to the chest X-ray domain. To overcome this limitation,
we propose a new method, the RadGraph reward, to further improve the factual
completeness and correctness of generated radiology reports. More precisely, we
leverage the RadGraph dataset containing annotated chest X-ray reports with
entities and relations between entities. On two open radiology report datasets,
our system substantially improves the scores up to 14.2% and 25.3% on metrics
evaluating the factual correctness and completeness of reports.",2022-10-21
Image Semantic Relation Generation,2022-10-19 16:15:19+00:00,http://arxiv.org/abs/2210.11253v1,Mingzhe Du,"cs.CV, cs.CL",image2text,"Scene graphs provide structured semantic understanding beyond images. For
downstream tasks, such as image retrieval, visual question answering, visual
relationship detection, and even autonomous vehicle technology, scene graphs
can not only distil complex image information but also correct the bias of
visual models using semantic-level relations, which has broad application
prospects. However, the heavy labour cost of constructing graph annotations may
hinder the application of PSG in practical scenarios. Inspired by the
observation that people usually identify the subject and object first and then
determine the relationship between them, we proposed to decouple the scene
graphs generation task into two sub-tasks: 1) an image segmentation task to
pick up the qualified objects. 2) a restricted auto-regressive text generation
task to generate the relation between given objects. Therefore, in this work,
we introduce image semantic relation generation (ISRG), a simple but effective
image-to-text model, which achieved 31 points on the OpenPSG dataset and
outperforms strong baselines respectively by 16 points (ResNet-50) and 5 points
(CLIP).",2022-10-19
"BioGPT: Generative Pre-trained Transformer for Biomedical Text
  Generation and Mining",2022-10-19 07:17:39+00:00,http://arxiv.org/abs/2210.10341v1,"Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu","cs.CL, cs.AI",image2text,"Pre-trained language models have attracted increasing attention in the
biomedical domain, inspired by their great success in the general natural
language domain. Among the two main branches of pre-trained language models in
the general language domain, i.e., BERT (and its variants) and GPT (and its
variants), the first one has been extensively studied in the biomedical domain,
such as BioBERT and PubMedBERT. While they have achieved great success on a
variety of discriminative downstream biomedical tasks, the lack of generation
ability constrains their application scope. In this paper, we propose BioGPT, a
domain-specific generative Transformer language model pre-trained on large
scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and
demonstrate that our model outperforms previous models on most tasks.
Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI
end-to-end relation extraction tasks respectively, and 78.2% accuracy on
PubMedQA, creating a new record. Our case study on text generation further
demonstrates the advantage of BioGPT on biomedical literature to generate
fluent descriptions for biomedical terms. Code is available at
https://github.com/microsoft/BioGPT.",2022-10-19
"Probing Cross-modal Semantics Alignment Capability from the Textual
  Perspective",2022-10-18 02:55:58+00:00,http://arxiv.org/abs/2210.09550v1,"Zheng Ma, Shi Zong, Mianzhi Pan, Jianbing Zhang, Shujian Huang, Xinyu Dai, Jiajun Chen",cs.CL,image2text,"In recent years, vision and language pre-training (VLP) models have advanced
the state-of-the-art results in a variety of cross-modal downstream tasks.
Aligning cross-modal semantics is claimed to be one of the essential
capabilities of VLP models. However, it still remains unclear about the inner
working mechanism of alignment in VLP models. In this paper, we propose a new
probing method that is based on image captioning to first empirically study the
cross-modal semantics alignment of VLP models. Our probing method is built upon
the fact that given an image-caption pair, the VLP models will give a score,
indicating how well two modalities are aligned; maximizing such scores will
generate sentences that VLP models believe are of good alignment. Analyzing
these sentences thus will reveal in what way different modalities are aligned
and how well these alignments are in VLP models. We apply our probing method to
five popular VLP models, including UNITER, ROSITA, ViLBERT, CLIP, and LXMERT,
and provide a comprehensive analysis of the generated captions guided by these
models. Our results show that VLP models (1) focus more on just aligning
objects with visual words, while neglecting global semantics; (2) prefer fixed
sentence patterns, thus ignoring more important textual information including
fluency and grammar; and (3) deem the captions with more visual words are
better aligned with images. These findings indicate that VLP models still have
weaknesses in cross-modal semantics alignment and we hope this work will draw
researchers' attention to such problems when designing a new VLP model.",2022-10-18
"UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation
  Model on a Single Image",2022-10-17 23:46:05+00:00,http://arxiv.org/abs/2210.09477v3,"Dani Valevski, Matan Kalman, Yossi Matias, Yaniv Leviathan","cs.CV, cs.GR, cs.LG",image2text,"We present UniTune, a simple and novel method for general text-driven image
editing. UniTune gets as input an arbitrary image and a textual edit
description, and carries out the edit while maintaining high semantic and
visual fidelity to the input image. UniTune uses text, an intuitive interface
for art-direction, and does not require additional inputs, like masks or
sketches. At the core of our method is the observation that with the right
choice of parameters, we can fine-tune a large text-to-image diffusion model on
a single image, encouraging the model to maintain fidelity to the input image
while still allowing expressive manipulations. We used Imagen as our
text-to-image model, but we expect UniTune to work with other large-scale
models as well. We test our method in a range of different use cases, and
demonstrate its wide applicability.",2022-10-17
Social Biases in Automatic Evaluation Metrics for NLG,2022-10-17 08:55:26+00:00,http://arxiv.org/abs/2210.08859v1,"Mingqi Gao, Xiaojun Wan","cs.CL, cs.AI",image2text,"Many studies have revealed that word embeddings, language models, and models
for specific downstream tasks in NLP are prone to social biases, especially
gender bias. Recently these techniques have been gradually applied to automatic
evaluation metrics for text generation. In the paper, we propose an evaluation
method based on Word Embeddings Association Test (WEAT) and Sentence Embeddings
Association Test (SEAT) to quantify social biases in evaluation metrics and
discover that social biases are also widely present in some model-based
automatic evaluation metrics. Moreover, we construct gender-swapped
meta-evaluation datasets to explore the potential impact of gender bias in
image caption and text summarization tasks. Results show that given
gender-neutral references in the evaluation, model-based evaluation metrics may
show a preference for the male hypothesis, and the performance of them, i.e.
the correlation between evaluation metrics and human judgments, usually has
more significant variation after gender swapping.",2022-10-17
"Plausible May Not Be Faithful: Probing Object Hallucination in
  Vision-Language Pre-training",2022-10-14 10:27:22+00:00,http://arxiv.org/abs/2210.07688v1,"Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, Pascale Fung","cs.CL, cs.CV",image2text,"Large-scale vision-language pre-trained (VLP) models are prone to hallucinate
non-existent visual objects when generating text based on visual information.
In this paper, we exhaustively probe the object hallucination problem from
three aspects. First, we examine various state-of-the-art VLP models, showing
that models achieving better scores on standard metrics(e.g., BLEU-4, CIDEr)
could hallucinate objects more frequently. Second, we investigate how different
types of visual features in VLP influence hallucination, including
region-based, grid-based, and patch-based. Surprisingly, we find that
patch-based features perform the best and smaller patch resolution yields a
non-trivial reduction in object hallucination. Third, we decouple various VLP
objectives and demonstrate their effectiveness in alleviating object
hallucination. Based on that, we propose a new pre-training loss, object masked
language modeling, to further reduce object hallucination. We evaluate models
on both COCO (in-domain) and NoCaps (out-of-domain) datasets with our improved
CHAIR metric. Furthermore, we investigate the effects of various text decoding
strategies and image augmentation methods on object hallucination.",2022-10-14
Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models,2022-10-13 08:45:23+00:00,http://arxiv.org/abs/2210.06475v1,"Sourya Basu, Prasanna Sattigeri, Karthikeyan Natesan Ramamurthy, Vijil Chenthamarakshan, Kush R. Varshney, Lav R. Varshney, Payel Das","cs.LG, cs.CL",image2text,"We introduce equi-tuning, a novel fine-tuning method that transforms
(potentially non-equivariant) pretrained models into group equivariant models
while incurring minimum $L_2$ loss between the feature representations of the
pretrained and the equivariant models. Large pretrained models can be
equi-tuned for different groups to satisfy the needs of various downstream
tasks. Equi-tuned models benefit from both group equivariance as an inductive
bias and semantic priors from pretrained models. We provide applications of
equi-tuning on three different tasks: image classification, compositional
generalization in language, and fairness in natural language generation (NLG).
We also provide a novel group-theoretic definition for fairness in NLG. The
effectiveness of this definition is shown by testing it against a standard
empirical method of fairness in NLG. We provide experimental results for
equi-tuning using a variety of pretrained models: Alexnet, Resnet, VGG, and
Densenet for image classification; RNNs, GRUs, and LSTMs for compositional
generalization; and GPT2 for fairness in NLG. We test these models on benchmark
datasets across all considered tasks to show the generality and effectiveness
of the proposed method.",2022-10-13
"Not All Errors are Equal: Learning Text Generation Metrics using
  Stratified Error Synthesis",2022-10-10 22:30:26+00:00,http://arxiv.org/abs/2210.05035v1,"Wenda Xu, Yilin Tuan, Yujie Lu, Michael Saxon, Lei Li, William Yang Wang","cs.CL, cs.AI",image2text,"Is it possible to build a general and automatic natural language generation
(NLG) evaluation metric? Existing learned metrics either perform
unsatisfactorily or are restricted to tasks where large human rating data is
already available. We introduce SESCORE, a model-based metric that is highly
correlated with human judgements without requiring human annotation, by
utilizing a novel, iterative error synthesis and severity scoring pipeline.
This pipeline applies a series of plausible errors to raw text and assigns
severity labels by simulating human judgements with entailment. We evaluate
SESCORE against existing metrics by comparing how their scores correlate with
human ratings. SESCORE outperforms all prior unsupervised metrics on multiple
diverse NLG tasks including machine translation, image captioning, and WebNLG
text generation. For WMT 20/21 En-De and Zh-En, SESCORE improve the average
Kendall correlation with human judgement from 0.154 to 0.195. SESCORE even
achieves comparable performance to the best supervised metric COMET, despite
receiving no human-annotated training data.",2022-10-10
CLIP-Diffusion-LM: Apply Diffusion Model on Image Captioning,2022-10-10 10:55:53+00:00,http://arxiv.org/abs/2210.04559v1,Shitong Xu,"cs.CV, cs.LG",image2text,"Image captioning task has been extensively researched by previous work.
However, limited experiments focus on generating captions based on
non-autoregressive text decoder. Inspired by the recent success of the
denoising diffusion model on image synthesis tasks, we apply denoising
diffusion probabilistic models to text generation in image captioning tasks. We
show that our CLIP-Diffusion-LM is capable of generating image captions using
significantly fewer inference steps than autoregressive models. On the Flickr8k
dataset, the model achieves 0.1876 BLEU-4 score. By training on the combined
Flickr8k and Flickr30k dataset, our model achieves 0.2470 BLEU-4 score. Our
code is available at https://github.com/xu-shitong/diffusion-image-captioning.",2022-10-10
ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models,2022-10-09 19:17:43+00:00,http://arxiv.org/abs/2210.04325v2,"Jiannan Xiang, Zhengzhong Liu, Yucheng Zhou, Eric P. Xing, Zhiting Hu",cs.CL,image2text,"Data-to-text generation is challenging due to the great variety of the input
data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse
predicates). Recent end-to-end neural methods thus require substantial training
examples to learn to disambiguate and describe the data. Yet, real-world
data-to-text problems often suffer from various data-scarce issues: one may
have access to only a handful of or no training examples, and/or have to rely
on examples in a different domain or schema. To fill this gap, we propose
Any-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse
settings by making efficient use of any given (or no) examples. ASDOT consists
of two steps, data disambiguation and sentence fusion, both of which are
amenable to be solved with off-the-shelf pretrained language models (LMs) with
optional finetuning. In the data disambiguation stage, we employ the prompted
GPT-3 model to understand possibly ambiguous triples from the input data and
convert each into a short sentence with reduced ambiguity. The sentence fusion
stage then uses an LM like T5 to fuse all the resulting sentences into a
coherent paragraph as the final description. We evaluate extensively on various
datasets in different scenarios, including the zero-/few-/full-shot settings,
and generalization to unseen predicates and out-of-domain data. Experimental
results show that ASDOT consistently achieves significant improvement over
baselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot
setting.",2022-10-09
"Visualize Before You Write: Imagination-Guided Open-Ended Text
  Generation",2022-10-07 18:01:09+00:00,http://arxiv.org/abs/2210.03765v1,"Wanrong Zhu, An Yan, Yujie Lu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, William Yang Wang","cs.CL, cs.AI",image2text,"Recent advances in text-to-image synthesis make it possible to visualize
machine imaginations for a given context. On the other hand, when generating
text, human writers are gifted at creative visualization, which enhances their
writings by forming imaginations as blueprints before putting down the stories
in words. Inspired by such a cognitive process, we ask the natural question of
whether we can endow machines with the same ability to utilize visual
information and construct a general picture of the context to guide text
generation. In this work, we propose iNLG that uses machine-generated images to
guide language models (LM) in open-ended text generation. The experiments and
analyses demonstrate the effectiveness of iNLG on open-ended text generation
tasks, including text completion, story generation, and concept-to-text
generation in few-shot scenarios. Both automatic metrics and human evaluations
verify that the text snippets generated by our iNLG are coherent and
informative while displaying minor degeneration.",2022-10-07
"Unsupervised Neural Stylistic Text Generation using Transfer learning
  and Adapters",2022-10-07 00:09:22+00:00,http://arxiv.org/abs/2210.03264v1,"Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah, Dan Roth",cs.CL,image2text,"Research has shown that personality is a key driver to improve engagement and
user experience in conversational systems. Conversational agents should also
maintain a consistent persona to have an engaging conversation with a user.
However, text generation datasets are often crowd sourced and thereby have an
averaging effect where the style of the generation model is an average style of
all the crowd workers that have contributed to the dataset. While one can
collect persona-specific datasets for each task, it would be an expensive and
time consuming annotation effort. In this work, we propose a novel transfer
learning framework which updates only $0.3\%$ of model parameters to learn
style specific attributes for response generation. For the purpose of this
study, we tackle the problem of stylistic story ending generation using the ROC
stories Corpus. We learn style specific attributes from the
PERSONALITY-CAPTIONS dataset. Through extensive experiments and evaluation
metrics we show that our novel training procedure can improve the style
generation by 200 over Encoder-Decoder baselines while maintaining on-par
content relevance metrics with",2022-10-07
"Co-Writing Screenplays and Theatre Scripts with Language Models: An
  Evaluation by Industry Professionals",2022-09-29 17:26:22+00:00,http://arxiv.org/abs/2209.14958v1,"Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, Richard Evans","cs.HC, cs.CL",image2text,"Language models are increasingly attracting interest from writers. However,
such models lack long-range semantic coherence, limiting their usefulness for
longform creative writing. We address this limitation by applying language
models hierarchically, in a system we call Dramatron. By building structural
context via prompt chaining, Dramatron can generate coherent scripts and
screenplays complete with title, characters, story beats, location
descriptions, and dialogue. We illustrate Dramatron's usefulness as an
interactive co-creative system with a user study of 15 theatre and film
industry professionals. Participants co-wrote theatre scripts and screenplays
with Dramatron and engaged in open-ended interviews. We report critical
reflections both from our interviewees and from independent reviewers who
watched stagings of the works to illustrate how both Dramatron and hierarchical
text generation could be useful for human-machine co-creativity. Finally, we
discuss the suitability of Dramatron for co-creativity, ethical considerations
-- including plagiarism and bias -- and participatory models for the design and
deployment of such tools.",2022-09-29
XF2T: Cross-lingual Fact-to-Text Generation for Low-Resource Languages,2022-09-22 18:01:27+00:00,http://arxiv.org/abs/2209.11252v1,"Shivprasad Sagare, Tushar Abhishek, Bhavyajeet Singh, Anubhav Sharma, Manish Gupta, Vasudeva Varma",cs.CL,image2text,"Multiple business scenarios require an automated generation of descriptive
human-readable text from structured input data. Hence, fact-to-text generation
systems have been developed for various downstream tasks like generating soccer
reports, weather and financial reports, medical reports, person biographies,
etc. Unfortunately, previous work on fact-to-text (F2T) generation has focused
primarily on English mainly due to the high availability of relevant datasets.
Only recently, the problem of cross-lingual fact-to-text (XF2T) was proposed
for generation across multiple languages alongwith a dataset, XALIGN for eight
languages. However, there has been no rigorous work on the actual XF2T
generation problem. We extend XALIGN dataset with annotated data for four more
languages: Punjabi, Malayalam, Assamese and Oriya. We conduct an extensive
study using popular Transformer-based text generation models on our extended
multi-lingual dataset, which we call XALIGNV2. Further, we investigate the
performance of different text generation strategies: multiple variations of
pretraining, fact-aware embeddings and structure-aware input encoding. Our
extensive experiments show that a multi-lingual mT5 model which uses fact-aware
embeddings with structure-aware input encoding leads to best results on average
across the twelve languages. We make our code, dataset and model publicly
available, and hope that this will help advance further research in this
critical area.",2022-09-22
Distribution Aware Metrics for Conditional Natural Language Generation,2022-09-15 17:58:13+00:00,http://arxiv.org/abs/2209.07518v1,"David M Chan, Yiming Ni, Austin Myers, Sudheendra Vijayanarasimhan, David A Ross, John Canny","cs.CL, cs.AI, cs.CV, cs.LG",image2text,"Traditional automated metrics for evaluating conditional natural language
generation use pairwise comparisons between a single generated text and the
best-matching gold-standard ground truth text. When multiple ground truths are
available, scores are aggregated using an average or max operation across
references. While this approach works well when diversity in the ground truth
data (i.e. dispersion of the distribution of conditional texts) can be ascribed
to noise, such as in automated speech recognition, it does not allow for robust
evaluation in the case where diversity in the ground truths represents signal
for the model. In this work we argue that existing metrics are not appropriate
for domains such as visual description or summarization where ground truths are
semantically diverse, and where the diversity in those captions captures useful
additional information about the context. We propose a novel paradigm for
multi-candidate evaluation of conditional language generation models, and a new
family of metrics that compare the distributions of reference and
model-generated caption sets using small sample sets of each. We demonstrate
the utility of our approach with a case study in visual description: where we
show that existing models optimize for single-description quality over
diversity, and gain some insights into how sampling methods and temperature
impact description quality and diversity.",2022-09-15
PaLI: A Jointly-Scaled Multilingual Language-Image Model,2022-09-14 17:24:07+00:00,http://arxiv.org/abs/2209.06794v2,"Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut","cs.CV, cs.CL",image2text,"Effective scaling and a flexible task interface enable large language models
to excel at many tasks. PaLI (Pathways Language and Image model) extends this
approach to the joint modeling of language and vision. PaLI generates text
based on visual and textual inputs, and with this interface performs many
vision, language, and multimodal tasks, in many languages. To train PaLI, we
make use of large pretrained encoder-decoder language models and Vision
Transformers (ViTs). This allows us to capitalize on their existing
capabilities and leverage the substantial cost of training them. We find that
joint scaling of the vision and language components is important. Since
existing Transformers for language are much larger than their vision
counterparts, we train the largest ViT to date (ViT-e) to quantify the benefits
from even larger-capacity vision models. To train PaLI, we create a large
multilingual mix of pretraining tasks, based on a new image-text training set
containing 10B images and texts in over 100 languages. PaLI achieves
state-of-the-art in multiple vision and language tasks (such as captioning,
visual question-answering, scene-text understanding), while retaining a simple,
modular, and scalable design.",2022-09-14
"Visual Recipe Flow: A Dataset for Learning Visual State Changes of
  Objects with Recipe Flows",2022-09-13 09:38:32+00:00,http://arxiv.org/abs/2209.05840v1,"Keisuke Shirai, Atsushi Hashimoto, Taichi Nishimura, Hirotaka Kameko, Shuhei Kurita, Yoshitaka Ushiku, Shinsuke Mori","cs.CL, cs.AI",image2text,"We present a new multimodal dataset called Visual Recipe Flow, which enables
us to learn each cooking action result in a recipe text. The dataset consists
of object state changes and the workflow of the recipe text. The state change
is represented as an image pair, while the workflow is represented as a recipe
flow graph (r-FG). The image pairs are grounded in the r-FG, which provides the
cross-modal relation. With our dataset, one can try a range of applications,
from multimodal commonsense reasoning and procedural text generation.",2022-09-13
"Bridging Music and Text with Crowdsourced Music Comments: A
  Sequence-to-Sequence Framework for Thematic Music Comments Generation",2022-09-05 14:51:51+00:00,http://arxiv.org/abs/2209.01996v1,"Peining Zhang, Junliang Guo, Linli Xu, Mu You, Junming Yin","cs.SD, cs.CL, eess.AS",image2text,"We consider a novel task of automatically generating text descriptions of
music. Compared with other well-established text generation tasks such as image
caption, the scarcity of well-paired music and text datasets makes it a much
more challenging task. In this paper, we exploit the crowd-sourced music
comments to construct a new dataset and propose a sequence-to-sequence model to
generate text descriptions of music. More concretely, we use the dilated
convolutional layer as the basic component of the encoder and a memory based
recurrent neural network as the decoder. To enhance the authenticity and
thematicity of generated texts, we further propose to fine-tune the model with
a discriminator as well as a novel topic evaluator. To measure the quality of
generated texts, we also propose two new evaluation metrics, which are more
aligned with human evaluation than traditional metrics such as BLEU.
Experimental results verify that our model is capable of generating fluent and
meaningful comments while containing thematic and content information of the
original music.",2022-09-05
"Every picture tells a story: Image-grounded controllable stylistic story
  generation",2022-09-04 15:07:53+00:00,http://arxiv.org/abs/2209.01638v1,"Holy Lovenia, Bryan Wilie, Romain Barraud, Samuel Cahyawijaya, Willy Chung, Pascale Fung",cs.CL,image2text,"Generating a short story out of an image is arduous. Unlike image captioning,
story generation from an image poses multiple challenges: preserving the story
coherence, appropriately assessing the quality of the story, steering the
generated story into a certain style, and addressing the scarcity of
image-story pair reference datasets limiting supervision during training. In
this work, we introduce Plug-and-Play Story Teller (PPST) and improve
image-to-story generation by: 1) alleviating the data scarcity problem by
incorporating large pre-trained models, namely CLIP and GPT-2, to facilitate a
fluent image-to-text generation with minimal supervision, and 2) enabling a
more style-relevant generation by incorporating stylistic adapters to control
the story generation. We conduct image-to-story generation experiments with
non-styled, romance-styled, and action-styled PPST approaches and compare our
generated stories with those of previous work over three aspects, i.e., story
coherence, image-story relevance, and style fitness, using both automatic and
human evaluation. The results show that PPST improves story coherence and has
better image-story relevance, but has yet to be adequately stylistic.",2022-09-04
Understanding Attention for Vision-and-Language Tasks,2022-08-17 06:45:07+00:00,http://arxiv.org/abs/2208.08104v1,"Feiqi Cao, Soyeon Caren Han, Siqu Long, Changwei Xu, Josiah Poon","cs.CV, cs.CL",image2text,"Attention mechanism has been used as an important component across
Vision-and-Language(VL) tasks in order to bridge the semantic gap between
visual and textual features. While attention has been widely used in VL tasks,
it has not been examined the capability of different attention alignment
calculation in bridging the semantic gap between visual and textual clues. In
this research, we conduct a comprehensive analysis on understanding the role of
attention alignment by looking into the attention score calculation methods and
check how it actually represents the visual region's and textual token's
significance for the global assessment. We also analyse the conditions which
attention score calculation mechanism would be more (or less) interpretable,
and which may impact the model performance on three different VL tasks,
including visual question answering, text-to-image generation, text-and-image
matching (both sentence and image retrieval). Our analysis is the first of its
kind and provides useful insights of the importance of each attention alignment
score calculation when applied at the training phase of VL tasks, commonly
ignored in attention-based cross modal models, and/or pretrained models.",2022-08-17
ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model,2022-07-19 17:59:01+00:00,http://arxiv.org/abs/2207.09446v1,"Rao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, Srinath Sridhar","cs.CV, cs.AI",image2text,"We present ShapeCrafter, a neural network for recursive text-conditioned 3D
shape generation. Existing methods to generate text-conditioned 3D shapes
consume an entire text prompt to generate a 3D shape in a single step. However,
humans tend to describe shapes recursively-we may start with an initial
description and progressively add details based on intermediate results. To
capture this recursive process, we introduce a method to generate a 3D shape
distribution, conditioned on an initial phrase, that gradually evolves as more
phrases are added. Since existing datasets are insufficient for training this
approach, we present Text2Shape++, a large dataset of 369K shape-text pairs
that supports recursive shape generation. To capture local details that are
often used to refine shape descriptions, we build on top of vector-quantized
deep implicit functions that generate a distribution of high-quality shapes.
Results show that our method can generate shapes consistent with text
descriptions, and shapes evolve gradually as more phrases are added. Our method
supports shape editing, extrapolation, and can enable new applications in
human-machine collaboration for creative design.",2022-07-19
"A Baseline for Detecting Out-of-Distribution Examples in Image
  Captioning",2022-07-12 09:29:57+00:00,http://arxiv.org/abs/2207.05418v1,"Gabi Shalev, Gal-Lev Shalev, Joseph Keshet","cs.CV, cs.LG",image2text,"Image captioning research achieved breakthroughs in recent years by
developing neural models that can generate diverse and high-quality
descriptions for images drawn from the same distribution as training images.
However, when facing out-of-distribution (OOD) images, such as corrupted
images, or images containing unknown objects, the models fail in generating
relevant captions.
  In this paper, we consider the problem of OOD detection in image captioning.
We formulate the problem and suggest an evaluation setup for assessing the
model's performance on the task. Then, we analyze and show the effectiveness of
the caption's likelihood score at detecting and rejecting OOD images, which
implies that the relatedness between the input image and the generated caption
is encapsulated within the score.",2022-07-12
Towards Multimodal Vision-Language Models Generating Non-Generic Text,2022-07-09 01:56:35+00:00,http://arxiv.org/abs/2207.04174v1,"Wes Robbins, Zanyar Zohourianshahzadi, Jugal Kalita","cs.CV, cs.AI",image2text,"Vision-language models can assess visual context in an image and generate
descriptive text. While the generated text may be accurate and syntactically
correct, it is often overly general. To address this, recent work has used
optical character recognition to supplement visual information with text
extracted from an image. In this work, we contend that vision-language models
can benefit from additional information that can be extracted from an image,
but are not used by current models. We modify previous multimodal frameworks to
accept relevant information from any number of auxiliary classifiers. In
particular, we focus on person names as an additional set of tokens and create
a novel image-caption dataset to facilitate captioning with person names. The
dataset, Politicians and Athletes in Captions (PAC), consists of captioned
images of well-known people in context. By fine-tuning pretrained models with
this dataset, we demonstrate a model that can naturally integrate facial
recognition tokens into generated text by training on limited data. For the PAC
dataset, we provide a discussion on collection and baseline benchmark scores.",2022-07-09
Dual-Stream Transformer for Generic Event Boundary Captioning,2022-07-07 01:47:19+00:00,http://arxiv.org/abs/2207.03038v1,"Xin Gu, Hanhua Ye, Guang Chen, Yufei Wang, Libo Zhang, Longyin Wen","cs.CV, cs.CL",image2text,"This paper describes our champion solution for the CVPR2022 Generic Event
Boundary Captioning (GEBC) competition. GEBC requires the captioning model to
have a comprehension of instantaneous status changes around the given video
boundary, which makes it much more challenging than conventional video
captioning task. In this paper, a Dual-Stream Transformer with improvements on
both video content encoding and captions generation is proposed: (1) We utilize
three pre-trained models to extract the video features from different
granularities. Moreover, we exploit the types of boundary as hints to help the
model generate captions. (2) We particularly design an model, termed as
Dual-Stream Transformer, to learn discriminative representations for boundary
captioning. (3) Towards generating content-relevant and human-like captions, we
improve the description quality by designing a word-level ensemble strategy.
The promising results on the GEBC test split demonstrate the efficacy of our
proposed model.",2022-07-07
"Syntax Controlled Knowledge Graph-to-Text Generation with Order and
  Semantic Consistency",2022-07-02 02:42:14+00:00,http://arxiv.org/abs/2207.00719v1,"Jin Liu, Chongfeng Fan, Fengyu Zhou, Huijuan Xu",cs.AI,image2text,"The knowledge graph (KG) stores a large amount of structural knowledge, while
it is not easy for direct human understanding. Knowledge graph-to-text
(KG-to-text) generation aims to generate easy-to-understand sentences from the
KG, and at the same time, maintains semantic consistency between generated
sentences and the KG. Existing KG-to-text generation methods phrase this task
as a sequence-to-sequence generation task with linearized KG as input and
consider the consistency issue of the generated texts and KG through a simple
selection between decoded sentence word and KG node word at each time step.
However, the linearized KG order is commonly obtained through a heuristic
search without data-driven optimization. In this paper, we optimize the
knowledge description order prediction under the order supervision extracted
from the caption and further enhance the consistency of the generated sentences
and KG through syntactic and semantic regularization. We incorporate the
Part-of-Speech (POS) syntactic tags to constrain the positions to copy words
from the KG and employ a semantic context scoring function to evaluate the
semantic fitness for each word in its local context when decoding each word in
the generated sentence. Extensive experiments are conducted on two datasets,
WebNLG and DART, and achieve state-of-the-art performances.",2022-07-02
Automatic Controllable Product Copywriting for E-Commerce,2022-06-21 04:18:52+00:00,http://arxiv.org/abs/2206.10103v1,"Xiaojie Guo, Qingkai Zeng, Meng Jiang, Yun Xiao, Bo Long, Lingfei Wu","cs.AI, cs.LG",image2text,"Automatic product description generation for e-commerce has witnessed
significant advancement in the past decade. Product copywriting aims to attract
users' interest and improve user experience by highlighting product
characteristics with textual descriptions. As the services provided by
e-commerce platforms become diverse, it is necessary to adapt the patterns of
automatically-generated descriptions dynamically. In this paper, we report our
experience in deploying an E-commerce Prefix-based Controllable Copywriting
Generation (EPCCG) system into the JD.com e-commerce product recommendation
platform. The development of the system contains two main components: 1)
copywriting aspect extraction; 2) weakly supervised aspect labeling; 3) text
generation with a prefix-based language model; 4) copywriting quality control.
We conduct experiments to validate the effectiveness of the proposed EPCCG. In
addition, we introduce the deployed architecture which cooperates with the
EPCCG into the real-time JD.com e-commerce recommendation platform and the
significant payoff since deployment.",2022-06-21
"niksss at HinglishEval: Language-agnostic BERT-based Contextual
  Embeddings with Catboost for Quality Evaluation of the Low-Resource
  Synthetically Generated Code-Mixed Hinglish Text",2022-06-17 17:36:03+00:00,http://arxiv.org/abs/2206.08910v1,Nikhil Singh,cs.CL,image2text,"This paper describes the system description for the HinglishEval challenge at
INLG 2022. The goal of this task was to investigate the factors influencing the
quality of the code-mixed text generation system. The task was divided into two
subtasks, quality rating prediction and annotators disagreement prediction of
the synthetic Hinglish dataset. We attempted to solve these tasks using
sentence-level embeddings, which are obtained from mean pooling the
contextualized word embeddings for all input tokens in our text. We
experimented with various classifiers on top of the embeddings produced for
respective tasks. Our best-performing system ranked 1st on subtask B and 3rd on
subtask A.",2022-06-17
Prefix Language Models are Unified Modal Learners,2022-06-15 17:49:38+00:00,http://arxiv.org/abs/2206.07699v1,"Shizhe Diao, Wangchunshu Zhou, Xinsong Zhang, Jiawei Wang","cs.CV, cs.CL, cs.LG",image2text,"With the success of vision-language pre-training, we have witnessed the
state-of-the-art has been pushed on multi-modal understanding and generation.
However, the current pre-training paradigm is either incapable of targeting all
modalities at once (e.g., text generation and image generation), or requires
multi-fold well-designed tasks which significantly limits the scalability. We
demonstrate that a unified modal model could be learned with a prefix language
modeling objective upon text and image sequences. Thanks to the simple but
powerful pre-training paradigm, our proposed model, DaVinci, is simple to
train, scalable to huge data, and adaptable to a variety of downstream tasks
across modalities (language / vision / vision+language), types (understanding /
generation) and settings (e.g., zero-shot, fine-tuning, linear evaluation) with
a single unified architecture. DaVinci achieves the competitive performance on
a wide range of 26 understanding / generation tasks, and outperforms previous
unified vision-language models on most tasks, including ImageNet classification
(+1.6%), VQAv2 (+1.4%), COCO caption generation (BLEU@4 +1.1%, CIDEr +1.5%) and
COCO image generation (IS +0.9%, FID -1.0%), at the comparable model and data
scale. Furthermore, we offer a well-defined benchmark for future research by
reporting the performance on different scales of the pre-training dataset on a
heterogeneous and wide distribution coverage. Our results establish new,
stronger baselines for future comparisons at different data scales and shed
light on the difficulties of comparing VLP models more generally.",2022-06-15
Exploring industrial safety knowledge via Zipf law,2022-05-25 10:22:14+00:00,http://arxiv.org/abs/2205.12636v1,"Zhenhua Wang, Ming Ren, Dong Gao, Zhuang Li",cs.CL,image2text,"The hazard and operability analysis (HAZOP) report contains precious
industrial safety knowledge (ISK) with expert experience and process nature,
which is of great significance to the development of industrial intelligence.
Subject to the attributes of ISK, existing researches mine them through
sequence labeling in deep learning. Yet, there are two thorny issues: (1)
Uneven distribution of ISK and (2) Consistent importance of ISK: for safety
review. In this study, we propose a novel generative mining strategy called
CRGM to explore ISK. Inspired Zipf law in linguistics, CRGM consists of
common-rare discriminator, induction-extension generator and ISK extractor.
Firstly, the common-rare discriminator divides HAZOP descriptions into common
words and rare words, and obtains the common description and the rare
description, where the latter contains more industrial substances. Then, they
are operated by the induction-extension generator in the way of deep text
generation, the common description is induced and the rare description is
extended, the material knowledge and the equipment knowledge can be enriched.
Finally, the ISK extractor processes the material knowledge and equipment
knowledge from the generated description through the rule template method, the
additional ISK is regarded as the supplement of the training set to train the
proposed sequence labeling model. We conduct multiple evaluation experiments on
two industrial safety datasets. The results show that CRGM has promising and
gratifying aptitudes, greatly improves the performance of the model, and is
efficient and generalized. Our sequence labeling model also shows the expected
performance, which is better than the existing research. Our research provides
a new perspective for exploring ISK, we hope it can contribute support for the
intelligent progress of industrial safety.",2022-05-25
"The Dialog Must Go On: Improving Visual Dialog via Generative
  Self-Training",2022-05-25 05:40:00+00:00,http://arxiv.org/abs/2205.12502v1,"Gi-Cheon Kang, Sungdong Kim, Jin-Hwa Kim, Donghyun Kwak, Byoung-Tak Zhang","cs.CV, cs.CL, cs.LG",image2text,"Visual dialog (VisDial) is a task of answering a sequence of questions
grounded in an image, using the dialog history as context. Prior work has
trained the dialog agents solely on VisDial data via supervised learning or
leveraged pre-training on related vision-and-language datasets. This paper
presents a semi-supervised learning approach for visually-grounded dialog,
called Generative Self-Training (GST), to leverage unlabeled images on the Web.
Specifically, GST first retrieves in-domain images through out-of-distribution
detection and generates synthetic dialogs regarding the images via multimodal
conditional text generation. GST then trains a dialog agent on the synthetic
and the original VisDial data. As a result, GST scales the amount of training
data up to an order of magnitude that of VisDial (1.2M to 12.9M QA data). For
robust training of the generated dialogs, we also propose perplexity-based data
selection and multimodal consistency regularization. Evaluation on VisDial v1.0
and v0.9 datasets shows that GST achieves new state-of-the-art results on both
datasets. We further observe strong performance gains in the low-data regime
(up to 9.35 absolute points on NDCG).",2022-05-25
"Rethinking Evaluation Practices in Visual Question Answering: A Case
  Study on Out-of-Distribution Generalization",2022-05-24 16:44:45+00:00,http://arxiv.org/abs/2205.12191v1,"Aishwarya Agrawal, Ivana Kajić, Emanuele Bugliarello, Elnaz Davoodi, Anita Gergely, Phil Blunsom, Aida Nematzadeh","cs.CL, cs.AI, cs.CV, cs.LG",image2text,"Vision-and-language (V&L) models pretrained on large-scale multimodal data
have demonstrated strong performance on various tasks such as image captioning
and visual question answering (VQA). The quality of such models is commonly
assessed by measuring their performance on unseen data that typically comes
from the same distribution as the training data. However, we observe that these
models exhibit poor out-of-distribution (OOD) generalization on the task of
VQA. To better understand the underlying causes of poor generalization, we
comprehensively investigate performance of two pretrained V&L models under
different settings (i.e. classification and open-ended text generation) by
conducting cross-dataset evaluations. We find that these models tend to learn
to solve the benchmark, rather than learning the high-level skills required by
the VQA task. We also argue that in most cases generative models are less
susceptible to shifts in data distribution, while frequently performing better
on our tested benchmarks. Moreover, we find that multimodal pretraining
improves OOD performance in most settings. Finally, we revisit assumptions
underlying the use of automatic VQA evaluation metrics, and empirically show
that their stringent nature repeatedly penalizes models for correct responses.",2022-05-24
"On Advances in Text Generation from Images Beyond Captioning: A Case
  Study in Self-Rationalization",2022-05-24 00:52:40+00:00,http://arxiv.org/abs/2205.11686v1,"Shruti Palaskar, Akshita Bhagia, Yonatan Bisk, Florian Metze, Alan W Black, Ana Marasovic","cs.CL, cs.CV",image2text,"Integrating vision and language has gained notable attention following the
success of pretrained language models. Despite that, a fraction of emerging
multimodal models is suitable for text generation conditioned on images. This
minority is typically developed and evaluated for image captioning, a text
generation task conditioned solely on images with the goal to describe what is
explicitly visible in an image. In this paper, we take a step back and ask: How
do these models work for more complex generative tasks, conditioned on both
text and images? Are models based on joint multimodal pretraining, visually
adapted pretrained language models, or models that combine these two
approaches, more promising for such tasks? We address these questions in the
context of self-rationalization (jointly generating task labels/answers and
free-text explanations) of three tasks: (i) visual question answering in VQA-X,
(ii) visual commonsense reasoning in VCR, and (iii) visual-textual entailment
in E-SNLI-VE. We show that recent advances in each modality, CLIP image
representations and scaling of language models, do not consistently improve
multimodal self-rationalization of tasks with multimodal inputs. We also
observe that no model type works universally the best across tasks/datasets and
finetuning data sizes. Our findings call for a backbone modelling approach that
can be built on to advance text generation from images and text beyond image
captioning.",2022-05-24
What Makes Data-to-Text Generation Hard for Pretrained Language Models?,2022-05-23 17:58:39+00:00,http://arxiv.org/abs/2205.11505v1,"Moniba Keymanesh, Adrian Benton, Mark Dredze","cs.CL, cs.AI, cs.IR, cs.LG",image2text,"Expressing natural language descriptions of structured facts or relations --
data-to-text generation (D2T) -- increases the accessibility of structured
knowledge repositories. Previous work shows that pre-trained language
models(PLMs) perform remarkably well on this task after fine-tuning on a
significant amount of task-specific training data. On the other hand, while
auto-regressive PLMs can generalize from a few task examples, their efficacy at
D2T is largely unexplored. Furthermore, we have an incomplete understanding of
the limits of PLMs on D2T.
  In this work, we conduct an empirical study of both fine-tuned and
auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their
performance as a function of the amount of task-specific data and how these
data are incorporated into the models: zero and few-shot learning, and
fine-tuning of model weights. In addition, we probe the limits of PLMs by
measuring performance on subsets of the evaluation data: novel predicates and
abstractive test examples. To improve the performance on these subsets, we
investigate two techniques: providing predicate descriptions in the context and
re-ranking generated candidates by information reflected in the source.
Finally, we conduct a human evaluation of model errors and show that D2T
generation tasks would benefit from datasets with more careful manual curation.",2022-05-23
"Language Models with Image Descriptors are Strong Few-Shot
  Video-Language Learners",2022-05-22 05:18:27+00:00,http://arxiv.org/abs/2205.10747v2,"Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal, Heng Ji","cs.CV, cs.AI",image2text,"The goal of this work is to build flexible video-language models that can
generalize to various video-to-text tasks from few examples, such as
domain-specific captioning, question answering, and future event prediction.
Existing few-shot video-language learners focus exclusively on the encoder,
resulting in the absence of a video-to-text decoder to handle generative tasks.
Video captioners have been pretrained on large-scale video-language datasets,
but they rely heavily on finetuning and lack the ability to generate text for
unseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language
Learner via Image and Language models, which demonstrates strong performance on
few-shot video-to-text tasks without the necessity of pretraining or finetuning
on any video datasets. We use the image-language models to translate the video
content into frame captions, object, attribute, and event phrases, and compose
them into a temporal structure template. We then instruct a language model,
with a prompt containing a few in-context examples, to generate a target output
from the composed content. The flexibility of prompting allows the model to
capture any form of text input, such as automatic speech recognition (ASR)
transcripts. Our experiments demonstrate the power of language models in
understanding videos on a wide variety of video-language tasks, including video
captioning, video question answering, video caption retrieval, and video future
event prediction. Especially, on video future event prediction, our few-shot
model significantly outperforms state-of-the-art supervised models trained on
large-scale video datasets. Code and resources are publicly available for
research purposes at https://github.com/MikeWangWZHL/VidIL .",2022-05-22
"Context Matters for Image Descriptions for Accessibility: Challenges for
  Referenceless Evaluation Metrics",2022-05-21 17:35:26+00:00,http://arxiv.org/abs/2205.10646v1,"Elisa Kreiss, Cynthia Bennett, Shayan Hooshmand, Eric Zelikman, Meredith Ringel Morris, Christopher Potts",cs.CL,image2text,"Few images on the Web receive alt-text descriptions that would make them
accessible to blind and low vision (BLV) users. Image-based NLG systems have
progressed to the point where they can begin to address this persistent
societal problem, but these systems will not be fully successful unless we
evaluate them on metrics that guide their development correctly. Here, we argue
against current referenceless metrics -- those that don't rely on
human-generated ground-truth descriptions -- on the grounds that they do not
align with the needs of BLV users. The fundamental shortcoming of these metrics
is that they cannot take context into account, whereas contextual information
is highly valued by BLV users. To substantiate these claims, we present a study
with BLV participants who rated descriptions along a variety of dimensions. An
in-depth analysis reveals that the lack of context-awareness makes current
referenceless metrics inadequate for advancing image accessibility, requiring a
rethinking of referenceless evaluation metrics for image-based NLG systems.",2022-05-21
"It Isn't Sh!tposting, It's My CAT Posting",2022-05-18 04:11:55+00:00,http://arxiv.org/abs/2205.08710v1,"Parthsarthi Rawat, Sayan Das, Jorge Aguirre, Akhil Daphara","cs.CV, cs.AI, cs.LG",image2text,"In this paper, we describe a novel architecture which can generate hilarious
captions for a given input image. The architecture is split into two halves,
i.e. image captioning and hilarious text conversion. The architecture starts
with a pre-trained CNN model, VGG16 in this implementation, and applies
attention LSTM on it to generate normal caption. These normal captions then are
fed forward to our hilarious text conversion transformer which converts this
text into something hilarious while maintaining the context of the input image.
The architecture can also be split into two halves and only the seq2seq
transformer can be used to generate hilarious caption by inputting a
sentence.This paper aims to help everyday user to be more lazy and hilarious at
the same time by generating captions using CATNet.",2022-05-18
"Breaking with Fixed Set Pathology Recognition through Report-Guided
  Contrastive Training",2022-05-14 21:44:05+00:00,http://arxiv.org/abs/2205.07139v1,"Constantin Seibold, Simon Reiß, M. Saquib Sarfraz, Rainer Stiefelhagen, Jens Kleesiek","cs.CV, cs.LG",image2text,"When reading images, radiologists generate text reports describing the
findings therein. Current state-of-the-art computer-aided diagnosis tools
utilize a fixed set of predefined categories automatically extracted from these
medical reports for training. This form of supervision limits the potential
usage of models as they are unable to pick up on anomalies outside of their
predefined set, thus, making it a necessity to retrain the classifier with
additional data when faced with novel classes. In contrast, we investigate
direct text supervision to break away from this closed set assumption. By doing
so, we avoid noisy label extraction via text classifiers and incorporate more
contextual information.
  We employ a contrastive global-local dual-encoder architecture to learn
concepts directly from unstructured medical reports while maintaining its
ability to perform free form classification.
  We investigate relevant properties of open set recognition for radiological
data and propose a method to employ currently weakly annotated data into
training.
  We evaluate our approach on the large-scale chest X-Ray datasets MIMIC-CXR,
CheXpert, and ChestX-Ray14 for disease classification. We show that despite
using unstructured medical report supervision, we perform on par with direct
label supervision through a sophisticated inference setting.",2022-05-14
"Robust (Controlled) Table-to-Text Generation with Structure-Aware
  Equivariance Learning",2022-05-08 23:37:27+00:00,http://arxiv.org/abs/2205.03972v1,"Fei Wang, Zhewei Xu, Pedro Szekely, Muhao Chen","cs.CL, cs.AI, cs.LG",image2text,"Controlled table-to-text generation seeks to generate natural language
descriptions for highlighted subparts of a table. Previous SOTA systems still
employ a sequence-to-sequence generation method, which merely captures the
table as a linear structure and is brittle when table layouts change. We seek
to go beyond this paradigm by (1) effectively expressing the relations of
content pieces in the table, and (2) making our model robust to
content-invariant structural transformations. Accordingly, we propose an
equivariance learning framework, which encodes tables with a structure-aware
self-attention mechanism. This prunes the full self-attention structure into an
order-invariant graph attention that captures the connected graph structure of
cells belonging to the same row or column, and it differentiates between
relevant cells and irrelevant cells from the structural perspective. Our
framework also modifies the positional encoding mechanism to preserve the
relative position of tokens in the same cell but enforce position invariance
among different cells. Our technology is free to be plugged into existing
table-to-text generation models, and has improved T5-based models to offer
better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo,
we preserve promising performance, while previous SOTA systems, even with
transformation-based data augmentation, have seen significant performance
drops. Our code is available at https://github.com/luka-group/Lattice.",2022-05-08
RoViST:Learning Robust Metrics for Visual Storytelling,2022-05-08 03:51:22+00:00,http://arxiv.org/abs/2205.03774v1,"Eileen Wang, Caren Han, Josiah Poon","cs.CV, cs.AI",image2text,"Visual storytelling (VST) is the task of generating a story paragraph that
describes a given image sequence. Most existing storytelling approaches have
evaluated their models using traditional natural language generation metrics
like BLEU or CIDEr. However, such metrics based on n-gram matching tend to have
poor correlation with human evaluation scores and do not explicitly consider
other criteria necessary for storytelling such as sentence structure or topic
coherence. Moreover, a single score is not enough to assess a story as it does
not inform us about what specific errors were made by the model. In this paper,
we propose 3 evaluation metrics sets that analyses which aspects we would look
for in a good story: 1) visual grounding, 2) coherence, and 3) non-redundancy.
We measure the reliability of our metric sets by analysing its correlation with
human judgement scores on a sample of machine stories obtained from 4
state-of-the-arts models trained on the Visual Storytelling Dataset (VIST). Our
metric sets outperforms other metrics on human correlation, and could be served
as a learning based evaluation metric set that is complementary to existing
rule-based metrics.",2022-05-08
"Attract me to Buy: Advertisement Copywriting Generation with Multimodal
  Multi-structured Information",2022-05-07 03:33:00+00:00,http://arxiv.org/abs/2205.03534v1,"Zhipeng Zhang, Xinglin Hou, Kai Niu, Zhongzhen Huang, Tiezheng Ge, Yuning Jiang, Qi Wu, Peng Wang","cs.CL, cs.CV, cs.MM",image2text,"Recently, online shopping has gradually become a common way of shopping for
people all over the world. Wonderful merchandise advertisements often attract
more people to buy. These advertisements properly integrate multimodal
multi-structured information of commodities, such as visual spatial information
and fine-grained structure information. However, traditional multimodal text
generation focuses on the conventional description of what existed and
happened, which does not match the requirement of advertisement copywriting in
the real world. Because advertisement copywriting has a vivid language style
and higher requirements of faithfulness. Unfortunately, there is a lack of
reusable evaluation frameworks and a scarcity of datasets. Therefore, we
present a dataset, E-MMAD (e-commercial multimodal multi-structured
advertisement copywriting), which requires, and supports much more detailed
information in text generation. Noticeably, it is one of the largest video
captioning datasets in this field. Accordingly, we propose a baseline method
and faithfulness evaluation metric on the strength of structured information
reasoning to solve the demand in reality on this dataset. It surpasses the
previous methods by a large margin on all metrics. The dataset and method are
coming soon on \url{https://e-mmad.github.io/e-mmad.net/index.html}.",2022-05-07
Language Models Can See: Plugging Visual Controls in Text Generation,2022-05-05 13:56:18+00:00,http://arxiv.org/abs/2205.02655v1,"Yixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani Yogatama, Yan Wang, Lingpeng Kong, Nigel Collier","cs.CV, cs.CL",image2text,"Generative language models (LMs) such as GPT-2/3 can be prompted to generate
text with remarkable quality. While they are designed for text-prompted
generation, it remains an open question how the generation process could be
guided by modalities beyond text such as images. In this work, we propose a
training-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP),
for plugging in visual controls in the generation process and enabling LMs to
perform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC
is a simple yet efficient plug-and-play framework, which directly combines an
off-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP)
for image-grounded text generation. During decoding, MAGIC influences the
generation of the LM by introducing a CLIP-induced score, called magic score,
which regularizes the generated result to be semantically related to a given
image while being coherent to the previously generated context. Notably, the
proposed decoding scheme does not involve any gradient update operation,
therefore being computationally efficient. On the challenging task of zero-shot
image captioning, MAGIC outperforms the state-of-the-art method by notable
margins with a nearly 27 times decoding speedup. MAGIC is a flexible framework
and is theoretically compatible with any text generation tasks that incorporate
image grounding. In the experiments, we showcase that it is also capable of
performing visually grounded story generation given both an image and a text
prompt.",2022-05-05
Diverse Image Captioning with Grounded Style,2022-05-03 22:57:59+00:00,http://arxiv.org/abs/2205.01813v1,"Franz Klein, Shweta Mahajan, Stefan Roth","cs.CV, cs.LG",image2text,"Stylized image captioning as presented in prior work aims to generate
captions that reflect characteristics beyond a factual description of the scene
composition, such as sentiments. Such prior work relies on given sentiment
identifiers, which are used to express a certain global style in the caption,
e.g. positive or negative, however without taking into account the stylistic
content of the visual scene. To address this shortcoming, we first analyze the
limitations of current stylized captioning datasets and propose COCO
attribute-based augmentations to obtain varied stylized captions from COCO
annotations. Furthermore, we encode the stylized information in the latent
space of a Variational Autoencoder; specifically, we leverage extracted image
attributes to explicitly structure its sequential latent space according to
different localized style characteristics. Our experiments on the Senticap and
COCO datasets show the ability of our approach to generate accurate captions
with diversity in styles that are grounded in the image.",2022-05-03
Cross-modal Memory Networks for Radiology Report Generation,2022-04-28 02:32:53+00:00,http://arxiv.org/abs/2204.13258v1,"Zhihong Chen, Yaling Shen, Yan Song, Xiang Wan",cs.CL,image2text,"Medical imaging plays a significant role in clinical practice of medical
diagnosis, where the text reports of the images are essential in understanding
them and facilitating later treatments. By generating the reports
automatically, it is beneficial to help lighten the burden of radiologists and
significantly promote clinical automation, which already attracts much
attention in applying artificial intelligence to medical domain. Previous
studies mainly follow the encoder-decoder paradigm and focus on the aspect of
text generation, with few studies considering the importance of cross-modal
mappings and explicitly exploit such mappings to facilitate radiology report
generation. In this paper, we propose a cross-modal memory networks (CMN) to
enhance the encoder-decoder framework for radiology report generation, where a
shared memory is designed to record the alignment between images and texts so
as to facilitate the interaction and generation across modalities. Experimental
results illustrate the effectiveness of our proposed model, where
state-of-the-art performance is achieved on two widely used benchmark datasets,
i.e., IU X-Ray and MIMIC-CXR. Further analyses also prove that our model is
able to better align information from radiology images and texts so as to help
generating more accurate reports in terms of clinical indicators.",2022-04-28
"Recovering Patient Journeys: A Corpus of Biomedical Entities and
  Relations on Twitter (BEAR)",2022-04-21 08:18:44+00:00,http://arxiv.org/abs/2204.09952v1,"Amelie Wührl, Roman Klinger","cs.CL, cs.IR",image2text,"Text mining and information extraction for the medical domain has focused on
scientific text generated by researchers. However, their direct access to
individual patient experiences or patient-doctor interactions can be limited.
Information provided on social media, e.g., by patients and their relatives,
complements the knowledge in scientific text. It reflects the patient's journey
and their subjective perspective on the process of developing symptoms, being
diagnosed and offered a treatment, being cured or learning to live with a
medical condition. The value of this type of data is therefore twofold:
Firstly, it offers direct access to people's perspectives. Secondly, it might
cover information that is not available elsewhere, including self-treatment or
self-diagnoses. Named entity recognition and relation extraction are methods to
structure information that is available in unstructured text. However, existing
medical social media corpora focused on a comparably small set of entities and
relations and particular domains, rather than putting the patient into the
center of analyses. With this paper we contribute a corpus with a rich set of
annotation layers following the motivation to uncover and model patients'
journeys and experiences in more detail. We label 14 entity classes (incl.
environmental factors, diagnostics, biochemical processes, patients'
quality-of-life descriptions, pathogens, medical conditions, and treatments)
and 20 relation classes (e.g., prevents, influences, interactions, causes) most
of which have not been considered before for social media data. The publicly
available dataset consists of 2,100 tweets with approx. 6,000 entity and 3,000
relation annotations. In a corpus analysis we find that over 80 % of documents
contain relevant entities. Over 50 % of tweets express relations which we
consider essential for uncovering patients' narratives about their journeys.",2022-04-21
"Evaluating Mixed-initiative Conversational Search Systems via User
  Simulation",2022-04-17 16:27:33+00:00,http://arxiv.org/abs/2204.08046v1,"Ivan Sekulić, Mohammad Aliannejadi, Fabio Crestani","cs.CL, cs.IR",image2text,"Clarifying the underlying user information need by asking clarifying
questions is an important feature of modern conversational search system.
However, evaluation of such systems through answering prompted clarifying
questions requires significant human effort, which can be time-consuming and
expensive. In this paper, we propose a conversational User Simulator, called
USi, for automatic evaluation of such conversational search systems. Given a
description of an information need, USi is capable of automatically answering
clarifying questions about the topic throughout the search session. Through a
set of experiments, including automated natural language generation metrics and
crowdsourcing studies, we show that responses generated by USi are both inline
with the underlying information need and comparable to human-generated answers.
Moreover, we make the first steps towards multi-turn interactions, where
conversational search systems asks multiple questions to the (simulated) user
with a goal of clarifying the user need. To this end, we expand on currently
available datasets for studying clarifying questions, i.e., Qulac and ClariQ,
by performing a crowdsourcing-based multi-turn data acquisition. We show that
our generative, GPT2-based model, is capable of providing accurate and natural
answers to unseen clarifying questions in the single-turn setting and discuss
capabilities of our model in the multi-turn setting. We provide the code, data,
and the pre-trained model to be used for further research on the topic.",2022-04-17
"Regularization-based Pruning of Irrelevant Weights in Deep Neural
  Architectures",2022-04-11 09:44:16+00:00,http://arxiv.org/abs/2204.04977v1,"Giovanni Bonetta, Matteo Ribero, Rossella Cancelliere","cs.CL, cs.AI",image2text,"Deep neural networks exploiting millions of parameters are nowadays the norm
in deep learning applications. This is a potential issue because of the great
amount of computational resources needed for training, and of the possible loss
of generalization performance of overparametrized networks. We propose in this
paper a method for learning sparse neural topologies via a regularization
technique which identifies non relevant weights and selectively shrinks their
norm, while performing a classic update for relevant ones. This technique,
which is an improvement of classical weight decay, is based on the definition
of a regularization term which can be added to any loss functional regardless
of its form, resulting in a unified general framework exploitable in many
different contexts. The actual elimination of parameters identified as
irrelevant is handled by an iterative pruning algorithm. We tested the proposed
technique on different image classification and Natural language generation
tasks, obtaining results on par or better then competitors in terms of sparsity
and metrics, while achieving strong models compression.",2022-04-11
"Explaining Deep Convolutional Neural Networks via Latent Visual-Semantic
  Filter Attention",2022-04-10 04:57:56+00:00,http://arxiv.org/abs/2204.04601v1,"Yu Yang, Seungbae Kim, Jungseock Joo","cs.CV, cs.AI, cs.LG",image2text,"Interpretability is an important property for visual models as it helps
researchers and users understand the internal mechanism of a complex model.
However, generating semantic explanations about the learned representation is
challenging without direct supervision to produce such explanations. We propose
a general framework, Latent Visual Semantic Explainer (LaViSE), to teach any
existing convolutional neural network to generate text descriptions about its
own latent representations at the filter level. Our method constructs a mapping
between the visual and semantic spaces using generic image datasets, using
images and category names. It then transfers the mapping to the target domain
which does not have semantic labels. The proposed framework employs a modular
structure and enables to analyze any trained network whether or not its
original training data is available. We show that our method can generate novel
descriptions for learned filters beyond the set of categories defined in the
training dataset and perform an extensive evaluation on multiple datasets. We
also demonstrate a novel application of our method for unsupervised dataset
bias analysis which allows us to automatically discover hidden biases in
datasets or compare different subsets without using additional labels. The
dataset and code are made public to facilitate further research.",2022-04-10
On Distinctive Image Captioning via Comparing and Reweighting,2022-04-08 08:59:23+00:00,http://arxiv.org/abs/2204.03938v1,"Jiuniu Wang, Wenjia Xu, Qingzhong Wang, Antoni B. Chan","cs.CV, cs.AI",image2text,"Recent image captioning models are achieving impressive results based on
popular metrics, i.e., BLEU, CIDEr, and SPICE. However, focusing on the most
popular metrics that only consider the overlap between the generated captions
and human annotation could result in using common words and phrases, which
lacks distinctiveness, i.e., many similar images have the same caption. In this
paper, we aim to improve the distinctiveness of image captions via comparing
and reweighting with a set of similar images. First, we propose a
distinctiveness metric -- between-set CIDEr (CIDErBtw) to evaluate the
distinctiveness of a caption with respect to those of similar images. Our
metric reveals that the human annotations of each image in the MSCOCO dataset
are not equivalent based on distinctiveness; however, previous works normally
treat the human annotations equally during training, which could be a reason
for generating less distinctive captions. In contrast, we reweight each
ground-truth caption according to its distinctiveness during training. We
further integrate a long-tailed weight strategy to highlight the rare words
that contain more information, and captions from the similar image set are
sampled as negative examples to encourage the generated sentence to be unique.
Finally, extensive experiments are conducted, showing that our proposed
approach significantly improves both distinctiveness (as measured by CIDErBtw
and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide
variety of image captioning baselines. These results are further confirmed
through a user study.",2022-04-08
CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations,2022-04-05 17:38:04+00:00,http://arxiv.org/abs/2204.02380v1,"Leonard Salewski, A. Sophia Koepke, Hendrik P. A. Lensch, Zeynep Akata","cs.CV, cs.CL",image2text,"Providing explanations in the context of Visual Question Answering (VQA)
presents a fundamental problem in machine learning. To obtain detailed insights
into the process of generating natural language explanations for VQA, we
introduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with
natural language explanations. For each image-question pair in the CLEVR
dataset, CLEVR-X contains multiple structured textual explanations which are
derived from the original scene graphs. By construction, the CLEVR-X
explanations are correct and describe the reasoning and visual information that
is necessary to answer a given question. We conducted a user study to confirm
that the ground-truth explanations in our proposed dataset are indeed complete
and relevant. We present baseline results for generating natural language
explanations in the context of VQA using two state-of-the-art frameworks on the
CLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation
generation quality for different question and answer types. Additionally, we
study the influence of using different numbers of ground-truth explanations on
the convergence of natural language generation (NLG) metrics. The CLEVR-X
dataset is publicly available at
\url{https://explainableml.github.io/CLEVR-X/}.",2022-04-05
Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language,2022-04-01 17:43:13+00:00,http://arxiv.org/abs/2204.00598v1,"Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Pete Florence","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Large foundation models can exhibit unique capabilities depending on the
domain of data they are trained on. While these domains are generic, they may
only barely overlap. For example, visual-language models (VLMs) are trained on
Internet-scale image captions, but large language models (LMs) are further
trained on Internet-scale text with no images (e.g. from spreadsheets, to SAT
questions). As a result, these models store different forms of commonsense
knowledge across different domains. In this work, we show that this model
diversity is symbiotic, and can be leveraged to build AI systems with
structured Socratic dialogue -- in which new multimodal tasks are formulated as
a guided language-based exchange between different pre-existing foundation
models, without additional finetuning. In the context of egocentric perception,
we present a case study of Socratic Models (SMs) that can provide meaningful
results for complex tasks such as generating free-form answers to contextual
questions about egocentric video, by formulating video Q&A as short story Q&A,
i.e. summarizing the video into a short story, then answering questions about
it. Additionally, SMs can generate captions for Internet images, and are
competitive with state-of-the-art on zero-shot video-to-text retrieval with
42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate how to compose foundation models
zero-shot to capture new multimodal functionalities, without domain-specific
data collection. Prototypes are available at socraticmodels.github.io.",2022-04-01
Neural Pipeline for Zero-Shot Data-to-Text Generation,2022-03-30 13:14:35+00:00,http://arxiv.org/abs/2203.16279v1,"Zdeněk Kasner, Ondřej Dušek",cs.CL,image2text,"In data-to-text (D2T) generation, training on in-domain data leads to
overfitting to the data representation and repeating training data noise. We
examine how to avoid finetuning pretrained language models (PLMs) on D2T
generation datasets while still taking advantage of surface realization
capabilities of PLMs. Inspired by pipeline approaches, we propose to generate
text by transforming single-item descriptions with a sequence of modules
trained on general-domain text-based operations: ordering, aggregation, and
paragraph compression. We train PLMs for performing these operations on a
synthetic corpus WikiFluent which we build from English Wikipedia. Our
experiments on two major triple-to-text datasets -- WebNLG and E2E -- show that
our approach enables D2T generation from RDF triples in zero-shot settings.",2022-03-30
"GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate
  Degradation of Artificial Neural Language Models",2022-03-25 00:25:42+00:00,http://arxiv.org/abs/2203.13397v1,"Changye Li, David Knopman, Weizhe Xu, Trevor Cohen, Serguei Pakhomov",cs.CL,image2text,"Deep learning (DL) techniques involving fine-tuning large numbers of model
parameters have delivered impressive performance on the task of discriminating
between language produced by cognitively healthy individuals, and those with
Alzheimer's disease (AD). However, questions remain about their ability to
generalize beyond the small reference sets that are publicly available for
research. As an alternative to fitting model parameters directly, we propose a
novel method by which a Transformer DL model (GPT-2) pre-trained on general
English text is paired with an artificially degraded version of itself (GPT-D),
to compute the ratio between these two models' \textit{perplexities} on
language from cognitively healthy and impaired individuals. This technique
approaches state-of-the-art performance on text data from a widely used ""Cookie
Theft"" picture description task, and unlike established alternatives also
generalizes well to spontaneous conversations. Furthermore, GPT-D generates
text with characteristics known to be associated with AD, demonstrating the
induction of dementia-related linguistic anomalies. Our study is a step toward
better understanding of the relationships between the inner workings of
generative neural language models, the language that they produce, and the
deleterious effects of dementia on human speech and language characteristics.",2022-03-25
Chart-to-Text: A Large-Scale Benchmark for Chart Summarization,2022-03-12 17:01:38+00:00,http://arxiv.org/abs/2203.06486v1,"Shankar Kanthara, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, Shafiq Joty",cs.CL,image2text,"Charts are commonly used for exploring data and communicating insights.
Generating natural language summaries from charts can be very helpful for
people in inferring key insights that would otherwise require a lot of
cognitive and perceptual efforts. We present Chart-to-text, a large-scale
benchmark with two datasets and a total of 44,096 charts covering a wide range
of topics and chart types. We explain the dataset construction process and
analyze the datasets. We also introduce a number of state-of-the-art neural
models as baselines that utilize image captioning and data-to-text generation
techniques to tackle two problem variations: one assumes the underlying data
table of the chart is available while the other needs to extract data from
chart images. Our analysis with automatic and human evaluation shows that while
our best models usually generate fluent summaries and yield reasonable BLEU
scores, they also suffer from hallucinations and factual errors as well as
difficulties in correctly explaining complex patterns and trends in charts.",2022-03-12
Compilable Neural Code Generation with Compiler Feedback,2022-03-10 03:15:17+00:00,http://arxiv.org/abs/2203.05132v1,"Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, Qun Liu","cs.CL, cs.AI, cs.PL",image2text,"Automatically generating compilable programs with (or without) natural
language descriptions has always been a touchstone problem for computational
linguistics and automated software engineering. Existing deep-learning
approaches model code generation as text generation, either constrained by
grammar structures in decoder, or driven by pre-trained language models on
large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of
them account for compilability of the generated programs. To improve
compilability of the generated programs, this paper proposes COMPCODER, a
three-stage pipeline utilizing compiler feedback for compilable code
generation, including language model fine-tuning, compilability reinforcement,
and compilability discrimination. Comprehensive experiments on two code
generation tasks demonstrate the effectiveness of our proposed approach,
improving the success rate of compilation from 44.18 to 89.18 in code
completion on average and from 70.3 to 96.2 in text-to-code generation,
respectively, when comparing with the state-of-the-art CodeGPT.",2022-03-10
"How to Fill the Optimum Set? Population Gradient Descent with Harmless
  Diversity",2022-02-16 23:40:18+00:00,http://arxiv.org/abs/2202.08376v1,"Chengyue Gong, Lemeng Wu, Qiang Liu","cs.LG, cs.CV",image2text,"Although traditional optimization methods focus on finding a single optimal
solution, most objective functions in modern machine learning problems,
especially those in deep learning, often have multiple or infinite numbers of
optima. Therefore, it is useful to consider the problem of finding a set of
diverse points in the optimum set of an objective function. In this work, we
frame this problem as a bi-level optimization problem of maximizing a diversity
score inside the optimum set of the main loss function, and solve it with a
simple population gradient descent framework that iteratively updates the
points to maximize the diversity score in a fashion that does not hurt the
optimization of the main loss. We demonstrate that our method can efficiently
generate diverse solutions on a variety of applications, including
text-to-image generation, text-to-mesh generation, molecular conformation
generation and ensemble neural network training.",2022-02-16
"Deep soccer captioning with transformer: dataset, semantics-related
  losses, and multi-level evaluation",2022-02-11 16:04:03+00:00,http://arxiv.org/abs/2202.05728v1,"Ahmad Hammoudeh, Bastein Vanderplaetse, Stéphane Dupont","cs.CV, cs.AI",image2text,"This work aims at generating captions for soccer videos using deep learning.
In this context, this paper introduces a dataset, model, and triple-level
evaluation. The dataset consists of 22k caption-clip pairs and three visual
features (images, optical flow, inpainting) for ~500 hours of \emph{SoccerNet}
videos. The model is divided into three parts: a transformer learns language,
ConvNets learn vision, and a fusion of linguistic and visual features generates
captions. The paper suggests evaluating generated captions at three levels:
syntax (the commonly used evaluation metrics such as BLEU-score and CIDEr),
meaning (the quality of descriptions for a domain expert), and corpus (the
diversity of generated captions). The paper shows that the diversity of
generated captions has improved (from 0.07 reaching 0.18) with
semantics-related losses that prioritize selected words. Semantics-related
losses and the utilization of more visual features (optical flow, inpainting)
improved the normalized captioning score by 28\%. The web page of this work:
https://sites.google.com/view/soccercaptioning}{https://sites.google.com/view/soccercaptioning",2022-02-11
"Unifying Architectures, Tasks, and Modalities Through a Simple
  Sequence-to-Sequence Learning Framework",2022-02-07 10:38:21+00:00,http://arxiv.org/abs/2202.03052v1,"Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang","cs.CV, cs.CL",image2text,"In this work, we pursue a unified paradigm for multimodal pretraining to
break the scaffolds of complex task/modality-specific customization. We propose
OFA, a unified multimodal pretrained model that unifies modalities (i.e.,
cross-modality, vision, language) and tasks (e.g., image generation, visual
grounding, image captioning, image classification, text generation, etc.) to a
simple sequence-to-sequence learning framework based on the encoder-decoder
architecture. OFA performs pretraining and finetuning with task instructions
and introduces no extra task-specific layers for finetuning. Experimental
results show that OFA achieves new state-of-the-arts on a series of multimodal
tasks, including image captioning (COCO test CIDEr: 149.6), text-to-image
generation (COCO test FID: 10.5), VQA (test-std acc.: 80.02), SNLI-VE (test
acc.: 90.20), and referring expression comprehension (RefCOCO / RefCOCO+ /
RefCOCOg test acc.: 92.93 / 90.10 / 85.20). Through extensive analyses, we
demonstrate that OFA reaches comparable performance with uni-modal pretrained
models (e.g., BERT, MAE, MoCo v3, SimCLR v2, etc.) in uni-modal tasks,
including NLU, NLG, and image classification, and it effectively transfers to
unseen tasks and domains. Code shall be released soon at
http://github.com/OFA-Sys/OFA",2022-02-07
"XAlign: Cross-lingual Fact-to-Text Alignment and Generation for
  Low-Resource Languages",2022-02-01 09:41:59+00:00,http://arxiv.org/abs/2202.00291v1,"Tushar Abhishek, Shivprasad Sagare, Bhavyajeet Singh, Anubhav Sharma, Manish Gupta, Vasudeva Varma",cs.CL,image2text,"Multiple critical scenarios (like Wikipedia text generation given English
Infoboxes) need automated generation of descriptive text in low resource (LR)
languages from English fact triples. Previous work has focused on English
fact-to-text (F2T) generation. To the best of our knowledge, there has been no
previous attempt on cross-lingual alignment or generation for LR languages.
Building an effective cross-lingual F2T (XF2T) system requires alignment
between English structured facts and LR sentences. We propose two unsupervised
methods for cross-lingual alignment. We contribute XALIGN, an XF2T dataset with
0.45M pairs across 8 languages, of which 5402 pairs have been manually
annotated. We also train strong baseline XF2T generation models on the XAlign
dataset.",2022-02-01
"BERTHA: Video Captioning Evaluation Via Transfer-Learned Human
  Assessment",2022-01-25 11:29:58+00:00,http://arxiv.org/abs/2201.10243v1,"Luis Lebron, Yvette Graham, Kevin McGuinness, Konstantinos Kouramas, Noel E. O'Connor","cs.CV, cs.LG",image2text,"Evaluating video captioning systems is a challenging task as there are
multiple factors to consider; for instance: the fluency of the caption,
multiple actions happening in a single scene, and the human bias of what is
considered important. Most metrics try to measure how similar the system
generated captions are to a single or a set of human-annotated captions. This
paper presents a new method based on a deep learning model to evaluate these
systems. The model is based on BERT, which is a language model that has been
shown to work well in multiple NLP tasks. The aim is for the model to learn to
perform an evaluation similar to that of a human. To do so, we use a dataset
that contains human evaluations of system generated captions. The dataset
consists of the human judgments of the captions produce by the system
participating in various years of the TRECVid video to text task. These
annotations will be made publicly available. BERTHA obtain favourable results,
outperforming the commonly used metrics in some setups.",2022-01-25
Pre-Trained Language Transformers are Universal Image Classifiers,2022-01-25 08:56:14+00:00,http://arxiv.org/abs/2201.10182v1,"Rahul Goel, Modar Sulaiman, Kimia Noorbakhsh, Mahdi Sharifi, Rajesh Sharma, Pooyan Jamshidi, Kallol Roy","cs.CV, cs.AI",image2text,"Facial images disclose many hidden personal traits such as age, gender, race,
health, emotion, and psychology. Understanding these traits will help to
classify the people in different attributes. In this paper, we have presented a
novel method for classifying images using a pretrained transformer model. We
apply the pretrained transformer for the binary classification of facial images
in criminal and non-criminal classes. The pretrained transformer of GPT-2 is
trained to generate text and then fine-tuned to classify facial images. During
the finetuning process with images, most of the layers of GT-2 are frozen
during backpropagation and the model is frozen pretrained transformer (FPT).
The FPT acts as a universal image classifier, and this paper shows the
application of FPT on facial images. We also use our FPT on encrypted images
for classification. Our FPT shows high accuracy on both raw facial images and
encrypted images. We hypothesize the meta-learning capacity FPT gained because
of its large size and trained on a large size with theory and experiments. The
GPT-2 trained to generate a single word token at a time, through the
autoregressive process, forced to heavy-tail distribution. Then the FPT uses
the heavy-tail property as its meta-learning capacity for classifying images.
Our work shows one way to avoid bias during the machine classification of
images.The FPT encodes worldly knowledge because of the pretraining of one
text, which it uses during the classification. The statistical error of
classification is reduced because of the added context gained from the text.Our
paper shows the ethical dimension of using encrypted data for
classification.Criminal images are sensitive to share across the boundary but
encrypted largely evades ethical concern.FPT showing good classification
accuracy on encrypted images shows promise for further research on
privacy-preserving machine learning.",2022-01-25
An Integrated Approach for Video Captioning and Applications,2022-01-23 01:06:00+00:00,http://arxiv.org/abs/2201.09153v1,"Soheyla Amirian, Thiab R. Taha, Khaled Rasheed, Hamid R. Arabnia","cs.CV, cs.AI",image2text,"Physical computing infrastructure, data gathering, and algorithms have
recently had significant advances to extract information from images and
videos. The growth has been especially outstanding in image captioning and
video captioning. However, most of the advancements in video captioning still
take place in short videos. In this research, we caption longer videos only by
using the keyframes, which are a small subset of the total video frames.
Instead of processing thousands of frames, only a few frames are processed
depending on the number of keyframes. There is a trade-off between the
computation of many frames and the speed of the captioning process. The
approach in this research is to allow the user to specify the trade-off between
execution time and accuracy. In addition, we argue that linking images, videos,
and natural language offers many practical benefits and immediate practical
applications. From the modeling perspective, instead of designing and staging
explicit algorithms to process videos and generate captions in complex
processing pipelines, our contribution lies in designing hybrid deep learning
architectures to apply in long videos by captioning video keyframes. We
consider the technology and the methodology that we have developed as steps
toward the applications discussed in this research.",2022-01-23
"Inferring Commonsense Explanations as Prompts for Future Event
  Generation",2022-01-18 16:21:23+00:00,http://arxiv.org/abs/2201.07099v1,"Li Lin, Yixin Cao, Lifu Huang, Shuang Li, Xuming Hu, Lijie Wen, Jianmin Wang","cs.CL, cs.LG, I.2.7; I.2.4",image2text,"Future Event Generation aims to generate fluent and reasonable future event
descriptions given preceding events. It requires not only fluent text
generation but also commonsense reasoning to maintain the coherence of the
entire event story. However, existing FEG methods are easily trapped into
repeated or general events without imposing any logical constraint to the
generation process. In this paper, we propose a novel explainable FEG framework
that consists of a commonsense inference model (IM) and an event generation
model (GM). The IM, which is pre-trained on a commonsense knowledge graph
ATOMIC, learns to interpret the preceding events and conducts commonsense
reasoning to reveal the characters psychology such as intent, reaction, and
needs as latent variables. GM further takes the commonsense knowledge as
prompts to guide and enforce the generation of logistically coherent future
events. As unique merit, the commonsense prompts can be further decoded into
textual descriptions, yielding explanations for the future event. Automatic and
human evaluation demonstrate that our approach can generate more coherent,
specific, and logical future events than the strong baselines.",2022-01-18
Local Information Assisted Attention-free Decoder for Audio Captioning,2022-01-10 08:55:52+00:00,http://arxiv.org/abs/2201.03217v1,"Feiyang Xiao, Jian Guan, Qiaoxi Zhu, Haiyan Lan, Wenwu Wang","cs.SD, cs.LG, eess.AS",image2text,"Automated audio captioning (AAC) aims to describe audio data with captions
using natural language. Most existing AAC methods adopt an encoder-decoder
structure, where the attention based mechanism is a popular choice in the
decoder (e.g., Transformer decoder) for predicting captions from audio
features. Such attention based decoders can capture the global information from
the audio features, however, their ability in extracting local information can
be limited, which may lead to degraded quality in the generated captions. In
this paper, we present an AAC method with an attention-free decoder, where an
encoder based on PANNs is employed for audio feature extraction, and the
attention-free decoder is designed to introduce local information. The proposed
method enables the effective use of both global and local information from
audio signals. Experiments show that our method outperforms the
state-of-the-art methods with the standard attention based decoder in Task 6 of
the DCASE 2021 Challenge.",2022-01-10
Self-Training Vision Language BERTs with a Unified Conditional Model,2022-01-06 11:00:52+00:00,http://arxiv.org/abs/2201.02010v1,"Xiaofeng Yang, Fengmao Lv, Fayao Liu, Guosheng Lin","cs.CV, cs.CL",image2text,"Natural language BERTs are trained with language corpus in a self-supervised
manner. Unlike natural language BERTs, vision language BERTs need paired data
to train, which restricts the scale of VL-BERT pretraining. We propose a
self-training approach that allows training VL-BERTs from unlabeled image data.
The proposed method starts with our unified conditional model -- a vision
language BERT model that can perform zero-shot conditional generation. Given
different conditions, the unified conditional model can generate captions,
dense captions, and even questions. We use the labeled image data to train a
teacher model and use the trained model to generate pseudo captions on
unlabeled image data. We then combine the labeled data and pseudo labeled data
to train a student model. The process is iterated by putting the student model
as a new teacher. By using the proposed self-training approach and only 300k
unlabeled extra data, we are able to get competitive or even better
performances compared to the models of similar model size trained with 3
million extra image data.",2022-01-06
Compact Bidirectional Transformer for Image Captioning,2022-01-06 09:23:18+00:00,http://arxiv.org/abs/2201.01984v1,"Yuanen Zhou, Zhenzhen Hu, Daqing Liu, Huixia Ben, Meng Wang","cs.CV, cs.CL",image2text,"Most current image captioning models typically generate captions from left to
right. This unidirectional property makes them can only leverage past context
but not future context. Though recent refinement-based models can exploit both
past and future context by generating a new caption in the second stage based
on pre-retrieved or pre-generated captions in the first stage, the decoder of
these models generally consists of two networks~(i.e. a retriever or captioner
in the first stage and a refiner in the second stage), which can only be
executed sequentially. In this paper, we introduce a Compact Bidirectional
Transformer model for image captioning that can leverage bidirectional context
implicitly and explicitly while the decoder can be executed parallelly.
Specifically, it is implemented by tightly coupling left-to-right(L2R) and
right-to-left(R2L) flows into a single compact model~(i.e. implicitly) and
optionally allowing interaction of the two flows(i.e. explicitly), while the
final caption is chosen from either L2R or R2L flow in a sentence-level
ensemble manner. We conduct extensive ablation studies on the MSCOCO benchmark
and find that the compact architecture, which serves as a regularization for
implicitly exploiting bidirectional context, and the sentence-level ensemble
play more important roles than the explicit interaction mechanism. By combining
with word-level ensemble seamlessly, the effect of the sentence-level ensemble
is further enlarged. We further extend the conventional one-flow self-critical
training to the two-flows version under this architecture and achieve new
state-of-the-art results in comparison with non-vision-language-pretraining
models. Source code is available at
{\color{magenta}\url{https://github.com/YuanEZhou/CBTrans}}.",2022-01-06
"StyleM: Stylized Metrics for Image Captioning Built with Contrastive
  N-grams",2022-01-04 04:44:05+00:00,http://arxiv.org/abs/2201.00975v1,"Chengxi Li, Brent Harrison","cs.CV, cs.AI, cs.CL",image2text,"In this paper, we build two automatic evaluation metrics for evaluating the
association between a machine-generated caption and a ground truth stylized
caption: OnlyStyle and StyleCIDEr.",2022-01-04
"ERNIE-ViLG: Unified Generative Pre-training for Bidirectional
  Vision-Language Generation",2021-12-31 03:53:33+00:00,http://arxiv.org/abs/2112.15283v1,"Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang","cs.CV, cs.CL",image2text,"Conventional methods for the image-text generation tasks mainly tackle the
naturally bidirectional generation tasks separately, focusing on designing
task-specific frameworks to improve the quality and fidelity of the generated
samples. Recently, Vision-Language Pre-training models have greatly improved
the performance of the image-to-text generation tasks, but large-scale
pre-training models for text-to-image synthesis task are still under-developed.
In this paper, we propose ERNIE-ViLG, a unified generative pre-training
framework for bidirectional image-text generation with transformer model. Based
on the image quantization models, we formulate both image generation and text
generation as autoregressive generative tasks conditioned on the text/image
input. The bidirectional image-text generative modeling eases the semantic
alignments across vision and language. For the text-to-image generation
process, we further propose an end-to-end training method to jointly learn the
visual sequence generator and the image reconstructor. To explore the landscape
of large-scale pre-training for bidirectional text-image generation, we train a
10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million
(Chinese) image-text pairs which achieves state-of-the-art performance for both
text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for
text-to-image synthesis and best results on COCO-CN and AIC-ICC for image
captioning.",2021-12-31
"Radiology Report Generation with a Learned Knowledge Base and
  Multi-modal Alignment",2021-12-30 10:43:56+00:00,http://arxiv.org/abs/2112.15011v1,"Shuxin Yang, Xian Wu, Shen Ge, Xingwang Wu, S. Kevin Zhou, Li Xiao","eess.IV, cs.CL, cs.CV",image2text,"In clinics, a radiology report is crucial for guiding a patient's treatment.
Unfortunately, report writing imposes a heavy burden on radiologists. To
effectively reduce such a burden, we hereby present an automatic, multi-modal
approach for report generation from chest x-ray. Our approach, motivated by the
observation that the descriptions in radiology reports are highly correlated
with the x-ray images, features two distinct modules: (i) Learned knowledge
base. To absorb the knowledge embedded in the above-mentioned correlation, we
automatically build a knowledge base based on textual embedding. (ii)
Multi-modal alignment. To promote the semantic alignment among reports, disease
labels and images, we explicitly utilize textual embedding to guide the
learning of the visual feature space. We evaluate the performance of the
proposed model using metrics from both natural language generation and clinic
efficacy on the public IU and MIMIC-CXR datasets. Our ablation study shows that
each module contributes to improving the quality of generated reports.
Furthermore, with the aid of both modules, our approach clearly outperforms
state-of-the-art methods.",2021-12-30
Automatic Product Copywriting for E-Commerce,2021-12-15 19:06:31+00:00,http://arxiv.org/abs/2112.11915v1,"Xueying Zhang, Yanyan Zou, Hainan Zhang, Jing Zhou, Shiliang Diao, Jiajia Chen, Zhuoye Ding, Zhen He, Xueqi He, Yun Xiao, Bo Long, Han Yu, Lingfei Wu","cs.CL, cs.AI",image2text,"Product copywriting is a critical component of e-commerce recommendation
platforms. It aims to attract users' interest and improve user experience by
highlighting product characteristics with textual descriptions. In this paper,
we report our experience deploying the proposed Automatic Product Copywriting
Generation (APCG) system into the JD.com e-commerce product recommendation
platform. It consists of two main components: 1) natural language generation,
which is built from a transformer-pointer network and a pre-trained
sequence-to-sequence model based on millions of training data from our in-house
platform; and 2) copywriting quality control, which is based on both automatic
evaluation and human screening. For selected domains, the models are trained
and updated daily with the updated training data. In addition, the model is
also used as a real-time writing assistant tool on our live broadcast platform.
The APCG system has been deployed in JD.com since Feb 2021. By Sep 2021, it has
generated 2.53 million product descriptions, and improved the overall averaged
click-through rate (CTR) and the Conversion Rate (CVR) by 4.22% and 3.61%,
compared to baselines, respectively on a year-on-year basis. The accumulated
Gross Merchandise Volume (GMV) made by our system is improved by 213.42%,
compared to the number in Feb 2021.",2021-12-15
Contextualized Scene Imagination for Generative Commonsense Reasoning,2021-12-12 20:38:08+00:00,http://arxiv.org/abs/2112.06318v1,"PeiFeng Wang, Jonathan Zamora, Junfeng Liu, Filip Ilievski, Muhao Chen, Xiang Ren",cs.CL,image2text,"Humans use natural language to compose common concepts from their environment
into plausible, day-to-day scene descriptions. However, such generative
commonsense reasoning (GCSR) skills are lacking in state-of-the-art text
generation methods. Descriptive sentences about arbitrary concepts generated by
neural text generation models (e.g., pre-trained text-to-text Transformers) are
often grammatically fluent but may not correspond to human common sense,
largely due to their lack of mechanisms to capture concept relations, to
identify implicit concepts, and to perform generalizable reasoning about unseen
concept compositions. In this paper, we propose an Imagine-and-Verbalize (I&V)
method, which learns to imagine a relational scene knowledge graph (SKG) with
relations between the input concepts, and leverage the SKG as a constraint when
generating a plausible scene description. We collect and harmonize a set of
knowledge resources from different domains and modalities, providing a rich
auxiliary supervision signal for I&V. The experiments demonstrate the
effectiveness of I&V in improving language models on both concept-to-sentence
and concept-to-story generation tasks, while enabling the model to learn well
from fewer task examples and generate SKGs that make common sense to human
annotators.",2021-12-12
"Improving Logical-Level Natural Language Generation with
  Topic-Conditioned Data Augmentation and Logical Form Generation",2021-12-12 13:50:18+00:00,http://arxiv.org/abs/2112.06240v1,"Ao Liu, Congjian Luo, Naoaki Okazaki",cs.CL,image2text,"Logical Natural Language Generation, i.e., generating textual descriptions
that can be logically entailed by a structured table, has been a challenge due
to the low fidelity of the generation. \citet{chen2020logic2text} have
addressed this problem by annotating interim logical programs to control the
generation contents and semantics, and presented the task of table-aware
logical form to text (Logic2text) generation. However, although table instances
are abundant in the real world, logical forms paired with textual descriptions
require costly human annotation work, which limits the performance of neural
models. To mitigate this, we propose topic-conditioned data augmentation
(TopicDA), which utilizes GPT-2 to generate unpaired logical forms and textual
descriptions directly from tables. We further introduce logical form generation
(LG), a dual task of Logic2text that requires generating a valid logical form
based on a text description of a table. We also propose a semi-supervised
learning approach to jointly train a Logic2text and an LG model with both
labeled and augmented data. The two models benefit from each other by providing
extra supervision signals through back-translation. Experimental results on the
Logic2text dataset and the LG task demonstrate that our approach can
effectively utilize the augmented data and outperform supervised baselines by a
substantial margin.",2021-12-12
Show and Write: Entity-aware News Generation with Image Information,2021-12-11 05:32:09+00:00,http://arxiv.org/abs/2112.05917v1,"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer",cs.CL,image2text,"Automatically writing long articles is a complex and challenging language
generation task. Prior work has primarily focused on generating these articles
using human-written prompt to provide some topical context and some metadata
about the article. That said, for many applications, such as generating news
stories, these articles are often paired with images and their captions or
alt-text, which in turn are based on real-world events and may reference many
different named entities that are difficult to be correctly recognized and
predicted by language models. To address these two problems, this paper
introduces an Entity-aware News Generation method with Image iNformation,
Engin, to incorporate news image information into language models. Engin
produces news articles conditioned on both metadata and information such as
captions and named entities extracted from images. We also propose an
Entity-aware mechanism to help our model better recognize and predict the
entity names in news. We perform experiments on two public large-scale news
datasets, GoodNews and VisualNews. Quantitative results show that our approach
improves article perplexity by 4-5 points over the base models. Qualitative
results demonstrate the text generated by Engin is more consistent with news
images. We also perform article quality annotation experiment on the generated
articles to validate that our model produces higher-quality articles. Finally,
we investigate the effect Engin has on methods that automatically detect
machine-generated articles.",2021-12-11
"Unified Multimodal Pre-training and Prompt-based Tuning for
  Vision-Language Understanding and Generation",2021-12-10 14:59:06+00:00,http://arxiv.org/abs/2112.05587v2,"Tianyi Liu, Zuxuan Wu, Wenhan Xiong, Jingjing Chen, Yu-Gang Jiang","cs.CV, cs.CL, cs.LG",image2text,"Most existing vision-language pre-training methods focus on understanding
tasks and use BERT-like objectives (masked language modeling and image-text
matching) during pretraining. Although they perform well in many understanding
downstream tasks, e.g., visual question answering, image-text retrieval and
visual entailment, they do not possess the ability to generate. To tackle this
problem, we propose Unified multimodal pre-training for both Vision-Language
understanding and generation (UniVL). The proposed UniVL is capable of handling
both understanding tasks and generative tasks. We augment existing pretraining
paradigms that only use random masks with causal masks, i.e., triangular masks
that mask out future tokens, such that the pre-trained models can have
autoregressive generation abilities by design. We formulate several previous
understanding tasks as a text generation task and propose to use prompt-based
method for fine-tuning on different downstream tasks. Our experiments show that
there is a trade-off between understanding tasks and generation tasks while
using the same model, and a feasible way to improve both tasks is to use more
data. Our UniVL framework attains comparable performance to recent
vision-language pre-training methods on both understanding tasks and generation
tasks. Moreover, we demostrate that prompt-based finetuning is more
data-efficient - it outperforms discriminative methods in few-shot scenarios.",2021-12-10
Self-Supervised Image-to-Text and Text-to-Image Synthesis,2021-12-09 13:54:56+00:00,http://arxiv.org/abs/2112.04928v1,"Anindya Sundar Das, Sriparna Saha","cs.CV, cs.CL, cs.LG",image2text,"A comprehensive understanding of vision and language and their interrelation
are crucial to realize the underlying similarities and differences between
these modalities and to learn more generalized, meaningful representations. In
recent years, most of the works related to Text-to-Image synthesis and
Image-to-Text generation, focused on supervised generative deep architectures
to solve the problems, where very little interest was placed on learning the
similarities between the embedding spaces across modalities. In this paper, we
propose a novel self-supervised deep learning based approach towards learning
the cross-modal embedding spaces; for both image to text and text to image
generations. In our approach, we first obtain dense vector representations of
images using StackGAN-based autoencoder model and also dense vector
representations on sentence-level utilizing LSTM based text-autoencoder; then
we study the mapping from embedding space of one modality to embedding space of
the other modality utilizing GAN and maximum mean discrepancy based generative
networks. We, also demonstrate that our model learns to generate textual
description from image data as well as images from textual data both
qualitatively and quantitatively.",2021-12-09
"Search and Learn: Improving Semantic Coverage for Data-to-Text
  Generation",2021-12-06 03:51:56+00:00,http://arxiv.org/abs/2112.02770v1,"Shailza Jolly, Zi Xuan Zhang, Andreas Dengel, Lili Mou",cs.CL,image2text,"Data-to-text generation systems aim to generate text descriptions based on
input data (often represented in the tabular form). A typical system uses huge
training samples for learning the correspondence between tables and texts.
However, large training sets are expensive to obtain, limiting the
applicability of these approaches in real-world scenarios. In this work, we
focus on few-shot data-to-text generation. We observe that, while fine-tuned
pretrained language models may generate plausible sentences, they suffer from
the low semantic coverage problem in the few-shot setting. In other words,
important input slots tend to be missing in the generated text. To this end, we
propose a search-and-learning approach that leverages pretrained language
models but inserts the missing slots to improve the semantic coverage. We
further fine-tune our system based on the search results to smooth out the
search noise, yielding better-quality text and improving inference efficiency
to a large extent. Experiments show that our model achieves high performance on
E2E and WikiBio datasets. Especially, we cover 98.35% of input slots on E2E,
largely alleviating the low coverage problem.",2021-12-06
"Protecting Intellectual Property of Language Generation APIs with
  Lexical Watermark",2021-12-05 22:54:54+00:00,http://arxiv.org/abs/2112.02701v1,"Xuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu, Chenguang Wang","cs.CR, cs.CL",image2text,"Nowadays, due to the breakthrough in natural language generation (NLG),
including machine translation, document summarization, image captioning, etc
NLG models have been encapsulated in cloud APIs to serve over half a billion
people worldwide and process over one hundred billion word generations per day.
Thus, NLG APIs have already become essential profitable services in many
commercial companies. Due to the substantial financial and intellectual
investments, service providers adopt a pay-as-you-use policy to promote
sustainable market growth. However, recent works have shown that cloud
platforms suffer from financial losses imposed by model extraction attacks,
which aim to imitate the functionality and utility of the victim services, thus
violating the intellectual property (IP) of cloud APIs. This work targets at
protecting IP of NLG APIs by identifying the attackers who have utilized
watermarked responses from the victim NLG APIs. However, most existing
watermarking techniques are not directly amenable for IP protection of NLG
APIs. To bridge this gap, we first present a novel watermarking method for text
generation APIs by conducting lexical modification to the original outputs.
Compared with the competitive baselines, our watermark approach achieves better
identifiable performance in terms of p-value, with fewer semantic losses. In
addition, our watermarks are more understandable and intuitive to humans than
the baselines. Finally, the empirical studies show our approach is also
applicable to queries from different domains, and is effective on the attacker
trained on a mixture of the corpus which includes less than 10\% watermarked
samples.",2021-12-05
"Representation Learning for Conversational Data using Discourse Mutual
  Information Maximization",2021-12-04 13:17:07+00:00,http://arxiv.org/abs/2112.05787v1,"Bishal Santra, Sumegh Roychowdhury, Aishik Mandal, Vasu Gurram, Atharva Naik, Manish Gupta, Pawan Goyal",cs.CL,image2text,"Although many pretrained models exist for text or images, there have been
relatively fewer attempts to train representations specifically for dialog
understanding. Prior works usually relied on finetuned representations based on
generic text representation models like BERT or GPT-2. But, existing
pretraining objectives do not take the structural information of text into
consideration. Although generative dialog models can learn structural features
too, we argue that the structure-unaware word-by-word generation is not
suitable for effective conversation modeling. We empirically demonstrate that
such representations do not perform consistently across various dialog
understanding tasks. Hence, we propose a structure-aware Mutual Information
based loss-function DMI (Discourse Mutual Information) for training
dialog-representation models, that additionally captures the inherent
uncertainty in response prediction. Extensive evaluation on nine diverse dialog
modeling tasks shows that our proposed DMI-based models outperform strong
baselines by significant margins, even with small-scale pretraining. Our models
show the most promising performance on the dialog evaluation task
DailyDialog++, in both random and adversarial negative scenarios.",2021-12-04
"LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with
  Self-training",2021-12-02 16:49:41+00:00,http://arxiv.org/abs/2112.01404v1,"Ningyu Zhang, Hongbin Ye, Jiacheng Yang, Shumin Deng, Chuanqi Tan, Mosha Chen, Songfang Huang, Fei Huang, Huajun Chen","cs.CL, cs.AI",image2text,"Natural language generation from structured data mainly focuses on
surface-level descriptions, suffering from uncontrollable content selection and
low fidelity. Previous works leverage logical forms to facilitate logical
knowledge-conditioned text generation. Though achieving remarkable progress,
they are data-hungry, which makes the adoption for real-world applications
challenging with limited data. To this end, this paper proposes a unified
framework for logical knowledge-conditioned text generation in the few-shot
setting. With only a few seeds logical forms (e.g., 20/100 shot), our approach
leverages self-training and samples pseudo logical forms based on content and
structure consistency. Experimental results demonstrate that our approach can
obtain better few-shot performance than baselines.",2021-12-02
"Translation-equivariant Image Quantizer for Bi-directional Image-Text
  Generation",2021-12-01 10:08:24+00:00,http://arxiv.org/abs/2112.00384v1,"Woncheol Shin, Gyubok Lee, Jiyoung Lee, Joonseok Lee, Edward Choi","cs.CV, cs.CL, cs.LG",image2text,"Recently, vector-quantized image modeling has demonstrated impressive
performance on generation tasks such as text-to-image generation. However, we
discover that the current image quantizers do not satisfy translation
equivariance in the quantized space due to aliasing, degrading performance in
the downstream text-to-image generation and image-to-text generation, even in
simple experimental setups. Instead of focusing on anti-aliasing, we take a
direct approach to encourage translation equivariance in the quantized space.
In particular, we explore a desirable property of image quantizers, called
'Translation Equivariance in the Quantized Space' and propose a simple but
effective way to achieve translation equivariance by regularizing orthogonality
in the codebook embedding vectors. Using this method, we improve accuracy by
+22% in text-to-image generation and +26% in image-to-text generation,
outperforming the VQGAN.",2021-12-01
Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic,2021-11-29 11:01:49+00:00,http://arxiv.org/abs/2111.14447v1,"Yoad Tewel, Yoav Shalev, Idan Schwartz, Lior Wolf","cs.CV, cs.AI, cs.CL",image2text,"Recent text-to-image matching models apply contrastive learning to large
corpora of uncurated pairs of images and sentences. While such models can
provide a powerful score for matching and subsequent zero-shot tasks, they are
not capable of generating caption given an image. In this work, we repurpose
such models to generate a descriptive text given an image at inference time,
without any further training or tuning step. This is done by combining the
visual-semantic model with a large language model, benefiting from the
knowledge in both web-scale models. The resulting captions are much less
restrictive than those obtained by supervised captioning methods. Moreover, as
a zero-shot learning method, it is extremely flexible and we demonstrate its
ability to perform image arithmetic in which the inputs can be either images or
text and the output is a sentence. This enables novel high-level vision
capabilities such as comparing two images or solving visual analogy tests.",2021-11-29
LAFITE: Towards Language-Free Training for Text-to-Image Generation,2021-11-27 01:54:45+00:00,http://arxiv.org/abs/2111.13792v1,"Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, Tong Sun","cs.CV, cs.LG",image2text,"One of the major challenges in training text-to-image generation models is
the need of a large number of high-quality image-text pairs. While image
samples are often easily accessible, the associated text descriptions typically
require careful human captioning, which is particularly time- and
cost-consuming. In this paper, we propose the first work to train text-to-image
generation models without any text data. Our method leverages the well-aligned
multi-modal semantic space of the powerful pre-trained CLIP model: the
requirement of text-conditioning is seamlessly alleviated via generating text
features from image features. Extensive experiments are conducted to illustrate
the effectiveness of the proposed method. We obtain state-of-the-art results in
the standard text-to-image generation tasks. Importantly, the proposed
language-free model outperforms most existing models trained with full
image-text pairs. Furthermore, our method can be applied in fine-tuning
pre-trained models, which saves both training time and cost in training
text-to-image generation models. Our pre-trained model obtains competitive
results in zero-shot text-to-image generation on the MS-COCO dataset, yet with
around only 1% of the model size and training data size relative to the
recently proposed large DALL-E model.",2021-11-27
"Octree Transformer: Autoregressive 3D Shape Generation on Hierarchically
  Structured Sequences",2021-11-24 13:17:16+00:00,http://arxiv.org/abs/2111.12480v1,"Moritz Ibing, Gregor Kobsik, Leif Kobbelt","cs.CV, cs.GR, cs.LG",image2text,"Autoregressive models have proven to be very powerful in NLP text generation
tasks and lately have gained popularity for image generation as well. However,
they have seen limited use for the synthesis of 3D shapes so far. This is
mainly due to the lack of a straightforward way to linearize 3D data as well as
to scaling problems with the length of the resulting sequences when describing
complex shapes. In this work we address both of these problems. We use octrees
as a compact hierarchical shape representation that can be sequentialized by
traversal ordering. Moreover, we introduce an adaptive compression scheme, that
significantly reduces sequence lengths and thus enables their effective
generation with a transformer, while still allowing fully autoregressive
sampling and parallel training. We demonstrate the performance of our model by
comparing against the state-of-the-art in shape generation.",2021-11-24
NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,2021-11-24 11:02:12+00:00,http://arxiv.org/abs/2111.12417v1,"Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan","cs.CV, cs.AI",image2text,"This paper presents a unified multimodal pre-trained model called N\""UWA that
can generate new or manipulate existing visual data (i.e., images and videos)
for various visual synthesis tasks. To cover language, image, and video at the
same time for different scenarios, a 3D transformer encoder-decoder framework
is designed, which can not only deal with videos as 3D data but also adapt to
texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA)
mechanism is also proposed to consider the nature of the visual data and reduce
the computational complexity. We evaluate N\""UWA on 8 downstream tasks.
Compared to several strong baselines, N\""UWA achieves state-of-the-art results
on text-to-image generation, text-to-video generation, video prediction, etc.
Furthermore, it also shows surprisingly good zero-shot capabilities on
text-guided image and video manipulation tasks. Project repo is
https://github.com/microsoft/NUWA.",2021-11-24
Scaling Up Vision-Language Pre-training for Image Captioning,2021-11-24 02:30:22+00:00,http://arxiv.org/abs/2111.12233v1,"Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, Lijuan Wang","cs.CV, cs.CL",image2text,"In recent years, we have witnessed significant performance boost in the image
captioning task based on vision-language pre-training (VLP). Scale is believed
to be an important factor for this advance. However, most existing work only
focuses on pre-training transformers with moderate sizes (e.g., 12 or 24
layers) on roughly 4 million images. In this paper, we present LEMON, a
LargE-scale iMage captiONer, and provide the first empirical study on the
scaling behavior of VLP for image captioning. We use the state-of-the-art VinVL
model as our reference model, which consists of an image feature extractor and
a transformer model, and scale the transformer both up and down, with model
sizes ranging from 13 to 675 million parameters. In terms of data, we conduct
experiments with up to 200 million image-text pairs which are automatically
collected from web based on the alt attribute of the image (dubbed as ALT200M).
Extensive analysis helps to characterize the performance trend as the model
size and the pre-training data size increase. We also compare different
training recipes, especially for training on large-scale noisy data. As a
result, LEMON achieves new state of the arts on several major image captioning
benchmarks, including COCO Caption, nocaps, and Conceptual Captions. We also
show LEMON can generate captions with long-tail visual concepts when used in a
zero-shot manner.",2021-11-24
L-Verse: Bidirectional Generation Between Image and Text,2021-11-22 11:48:26+00:00,http://arxiv.org/abs/2111.11133v3,"Taehoon Kim, Gwangmo Song, Sihaeng Lee, Sangyun Kim, Yewon Seo, Soonyoung Lee, Seung Hwan Kim, Honglak Lee, Kyunghoon Bae","cs.CV, cs.CL, cs.LG",image2text,"Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalabilty. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for text-to-image and image-to-text
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation tasks without any finetuning or extra
object detection frameworks. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial results of bidirectional vision-language representation learning on
general domain. Codes available at: https://github.com/tgisaturday/L-Verse",2021-11-22
