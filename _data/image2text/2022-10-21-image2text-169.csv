title,pubdate,id,authors,categories,search,abstract,displaydate
"Improving the Factual Correctness of Radiology Report Generation with
  Semantic Rewards",2022-10-21 18:27:45+00:00,http://arxiv.org/abs/2210.12186v1,"Jean-Benoit Delbrouck, Pierre Chambon, Christian Bluethgen, Emily Tsai, Omar Almusa, Curtis P. Langlotz","cs.CL, cs.AI",image2text,"Neural image-to-text radiology report generation systems offer the potential
to improve radiology reporting by reducing the repetitive process of report
drafting and identifying possible medical errors. These systems have achieved
promising performance as measured by widely used NLG metrics such as BLEU and
CIDEr. However, the current systems face important limitations. First, they
present an increased complexity in architecture that offers only marginal
improvements on NLG metrics. Secondly, these systems that achieve high
performance on these metrics are not always factually complete or consistent
due to both inadequate training and evaluation. Recent studies have shown the
systems can be substantially improved by using new methods encouraging 1) the
generation of domain entities consistent with the reference and 2) describing
these entities in inferentially consistent ways. So far, these methods rely on
weakly-supervised approaches (rule-based) and named entity recognition systems
that are not specific to the chest X-ray domain. To overcome this limitation,
we propose a new method, the RadGraph reward, to further improve the factual
completeness and correctness of generated radiology reports. More precisely, we
leverage the RadGraph dataset containing annotated chest X-ray reports with
entities and relations between entities. On two open radiology report datasets,
our system substantially improves the scores up to 14.2% and 25.3% on metrics
evaluating the factual correctness and completeness of reports.",2022-10-21
Image Semantic Relation Generation,2022-10-19 16:15:19+00:00,http://arxiv.org/abs/2210.11253v1,Mingzhe Du,"cs.CV, cs.CL",image2text,"Scene graphs provide structured semantic understanding beyond images. For
downstream tasks, such as image retrieval, visual question answering, visual
relationship detection, and even autonomous vehicle technology, scene graphs
can not only distil complex image information but also correct the bias of
visual models using semantic-level relations, which has broad application
prospects. However, the heavy labour cost of constructing graph annotations may
hinder the application of PSG in practical scenarios. Inspired by the
observation that people usually identify the subject and object first and then
determine the relationship between them, we proposed to decouple the scene
graphs generation task into two sub-tasks: 1) an image segmentation task to
pick up the qualified objects. 2) a restricted auto-regressive text generation
task to generate the relation between given objects. Therefore, in this work,
we introduce image semantic relation generation (ISRG), a simple but effective
image-to-text model, which achieved 31 points on the OpenPSG dataset and
outperforms strong baselines respectively by 16 points (ResNet-50) and 5 points
(CLIP).",2022-10-19
"BioGPT: Generative Pre-trained Transformer for Biomedical Text
  Generation and Mining",2022-10-19 07:17:39+00:00,http://arxiv.org/abs/2210.10341v1,"Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu","cs.CL, cs.AI",image2text,"Pre-trained language models have attracted increasing attention in the
biomedical domain, inspired by their great success in the general natural
language domain. Among the two main branches of pre-trained language models in
the general language domain, i.e., BERT (and its variants) and GPT (and its
variants), the first one has been extensively studied in the biomedical domain,
such as BioBERT and PubMedBERT. While they have achieved great success on a
variety of discriminative downstream biomedical tasks, the lack of generation
ability constrains their application scope. In this paper, we propose BioGPT, a
domain-specific generative Transformer language model pre-trained on large
scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and
demonstrate that our model outperforms previous models on most tasks.
Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI
end-to-end relation extraction tasks respectively, and 78.2% accuracy on
PubMedQA, creating a new record. Our case study on text generation further
demonstrates the advantage of BioGPT on biomedical literature to generate
fluent descriptions for biomedical terms. Code is available at
https://github.com/microsoft/BioGPT.",2022-10-19
"Probing Cross-modal Semantics Alignment Capability from the Textual
  Perspective",2022-10-18 02:55:58+00:00,http://arxiv.org/abs/2210.09550v1,"Zheng Ma, Shi Zong, Mianzhi Pan, Jianbing Zhang, Shujian Huang, Xinyu Dai, Jiajun Chen",cs.CL,image2text,"In recent years, vision and language pre-training (VLP) models have advanced
the state-of-the-art results in a variety of cross-modal downstream tasks.
Aligning cross-modal semantics is claimed to be one of the essential
capabilities of VLP models. However, it still remains unclear about the inner
working mechanism of alignment in VLP models. In this paper, we propose a new
probing method that is based on image captioning to first empirically study the
cross-modal semantics alignment of VLP models. Our probing method is built upon
the fact that given an image-caption pair, the VLP models will give a score,
indicating how well two modalities are aligned; maximizing such scores will
generate sentences that VLP models believe are of good alignment. Analyzing
these sentences thus will reveal in what way different modalities are aligned
and how well these alignments are in VLP models. We apply our probing method to
five popular VLP models, including UNITER, ROSITA, ViLBERT, CLIP, and LXMERT,
and provide a comprehensive analysis of the generated captions guided by these
models. Our results show that VLP models (1) focus more on just aligning
objects with visual words, while neglecting global semantics; (2) prefer fixed
sentence patterns, thus ignoring more important textual information including
fluency and grammar; and (3) deem the captions with more visual words are
better aligned with images. These findings indicate that VLP models still have
weaknesses in cross-modal semantics alignment and we hope this work will draw
researchers' attention to such problems when designing a new VLP model.",2022-10-18
"UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation
  Model on a Single Image",2022-10-17 23:46:05+00:00,http://arxiv.org/abs/2210.09477v3,"Dani Valevski, Matan Kalman, Yossi Matias, Yaniv Leviathan","cs.CV, cs.GR, cs.LG",image2text,"We present UniTune, a simple and novel method for general text-driven image
editing. UniTune gets as input an arbitrary image and a textual edit
description, and carries out the edit while maintaining high semantic and
visual fidelity to the input image. UniTune uses text, an intuitive interface
for art-direction, and does not require additional inputs, like masks or
sketches. At the core of our method is the observation that with the right
choice of parameters, we can fine-tune a large text-to-image diffusion model on
a single image, encouraging the model to maintain fidelity to the input image
while still allowing expressive manipulations. We used Imagen as our
text-to-image model, but we expect UniTune to work with other large-scale
models as well. We test our method in a range of different use cases, and
demonstrate its wide applicability.",2022-10-17
Social Biases in Automatic Evaluation Metrics for NLG,2022-10-17 08:55:26+00:00,http://arxiv.org/abs/2210.08859v1,"Mingqi Gao, Xiaojun Wan","cs.CL, cs.AI",image2text,"Many studies have revealed that word embeddings, language models, and models
for specific downstream tasks in NLP are prone to social biases, especially
gender bias. Recently these techniques have been gradually applied to automatic
evaluation metrics for text generation. In the paper, we propose an evaluation
method based on Word Embeddings Association Test (WEAT) and Sentence Embeddings
Association Test (SEAT) to quantify social biases in evaluation metrics and
discover that social biases are also widely present in some model-based
automatic evaluation metrics. Moreover, we construct gender-swapped
meta-evaluation datasets to explore the potential impact of gender bias in
image caption and text summarization tasks. Results show that given
gender-neutral references in the evaluation, model-based evaluation metrics may
show a preference for the male hypothesis, and the performance of them, i.e.
the correlation between evaluation metrics and human judgments, usually has
more significant variation after gender swapping.",2022-10-17
"Plausible May Not Be Faithful: Probing Object Hallucination in
  Vision-Language Pre-training",2022-10-14 10:27:22+00:00,http://arxiv.org/abs/2210.07688v1,"Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, Pascale Fung","cs.CL, cs.CV",image2text,"Large-scale vision-language pre-trained (VLP) models are prone to hallucinate
non-existent visual objects when generating text based on visual information.
In this paper, we exhaustively probe the object hallucination problem from
three aspects. First, we examine various state-of-the-art VLP models, showing
that models achieving better scores on standard metrics(e.g., BLEU-4, CIDEr)
could hallucinate objects more frequently. Second, we investigate how different
types of visual features in VLP influence hallucination, including
region-based, grid-based, and patch-based. Surprisingly, we find that
patch-based features perform the best and smaller patch resolution yields a
non-trivial reduction in object hallucination. Third, we decouple various VLP
objectives and demonstrate their effectiveness in alleviating object
hallucination. Based on that, we propose a new pre-training loss, object masked
language modeling, to further reduce object hallucination. We evaluate models
on both COCO (in-domain) and NoCaps (out-of-domain) datasets with our improved
CHAIR metric. Furthermore, we investigate the effects of various text decoding
strategies and image augmentation methods on object hallucination.",2022-10-14
Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models,2022-10-13 08:45:23+00:00,http://arxiv.org/abs/2210.06475v1,"Sourya Basu, Prasanna Sattigeri, Karthikeyan Natesan Ramamurthy, Vijil Chenthamarakshan, Kush R. Varshney, Lav R. Varshney, Payel Das","cs.LG, cs.CL",image2text,"We introduce equi-tuning, a novel fine-tuning method that transforms
(potentially non-equivariant) pretrained models into group equivariant models
while incurring minimum $L_2$ loss between the feature representations of the
pretrained and the equivariant models. Large pretrained models can be
equi-tuned for different groups to satisfy the needs of various downstream
tasks. Equi-tuned models benefit from both group equivariance as an inductive
bias and semantic priors from pretrained models. We provide applications of
equi-tuning on three different tasks: image classification, compositional
generalization in language, and fairness in natural language generation (NLG).
We also provide a novel group-theoretic definition for fairness in NLG. The
effectiveness of this definition is shown by testing it against a standard
empirical method of fairness in NLG. We provide experimental results for
equi-tuning using a variety of pretrained models: Alexnet, Resnet, VGG, and
Densenet for image classification; RNNs, GRUs, and LSTMs for compositional
generalization; and GPT2 for fairness in NLG. We test these models on benchmark
datasets across all considered tasks to show the generality and effectiveness
of the proposed method.",2022-10-13
"Not All Errors are Equal: Learning Text Generation Metrics using
  Stratified Error Synthesis",2022-10-10 22:30:26+00:00,http://arxiv.org/abs/2210.05035v1,"Wenda Xu, Yilin Tuan, Yujie Lu, Michael Saxon, Lei Li, William Yang Wang","cs.CL, cs.AI",image2text,"Is it possible to build a general and automatic natural language generation
(NLG) evaluation metric? Existing learned metrics either perform
unsatisfactorily or are restricted to tasks where large human rating data is
already available. We introduce SESCORE, a model-based metric that is highly
correlated with human judgements without requiring human annotation, by
utilizing a novel, iterative error synthesis and severity scoring pipeline.
This pipeline applies a series of plausible errors to raw text and assigns
severity labels by simulating human judgements with entailment. We evaluate
SESCORE against existing metrics by comparing how their scores correlate with
human ratings. SESCORE outperforms all prior unsupervised metrics on multiple
diverse NLG tasks including machine translation, image captioning, and WebNLG
text generation. For WMT 20/21 En-De and Zh-En, SESCORE improve the average
Kendall correlation with human judgement from 0.154 to 0.195. SESCORE even
achieves comparable performance to the best supervised metric COMET, despite
receiving no human-annotated training data.",2022-10-10
CLIP-Diffusion-LM: Apply Diffusion Model on Image Captioning,2022-10-10 10:55:53+00:00,http://arxiv.org/abs/2210.04559v1,Shitong Xu,"cs.CV, cs.LG",image2text,"Image captioning task has been extensively researched by previous work.
However, limited experiments focus on generating captions based on
non-autoregressive text decoder. Inspired by the recent success of the
denoising diffusion model on image synthesis tasks, we apply denoising
diffusion probabilistic models to text generation in image captioning tasks. We
show that our CLIP-Diffusion-LM is capable of generating image captions using
significantly fewer inference steps than autoregressive models. On the Flickr8k
dataset, the model achieves 0.1876 BLEU-4 score. By training on the combined
Flickr8k and Flickr30k dataset, our model achieves 0.2470 BLEU-4 score. Our
code is available at https://github.com/xu-shitong/diffusion-image-captioning.",2022-10-10
ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models,2022-10-09 19:17:43+00:00,http://arxiv.org/abs/2210.04325v2,"Jiannan Xiang, Zhengzhong Liu, Yucheng Zhou, Eric P. Xing, Zhiting Hu",cs.CL,image2text,"Data-to-text generation is challenging due to the great variety of the input
data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse
predicates). Recent end-to-end neural methods thus require substantial training
examples to learn to disambiguate and describe the data. Yet, real-world
data-to-text problems often suffer from various data-scarce issues: one may
have access to only a handful of or no training examples, and/or have to rely
on examples in a different domain or schema. To fill this gap, we propose
Any-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse
settings by making efficient use of any given (or no) examples. ASDOT consists
of two steps, data disambiguation and sentence fusion, both of which are
amenable to be solved with off-the-shelf pretrained language models (LMs) with
optional finetuning. In the data disambiguation stage, we employ the prompted
GPT-3 model to understand possibly ambiguous triples from the input data and
convert each into a short sentence with reduced ambiguity. The sentence fusion
stage then uses an LM like T5 to fuse all the resulting sentences into a
coherent paragraph as the final description. We evaluate extensively on various
datasets in different scenarios, including the zero-/few-/full-shot settings,
and generalization to unseen predicates and out-of-domain data. Experimental
results show that ASDOT consistently achieves significant improvement over
baselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot
setting.",2022-10-09
"Visualize Before You Write: Imagination-Guided Open-Ended Text
  Generation",2022-10-07 18:01:09+00:00,http://arxiv.org/abs/2210.03765v1,"Wanrong Zhu, An Yan, Yujie Lu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, William Yang Wang","cs.CL, cs.AI",image2text,"Recent advances in text-to-image synthesis make it possible to visualize
machine imaginations for a given context. On the other hand, when generating
text, human writers are gifted at creative visualization, which enhances their
writings by forming imaginations as blueprints before putting down the stories
in words. Inspired by such a cognitive process, we ask the natural question of
whether we can endow machines with the same ability to utilize visual
information and construct a general picture of the context to guide text
generation. In this work, we propose iNLG that uses machine-generated images to
guide language models (LM) in open-ended text generation. The experiments and
analyses demonstrate the effectiveness of iNLG on open-ended text generation
tasks, including text completion, story generation, and concept-to-text
generation in few-shot scenarios. Both automatic metrics and human evaluations
verify that the text snippets generated by our iNLG are coherent and
informative while displaying minor degeneration.",2022-10-07
"Unsupervised Neural Stylistic Text Generation using Transfer learning
  and Adapters",2022-10-07 00:09:22+00:00,http://arxiv.org/abs/2210.03264v1,"Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah, Dan Roth",cs.CL,image2text,"Research has shown that personality is a key driver to improve engagement and
user experience in conversational systems. Conversational agents should also
maintain a consistent persona to have an engaging conversation with a user.
However, text generation datasets are often crowd sourced and thereby have an
averaging effect where the style of the generation model is an average style of
all the crowd workers that have contributed to the dataset. While one can
collect persona-specific datasets for each task, it would be an expensive and
time consuming annotation effort. In this work, we propose a novel transfer
learning framework which updates only $0.3\%$ of model parameters to learn
style specific attributes for response generation. For the purpose of this
study, we tackle the problem of stylistic story ending generation using the ROC
stories Corpus. We learn style specific attributes from the
PERSONALITY-CAPTIONS dataset. Through extensive experiments and evaluation
metrics we show that our novel training procedure can improve the style
generation by 200 over Encoder-Decoder baselines while maintaining on-par
content relevance metrics with",2022-10-07
"Co-Writing Screenplays and Theatre Scripts with Language Models: An
  Evaluation by Industry Professionals",2022-09-29 17:26:22+00:00,http://arxiv.org/abs/2209.14958v1,"Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, Richard Evans","cs.HC, cs.CL",image2text,"Language models are increasingly attracting interest from writers. However,
such models lack long-range semantic coherence, limiting their usefulness for
longform creative writing. We address this limitation by applying language
models hierarchically, in a system we call Dramatron. By building structural
context via prompt chaining, Dramatron can generate coherent scripts and
screenplays complete with title, characters, story beats, location
descriptions, and dialogue. We illustrate Dramatron's usefulness as an
interactive co-creative system with a user study of 15 theatre and film
industry professionals. Participants co-wrote theatre scripts and screenplays
with Dramatron and engaged in open-ended interviews. We report critical
reflections both from our interviewees and from independent reviewers who
watched stagings of the works to illustrate how both Dramatron and hierarchical
text generation could be useful for human-machine co-creativity. Finally, we
discuss the suitability of Dramatron for co-creativity, ethical considerations
-- including plagiarism and bias -- and participatory models for the design and
deployment of such tools.",2022-09-29
XF2T: Cross-lingual Fact-to-Text Generation for Low-Resource Languages,2022-09-22 18:01:27+00:00,http://arxiv.org/abs/2209.11252v1,"Shivprasad Sagare, Tushar Abhishek, Bhavyajeet Singh, Anubhav Sharma, Manish Gupta, Vasudeva Varma",cs.CL,image2text,"Multiple business scenarios require an automated generation of descriptive
human-readable text from structured input data. Hence, fact-to-text generation
systems have been developed for various downstream tasks like generating soccer
reports, weather and financial reports, medical reports, person biographies,
etc. Unfortunately, previous work on fact-to-text (F2T) generation has focused
primarily on English mainly due to the high availability of relevant datasets.
Only recently, the problem of cross-lingual fact-to-text (XF2T) was proposed
for generation across multiple languages alongwith a dataset, XALIGN for eight
languages. However, there has been no rigorous work on the actual XF2T
generation problem. We extend XALIGN dataset with annotated data for four more
languages: Punjabi, Malayalam, Assamese and Oriya. We conduct an extensive
study using popular Transformer-based text generation models on our extended
multi-lingual dataset, which we call XALIGNV2. Further, we investigate the
performance of different text generation strategies: multiple variations of
pretraining, fact-aware embeddings and structure-aware input encoding. Our
extensive experiments show that a multi-lingual mT5 model which uses fact-aware
embeddings with structure-aware input encoding leads to best results on average
across the twelve languages. We make our code, dataset and model publicly
available, and hope that this will help advance further research in this
critical area.",2022-09-22
Distribution Aware Metrics for Conditional Natural Language Generation,2022-09-15 17:58:13+00:00,http://arxiv.org/abs/2209.07518v1,"David M Chan, Yiming Ni, Austin Myers, Sudheendra Vijayanarasimhan, David A Ross, John Canny","cs.CL, cs.AI, cs.CV, cs.LG",image2text,"Traditional automated metrics for evaluating conditional natural language
generation use pairwise comparisons between a single generated text and the
best-matching gold-standard ground truth text. When multiple ground truths are
available, scores are aggregated using an average or max operation across
references. While this approach works well when diversity in the ground truth
data (i.e. dispersion of the distribution of conditional texts) can be ascribed
to noise, such as in automated speech recognition, it does not allow for robust
evaluation in the case where diversity in the ground truths represents signal
for the model. In this work we argue that existing metrics are not appropriate
for domains such as visual description or summarization where ground truths are
semantically diverse, and where the diversity in those captions captures useful
additional information about the context. We propose a novel paradigm for
multi-candidate evaluation of conditional language generation models, and a new
family of metrics that compare the distributions of reference and
model-generated caption sets using small sample sets of each. We demonstrate
the utility of our approach with a case study in visual description: where we
show that existing models optimize for single-description quality over
diversity, and gain some insights into how sampling methods and temperature
impact description quality and diversity.",2022-09-15
PaLI: A Jointly-Scaled Multilingual Language-Image Model,2022-09-14 17:24:07+00:00,http://arxiv.org/abs/2209.06794v2,"Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut","cs.CV, cs.CL",image2text,"Effective scaling and a flexible task interface enable large language models
to excel at many tasks. PaLI (Pathways Language and Image model) extends this
approach to the joint modeling of language and vision. PaLI generates text
based on visual and textual inputs, and with this interface performs many
vision, language, and multimodal tasks, in many languages. To train PaLI, we
make use of large pretrained encoder-decoder language models and Vision
Transformers (ViTs). This allows us to capitalize on their existing
capabilities and leverage the substantial cost of training them. We find that
joint scaling of the vision and language components is important. Since
existing Transformers for language are much larger than their vision
counterparts, we train the largest ViT to date (ViT-e) to quantify the benefits
from even larger-capacity vision models. To train PaLI, we create a large
multilingual mix of pretraining tasks, based on a new image-text training set
containing 10B images and texts in over 100 languages. PaLI achieves
state-of-the-art in multiple vision and language tasks (such as captioning,
visual question-answering, scene-text understanding), while retaining a simple,
modular, and scalable design.",2022-09-14
"Visual Recipe Flow: A Dataset for Learning Visual State Changes of
  Objects with Recipe Flows",2022-09-13 09:38:32+00:00,http://arxiv.org/abs/2209.05840v1,"Keisuke Shirai, Atsushi Hashimoto, Taichi Nishimura, Hirotaka Kameko, Shuhei Kurita, Yoshitaka Ushiku, Shinsuke Mori","cs.CL, cs.AI",image2text,"We present a new multimodal dataset called Visual Recipe Flow, which enables
us to learn each cooking action result in a recipe text. The dataset consists
of object state changes and the workflow of the recipe text. The state change
is represented as an image pair, while the workflow is represented as a recipe
flow graph (r-FG). The image pairs are grounded in the r-FG, which provides the
cross-modal relation. With our dataset, one can try a range of applications,
from multimodal commonsense reasoning and procedural text generation.",2022-09-13
"Bridging Music and Text with Crowdsourced Music Comments: A
  Sequence-to-Sequence Framework for Thematic Music Comments Generation",2022-09-05 14:51:51+00:00,http://arxiv.org/abs/2209.01996v1,"Peining Zhang, Junliang Guo, Linli Xu, Mu You, Junming Yin","cs.SD, cs.CL, eess.AS",image2text,"We consider a novel task of automatically generating text descriptions of
music. Compared with other well-established text generation tasks such as image
caption, the scarcity of well-paired music and text datasets makes it a much
more challenging task. In this paper, we exploit the crowd-sourced music
comments to construct a new dataset and propose a sequence-to-sequence model to
generate text descriptions of music. More concretely, we use the dilated
convolutional layer as the basic component of the encoder and a memory based
recurrent neural network as the decoder. To enhance the authenticity and
thematicity of generated texts, we further propose to fine-tune the model with
a discriminator as well as a novel topic evaluator. To measure the quality of
generated texts, we also propose two new evaluation metrics, which are more
aligned with human evaluation than traditional metrics such as BLEU.
Experimental results verify that our model is capable of generating fluent and
meaningful comments while containing thematic and content information of the
original music.",2022-09-05
"Every picture tells a story: Image-grounded controllable stylistic story
  generation",2022-09-04 15:07:53+00:00,http://arxiv.org/abs/2209.01638v1,"Holy Lovenia, Bryan Wilie, Romain Barraud, Samuel Cahyawijaya, Willy Chung, Pascale Fung",cs.CL,image2text,"Generating a short story out of an image is arduous. Unlike image captioning,
story generation from an image poses multiple challenges: preserving the story
coherence, appropriately assessing the quality of the story, steering the
generated story into a certain style, and addressing the scarcity of
image-story pair reference datasets limiting supervision during training. In
this work, we introduce Plug-and-Play Story Teller (PPST) and improve
image-to-story generation by: 1) alleviating the data scarcity problem by
incorporating large pre-trained models, namely CLIP and GPT-2, to facilitate a
fluent image-to-text generation with minimal supervision, and 2) enabling a
more style-relevant generation by incorporating stylistic adapters to control
the story generation. We conduct image-to-story generation experiments with
non-styled, romance-styled, and action-styled PPST approaches and compare our
generated stories with those of previous work over three aspects, i.e., story
coherence, image-story relevance, and style fitness, using both automatic and
human evaluation. The results show that PPST improves story coherence and has
better image-story relevance, but has yet to be adequately stylistic.",2022-09-04
Understanding Attention for Vision-and-Language Tasks,2022-08-17 06:45:07+00:00,http://arxiv.org/abs/2208.08104v1,"Feiqi Cao, Soyeon Caren Han, Siqu Long, Changwei Xu, Josiah Poon","cs.CV, cs.CL",image2text,"Attention mechanism has been used as an important component across
Vision-and-Language(VL) tasks in order to bridge the semantic gap between
visual and textual features. While attention has been widely used in VL tasks,
it has not been examined the capability of different attention alignment
calculation in bridging the semantic gap between visual and textual clues. In
this research, we conduct a comprehensive analysis on understanding the role of
attention alignment by looking into the attention score calculation methods and
check how it actually represents the visual region's and textual token's
significance for the global assessment. We also analyse the conditions which
attention score calculation mechanism would be more (or less) interpretable,
and which may impact the model performance on three different VL tasks,
including visual question answering, text-to-image generation, text-and-image
matching (both sentence and image retrieval). Our analysis is the first of its
kind and provides useful insights of the importance of each attention alignment
score calculation when applied at the training phase of VL tasks, commonly
ignored in attention-based cross modal models, and/or pretrained models.",2022-08-17
ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model,2022-07-19 17:59:01+00:00,http://arxiv.org/abs/2207.09446v1,"Rao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, Srinath Sridhar","cs.CV, cs.AI",image2text,"We present ShapeCrafter, a neural network for recursive text-conditioned 3D
shape generation. Existing methods to generate text-conditioned 3D shapes
consume an entire text prompt to generate a 3D shape in a single step. However,
humans tend to describe shapes recursively-we may start with an initial
description and progressively add details based on intermediate results. To
capture this recursive process, we introduce a method to generate a 3D shape
distribution, conditioned on an initial phrase, that gradually evolves as more
phrases are added. Since existing datasets are insufficient for training this
approach, we present Text2Shape++, a large dataset of 369K shape-text pairs
that supports recursive shape generation. To capture local details that are
often used to refine shape descriptions, we build on top of vector-quantized
deep implicit functions that generate a distribution of high-quality shapes.
Results show that our method can generate shapes consistent with text
descriptions, and shapes evolve gradually as more phrases are added. Our method
supports shape editing, extrapolation, and can enable new applications in
human-machine collaboration for creative design.",2022-07-19
"A Baseline for Detecting Out-of-Distribution Examples in Image
  Captioning",2022-07-12 09:29:57+00:00,http://arxiv.org/abs/2207.05418v1,"Gabi Shalev, Gal-Lev Shalev, Joseph Keshet","cs.CV, cs.LG",image2text,"Image captioning research achieved breakthroughs in recent years by
developing neural models that can generate diverse and high-quality
descriptions for images drawn from the same distribution as training images.
However, when facing out-of-distribution (OOD) images, such as corrupted
images, or images containing unknown objects, the models fail in generating
relevant captions.
  In this paper, we consider the problem of OOD detection in image captioning.
We formulate the problem and suggest an evaluation setup for assessing the
model's performance on the task. Then, we analyze and show the effectiveness of
the caption's likelihood score at detecting and rejecting OOD images, which
implies that the relatedness between the input image and the generated caption
is encapsulated within the score.",2022-07-12
Towards Multimodal Vision-Language Models Generating Non-Generic Text,2022-07-09 01:56:35+00:00,http://arxiv.org/abs/2207.04174v1,"Wes Robbins, Zanyar Zohourianshahzadi, Jugal Kalita","cs.CV, cs.AI",image2text,"Vision-language models can assess visual context in an image and generate
descriptive text. While the generated text may be accurate and syntactically
correct, it is often overly general. To address this, recent work has used
optical character recognition to supplement visual information with text
extracted from an image. In this work, we contend that vision-language models
can benefit from additional information that can be extracted from an image,
but are not used by current models. We modify previous multimodal frameworks to
accept relevant information from any number of auxiliary classifiers. In
particular, we focus on person names as an additional set of tokens and create
a novel image-caption dataset to facilitate captioning with person names. The
dataset, Politicians and Athletes in Captions (PAC), consists of captioned
images of well-known people in context. By fine-tuning pretrained models with
this dataset, we demonstrate a model that can naturally integrate facial
recognition tokens into generated text by training on limited data. For the PAC
dataset, we provide a discussion on collection and baseline benchmark scores.",2022-07-09
Dual-Stream Transformer for Generic Event Boundary Captioning,2022-07-07 01:47:19+00:00,http://arxiv.org/abs/2207.03038v1,"Xin Gu, Hanhua Ye, Guang Chen, Yufei Wang, Libo Zhang, Longyin Wen","cs.CV, cs.CL",image2text,"This paper describes our champion solution for the CVPR2022 Generic Event
Boundary Captioning (GEBC) competition. GEBC requires the captioning model to
have a comprehension of instantaneous status changes around the given video
boundary, which makes it much more challenging than conventional video
captioning task. In this paper, a Dual-Stream Transformer with improvements on
both video content encoding and captions generation is proposed: (1) We utilize
three pre-trained models to extract the video features from different
granularities. Moreover, we exploit the types of boundary as hints to help the
model generate captions. (2) We particularly design an model, termed as
Dual-Stream Transformer, to learn discriminative representations for boundary
captioning. (3) Towards generating content-relevant and human-like captions, we
improve the description quality by designing a word-level ensemble strategy.
The promising results on the GEBC test split demonstrate the efficacy of our
proposed model.",2022-07-07
"Syntax Controlled Knowledge Graph-to-Text Generation with Order and
  Semantic Consistency",2022-07-02 02:42:14+00:00,http://arxiv.org/abs/2207.00719v1,"Jin Liu, Chongfeng Fan, Fengyu Zhou, Huijuan Xu",cs.AI,image2text,"The knowledge graph (KG) stores a large amount of structural knowledge, while
it is not easy for direct human understanding. Knowledge graph-to-text
(KG-to-text) generation aims to generate easy-to-understand sentences from the
KG, and at the same time, maintains semantic consistency between generated
sentences and the KG. Existing KG-to-text generation methods phrase this task
as a sequence-to-sequence generation task with linearized KG as input and
consider the consistency issue of the generated texts and KG through a simple
selection between decoded sentence word and KG node word at each time step.
However, the linearized KG order is commonly obtained through a heuristic
search without data-driven optimization. In this paper, we optimize the
knowledge description order prediction under the order supervision extracted
from the caption and further enhance the consistency of the generated sentences
and KG through syntactic and semantic regularization. We incorporate the
Part-of-Speech (POS) syntactic tags to constrain the positions to copy words
from the KG and employ a semantic context scoring function to evaluate the
semantic fitness for each word in its local context when decoding each word in
the generated sentence. Extensive experiments are conducted on two datasets,
WebNLG and DART, and achieve state-of-the-art performances.",2022-07-02
Automatic Controllable Product Copywriting for E-Commerce,2022-06-21 04:18:52+00:00,http://arxiv.org/abs/2206.10103v1,"Xiaojie Guo, Qingkai Zeng, Meng Jiang, Yun Xiao, Bo Long, Lingfei Wu","cs.AI, cs.LG",image2text,"Automatic product description generation for e-commerce has witnessed
significant advancement in the past decade. Product copywriting aims to attract
users' interest and improve user experience by highlighting product
characteristics with textual descriptions. As the services provided by
e-commerce platforms become diverse, it is necessary to adapt the patterns of
automatically-generated descriptions dynamically. In this paper, we report our
experience in deploying an E-commerce Prefix-based Controllable Copywriting
Generation (EPCCG) system into the JD.com e-commerce product recommendation
platform. The development of the system contains two main components: 1)
copywriting aspect extraction; 2) weakly supervised aspect labeling; 3) text
generation with a prefix-based language model; 4) copywriting quality control.
We conduct experiments to validate the effectiveness of the proposed EPCCG. In
addition, we introduce the deployed architecture which cooperates with the
EPCCG into the real-time JD.com e-commerce recommendation platform and the
significant payoff since deployment.",2022-06-21
"niksss at HinglishEval: Language-agnostic BERT-based Contextual
  Embeddings with Catboost for Quality Evaluation of the Low-Resource
  Synthetically Generated Code-Mixed Hinglish Text",2022-06-17 17:36:03+00:00,http://arxiv.org/abs/2206.08910v1,Nikhil Singh,cs.CL,image2text,"This paper describes the system description for the HinglishEval challenge at
INLG 2022. The goal of this task was to investigate the factors influencing the
quality of the code-mixed text generation system. The task was divided into two
subtasks, quality rating prediction and annotators disagreement prediction of
the synthetic Hinglish dataset. We attempted to solve these tasks using
sentence-level embeddings, which are obtained from mean pooling the
contextualized word embeddings for all input tokens in our text. We
experimented with various classifiers on top of the embeddings produced for
respective tasks. Our best-performing system ranked 1st on subtask B and 3rd on
subtask A.",2022-06-17
Prefix Language Models are Unified Modal Learners,2022-06-15 17:49:38+00:00,http://arxiv.org/abs/2206.07699v1,"Shizhe Diao, Wangchunshu Zhou, Xinsong Zhang, Jiawei Wang","cs.CV, cs.CL, cs.LG",image2text,"With the success of vision-language pre-training, we have witnessed the
state-of-the-art has been pushed on multi-modal understanding and generation.
However, the current pre-training paradigm is either incapable of targeting all
modalities at once (e.g., text generation and image generation), or requires
multi-fold well-designed tasks which significantly limits the scalability. We
demonstrate that a unified modal model could be learned with a prefix language
modeling objective upon text and image sequences. Thanks to the simple but
powerful pre-training paradigm, our proposed model, DaVinci, is simple to
train, scalable to huge data, and adaptable to a variety of downstream tasks
across modalities (language / vision / vision+language), types (understanding /
generation) and settings (e.g., zero-shot, fine-tuning, linear evaluation) with
a single unified architecture. DaVinci achieves the competitive performance on
a wide range of 26 understanding / generation tasks, and outperforms previous
unified vision-language models on most tasks, including ImageNet classification
(+1.6%), VQAv2 (+1.4%), COCO caption generation (BLEU@4 +1.1%, CIDEr +1.5%) and
COCO image generation (IS +0.9%, FID -1.0%), at the comparable model and data
scale. Furthermore, we offer a well-defined benchmark for future research by
reporting the performance on different scales of the pre-training dataset on a
heterogeneous and wide distribution coverage. Our results establish new,
stronger baselines for future comparisons at different data scales and shed
light on the difficulties of comparing VLP models more generally.",2022-06-15
Exploring industrial safety knowledge via Zipf law,2022-05-25 10:22:14+00:00,http://arxiv.org/abs/2205.12636v1,"Zhenhua Wang, Ming Ren, Dong Gao, Zhuang Li",cs.CL,image2text,"The hazard and operability analysis (HAZOP) report contains precious
industrial safety knowledge (ISK) with expert experience and process nature,
which is of great significance to the development of industrial intelligence.
Subject to the attributes of ISK, existing researches mine them through
sequence labeling in deep learning. Yet, there are two thorny issues: (1)
Uneven distribution of ISK and (2) Consistent importance of ISK: for safety
review. In this study, we propose a novel generative mining strategy called
CRGM to explore ISK. Inspired Zipf law in linguistics, CRGM consists of
common-rare discriminator, induction-extension generator and ISK extractor.
Firstly, the common-rare discriminator divides HAZOP descriptions into common
words and rare words, and obtains the common description and the rare
description, where the latter contains more industrial substances. Then, they
are operated by the induction-extension generator in the way of deep text
generation, the common description is induced and the rare description is
extended, the material knowledge and the equipment knowledge can be enriched.
Finally, the ISK extractor processes the material knowledge and equipment
knowledge from the generated description through the rule template method, the
additional ISK is regarded as the supplement of the training set to train the
proposed sequence labeling model. We conduct multiple evaluation experiments on
two industrial safety datasets. The results show that CRGM has promising and
gratifying aptitudes, greatly improves the performance of the model, and is
efficient and generalized. Our sequence labeling model also shows the expected
performance, which is better than the existing research. Our research provides
a new perspective for exploring ISK, we hope it can contribute support for the
intelligent progress of industrial safety.",2022-05-25
"The Dialog Must Go On: Improving Visual Dialog via Generative
  Self-Training",2022-05-25 05:40:00+00:00,http://arxiv.org/abs/2205.12502v1,"Gi-Cheon Kang, Sungdong Kim, Jin-Hwa Kim, Donghyun Kwak, Byoung-Tak Zhang","cs.CV, cs.CL, cs.LG",image2text,"Visual dialog (VisDial) is a task of answering a sequence of questions
grounded in an image, using the dialog history as context. Prior work has
trained the dialog agents solely on VisDial data via supervised learning or
leveraged pre-training on related vision-and-language datasets. This paper
presents a semi-supervised learning approach for visually-grounded dialog,
called Generative Self-Training (GST), to leverage unlabeled images on the Web.
Specifically, GST first retrieves in-domain images through out-of-distribution
detection and generates synthetic dialogs regarding the images via multimodal
conditional text generation. GST then trains a dialog agent on the synthetic
and the original VisDial data. As a result, GST scales the amount of training
data up to an order of magnitude that of VisDial (1.2M to 12.9M QA data). For
robust training of the generated dialogs, we also propose perplexity-based data
selection and multimodal consistency regularization. Evaluation on VisDial v1.0
and v0.9 datasets shows that GST achieves new state-of-the-art results on both
datasets. We further observe strong performance gains in the low-data regime
(up to 9.35 absolute points on NDCG).",2022-05-25
"Rethinking Evaluation Practices in Visual Question Answering: A Case
  Study on Out-of-Distribution Generalization",2022-05-24 16:44:45+00:00,http://arxiv.org/abs/2205.12191v1,"Aishwarya Agrawal, Ivana KajiÄ‡, Emanuele Bugliarello, Elnaz Davoodi, Anita Gergely, Phil Blunsom, Aida Nematzadeh","cs.CL, cs.AI, cs.CV, cs.LG",image2text,"Vision-and-language (V&L) models pretrained on large-scale multimodal data
have demonstrated strong performance on various tasks such as image captioning
and visual question answering (VQA). The quality of such models is commonly
assessed by measuring their performance on unseen data that typically comes
from the same distribution as the training data. However, we observe that these
models exhibit poor out-of-distribution (OOD) generalization on the task of
VQA. To better understand the underlying causes of poor generalization, we
comprehensively investigate performance of two pretrained V&L models under
different settings (i.e. classification and open-ended text generation) by
conducting cross-dataset evaluations. We find that these models tend to learn
to solve the benchmark, rather than learning the high-level skills required by
the VQA task. We also argue that in most cases generative models are less
susceptible to shifts in data distribution, while frequently performing better
on our tested benchmarks. Moreover, we find that multimodal pretraining
improves OOD performance in most settings. Finally, we revisit assumptions
underlying the use of automatic VQA evaluation metrics, and empirically show
that their stringent nature repeatedly penalizes models for correct responses.",2022-05-24
"On Advances in Text Generation from Images Beyond Captioning: A Case
  Study in Self-Rationalization",2022-05-24 00:52:40+00:00,http://arxiv.org/abs/2205.11686v1,"Shruti Palaskar, Akshita Bhagia, Yonatan Bisk, Florian Metze, Alan W Black, Ana Marasovic","cs.CL, cs.CV",image2text,"Integrating vision and language has gained notable attention following the
success of pretrained language models. Despite that, a fraction of emerging
multimodal models is suitable for text generation conditioned on images. This
minority is typically developed and evaluated for image captioning, a text
generation task conditioned solely on images with the goal to describe what is
explicitly visible in an image. In this paper, we take a step back and ask: How
do these models work for more complex generative tasks, conditioned on both
text and images? Are models based on joint multimodal pretraining, visually
adapted pretrained language models, or models that combine these two
approaches, more promising for such tasks? We address these questions in the
context of self-rationalization (jointly generating task labels/answers and
free-text explanations) of three tasks: (i) visual question answering in VQA-X,
(ii) visual commonsense reasoning in VCR, and (iii) visual-textual entailment
in E-SNLI-VE. We show that recent advances in each modality, CLIP image
representations and scaling of language models, do not consistently improve
multimodal self-rationalization of tasks with multimodal inputs. We also
observe that no model type works universally the best across tasks/datasets and
finetuning data sizes. Our findings call for a backbone modelling approach that
can be built on to advance text generation from images and text beyond image
captioning.",2022-05-24
What Makes Data-to-Text Generation Hard for Pretrained Language Models?,2022-05-23 17:58:39+00:00,http://arxiv.org/abs/2205.11505v1,"Moniba Keymanesh, Adrian Benton, Mark Dredze","cs.CL, cs.AI, cs.IR, cs.LG",image2text,"Expressing natural language descriptions of structured facts or relations --
data-to-text generation (D2T) -- increases the accessibility of structured
knowledge repositories. Previous work shows that pre-trained language
models(PLMs) perform remarkably well on this task after fine-tuning on a
significant amount of task-specific training data. On the other hand, while
auto-regressive PLMs can generalize from a few task examples, their efficacy at
D2T is largely unexplored. Furthermore, we have an incomplete understanding of
the limits of PLMs on D2T.
  In this work, we conduct an empirical study of both fine-tuned and
auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their
performance as a function of the amount of task-specific data and how these
data are incorporated into the models: zero and few-shot learning, and
fine-tuning of model weights. In addition, we probe the limits of PLMs by
measuring performance on subsets of the evaluation data: novel predicates and
abstractive test examples. To improve the performance on these subsets, we
investigate two techniques: providing predicate descriptions in the context and
re-ranking generated candidates by information reflected in the source.
Finally, we conduct a human evaluation of model errors and show that D2T
generation tasks would benefit from datasets with more careful manual curation.",2022-05-23
"Language Models with Image Descriptors are Strong Few-Shot
  Video-Language Learners",2022-05-22 05:18:27+00:00,http://arxiv.org/abs/2205.10747v2,"Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal, Heng Ji","cs.CV, cs.AI",image2text,"The goal of this work is to build flexible video-language models that can
generalize to various video-to-text tasks from few examples, such as
domain-specific captioning, question answering, and future event prediction.
Existing few-shot video-language learners focus exclusively on the encoder,
resulting in the absence of a video-to-text decoder to handle generative tasks.
Video captioners have been pretrained on large-scale video-language datasets,
but they rely heavily on finetuning and lack the ability to generate text for
unseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language
Learner via Image and Language models, which demonstrates strong performance on
few-shot video-to-text tasks without the necessity of pretraining or finetuning
on any video datasets. We use the image-language models to translate the video
content into frame captions, object, attribute, and event phrases, and compose
them into a temporal structure template. We then instruct a language model,
with a prompt containing a few in-context examples, to generate a target output
from the composed content. The flexibility of prompting allows the model to
capture any form of text input, such as automatic speech recognition (ASR)
transcripts. Our experiments demonstrate the power of language models in
understanding videos on a wide variety of video-language tasks, including video
captioning, video question answering, video caption retrieval, and video future
event prediction. Especially, on video future event prediction, our few-shot
model significantly outperforms state-of-the-art supervised models trained on
large-scale video datasets. Code and resources are publicly available for
research purposes at https://github.com/MikeWangWZHL/VidIL .",2022-05-22
"Context Matters for Image Descriptions for Accessibility: Challenges for
  Referenceless Evaluation Metrics",2022-05-21 17:35:26+00:00,http://arxiv.org/abs/2205.10646v1,"Elisa Kreiss, Cynthia Bennett, Shayan Hooshmand, Eric Zelikman, Meredith Ringel Morris, Christopher Potts",cs.CL,image2text,"Few images on the Web receive alt-text descriptions that would make them
accessible to blind and low vision (BLV) users. Image-based NLG systems have
progressed to the point where they can begin to address this persistent
societal problem, but these systems will not be fully successful unless we
evaluate them on metrics that guide their development correctly. Here, we argue
against current referenceless metrics -- those that don't rely on
human-generated ground-truth descriptions -- on the grounds that they do not
align with the needs of BLV users. The fundamental shortcoming of these metrics
is that they cannot take context into account, whereas contextual information
is highly valued by BLV users. To substantiate these claims, we present a study
with BLV participants who rated descriptions along a variety of dimensions. An
in-depth analysis reveals that the lack of context-awareness makes current
referenceless metrics inadequate for advancing image accessibility, requiring a
rethinking of referenceless evaluation metrics for image-based NLG systems.",2022-05-21
"It Isn't Sh!tposting, It's My CAT Posting",2022-05-18 04:11:55+00:00,http://arxiv.org/abs/2205.08710v1,"Parthsarthi Rawat, Sayan Das, Jorge Aguirre, Akhil Daphara","cs.CV, cs.AI, cs.LG",image2text,"In this paper, we describe a novel architecture which can generate hilarious
captions for a given input image. The architecture is split into two halves,
i.e. image captioning and hilarious text conversion. The architecture starts
with a pre-trained CNN model, VGG16 in this implementation, and applies
attention LSTM on it to generate normal caption. These normal captions then are
fed forward to our hilarious text conversion transformer which converts this
text into something hilarious while maintaining the context of the input image.
The architecture can also be split into two halves and only the seq2seq
transformer can be used to generate hilarious caption by inputting a
sentence.This paper aims to help everyday user to be more lazy and hilarious at
the same time by generating captions using CATNet.",2022-05-18
"Breaking with Fixed Set Pathology Recognition through Report-Guided
  Contrastive Training",2022-05-14 21:44:05+00:00,http://arxiv.org/abs/2205.07139v1,"Constantin Seibold, Simon ReiÃŸ, M. Saquib Sarfraz, Rainer Stiefelhagen, Jens Kleesiek","cs.CV, cs.LG",image2text,"When reading images, radiologists generate text reports describing the
findings therein. Current state-of-the-art computer-aided diagnosis tools
utilize a fixed set of predefined categories automatically extracted from these
medical reports for training. This form of supervision limits the potential
usage of models as they are unable to pick up on anomalies outside of their
predefined set, thus, making it a necessity to retrain the classifier with
additional data when faced with novel classes. In contrast, we investigate
direct text supervision to break away from this closed set assumption. By doing
so, we avoid noisy label extraction via text classifiers and incorporate more
contextual information.
  We employ a contrastive global-local dual-encoder architecture to learn
concepts directly from unstructured medical reports while maintaining its
ability to perform free form classification.
  We investigate relevant properties of open set recognition for radiological
data and propose a method to employ currently weakly annotated data into
training.
  We evaluate our approach on the large-scale chest X-Ray datasets MIMIC-CXR,
CheXpert, and ChestX-Ray14 for disease classification. We show that despite
using unstructured medical report supervision, we perform on par with direct
label supervision through a sophisticated inference setting.",2022-05-14
"Robust (Controlled) Table-to-Text Generation with Structure-Aware
  Equivariance Learning",2022-05-08 23:37:27+00:00,http://arxiv.org/abs/2205.03972v1,"Fei Wang, Zhewei Xu, Pedro Szekely, Muhao Chen","cs.CL, cs.AI, cs.LG",image2text,"Controlled table-to-text generation seeks to generate natural language
descriptions for highlighted subparts of a table. Previous SOTA systems still
employ a sequence-to-sequence generation method, which merely captures the
table as a linear structure and is brittle when table layouts change. We seek
to go beyond this paradigm by (1) effectively expressing the relations of
content pieces in the table, and (2) making our model robust to
content-invariant structural transformations. Accordingly, we propose an
equivariance learning framework, which encodes tables with a structure-aware
self-attention mechanism. This prunes the full self-attention structure into an
order-invariant graph attention that captures the connected graph structure of
cells belonging to the same row or column, and it differentiates between
relevant cells and irrelevant cells from the structural perspective. Our
framework also modifies the positional encoding mechanism to preserve the
relative position of tokens in the same cell but enforce position invariance
among different cells. Our technology is free to be plugged into existing
table-to-text generation models, and has improved T5-based models to offer
better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo,
we preserve promising performance, while previous SOTA systems, even with
transformation-based data augmentation, have seen significant performance
drops. Our code is available at https://github.com/luka-group/Lattice.",2022-05-08
RoViST:Learning Robust Metrics for Visual Storytelling,2022-05-08 03:51:22+00:00,http://arxiv.org/abs/2205.03774v1,"Eileen Wang, Caren Han, Josiah Poon","cs.CV, cs.AI",image2text,"Visual storytelling (VST) is the task of generating a story paragraph that
describes a given image sequence. Most existing storytelling approaches have
evaluated their models using traditional natural language generation metrics
like BLEU or CIDEr. However, such metrics based on n-gram matching tend to have
poor correlation with human evaluation scores and do not explicitly consider
other criteria necessary for storytelling such as sentence structure or topic
coherence. Moreover, a single score is not enough to assess a story as it does
not inform us about what specific errors were made by the model. In this paper,
we propose 3 evaluation metrics sets that analyses which aspects we would look
for in a good story: 1) visual grounding, 2) coherence, and 3) non-redundancy.
We measure the reliability of our metric sets by analysing its correlation with
human judgement scores on a sample of machine stories obtained from 4
state-of-the-arts models trained on the Visual Storytelling Dataset (VIST). Our
metric sets outperforms other metrics on human correlation, and could be served
as a learning based evaluation metric set that is complementary to existing
rule-based metrics.",2022-05-08
"Attract me to Buy: Advertisement Copywriting Generation with Multimodal
  Multi-structured Information",2022-05-07 03:33:00+00:00,http://arxiv.org/abs/2205.03534v1,"Zhipeng Zhang, Xinglin Hou, Kai Niu, Zhongzhen Huang, Tiezheng Ge, Yuning Jiang, Qi Wu, Peng Wang","cs.CL, cs.CV, cs.MM",image2text,"Recently, online shopping has gradually become a common way of shopping for
people all over the world. Wonderful merchandise advertisements often attract
more people to buy. These advertisements properly integrate multimodal
multi-structured information of commodities, such as visual spatial information
and fine-grained structure information. However, traditional multimodal text
generation focuses on the conventional description of what existed and
happened, which does not match the requirement of advertisement copywriting in
the real world. Because advertisement copywriting has a vivid language style
and higher requirements of faithfulness. Unfortunately, there is a lack of
reusable evaluation frameworks and a scarcity of datasets. Therefore, we
present a dataset, E-MMAD (e-commercial multimodal multi-structured
advertisement copywriting), which requires, and supports much more detailed
information in text generation. Noticeably, it is one of the largest video
captioning datasets in this field. Accordingly, we propose a baseline method
and faithfulness evaluation metric on the strength of structured information
reasoning to solve the demand in reality on this dataset. It surpasses the
previous methods by a large margin on all metrics. The dataset and method are
coming soon on \url{https://e-mmad.github.io/e-mmad.net/index.html}.",2022-05-07
Language Models Can See: Plugging Visual Controls in Text Generation,2022-05-05 13:56:18+00:00,http://arxiv.org/abs/2205.02655v1,"Yixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani Yogatama, Yan Wang, Lingpeng Kong, Nigel Collier","cs.CV, cs.CL",image2text,"Generative language models (LMs) such as GPT-2/3 can be prompted to generate
text with remarkable quality. While they are designed for text-prompted
generation, it remains an open question how the generation process could be
guided by modalities beyond text such as images. In this work, we propose a
training-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP),
for plugging in visual controls in the generation process and enabling LMs to
perform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC
is a simple yet efficient plug-and-play framework, which directly combines an
off-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP)
for image-grounded text generation. During decoding, MAGIC influences the
generation of the LM by introducing a CLIP-induced score, called magic score,
which regularizes the generated result to be semantically related to a given
image while being coherent to the previously generated context. Notably, the
proposed decoding scheme does not involve any gradient update operation,
therefore being computationally efficient. On the challenging task of zero-shot
image captioning, MAGIC outperforms the state-of-the-art method by notable
margins with a nearly 27 times decoding speedup. MAGIC is a flexible framework
and is theoretically compatible with any text generation tasks that incorporate
image grounding. In the experiments, we showcase that it is also capable of
performing visually grounded story generation given both an image and a text
prompt.",2022-05-05
Diverse Image Captioning with Grounded Style,2022-05-03 22:57:59+00:00,http://arxiv.org/abs/2205.01813v1,"Franz Klein, Shweta Mahajan, Stefan Roth","cs.CV, cs.LG",image2text,"Stylized image captioning as presented in prior work aims to generate
captions that reflect characteristics beyond a factual description of the scene
composition, such as sentiments. Such prior work relies on given sentiment
identifiers, which are used to express a certain global style in the caption,
e.g. positive or negative, however without taking into account the stylistic
content of the visual scene. To address this shortcoming, we first analyze the
limitations of current stylized captioning datasets and propose COCO
attribute-based augmentations to obtain varied stylized captions from COCO
annotations. Furthermore, we encode the stylized information in the latent
space of a Variational Autoencoder; specifically, we leverage extracted image
attributes to explicitly structure its sequential latent space according to
different localized style characteristics. Our experiments on the Senticap and
COCO datasets show the ability of our approach to generate accurate captions
with diversity in styles that are grounded in the image.",2022-05-03
Cross-modal Memory Networks for Radiology Report Generation,2022-04-28 02:32:53+00:00,http://arxiv.org/abs/2204.13258v1,"Zhihong Chen, Yaling Shen, Yan Song, Xiang Wan",cs.CL,image2text,"Medical imaging plays a significant role in clinical practice of medical
diagnosis, where the text reports of the images are essential in understanding
them and facilitating later treatments. By generating the reports
automatically, it is beneficial to help lighten the burden of radiologists and
significantly promote clinical automation, which already attracts much
attention in applying artificial intelligence to medical domain. Previous
studies mainly follow the encoder-decoder paradigm and focus on the aspect of
text generation, with few studies considering the importance of cross-modal
mappings and explicitly exploit such mappings to facilitate radiology report
generation. In this paper, we propose a cross-modal memory networks (CMN) to
enhance the encoder-decoder framework for radiology report generation, where a
shared memory is designed to record the alignment between images and texts so
as to facilitate the interaction and generation across modalities. Experimental
results illustrate the effectiveness of our proposed model, where
state-of-the-art performance is achieved on two widely used benchmark datasets,
i.e., IU X-Ray and MIMIC-CXR. Further analyses also prove that our model is
able to better align information from radiology images and texts so as to help
generating more accurate reports in terms of clinical indicators.",2022-04-28
"Recovering Patient Journeys: A Corpus of Biomedical Entities and
  Relations on Twitter (BEAR)",2022-04-21 08:18:44+00:00,http://arxiv.org/abs/2204.09952v1,"Amelie WÃ¼hrl, Roman Klinger","cs.CL, cs.IR",image2text,"Text mining and information extraction for the medical domain has focused on
scientific text generated by researchers. However, their direct access to
individual patient experiences or patient-doctor interactions can be limited.
Information provided on social media, e.g., by patients and their relatives,
complements the knowledge in scientific text. It reflects the patient's journey
and their subjective perspective on the process of developing symptoms, being
diagnosed and offered a treatment, being cured or learning to live with a
medical condition. The value of this type of data is therefore twofold:
Firstly, it offers direct access to people's perspectives. Secondly, it might
cover information that is not available elsewhere, including self-treatment or
self-diagnoses. Named entity recognition and relation extraction are methods to
structure information that is available in unstructured text. However, existing
medical social media corpora focused on a comparably small set of entities and
relations and particular domains, rather than putting the patient into the
center of analyses. With this paper we contribute a corpus with a rich set of
annotation layers following the motivation to uncover and model patients'
journeys and experiences in more detail. We label 14 entity classes (incl.
environmental factors, diagnostics, biochemical processes, patients'
quality-of-life descriptions, pathogens, medical conditions, and treatments)
and 20 relation classes (e.g., prevents, influences, interactions, causes) most
of which have not been considered before for social media data. The publicly
available dataset consists of 2,100 tweets with approx. 6,000 entity and 3,000
relation annotations. In a corpus analysis we find that over 80 % of documents
contain relevant entities. Over 50 % of tweets express relations which we
consider essential for uncovering patients' narratives about their journeys.",2022-04-21
"Evaluating Mixed-initiative Conversational Search Systems via User
  Simulation",2022-04-17 16:27:33+00:00,http://arxiv.org/abs/2204.08046v1,"Ivan SekuliÄ‡, Mohammad Aliannejadi, Fabio Crestani","cs.CL, cs.IR",image2text,"Clarifying the underlying user information need by asking clarifying
questions is an important feature of modern conversational search system.
However, evaluation of such systems through answering prompted clarifying
questions requires significant human effort, which can be time-consuming and
expensive. In this paper, we propose a conversational User Simulator, called
USi, for automatic evaluation of such conversational search systems. Given a
description of an information need, USi is capable of automatically answering
clarifying questions about the topic throughout the search session. Through a
set of experiments, including automated natural language generation metrics and
crowdsourcing studies, we show that responses generated by USi are both inline
with the underlying information need and comparable to human-generated answers.
Moreover, we make the first steps towards multi-turn interactions, where
conversational search systems asks multiple questions to the (simulated) user
with a goal of clarifying the user need. To this end, we expand on currently
available datasets for studying clarifying questions, i.e., Qulac and ClariQ,
by performing a crowdsourcing-based multi-turn data acquisition. We show that
our generative, GPT2-based model, is capable of providing accurate and natural
answers to unseen clarifying questions in the single-turn setting and discuss
capabilities of our model in the multi-turn setting. We provide the code, data,
and the pre-trained model to be used for further research on the topic.",2022-04-17
"Regularization-based Pruning of Irrelevant Weights in Deep Neural
  Architectures",2022-04-11 09:44:16+00:00,http://arxiv.org/abs/2204.04977v1,"Giovanni Bonetta, Matteo Ribero, Rossella Cancelliere","cs.CL, cs.AI",image2text,"Deep neural networks exploiting millions of parameters are nowadays the norm
in deep learning applications. This is a potential issue because of the great
amount of computational resources needed for training, and of the possible loss
of generalization performance of overparametrized networks. We propose in this
paper a method for learning sparse neural topologies via a regularization
technique which identifies non relevant weights and selectively shrinks their
norm, while performing a classic update for relevant ones. This technique,
which is an improvement of classical weight decay, is based on the definition
of a regularization term which can be added to any loss functional regardless
of its form, resulting in a unified general framework exploitable in many
different contexts. The actual elimination of parameters identified as
irrelevant is handled by an iterative pruning algorithm. We tested the proposed
technique on different image classification and Natural language generation
tasks, obtaining results on par or better then competitors in terms of sparsity
and metrics, while achieving strong models compression.",2022-04-11
"Explaining Deep Convolutional Neural Networks via Latent Visual-Semantic
  Filter Attention",2022-04-10 04:57:56+00:00,http://arxiv.org/abs/2204.04601v1,"Yu Yang, Seungbae Kim, Jungseock Joo","cs.CV, cs.AI, cs.LG",image2text,"Interpretability is an important property for visual models as it helps
researchers and users understand the internal mechanism of a complex model.
However, generating semantic explanations about the learned representation is
challenging without direct supervision to produce such explanations. We propose
a general framework, Latent Visual Semantic Explainer (LaViSE), to teach any
existing convolutional neural network to generate text descriptions about its
own latent representations at the filter level. Our method constructs a mapping
between the visual and semantic spaces using generic image datasets, using
images and category names. It then transfers the mapping to the target domain
which does not have semantic labels. The proposed framework employs a modular
structure and enables to analyze any trained network whether or not its
original training data is available. We show that our method can generate novel
descriptions for learned filters beyond the set of categories defined in the
training dataset and perform an extensive evaluation on multiple datasets. We
also demonstrate a novel application of our method for unsupervised dataset
bias analysis which allows us to automatically discover hidden biases in
datasets or compare different subsets without using additional labels. The
dataset and code are made public to facilitate further research.",2022-04-10
On Distinctive Image Captioning via Comparing and Reweighting,2022-04-08 08:59:23+00:00,http://arxiv.org/abs/2204.03938v1,"Jiuniu Wang, Wenjia Xu, Qingzhong Wang, Antoni B. Chan","cs.CV, cs.AI",image2text,"Recent image captioning models are achieving impressive results based on
popular metrics, i.e., BLEU, CIDEr, and SPICE. However, focusing on the most
popular metrics that only consider the overlap between the generated captions
and human annotation could result in using common words and phrases, which
lacks distinctiveness, i.e., many similar images have the same caption. In this
paper, we aim to improve the distinctiveness of image captions via comparing
and reweighting with a set of similar images. First, we propose a
distinctiveness metric -- between-set CIDEr (CIDErBtw) to evaluate the
distinctiveness of a caption with respect to those of similar images. Our
metric reveals that the human annotations of each image in the MSCOCO dataset
are not equivalent based on distinctiveness; however, previous works normally
treat the human annotations equally during training, which could be a reason
for generating less distinctive captions. In contrast, we reweight each
ground-truth caption according to its distinctiveness during training. We
further integrate a long-tailed weight strategy to highlight the rare words
that contain more information, and captions from the similar image set are
sampled as negative examples to encourage the generated sentence to be unique.
Finally, extensive experiments are conducted, showing that our proposed
approach significantly improves both distinctiveness (as measured by CIDErBtw
and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide
variety of image captioning baselines. These results are further confirmed
through a user study.",2022-04-08
CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations,2022-04-05 17:38:04+00:00,http://arxiv.org/abs/2204.02380v1,"Leonard Salewski, A. Sophia Koepke, Hendrik P. A. Lensch, Zeynep Akata","cs.CV, cs.CL",image2text,"Providing explanations in the context of Visual Question Answering (VQA)
presents a fundamental problem in machine learning. To obtain detailed insights
into the process of generating natural language explanations for VQA, we
introduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with
natural language explanations. For each image-question pair in the CLEVR
dataset, CLEVR-X contains multiple structured textual explanations which are
derived from the original scene graphs. By construction, the CLEVR-X
explanations are correct and describe the reasoning and visual information that
is necessary to answer a given question. We conducted a user study to confirm
that the ground-truth explanations in our proposed dataset are indeed complete
and relevant. We present baseline results for generating natural language
explanations in the context of VQA using two state-of-the-art frameworks on the
CLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation
generation quality for different question and answer types. Additionally, we
study the influence of using different numbers of ground-truth explanations on
the convergence of natural language generation (NLG) metrics. The CLEVR-X
dataset is publicly available at
\url{https://explainableml.github.io/CLEVR-X/}.",2022-04-05
Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language,2022-04-01 17:43:13+00:00,http://arxiv.org/abs/2204.00598v1,"Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Pete Florence","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Large foundation models can exhibit unique capabilities depending on the
domain of data they are trained on. While these domains are generic, they may
only barely overlap. For example, visual-language models (VLMs) are trained on
Internet-scale image captions, but large language models (LMs) are further
trained on Internet-scale text with no images (e.g. from spreadsheets, to SAT
questions). As a result, these models store different forms of commonsense
knowledge across different domains. In this work, we show that this model
diversity is symbiotic, and can be leveraged to build AI systems with
structured Socratic dialogue -- in which new multimodal tasks are formulated as
a guided language-based exchange between different pre-existing foundation
models, without additional finetuning. In the context of egocentric perception,
we present a case study of Socratic Models (SMs) that can provide meaningful
results for complex tasks such as generating free-form answers to contextual
questions about egocentric video, by formulating video Q&A as short story Q&A,
i.e. summarizing the video into a short story, then answering questions about
it. Additionally, SMs can generate captions for Internet images, and are
competitive with state-of-the-art on zero-shot video-to-text retrieval with
42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate how to compose foundation models
zero-shot to capture new multimodal functionalities, without domain-specific
data collection. Prototypes are available at socraticmodels.github.io.",2022-04-01
Neural Pipeline for Zero-Shot Data-to-Text Generation,2022-03-30 13:14:35+00:00,http://arxiv.org/abs/2203.16279v1,"ZdenÄ›k Kasner, OndÅ™ej DuÅ¡ek",cs.CL,image2text,"In data-to-text (D2T) generation, training on in-domain data leads to
overfitting to the data representation and repeating training data noise. We
examine how to avoid finetuning pretrained language models (PLMs) on D2T
generation datasets while still taking advantage of surface realization
capabilities of PLMs. Inspired by pipeline approaches, we propose to generate
text by transforming single-item descriptions with a sequence of modules
trained on general-domain text-based operations: ordering, aggregation, and
paragraph compression. We train PLMs for performing these operations on a
synthetic corpus WikiFluent which we build from English Wikipedia. Our
experiments on two major triple-to-text datasets -- WebNLG and E2E -- show that
our approach enables D2T generation from RDF triples in zero-shot settings.",2022-03-30
"GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate
  Degradation of Artificial Neural Language Models",2022-03-25 00:25:42+00:00,http://arxiv.org/abs/2203.13397v1,"Changye Li, David Knopman, Weizhe Xu, Trevor Cohen, Serguei Pakhomov",cs.CL,image2text,"Deep learning (DL) techniques involving fine-tuning large numbers of model
parameters have delivered impressive performance on the task of discriminating
between language produced by cognitively healthy individuals, and those with
Alzheimer's disease (AD). However, questions remain about their ability to
generalize beyond the small reference sets that are publicly available for
research. As an alternative to fitting model parameters directly, we propose a
novel method by which a Transformer DL model (GPT-2) pre-trained on general
English text is paired with an artificially degraded version of itself (GPT-D),
to compute the ratio between these two models' \textit{perplexities} on
language from cognitively healthy and impaired individuals. This technique
approaches state-of-the-art performance on text data from a widely used ""Cookie
Theft"" picture description task, and unlike established alternatives also
generalizes well to spontaneous conversations. Furthermore, GPT-D generates
text with characteristics known to be associated with AD, demonstrating the
induction of dementia-related linguistic anomalies. Our study is a step toward
better understanding of the relationships between the inner workings of
generative neural language models, the language that they produce, and the
deleterious effects of dementia on human speech and language characteristics.",2022-03-25
Chart-to-Text: A Large-Scale Benchmark for Chart Summarization,2022-03-12 17:01:38+00:00,http://arxiv.org/abs/2203.06486v1,"Shankar Kanthara, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, Shafiq Joty",cs.CL,image2text,"Charts are commonly used for exploring data and communicating insights.
Generating natural language summaries from charts can be very helpful for
people in inferring key insights that would otherwise require a lot of
cognitive and perceptual efforts. We present Chart-to-text, a large-scale
benchmark with two datasets and a total of 44,096 charts covering a wide range
of topics and chart types. We explain the dataset construction process and
analyze the datasets. We also introduce a number of state-of-the-art neural
models as baselines that utilize image captioning and data-to-text generation
techniques to tackle two problem variations: one assumes the underlying data
table of the chart is available while the other needs to extract data from
chart images. Our analysis with automatic and human evaluation shows that while
our best models usually generate fluent summaries and yield reasonable BLEU
scores, they also suffer from hallucinations and factual errors as well as
difficulties in correctly explaining complex patterns and trends in charts.",2022-03-12
Compilable Neural Code Generation with Compiler Feedback,2022-03-10 03:15:17+00:00,http://arxiv.org/abs/2203.05132v1,"Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, Qun Liu","cs.CL, cs.AI, cs.PL",image2text,"Automatically generating compilable programs with (or without) natural
language descriptions has always been a touchstone problem for computational
linguistics and automated software engineering. Existing deep-learning
approaches model code generation as text generation, either constrained by
grammar structures in decoder, or driven by pre-trained language models on
large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of
them account for compilability of the generated programs. To improve
compilability of the generated programs, this paper proposes COMPCODER, a
three-stage pipeline utilizing compiler feedback for compilable code
generation, including language model fine-tuning, compilability reinforcement,
and compilability discrimination. Comprehensive experiments on two code
generation tasks demonstrate the effectiveness of our proposed approach,
improving the success rate of compilation from 44.18 to 89.18 in code
completion on average and from 70.3 to 96.2 in text-to-code generation,
respectively, when comparing with the state-of-the-art CodeGPT.",2022-03-10
"How to Fill the Optimum Set? Population Gradient Descent with Harmless
  Diversity",2022-02-16 23:40:18+00:00,http://arxiv.org/abs/2202.08376v1,"Chengyue Gong, Lemeng Wu, Qiang Liu","cs.LG, cs.CV",image2text,"Although traditional optimization methods focus on finding a single optimal
solution, most objective functions in modern machine learning problems,
especially those in deep learning, often have multiple or infinite numbers of
optima. Therefore, it is useful to consider the problem of finding a set of
diverse points in the optimum set of an objective function. In this work, we
frame this problem as a bi-level optimization problem of maximizing a diversity
score inside the optimum set of the main loss function, and solve it with a
simple population gradient descent framework that iteratively updates the
points to maximize the diversity score in a fashion that does not hurt the
optimization of the main loss. We demonstrate that our method can efficiently
generate diverse solutions on a variety of applications, including
text-to-image generation, text-to-mesh generation, molecular conformation
generation and ensemble neural network training.",2022-02-16
"Deep soccer captioning with transformer: dataset, semantics-related
  losses, and multi-level evaluation",2022-02-11 16:04:03+00:00,http://arxiv.org/abs/2202.05728v1,"Ahmad Hammoudeh, Bastein Vanderplaetse, StÃ©phane Dupont","cs.CV, cs.AI",image2text,"This work aims at generating captions for soccer videos using deep learning.
In this context, this paper introduces a dataset, model, and triple-level
evaluation. The dataset consists of 22k caption-clip pairs and three visual
features (images, optical flow, inpainting) for ~500 hours of \emph{SoccerNet}
videos. The model is divided into three parts: a transformer learns language,
ConvNets learn vision, and a fusion of linguistic and visual features generates
captions. The paper suggests evaluating generated captions at three levels:
syntax (the commonly used evaluation metrics such as BLEU-score and CIDEr),
meaning (the quality of descriptions for a domain expert), and corpus (the
diversity of generated captions). The paper shows that the diversity of
generated captions has improved (from 0.07 reaching 0.18) with
semantics-related losses that prioritize selected words. Semantics-related
losses and the utilization of more visual features (optical flow, inpainting)
improved the normalized captioning score by 28\%. The web page of this work:
https://sites.google.com/view/soccercaptioning}{https://sites.google.com/view/soccercaptioning",2022-02-11
"Unifying Architectures, Tasks, and Modalities Through a Simple
  Sequence-to-Sequence Learning Framework",2022-02-07 10:38:21+00:00,http://arxiv.org/abs/2202.03052v1,"Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang","cs.CV, cs.CL",image2text,"In this work, we pursue a unified paradigm for multimodal pretraining to
break the scaffolds of complex task/modality-specific customization. We propose
OFA, a unified multimodal pretrained model that unifies modalities (i.e.,
cross-modality, vision, language) and tasks (e.g., image generation, visual
grounding, image captioning, image classification, text generation, etc.) to a
simple sequence-to-sequence learning framework based on the encoder-decoder
architecture. OFA performs pretraining and finetuning with task instructions
and introduces no extra task-specific layers for finetuning. Experimental
results show that OFA achieves new state-of-the-arts on a series of multimodal
tasks, including image captioning (COCO test CIDEr: 149.6), text-to-image
generation (COCO test FID: 10.5), VQA (test-std acc.: 80.02), SNLI-VE (test
acc.: 90.20), and referring expression comprehension (RefCOCO / RefCOCO+ /
RefCOCOg test acc.: 92.93 / 90.10 / 85.20). Through extensive analyses, we
demonstrate that OFA reaches comparable performance with uni-modal pretrained
models (e.g., BERT, MAE, MoCo v3, SimCLR v2, etc.) in uni-modal tasks,
including NLU, NLG, and image classification, and it effectively transfers to
unseen tasks and domains. Code shall be released soon at
http://github.com/OFA-Sys/OFA",2022-02-07
"XAlign: Cross-lingual Fact-to-Text Alignment and Generation for
  Low-Resource Languages",2022-02-01 09:41:59+00:00,http://arxiv.org/abs/2202.00291v1,"Tushar Abhishek, Shivprasad Sagare, Bhavyajeet Singh, Anubhav Sharma, Manish Gupta, Vasudeva Varma",cs.CL,image2text,"Multiple critical scenarios (like Wikipedia text generation given English
Infoboxes) need automated generation of descriptive text in low resource (LR)
languages from English fact triples. Previous work has focused on English
fact-to-text (F2T) generation. To the best of our knowledge, there has been no
previous attempt on cross-lingual alignment or generation for LR languages.
Building an effective cross-lingual F2T (XF2T) system requires alignment
between English structured facts and LR sentences. We propose two unsupervised
methods for cross-lingual alignment. We contribute XALIGN, an XF2T dataset with
0.45M pairs across 8 languages, of which 5402 pairs have been manually
annotated. We also train strong baseline XF2T generation models on the XAlign
dataset.",2022-02-01
"BERTHA: Video Captioning Evaluation Via Transfer-Learned Human
  Assessment",2022-01-25 11:29:58+00:00,http://arxiv.org/abs/2201.10243v1,"Luis Lebron, Yvette Graham, Kevin McGuinness, Konstantinos Kouramas, Noel E. O'Connor","cs.CV, cs.LG",image2text,"Evaluating video captioning systems is a challenging task as there are
multiple factors to consider; for instance: the fluency of the caption,
multiple actions happening in a single scene, and the human bias of what is
considered important. Most metrics try to measure how similar the system
generated captions are to a single or a set of human-annotated captions. This
paper presents a new method based on a deep learning model to evaluate these
systems. The model is based on BERT, which is a language model that has been
shown to work well in multiple NLP tasks. The aim is for the model to learn to
perform an evaluation similar to that of a human. To do so, we use a dataset
that contains human evaluations of system generated captions. The dataset
consists of the human judgments of the captions produce by the system
participating in various years of the TRECVid video to text task. These
annotations will be made publicly available. BERTHA obtain favourable results,
outperforming the commonly used metrics in some setups.",2022-01-25
Pre-Trained Language Transformers are Universal Image Classifiers,2022-01-25 08:56:14+00:00,http://arxiv.org/abs/2201.10182v1,"Rahul Goel, Modar Sulaiman, Kimia Noorbakhsh, Mahdi Sharifi, Rajesh Sharma, Pooyan Jamshidi, Kallol Roy","cs.CV, cs.AI",image2text,"Facial images disclose many hidden personal traits such as age, gender, race,
health, emotion, and psychology. Understanding these traits will help to
classify the people in different attributes. In this paper, we have presented a
novel method for classifying images using a pretrained transformer model. We
apply the pretrained transformer for the binary classification of facial images
in criminal and non-criminal classes. The pretrained transformer of GPT-2 is
trained to generate text and then fine-tuned to classify facial images. During
the finetuning process with images, most of the layers of GT-2 are frozen
during backpropagation and the model is frozen pretrained transformer (FPT).
The FPT acts as a universal image classifier, and this paper shows the
application of FPT on facial images. We also use our FPT on encrypted images
for classification. Our FPT shows high accuracy on both raw facial images and
encrypted images. We hypothesize the meta-learning capacity FPT gained because
of its large size and trained on a large size with theory and experiments. The
GPT-2 trained to generate a single word token at a time, through the
autoregressive process, forced to heavy-tail distribution. Then the FPT uses
the heavy-tail property as its meta-learning capacity for classifying images.
Our work shows one way to avoid bias during the machine classification of
images.The FPT encodes worldly knowledge because of the pretraining of one
text, which it uses during the classification. The statistical error of
classification is reduced because of the added context gained from the text.Our
paper shows the ethical dimension of using encrypted data for
classification.Criminal images are sensitive to share across the boundary but
encrypted largely evades ethical concern.FPT showing good classification
accuracy on encrypted images shows promise for further research on
privacy-preserving machine learning.",2022-01-25
An Integrated Approach for Video Captioning and Applications,2022-01-23 01:06:00+00:00,http://arxiv.org/abs/2201.09153v1,"Soheyla Amirian, Thiab R. Taha, Khaled Rasheed, Hamid R. Arabnia","cs.CV, cs.AI",image2text,"Physical computing infrastructure, data gathering, and algorithms have
recently had significant advances to extract information from images and
videos. The growth has been especially outstanding in image captioning and
video captioning. However, most of the advancements in video captioning still
take place in short videos. In this research, we caption longer videos only by
using the keyframes, which are a small subset of the total video frames.
Instead of processing thousands of frames, only a few frames are processed
depending on the number of keyframes. There is a trade-off between the
computation of many frames and the speed of the captioning process. The
approach in this research is to allow the user to specify the trade-off between
execution time and accuracy. In addition, we argue that linking images, videos,
and natural language offers many practical benefits and immediate practical
applications. From the modeling perspective, instead of designing and staging
explicit algorithms to process videos and generate captions in complex
processing pipelines, our contribution lies in designing hybrid deep learning
architectures to apply in long videos by captioning video keyframes. We
consider the technology and the methodology that we have developed as steps
toward the applications discussed in this research.",2022-01-23
"Inferring Commonsense Explanations as Prompts for Future Event
  Generation",2022-01-18 16:21:23+00:00,http://arxiv.org/abs/2201.07099v1,"Li Lin, Yixin Cao, Lifu Huang, Shuang Li, Xuming Hu, Lijie Wen, Jianmin Wang","cs.CL, cs.LG, I.2.7; I.2.4",image2text,"Future Event Generation aims to generate fluent and reasonable future event
descriptions given preceding events. It requires not only fluent text
generation but also commonsense reasoning to maintain the coherence of the
entire event story. However, existing FEG methods are easily trapped into
repeated or general events without imposing any logical constraint to the
generation process. In this paper, we propose a novel explainable FEG framework
that consists of a commonsense inference model (IM) and an event generation
model (GM). The IM, which is pre-trained on a commonsense knowledge graph
ATOMIC, learns to interpret the preceding events and conducts commonsense
reasoning to reveal the characters psychology such as intent, reaction, and
needs as latent variables. GM further takes the commonsense knowledge as
prompts to guide and enforce the generation of logistically coherent future
events. As unique merit, the commonsense prompts can be further decoded into
textual descriptions, yielding explanations for the future event. Automatic and
human evaluation demonstrate that our approach can generate more coherent,
specific, and logical future events than the strong baselines.",2022-01-18
Local Information Assisted Attention-free Decoder for Audio Captioning,2022-01-10 08:55:52+00:00,http://arxiv.org/abs/2201.03217v1,"Feiyang Xiao, Jian Guan, Qiaoxi Zhu, Haiyan Lan, Wenwu Wang","cs.SD, cs.LG, eess.AS",image2text,"Automated audio captioning (AAC) aims to describe audio data with captions
using natural language. Most existing AAC methods adopt an encoder-decoder
structure, where the attention based mechanism is a popular choice in the
decoder (e.g., Transformer decoder) for predicting captions from audio
features. Such attention based decoders can capture the global information from
the audio features, however, their ability in extracting local information can
be limited, which may lead to degraded quality in the generated captions. In
this paper, we present an AAC method with an attention-free decoder, where an
encoder based on PANNs is employed for audio feature extraction, and the
attention-free decoder is designed to introduce local information. The proposed
method enables the effective use of both global and local information from
audio signals. Experiments show that our method outperforms the
state-of-the-art methods with the standard attention based decoder in Task 6 of
the DCASE 2021 Challenge.",2022-01-10
Self-Training Vision Language BERTs with a Unified Conditional Model,2022-01-06 11:00:52+00:00,http://arxiv.org/abs/2201.02010v1,"Xiaofeng Yang, Fengmao Lv, Fayao Liu, Guosheng Lin","cs.CV, cs.CL",image2text,"Natural language BERTs are trained with language corpus in a self-supervised
manner. Unlike natural language BERTs, vision language BERTs need paired data
to train, which restricts the scale of VL-BERT pretraining. We propose a
self-training approach that allows training VL-BERTs from unlabeled image data.
The proposed method starts with our unified conditional model -- a vision
language BERT model that can perform zero-shot conditional generation. Given
different conditions, the unified conditional model can generate captions,
dense captions, and even questions. We use the labeled image data to train a
teacher model and use the trained model to generate pseudo captions on
unlabeled image data. We then combine the labeled data and pseudo labeled data
to train a student model. The process is iterated by putting the student model
as a new teacher. By using the proposed self-training approach and only 300k
unlabeled extra data, we are able to get competitive or even better
performances compared to the models of similar model size trained with 3
million extra image data.",2022-01-06
Compact Bidirectional Transformer for Image Captioning,2022-01-06 09:23:18+00:00,http://arxiv.org/abs/2201.01984v1,"Yuanen Zhou, Zhenzhen Hu, Daqing Liu, Huixia Ben, Meng Wang","cs.CV, cs.CL",image2text,"Most current image captioning models typically generate captions from left to
right. This unidirectional property makes them can only leverage past context
but not future context. Though recent refinement-based models can exploit both
past and future context by generating a new caption in the second stage based
on pre-retrieved or pre-generated captions in the first stage, the decoder of
these models generally consists of two networks~(i.e. a retriever or captioner
in the first stage and a refiner in the second stage), which can only be
executed sequentially. In this paper, we introduce a Compact Bidirectional
Transformer model for image captioning that can leverage bidirectional context
implicitly and explicitly while the decoder can be executed parallelly.
Specifically, it is implemented by tightly coupling left-to-right(L2R) and
right-to-left(R2L) flows into a single compact model~(i.e. implicitly) and
optionally allowing interaction of the two flows(i.e. explicitly), while the
final caption is chosen from either L2R or R2L flow in a sentence-level
ensemble manner. We conduct extensive ablation studies on the MSCOCO benchmark
and find that the compact architecture, which serves as a regularization for
implicitly exploiting bidirectional context, and the sentence-level ensemble
play more important roles than the explicit interaction mechanism. By combining
with word-level ensemble seamlessly, the effect of the sentence-level ensemble
is further enlarged. We further extend the conventional one-flow self-critical
training to the two-flows version under this architecture and achieve new
state-of-the-art results in comparison with non-vision-language-pretraining
models. Source code is available at
{\color{magenta}\url{https://github.com/YuanEZhou/CBTrans}}.",2022-01-06
"StyleM: Stylized Metrics for Image Captioning Built with Contrastive
  N-grams",2022-01-04 04:44:05+00:00,http://arxiv.org/abs/2201.00975v1,"Chengxi Li, Brent Harrison","cs.CV, cs.AI, cs.CL",image2text,"In this paper, we build two automatic evaluation metrics for evaluating the
association between a machine-generated caption and a ground truth stylized
caption: OnlyStyle and StyleCIDEr.",2022-01-04
"ERNIE-ViLG: Unified Generative Pre-training for Bidirectional
  Vision-Language Generation",2021-12-31 03:53:33+00:00,http://arxiv.org/abs/2112.15283v1,"Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang","cs.CV, cs.CL",image2text,"Conventional methods for the image-text generation tasks mainly tackle the
naturally bidirectional generation tasks separately, focusing on designing
task-specific frameworks to improve the quality and fidelity of the generated
samples. Recently, Vision-Language Pre-training models have greatly improved
the performance of the image-to-text generation tasks, but large-scale
pre-training models for text-to-image synthesis task are still under-developed.
In this paper, we propose ERNIE-ViLG, a unified generative pre-training
framework for bidirectional image-text generation with transformer model. Based
on the image quantization models, we formulate both image generation and text
generation as autoregressive generative tasks conditioned on the text/image
input. The bidirectional image-text generative modeling eases the semantic
alignments across vision and language. For the text-to-image generation
process, we further propose an end-to-end training method to jointly learn the
visual sequence generator and the image reconstructor. To explore the landscape
of large-scale pre-training for bidirectional text-image generation, we train a
10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million
(Chinese) image-text pairs which achieves state-of-the-art performance for both
text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for
text-to-image synthesis and best results on COCO-CN and AIC-ICC for image
captioning.",2021-12-31
"Radiology Report Generation with a Learned Knowledge Base and
  Multi-modal Alignment",2021-12-30 10:43:56+00:00,http://arxiv.org/abs/2112.15011v1,"Shuxin Yang, Xian Wu, Shen Ge, Xingwang Wu, S. Kevin Zhou, Li Xiao","eess.IV, cs.CL, cs.CV",image2text,"In clinics, a radiology report is crucial for guiding a patient's treatment.
Unfortunately, report writing imposes a heavy burden on radiologists. To
effectively reduce such a burden, we hereby present an automatic, multi-modal
approach for report generation from chest x-ray. Our approach, motivated by the
observation that the descriptions in radiology reports are highly correlated
with the x-ray images, features two distinct modules: (i) Learned knowledge
base. To absorb the knowledge embedded in the above-mentioned correlation, we
automatically build a knowledge base based on textual embedding. (ii)
Multi-modal alignment. To promote the semantic alignment among reports, disease
labels and images, we explicitly utilize textual embedding to guide the
learning of the visual feature space. We evaluate the performance of the
proposed model using metrics from both natural language generation and clinic
efficacy on the public IU and MIMIC-CXR datasets. Our ablation study shows that
each module contributes to improving the quality of generated reports.
Furthermore, with the aid of both modules, our approach clearly outperforms
state-of-the-art methods.",2021-12-30
Automatic Product Copywriting for E-Commerce,2021-12-15 19:06:31+00:00,http://arxiv.org/abs/2112.11915v1,"Xueying Zhang, Yanyan Zou, Hainan Zhang, Jing Zhou, Shiliang Diao, Jiajia Chen, Zhuoye Ding, Zhen He, Xueqi He, Yun Xiao, Bo Long, Han Yu, Lingfei Wu","cs.CL, cs.AI",image2text,"Product copywriting is a critical component of e-commerce recommendation
platforms. It aims to attract users' interest and improve user experience by
highlighting product characteristics with textual descriptions. In this paper,
we report our experience deploying the proposed Automatic Product Copywriting
Generation (APCG) system into the JD.com e-commerce product recommendation
platform. It consists of two main components: 1) natural language generation,
which is built from a transformer-pointer network and a pre-trained
sequence-to-sequence model based on millions of training data from our in-house
platform; and 2) copywriting quality control, which is based on both automatic
evaluation and human screening. For selected domains, the models are trained
and updated daily with the updated training data. In addition, the model is
also used as a real-time writing assistant tool on our live broadcast platform.
The APCG system has been deployed in JD.com since Feb 2021. By Sep 2021, it has
generated 2.53 million product descriptions, and improved the overall averaged
click-through rate (CTR) and the Conversion Rate (CVR) by 4.22% and 3.61%,
compared to baselines, respectively on a year-on-year basis. The accumulated
Gross Merchandise Volume (GMV) made by our system is improved by 213.42%,
compared to the number in Feb 2021.",2021-12-15
Contextualized Scene Imagination for Generative Commonsense Reasoning,2021-12-12 20:38:08+00:00,http://arxiv.org/abs/2112.06318v1,"PeiFeng Wang, Jonathan Zamora, Junfeng Liu, Filip Ilievski, Muhao Chen, Xiang Ren",cs.CL,image2text,"Humans use natural language to compose common concepts from their environment
into plausible, day-to-day scene descriptions. However, such generative
commonsense reasoning (GCSR) skills are lacking in state-of-the-art text
generation methods. Descriptive sentences about arbitrary concepts generated by
neural text generation models (e.g., pre-trained text-to-text Transformers) are
often grammatically fluent but may not correspond to human common sense,
largely due to their lack of mechanisms to capture concept relations, to
identify implicit concepts, and to perform generalizable reasoning about unseen
concept compositions. In this paper, we propose an Imagine-and-Verbalize (I&V)
method, which learns to imagine a relational scene knowledge graph (SKG) with
relations between the input concepts, and leverage the SKG as a constraint when
generating a plausible scene description. We collect and harmonize a set of
knowledge resources from different domains and modalities, providing a rich
auxiliary supervision signal for I&V. The experiments demonstrate the
effectiveness of I&V in improving language models on both concept-to-sentence
and concept-to-story generation tasks, while enabling the model to learn well
from fewer task examples and generate SKGs that make common sense to human
annotators.",2021-12-12
"Improving Logical-Level Natural Language Generation with
  Topic-Conditioned Data Augmentation and Logical Form Generation",2021-12-12 13:50:18+00:00,http://arxiv.org/abs/2112.06240v1,"Ao Liu, Congjian Luo, Naoaki Okazaki",cs.CL,image2text,"Logical Natural Language Generation, i.e., generating textual descriptions
that can be logically entailed by a structured table, has been a challenge due
to the low fidelity of the generation. \citet{chen2020logic2text} have
addressed this problem by annotating interim logical programs to control the
generation contents and semantics, and presented the task of table-aware
logical form to text (Logic2text) generation. However, although table instances
are abundant in the real world, logical forms paired with textual descriptions
require costly human annotation work, which limits the performance of neural
models. To mitigate this, we propose topic-conditioned data augmentation
(TopicDA), which utilizes GPT-2 to generate unpaired logical forms and textual
descriptions directly from tables. We further introduce logical form generation
(LG), a dual task of Logic2text that requires generating a valid logical form
based on a text description of a table. We also propose a semi-supervised
learning approach to jointly train a Logic2text and an LG model with both
labeled and augmented data. The two models benefit from each other by providing
extra supervision signals through back-translation. Experimental results on the
Logic2text dataset and the LG task demonstrate that our approach can
effectively utilize the augmented data and outperform supervised baselines by a
substantial margin.",2021-12-12
Show and Write: Entity-aware News Generation with Image Information,2021-12-11 05:32:09+00:00,http://arxiv.org/abs/2112.05917v1,"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer",cs.CL,image2text,"Automatically writing long articles is a complex and challenging language
generation task. Prior work has primarily focused on generating these articles
using human-written prompt to provide some topical context and some metadata
about the article. That said, for many applications, such as generating news
stories, these articles are often paired with images and their captions or
alt-text, which in turn are based on real-world events and may reference many
different named entities that are difficult to be correctly recognized and
predicted by language models. To address these two problems, this paper
introduces an Entity-aware News Generation method with Image iNformation,
Engin, to incorporate news image information into language models. Engin
produces news articles conditioned on both metadata and information such as
captions and named entities extracted from images. We also propose an
Entity-aware mechanism to help our model better recognize and predict the
entity names in news. We perform experiments on two public large-scale news
datasets, GoodNews and VisualNews. Quantitative results show that our approach
improves article perplexity by 4-5 points over the base models. Qualitative
results demonstrate the text generated by Engin is more consistent with news
images. We also perform article quality annotation experiment on the generated
articles to validate that our model produces higher-quality articles. Finally,
we investigate the effect Engin has on methods that automatically detect
machine-generated articles.",2021-12-11
"Unified Multimodal Pre-training and Prompt-based Tuning for
  Vision-Language Understanding and Generation",2021-12-10 14:59:06+00:00,http://arxiv.org/abs/2112.05587v2,"Tianyi Liu, Zuxuan Wu, Wenhan Xiong, Jingjing Chen, Yu-Gang Jiang","cs.CV, cs.CL, cs.LG",image2text,"Most existing vision-language pre-training methods focus on understanding
tasks and use BERT-like objectives (masked language modeling and image-text
matching) during pretraining. Although they perform well in many understanding
downstream tasks, e.g., visual question answering, image-text retrieval and
visual entailment, they do not possess the ability to generate. To tackle this
problem, we propose Unified multimodal pre-training for both Vision-Language
understanding and generation (UniVL). The proposed UniVL is capable of handling
both understanding tasks and generative tasks. We augment existing pretraining
paradigms that only use random masks with causal masks, i.e., triangular masks
that mask out future tokens, such that the pre-trained models can have
autoregressive generation abilities by design. We formulate several previous
understanding tasks as a text generation task and propose to use prompt-based
method for fine-tuning on different downstream tasks. Our experiments show that
there is a trade-off between understanding tasks and generation tasks while
using the same model, and a feasible way to improve both tasks is to use more
data. Our UniVL framework attains comparable performance to recent
vision-language pre-training methods on both understanding tasks and generation
tasks. Moreover, we demostrate that prompt-based finetuning is more
data-efficient - it outperforms discriminative methods in few-shot scenarios.",2021-12-10
Self-Supervised Image-to-Text and Text-to-Image Synthesis,2021-12-09 13:54:56+00:00,http://arxiv.org/abs/2112.04928v1,"Anindya Sundar Das, Sriparna Saha","cs.CV, cs.CL, cs.LG",image2text,"A comprehensive understanding of vision and language and their interrelation
are crucial to realize the underlying similarities and differences between
these modalities and to learn more generalized, meaningful representations. In
recent years, most of the works related to Text-to-Image synthesis and
Image-to-Text generation, focused on supervised generative deep architectures
to solve the problems, where very little interest was placed on learning the
similarities between the embedding spaces across modalities. In this paper, we
propose a novel self-supervised deep learning based approach towards learning
the cross-modal embedding spaces; for both image to text and text to image
generations. In our approach, we first obtain dense vector representations of
images using StackGAN-based autoencoder model and also dense vector
representations on sentence-level utilizing LSTM based text-autoencoder; then
we study the mapping from embedding space of one modality to embedding space of
the other modality utilizing GAN and maximum mean discrepancy based generative
networks. We, also demonstrate that our model learns to generate textual
description from image data as well as images from textual data both
qualitatively and quantitatively.",2021-12-09
"Search and Learn: Improving Semantic Coverage for Data-to-Text
  Generation",2021-12-06 03:51:56+00:00,http://arxiv.org/abs/2112.02770v1,"Shailza Jolly, Zi Xuan Zhang, Andreas Dengel, Lili Mou",cs.CL,image2text,"Data-to-text generation systems aim to generate text descriptions based on
input data (often represented in the tabular form). A typical system uses huge
training samples for learning the correspondence between tables and texts.
However, large training sets are expensive to obtain, limiting the
applicability of these approaches in real-world scenarios. In this work, we
focus on few-shot data-to-text generation. We observe that, while fine-tuned
pretrained language models may generate plausible sentences, they suffer from
the low semantic coverage problem in the few-shot setting. In other words,
important input slots tend to be missing in the generated text. To this end, we
propose a search-and-learning approach that leverages pretrained language
models but inserts the missing slots to improve the semantic coverage. We
further fine-tune our system based on the search results to smooth out the
search noise, yielding better-quality text and improving inference efficiency
to a large extent. Experiments show that our model achieves high performance on
E2E and WikiBio datasets. Especially, we cover 98.35% of input slots on E2E,
largely alleviating the low coverage problem.",2021-12-06
"Protecting Intellectual Property of Language Generation APIs with
  Lexical Watermark",2021-12-05 22:54:54+00:00,http://arxiv.org/abs/2112.02701v1,"Xuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu, Chenguang Wang","cs.CR, cs.CL",image2text,"Nowadays, due to the breakthrough in natural language generation (NLG),
including machine translation, document summarization, image captioning, etc
NLG models have been encapsulated in cloud APIs to serve over half a billion
people worldwide and process over one hundred billion word generations per day.
Thus, NLG APIs have already become essential profitable services in many
commercial companies. Due to the substantial financial and intellectual
investments, service providers adopt a pay-as-you-use policy to promote
sustainable market growth. However, recent works have shown that cloud
platforms suffer from financial losses imposed by model extraction attacks,
which aim to imitate the functionality and utility of the victim services, thus
violating the intellectual property (IP) of cloud APIs. This work targets at
protecting IP of NLG APIs by identifying the attackers who have utilized
watermarked responses from the victim NLG APIs. However, most existing
watermarking techniques are not directly amenable for IP protection of NLG
APIs. To bridge this gap, we first present a novel watermarking method for text
generation APIs by conducting lexical modification to the original outputs.
Compared with the competitive baselines, our watermark approach achieves better
identifiable performance in terms of p-value, with fewer semantic losses. In
addition, our watermarks are more understandable and intuitive to humans than
the baselines. Finally, the empirical studies show our approach is also
applicable to queries from different domains, and is effective on the attacker
trained on a mixture of the corpus which includes less than 10\% watermarked
samples.",2021-12-05
"Representation Learning for Conversational Data using Discourse Mutual
  Information Maximization",2021-12-04 13:17:07+00:00,http://arxiv.org/abs/2112.05787v1,"Bishal Santra, Sumegh Roychowdhury, Aishik Mandal, Vasu Gurram, Atharva Naik, Manish Gupta, Pawan Goyal",cs.CL,image2text,"Although many pretrained models exist for text or images, there have been
relatively fewer attempts to train representations specifically for dialog
understanding. Prior works usually relied on finetuned representations based on
generic text representation models like BERT or GPT-2. But, existing
pretraining objectives do not take the structural information of text into
consideration. Although generative dialog models can learn structural features
too, we argue that the structure-unaware word-by-word generation is not
suitable for effective conversation modeling. We empirically demonstrate that
such representations do not perform consistently across various dialog
understanding tasks. Hence, we propose a structure-aware Mutual Information
based loss-function DMI (Discourse Mutual Information) for training
dialog-representation models, that additionally captures the inherent
uncertainty in response prediction. Extensive evaluation on nine diverse dialog
modeling tasks shows that our proposed DMI-based models outperform strong
baselines by significant margins, even with small-scale pretraining. Our models
show the most promising performance on the dialog evaluation task
DailyDialog++, in both random and adversarial negative scenarios.",2021-12-04
"LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with
  Self-training",2021-12-02 16:49:41+00:00,http://arxiv.org/abs/2112.01404v1,"Ningyu Zhang, Hongbin Ye, Jiacheng Yang, Shumin Deng, Chuanqi Tan, Mosha Chen, Songfang Huang, Fei Huang, Huajun Chen","cs.CL, cs.AI",image2text,"Natural language generation from structured data mainly focuses on
surface-level descriptions, suffering from uncontrollable content selection and
low fidelity. Previous works leverage logical forms to facilitate logical
knowledge-conditioned text generation. Though achieving remarkable progress,
they are data-hungry, which makes the adoption for real-world applications
challenging with limited data. To this end, this paper proposes a unified
framework for logical knowledge-conditioned text generation in the few-shot
setting. With only a few seeds logical forms (e.g., 20/100 shot), our approach
leverages self-training and samples pseudo logical forms based on content and
structure consistency. Experimental results demonstrate that our approach can
obtain better few-shot performance than baselines.",2021-12-02
"Translation-equivariant Image Quantizer for Bi-directional Image-Text
  Generation",2021-12-01 10:08:24+00:00,http://arxiv.org/abs/2112.00384v1,"Woncheol Shin, Gyubok Lee, Jiyoung Lee, Joonseok Lee, Edward Choi","cs.CV, cs.CL, cs.LG",image2text,"Recently, vector-quantized image modeling has demonstrated impressive
performance on generation tasks such as text-to-image generation. However, we
discover that the current image quantizers do not satisfy translation
equivariance in the quantized space due to aliasing, degrading performance in
the downstream text-to-image generation and image-to-text generation, even in
simple experimental setups. Instead of focusing on anti-aliasing, we take a
direct approach to encourage translation equivariance in the quantized space.
In particular, we explore a desirable property of image quantizers, called
'Translation Equivariance in the Quantized Space' and propose a simple but
effective way to achieve translation equivariance by regularizing orthogonality
in the codebook embedding vectors. Using this method, we improve accuracy by
+22% in text-to-image generation and +26% in image-to-text generation,
outperforming the VQGAN.",2021-12-01
Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic,2021-11-29 11:01:49+00:00,http://arxiv.org/abs/2111.14447v1,"Yoad Tewel, Yoav Shalev, Idan Schwartz, Lior Wolf","cs.CV, cs.AI, cs.CL",image2text,"Recent text-to-image matching models apply contrastive learning to large
corpora of uncurated pairs of images and sentences. While such models can
provide a powerful score for matching and subsequent zero-shot tasks, they are
not capable of generating caption given an image. In this work, we repurpose
such models to generate a descriptive text given an image at inference time,
without any further training or tuning step. This is done by combining the
visual-semantic model with a large language model, benefiting from the
knowledge in both web-scale models. The resulting captions are much less
restrictive than those obtained by supervised captioning methods. Moreover, as
a zero-shot learning method, it is extremely flexible and we demonstrate its
ability to perform image arithmetic in which the inputs can be either images or
text and the output is a sentence. This enables novel high-level vision
capabilities such as comparing two images or solving visual analogy tests.",2021-11-29
LAFITE: Towards Language-Free Training for Text-to-Image Generation,2021-11-27 01:54:45+00:00,http://arxiv.org/abs/2111.13792v1,"Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, Tong Sun","cs.CV, cs.LG",image2text,"One of the major challenges in training text-to-image generation models is
the need of a large number of high-quality image-text pairs. While image
samples are often easily accessible, the associated text descriptions typically
require careful human captioning, which is particularly time- and
cost-consuming. In this paper, we propose the first work to train text-to-image
generation models without any text data. Our method leverages the well-aligned
multi-modal semantic space of the powerful pre-trained CLIP model: the
requirement of text-conditioning is seamlessly alleviated via generating text
features from image features. Extensive experiments are conducted to illustrate
the effectiveness of the proposed method. We obtain state-of-the-art results in
the standard text-to-image generation tasks. Importantly, the proposed
language-free model outperforms most existing models trained with full
image-text pairs. Furthermore, our method can be applied in fine-tuning
pre-trained models, which saves both training time and cost in training
text-to-image generation models. Our pre-trained model obtains competitive
results in zero-shot text-to-image generation on the MS-COCO dataset, yet with
around only 1% of the model size and training data size relative to the
recently proposed large DALL-E model.",2021-11-27
"Octree Transformer: Autoregressive 3D Shape Generation on Hierarchically
  Structured Sequences",2021-11-24 13:17:16+00:00,http://arxiv.org/abs/2111.12480v1,"Moritz Ibing, Gregor Kobsik, Leif Kobbelt","cs.CV, cs.GR, cs.LG",image2text,"Autoregressive models have proven to be very powerful in NLP text generation
tasks and lately have gained popularity for image generation as well. However,
they have seen limited use for the synthesis of 3D shapes so far. This is
mainly due to the lack of a straightforward way to linearize 3D data as well as
to scaling problems with the length of the resulting sequences when describing
complex shapes. In this work we address both of these problems. We use octrees
as a compact hierarchical shape representation that can be sequentialized by
traversal ordering. Moreover, we introduce an adaptive compression scheme, that
significantly reduces sequence lengths and thus enables their effective
generation with a transformer, while still allowing fully autoregressive
sampling and parallel training. We demonstrate the performance of our model by
comparing against the state-of-the-art in shape generation.",2021-11-24
NÃœWA: Visual Synthesis Pre-training for Neural visUal World creAtion,2021-11-24 11:02:12+00:00,http://arxiv.org/abs/2111.12417v1,"Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan","cs.CV, cs.AI",image2text,"This paper presents a unified multimodal pre-trained model called N\""UWA that
can generate new or manipulate existing visual data (i.e., images and videos)
for various visual synthesis tasks. To cover language, image, and video at the
same time for different scenarios, a 3D transformer encoder-decoder framework
is designed, which can not only deal with videos as 3D data but also adapt to
texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA)
mechanism is also proposed to consider the nature of the visual data and reduce
the computational complexity. We evaluate N\""UWA on 8 downstream tasks.
Compared to several strong baselines, N\""UWA achieves state-of-the-art results
on text-to-image generation, text-to-video generation, video prediction, etc.
Furthermore, it also shows surprisingly good zero-shot capabilities on
text-guided image and video manipulation tasks. Project repo is
https://github.com/microsoft/NUWA.",2021-11-24
Scaling Up Vision-Language Pre-training for Image Captioning,2021-11-24 02:30:22+00:00,http://arxiv.org/abs/2111.12233v1,"Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, Lijuan Wang","cs.CV, cs.CL",image2text,"In recent years, we have witnessed significant performance boost in the image
captioning task based on vision-language pre-training (VLP). Scale is believed
to be an important factor for this advance. However, most existing work only
focuses on pre-training transformers with moderate sizes (e.g., 12 or 24
layers) on roughly 4 million images. In this paper, we present LEMON, a
LargE-scale iMage captiONer, and provide the first empirical study on the
scaling behavior of VLP for image captioning. We use the state-of-the-art VinVL
model as our reference model, which consists of an image feature extractor and
a transformer model, and scale the transformer both up and down, with model
sizes ranging from 13 to 675 million parameters. In terms of data, we conduct
experiments with up to 200 million image-text pairs which are automatically
collected from web based on the alt attribute of the image (dubbed as ALT200M).
Extensive analysis helps to characterize the performance trend as the model
size and the pre-training data size increase. We also compare different
training recipes, especially for training on large-scale noisy data. As a
result, LEMON achieves new state of the arts on several major image captioning
benchmarks, including COCO Caption, nocaps, and Conceptual Captions. We also
show LEMON can generate captions with long-tail visual concepts when used in a
zero-shot manner.",2021-11-24
L-Verse: Bidirectional Generation Between Image and Text,2021-11-22 11:48:26+00:00,http://arxiv.org/abs/2111.11133v3,"Taehoon Kim, Gwangmo Song, Sihaeng Lee, Sangyun Kim, Yewon Seo, Soonyoung Lee, Seung Hwan Kim, Honglak Lee, Kyunghoon Bae","cs.CV, cs.CL, cs.LG",image2text,"Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalabilty. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for text-to-image and image-to-text
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation tasks without any finetuning or extra
object detection frameworks. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial results of bidirectional vision-language representation learning on
general domain. Codes available at: https://github.com/tgisaturday/L-Verse",2021-11-22
"RDF-to-Text Generation with Reinforcement Learning Based Graph-augmented
  Structural Neural Encoders",2021-11-20 08:41:54+00:00,http://arxiv.org/abs/2111.10545v1,"Hanning Gao, Lingfei Wu, Po Hu, Zhihua Wei, Fangli Xu, Bo Long","cs.CL, cs.AI",image2text,"Considering a collection of RDF triples, the RDF-to-text generation task aims
to generate a text description. Most previous methods solve this task using a
sequence-to-sequence model or using a graph-based model to encode RDF triples
and to generate a text sequence. Nevertheless, these approaches fail to clearly
model the local and global structural information between and within RDF
triples. Moreover, the previous methods also face the non-negligible problem of
low faithfulness of the generated text, which seriously affects the overall
performance of these models. To solve these problems, we propose a model
combining two new graph-augmented structural neural encoders to jointly learn
both local and global structural information in the input RDF triples. To
further improve text faithfulness, we innovatively introduce a reinforcement
learning (RL) reward based on information extraction (IE). We first extract
triples from the generated text using a pretrained IE model and regard the
correct number of the extracted triples as the additional RL reward.
Experimental results on two benchmark datasets demonstrate that our proposed
model outperforms the state-of-the-art baselines, and the additional
reinforcement learning reward does help to improve the faithfulness of the
generated text.",2021-11-20
Transparent Human Evaluation for Image Captioning,2021-11-17 07:09:59+00:00,http://arxiv.org/abs/2111.08940v1,"Jungo Kasai, Keisuke Sakaguchi, Lavinia Dunagan, Jacob Morrison, Ronan Le Bras, Yejin Choi, Noah A. Smith","cs.CL, cs.CV",image2text,"We establish a rubric-based human evaluation protocol for image captioning
models. Our scoring rubrics and their definitions are carefully developed based
on machine- and human-generated captions on the MSCOCO dataset. Each caption is
evaluated along two main dimensions in a tradeoff (precision and recall) as
well as other aspects that measure the text quality (fluency, conciseness, and
inclusive language). Our evaluations demonstrate several critical problems of
the current evaluation practice. Human-generated captions show substantially
higher quality than machine-generated ones, especially in coverage of salient
information (i.e., recall), while all automatic metrics say the opposite. Our
rubric-based results reveal that CLIPScore, a recent metric that uses image
features, better correlates with human judgments than conventional text-only
metrics because it is more sensitive to recall. We hope that this work will
promote a more transparent evaluation protocol for image captioning and its
automatic metrics.",2021-11-17
Explaining Face Presentation Attack Detection Using Natural Language,2021-11-08 22:55:55+00:00,http://arxiv.org/abs/2111.04862v1,"Hengameh Mirzaalian, Mohamed E. Hussein, Leonidas Spinoulas, Jonathan May, Wael Abd-Almageed","cs.CV, cs.AI, cs.CL, cs.CR",image2text,"A large number of deep neural network based techniques have been developed to
address the challenging problem of face presentation attack detection (PAD).
Whereas such techniques' focus has been on improving PAD performance in terms
of classification accuracy and robustness against unseen attacks and
environmental conditions, there exists little attention on the explainability
of PAD predictions. In this paper, we tackle the problem of explaining PAD
predictions through natural language. Our approach passes feature
representations of a deep layer of the PAD model to a language model to
generate text describing the reasoning behind the PAD prediction. Due to the
limited amount of annotated data in our study, we apply a light-weight LSTM
network as our natural language generation model. We investigate how the
quality of the generated explanations is affected by different loss functions,
including the commonly used word-wise cross entropy loss, a sentence
discriminative loss, and a sentence semantic loss. We perform our experiments
using face images from a dataset consisting of 1,105 bona-fide and 924
presentation attack samples. Our quantitative and qualitative results show the
effectiveness of our model for generating proper PAD explanations through text
as well as the power of the sentence-wise losses. To the best of our knowledge,
this is the first introduction of a joint biometrics-NLP task. Our dataset can
be obtained through our GitHub page.",2021-11-08
Machine-in-the-Loop Rewriting for Creative Image Captioning,2021-11-07 22:17:41+00:00,http://arxiv.org/abs/2111.04193v1,"Vishakh Padmakumar, He He",cs.CL,image2text,"Machine-in-the-loop writing aims to enable humans to collaborate with models
to complete their writing tasks more effectively. Prior work has found that
providing humans a machine-written draft or sentence-level continuations has
limited success since the generated text tends to deviate from humans'
intention. To allow the user to retain control over the content, we train a
rewriting model that, when prompted, modifies specified spans of text within
the user's original draft to introduce descriptive and figurative elements
locally in the text. We evaluate the model on its ability to collaborate with
humans on the task of creative image captioning. On a user study through Amazon
Mechanical Turk, our model is rated to be more helpful than a baseline
infilling language model. In addition, third-party evaluation shows that users
write more descriptive and figurative captions when collaborating with our
model compared to completing the task alone.",2021-11-07
"Exploiting Cross-Modal Prediction and Relation Consistency for
  Semi-Supervised Image Captioning",2021-10-22 13:14:32+00:00,http://arxiv.org/abs/2110.11767v2,"Yang Yang, Hongchen Wei, Hengshu Zhu, Dianhai Yu, Hui Xiong, Jian Yang","cs.CV, cs.AI",image2text,"The task of image captioning aims to generate captions directly from images
via the automatically learned cross-modal generator. To build a well-performing
generator, existing approaches usually need a large number of described images,
which requires a huge effects on manual labeling. However, in real-world
applications, a more general scenario is that we only have limited amount of
described images and a large number of undescribed images. Therefore, a
resulting challenge is how to effectively combine the undescribed images into
the learning of cross-modal generator. To solve this problem, we propose a
novel image captioning method by exploiting the Cross-modal Prediction and
Relation Consistency (CPRC), which aims to utilize the raw image input to
constrain the generated sentence in the commonly semantic space. In detail,
considering that the heterogeneous gap between modalities always leads to the
supervision difficulty of using the global embedding directly, CPRC turns to
transform both the raw image and corresponding generated sentence into the
shared semantic space, and measure the generated sentence from two aspects: 1)
Prediction consistency. CPRC utilizes the prediction of raw image as soft label
to distill useful supervision for the generated sentence, rather than employing
the traditional pseudo labeling; 2) Relation consistency. CPRC develops a novel
relation consistency between augmented images and corresponding generated
sentences to retain the important relational knowledge. In result, CPRC
supervises the generated sentence from both the informativeness and
representativeness perspectives, and can reasonably use the undescribed images
to learn a more effective generator under the semi-supervised scenario.",2021-10-22
SciCap: Generating Captions for Scientific Figures,2021-10-22 07:10:41+00:00,http://arxiv.org/abs/2110.11624v2,"Ting-Yao Hsu, C. Lee Giles, Ting-Hao 'Kenneth' Huang","cs.CL, cs.AI, cs.CV",image2text,"Researchers use figures to communicate rich, complex information in
scientific papers. The captions of these figures are critical to conveying
effective messages. However, low-quality figure captions commonly occur in
scientific articles and may decrease understanding. In this paper, we propose
an end-to-end neural framework to automatically generate informative,
high-quality captions for scientific figures. To this end, we introduce SCICAP,
a large-scale figure-caption dataset based on computer science arXiv papers
published between 2010 and 2020. After pre-processing - including figure-type
classification, sub-figure identification, text normalization, and caption text
selection - SCICAP contained more than two million figures extracted from over
290,000 papers. We then established baseline models that caption graph plots,
the dominant (19.2%) figure type. The experimental results showed both
opportunities and steep challenges of generating captions for scientific
figures.",2021-10-22
SciXGen: A Scientific Paper Dataset for Context-Aware Text Generation,2021-10-20 20:37:11+00:00,http://arxiv.org/abs/2110.10774v1,"Hong Chen, Hiroya Takamura, Hideki Nakayama",cs.CL,image2text,"Generating texts in scientific papers requires not only capturing the content
contained within the given input but also frequently acquiring the external
information called \textit{context}. We push forward the scientific text
generation by proposing a new task, namely \textbf{context-aware text
generation} in the scientific domain, aiming at exploiting the contributions of
context in generated texts. To this end, we present a novel challenging
large-scale \textbf{Sci}entific Paper Dataset for Conte\textbf{X}t-Aware Text
\textbf{Gen}eration (SciXGen), consisting of well-annotated 205,304 papers with
full references to widely-used objects (e.g., tables, figures, algorithms) in a
paper. We comprehensively benchmark, using state-of-the-arts, the efficacy of
our newly constructed SciXGen dataset in generating description and paragraph.
Our dataset and benchmarks will be made publicly available to hopefully
facilitate the scientific text generation research.",2021-10-20
"A Picture is Worth a Thousand Words: A Unified System for Diverse
  Captions and Rich Images Generation",2021-10-19 06:10:42+00:00,http://arxiv.org/abs/2110.09756v1,"Yupan Huang, Bei Liu, Jianlong Fu, Yutong Lu","cs.CV, cs.CL, cs.MM",image2text,"A creative image-and-text generative AI system mimics humans' extraordinary
abilities to provide users with diverse and comprehensive caption suggestions,
as well as rich image creations. In this work, we demonstrate such an AI
creation system to produce both diverse captions and rich images. When users
imagine an image and associate it with multiple captions, our system paints a
rich image to reflect all captions faithfully. Likewise, when users upload an
image, our system depicts it with multiple diverse captions. We propose a
unified multi-modal framework to achieve this goal. Specifically, our framework
jointly models image-and-text representations with a Transformer network, which
supports rich image creation by accepting multiple captions as input. We
consider the relations among input captions to encourage diversity in training
and adopt a non-autoregressive decoding strategy to enable real-time inference.
Based on these, our system supports both diverse captions and rich images
generations. Our code is available online.",2021-10-19
"Unifying Multimodal Transformer for Bi-directional Image and Text
  Generation",2021-10-19 06:01:24+00:00,http://arxiv.org/abs/2110.09753v1,"Yupan Huang, Hongwei Xue, Bei Liu, Yutong Lu","cs.CV, cs.CL, cs.MM",image2text,"We study the joint learning of image-to-text and text-to-image generations,
which are naturally bi-directional tasks. Typical existing works design two
separate task-specific models for each task, which impose expensive design
efforts. In this work, we propose a unified image-and-text generative framework
based on a single multimodal model to jointly study the bi-directional tasks.
We adopt Transformer as our unified architecture for its strong performance and
task-agnostic design. Specifically, we formulate both tasks as sequence
generation tasks, where we represent images and text as unified sequences of
tokens, and the Transformer learns multimodal interactions to generate
sequences. We further propose two-level granularity feature representations and
sequence-level training to improve the Transformer-based unified framework.
Experiments show that our approach significantly improves previous
Transformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for
text-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for
fine-tuned image-to-text generation on the MS-COCO dataset. Our code is
available online.",2021-10-19
Self-Annotated Training for Controllable Image Captioning,2021-10-16 02:10:23+00:00,http://arxiv.org/abs/2110.08446v1,"Zhangzi Zhu, Tianlei Wang, Hong Qu","cs.AI, cs.CV",image2text,"The Controllable Image Captioning (CIC) task aims to generate captions
conditioned on designated control signals. In this paper, we improve CIC from
two aspects: 1) Existing reinforcement training methods are not applicable to
structure-related CIC models due to the fact that the accuracy-based reward
focuses mainly on contents rather than semantic structures. The lack of
reinforcement training prevents the model from generating more accurate and
controllable sentences. To solve the problem above, we propose a novel
reinforcement training method for structure-related CIC models: Self-Annotated
Training (SAT), where a recursive sampling mechanism (RSM) is designed to force
the input control signal to match the actual output sentence. Extensive
experiments conducted on MSCOCO show that our SAT method improves C-Transformer
(XE) on CIDEr-D score from 118.6 to 130.1 in the length-control task and from
132.2 to 142.7 in the tense-control task, while maintaining more than 99$\%$
matching accuracy with the control signal. 2) We introduce a new control
signal: sentence quality. Equipped with it, CIC models are able to generate
captions of different quality levels as needed. Experiments show that without
additional information of ground truth captions, models controlled by the
highest level of sentence quality perform much better in accuracy than baseline
models.",2021-10-16
How Well Do You Know Your Audience? Reader-aware Question Generation,2021-10-16 02:10:16+00:00,http://arxiv.org/abs/2110.08445v1,"Ian Stewart, Rada Mihalcea","cs.CL, I.7",image2text,"When writing, a person may need to anticipate questions from their readers,
but different types of readers may ask very different types of questions. If
someone is writing for advice about a problem, what question will a domain
expert ask, and is this different from how a novice might react? In this paper,
we address the task of reader-aware question generation. We collect a new data
set of questions and posts from social media, augmented with background
information about the post readers. Based on predictive analysis and
descriptive differences, we find that different readers, such as experts and
novices, consistently ask different types of questions. We next develop several
text generation models that incorporate different types of reader background,
including discrete and continuous reader representations based on the readers'
prior behavior. We demonstrate that reader-aware models can perform on par or
slightly better than the text-only model in some cases, particularly in cases
where a post attracts very different questions from readers of different
groups. Our work has the potential to help writers anticipate the information
needs of different readers.",2021-10-16
CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation,2021-10-06 09:55:19+00:00,http://arxiv.org/abs/2110.02624v1,"Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero","cs.CV, cs.AI, 68T07, I.2.10",image2text,"While recent progress has been made in text-to-image generation,
text-to-shape generation remains a challenging problem due to the
unavailability of paired text and shape data at a large scale. We present a
simple yet effective method for zero-shot text-to-shape generation based on a
two-stage training process, which only depends on an unlabelled shape dataset
and a pre-trained image-text network such as CLIP. Our method not only
demonstrates promising zero-shot generalization, but also avoids expensive
inference time optimization and can generate multiple shapes for a given text.",2021-10-06
CIDEr-R: Robust Consensus-based Image Description Evaluation,2021-09-28 13:13:21+00:00,http://arxiv.org/abs/2109.13701v1,"Gabriel Oliveira dos Santos, Esther Luna Colombini, Sandra Avila","cs.CV, cs.CL",image2text,"This paper shows that CIDEr-D, a traditional evaluation metric for image
description, does not work properly on datasets where the number of words in
the sentence is significantly greater than those in the MS COCO Captions
dataset. We also show that CIDEr-D has performance hampered by the lack of
multiple reference sentences and high variance of sentence length. To bypass
this problem, we introduce CIDEr-R, which improves CIDEr-D, making it more
flexible in dealing with datasets with high sentence length variance. We
demonstrate that CIDEr-R is more accurate and closer to human judgment than
CIDEr-D; CIDEr-R is more robust regarding the number of available references.
Our results reveal that using Self-Critical Sequence Training to optimize
CIDEr-R generates descriptive captions. In contrast, when CIDEr-D is optimized,
the generated captions' length tends to be similar to the reference length.
However, the models also repeat several times the same word to increase the
sentence length.",2021-09-28
Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation,2021-09-25 00:06:23+00:00,http://arxiv.org/abs/2109.12242v1,"An Yan, Zexue He, Xing Lu, Jiang Du, Eric Chang, Amilcare Gentili, Julian McAuley, Chun-Nan Hsu",cs.CL,image2text,"Radiology report generation aims at generating descriptive text from
radiology images automatically, which may present an opportunity to improve
radiology reporting and interpretation. A typical setting consists of training
encoder-decoder models on image-report pairs with a cross entropy loss, which
struggles to generate informative sentences for clinical diagnoses since normal
findings dominate the datasets. To tackle this challenge and encourage more
clinically-accurate text outputs, we propose a novel weakly supervised
contrastive loss for medical report generation. Experimental results
demonstrate that our method benefits from contrasting target reports with
incorrect but semantically-close ones. It outperforms previous work on both
clinical correctness and text generation metrics for two public benchmarks.",2021-09-25
"An animated picture says at least a thousand words: Selecting Gif-based
  Replies in Multimodal Dialog",2021-09-24 21:48:27+00:00,http://arxiv.org/abs/2109.12212v1,"Xingyao Wang, David Jurgens","cs.CL, cs.CV, cs.CY",image2text,"Online conversations include more than just text. Increasingly, image-based
responses such as memes and animated gifs serve as culturally recognized and
often humorous responses in conversation. However, while NLP has broadened to
multimodal models, conversational dialog systems have largely focused only on
generating text replies. Here, we introduce a new dataset of 1.56M text-gif
conversation turns and introduce a new multimodal conversational model Pepe the
King Prawn for selecting gif-based replies. We demonstrate that our model
produces relevant and high-quality gif responses and, in a large randomized
control trial of multiple models replying to real users, we show that our model
replies with gifs that are significantly better received by the community.",2021-09-24
Style Control for Schema-Guided Natural Language Generation,2021-09-24 21:47:58+00:00,http://arxiv.org/abs/2109.12211v1,"Alicia Y. Tsai, Shereen Oraby, Vittorio Perera, Jiun-Yu Kao, Yuheng Du, Anjali Narayan-Chen, Tagyoung Chung, Dilek Hakkani-Tur",cs.CL,image2text,"Natural Language Generation (NLG) for task-oriented dialogue systems focuses
on communicating specific content accurately, fluently, and coherently. While
these attributes are crucial for a successful dialogue, it is also desirable to
simultaneously accomplish specific stylistic goals, such as response length,
point-of-view, descriptiveness, sentiment, formality, and empathy. In this
work, we focus on stylistic control and evaluation for schema-guided NLG, with
joint goals of achieving both semantic and stylistic control. We experiment in
detail with various controlled generation methods for large pretrained language
models: specifically, conditional training, guided fine-tuning, and guided
decoding. We discuss their advantages and limitations, and evaluate them with a
broad range of automatic and human evaluation metrics. Our results show that
while high style accuracy and semantic correctness are easier to achieve for
more lexically-defined styles with conditional training, stylistic control is
also achievable for more semantically complex styles using discriminator-based
guided decoding methods. The results also suggest that methods that are more
scalable (with less hyper-parameters tuning) and that disentangle content
generation and stylistic variations are more effective at achieving semantic
correctness and style accuracy.",2021-09-24
"TrOCR: Transformer-based Optical Character Recognition with Pre-trained
  Models",2021-09-21 16:01:56+00:00,http://arxiv.org/abs/2109.10282v3,"Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei","cs.CL, cs.CV",image2text,"Text recognition is a long-standing research problem for document
digitalization. Existing approaches for text recognition are usually built
based on CNN for image understanding and RNN for char-level text generation. In
addition, another language model is usually needed to improve the overall
accuracy as a post-processing step. In this paper, we propose an end-to-end
text recognition approach with pre-trained image Transformer and text
Transformer models, namely TrOCR, which leverages the Transformer architecture
for both image understanding and wordpiece-level text generation. The TrOCR
model is simple but effective, and can be pre-trained with large-scale
synthetic data and fine-tuned with human-labeled datasets. Experiments show
that the TrOCR model outperforms the current state-of-the-art models on both
printed and handwritten text recognition tasks. The code and models will be
publicly available at https://aka.ms/TrOCR.",2021-09-21
PluGeN: Multi-Label Conditional Generation From Pre-Trained Models,2021-09-18 21:02:24+00:00,http://arxiv.org/abs/2109.09011v1,"Maciej WoÅ‚czyk, Magdalena Proszewska, Åukasz Maziarka, Maciej ZiÄ™ba, Patryk Wielopolski, RafaÅ‚ Kurczab, Marek Åšmieja",cs.LG,image2text,"Modern generative models achieve excellent quality in a variety of tasks
including image or text generation and chemical molecule modeling. However,
existing methods often lack the essential ability to generate examples with
requested properties, such as the age of the person in the photo or the weight
of the generated molecule. Incorporating such additional conditioning factors
would require rebuilding the entire architecture and optimizing the parameters
from scratch. Moreover, it is difficult to disentangle selected attributes so
that to perform edits of only one attribute while leaving the others unchanged.
To overcome these limitations we propose PluGeN (Plugin Generative Network), a
simple yet effective generative technique that can be used as a plugin to
pre-trained generative models. The idea behind our approach is to transform the
entangled latent representation using a flow-based module into a
multi-dimensional space where the values of each attribute are modeled as an
independent one-dimensional distribution. In consequence, PluGeN can generate
new samples with desired attributes as well as manipulate labeled attributes of
existing examples. Due to the disentangling of the latent representation, we
are even able to generate samples with rare or unseen combinations of
attributes in the dataset, such as a young person with gray hair, men with
make-up, or women with beards. We combined PluGeN with GAN and VAE models and
applied it to conditional generation and manipulation of images and chemical
molecule modeling. Experiments demonstrate that PluGeN preserves the quality of
backbone models while adding the ability to control the values of labeled
attributes.",2021-09-18
"Label-Attention Transformer with Geometrically Coherent Objects for
  Image Captioning",2021-09-16 08:43:46+00:00,http://arxiv.org/abs/2109.07799v1,"Shikha Dubey, Farrukh Olimov, Muhammad Aasim Rafique, Joonmo Kim, Moongu Jeon","cs.CV, cs.AI",image2text,"Automatic transcription of scene understanding in images and videos is a step
towards artificial general intelligence. Image captioning is a nomenclature for
describing meaningful information in an image using computer vision techniques.
Automated image captioning techniques utilize encoder and decoder architecture,
where the encoder extracts features from an image and the decoder generates a
transcript. In this work, we investigate two unexplored ideas for image
captioning using transformers: First, we demonstrate the enforcement of using
objects' relevance in the surrounding environment. Second, learning an explicit
association between labels and language constructs. We propose label-attention
Transformer with geometrically coherent objects (LATGeO). The proposed
technique acquires a proposal of geometrically coherent objects using a deep
neural network (DNN) and generates captions by investigating their
relationships using a label-attention module. Object coherence is defined using
the localized ratio of the geometrical properties of the proposals. The
label-attention module associates the extracted objects classes to the
available dictionary using self-attention layers. The experimentation results
show that objects' relevance in surroundings and binding of their visual
feature with their geometrically localized ratios combined with its associated
labels help in defining meaningful captions. The proposed framework is tested
on the MSCOCO dataset, and a thorough evaluation resulting in overall better
quantitative scores pronounces its superiority.",2021-09-16
"UniMS: A Unified Framework for Multimodal Summarization with Knowledge
  Distillation",2021-09-13 09:36:04+00:00,http://arxiv.org/abs/2109.05812v1,"Zhengkun Zhang, Xiaojun Meng, Yasheng Wang, Xin Jiang, Qun Liu, Zhenglu Yang",cs.CL,image2text,"With the rapid increase of multimedia data, a large body of literature has
emerged to work on multimodal summarization, the majority of which target at
refining salient information from textual and visual modalities to output a
pictorial summary with the most relevant images. Existing methods mostly focus
on either extractive or abstractive summarization and rely on qualified image
captions to build image references. We are the first to propose a Unified
framework for Multimodal Summarization grounding on BART, UniMS, that
integrates extractive and abstractive objectives, as well as selecting the
image output. Specially, we adopt knowledge distillation from a vision-language
pretrained model to improve image selection, which avoids any requirement on
the existence and quality of image captions. Besides, we introduce a visual
guided decoder to better integrate textual and visual modalities in guiding
abstractive text generation. Results show that our best model achieves a new
state-of-the-art result on a large-scale benchmark dataset. The newly involved
extractive objective as well as the knowledge distillation technique are proven
to bring a noticeable improvement to the multimodal summarization task.",2021-09-13
COSMic: A Coherence-Aware Generation Metric for Image Descriptions,2021-09-11 13:43:36+00:00,http://arxiv.org/abs/2109.05281v1,"Mert Ä°nan, Piyush Sharma, Baber Khalid, Radu Soricut, Matthew Stone, Malihe Alikhani","cs.CL, cs.AI, cs.CV, cs.LG",image2text,"Developers of text generation models rely on automated evaluation metrics as
a stand-in for slow and expensive manual evaluations. However, image captioning
metrics have struggled to give accurate learned estimates of the semantic and
pragmatic success of output text. We address this weakness by introducing the
first discourse-aware learned generation metric for evaluating image
descriptions. Our approach is inspired by computational theories of discourse
for capturing information goals using coherence. We present a dataset of
image$\unicode{x2013}$description pairs annotated with coherence relations. We
then train a coherence-aware metric on a subset of the Conceptual Captions
dataset and measure its effectiveness$\unicode{x2014}$its ability to predict
human ratings of output captions$\unicode{x2014}$on a test set composed of
out-of-domain images. We demonstrate a higher Kendall Correlation Coefficient
for our proposed metric with the human judgments for the results of a number of
state-of-the-art coherence-aware caption generation models when compared to
several other metrics including recently proposed learned metrics such as
BLEURT and BERTScore.",2021-09-11
"Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense
  in Text Generation Models",2021-09-08 19:38:11+00:00,http://arxiv.org/abs/2109.03892v1,"Steven Y. Feng, Kevin Lu, Zhuofu Tao, Malihe Alikhani, Teruko Mitamura, Eduard Hovy, Varun Gangal","cs.CL, cs.AI, cs.LG",image2text,"We investigate the use of multimodal information contained in images as an
effective method for enhancing the commonsense of Transformer models for text
generation. We perform experiments using BART and T5 on concept-to-text
generation, specifically the task of generative commonsense reasoning, or
CommonGen. We call our approach VisCTG: Visually Grounded Concept-to-Text
Generation. VisCTG involves captioning images representing appropriate everyday
scenarios, and using these captions to enrich and steer the generation process.
Comprehensive evaluation and analysis demonstrate that VisCTG noticeably
improves model performance while successfully addressing several issues of the
baseline generations, including poor commonsense, fluency, and specificity.",2021-09-08
Sequence Level Contrastive Learning for Text Summarization,2021-09-08 08:00:36+00:00,http://arxiv.org/abs/2109.03481v1,"Shusheng Xu, Xingxing Zhang, Yi Wu, Furu Wei",cs.CL,image2text,"Contrastive learning models have achieved great success in unsupervised
visual representation learning, which maximize the similarities between feature
representations of different views of the same image, while minimize the
similarities between feature representations of views of different images. In
text summarization, the output summary is a shorter form of the input document
and they have similar meanings. In this paper, we propose a contrastive
learning model for supervised abstractive text summarization, where we view a
document, its gold summary and its model generated summaries as different views
of the same mean representation and maximize the similarities between them
during training. We improve over a strong sequence-to-sequence text generation
model (i.e., BART) on three different summarization datasets. Human evaluation
also shows that our model achieves better faithfulness ratings compared to its
counterpart without contrastive objectives.",2021-09-08
Multimodal Conditionality for Natural Language Generation,2021-09-02 22:06:07+00:00,http://arxiv.org/abs/2109.01229v1,"Michael Sollami, Aashish Jain","cs.CL, cs.LG",image2text,"Large scale pretrained language models have demonstrated state-of-the-art
performance in language understanding tasks. Their application has recently
expanded into multimodality learning, leading to improved representations
combining vision and language. However, progress in adapting language models
towards conditional Natural Language Generation (NLG) has been limited to a
single modality, generally text. We propose MAnTiS, Multimodal Adaptation for
Text Synthesis, a general approach for multimodal conditionality in
transformer-based NLG models. In this method, we pass inputs from each modality
through modality-specific encoders, project to textual token space, and finally
join to form a conditionality prefix. We fine-tune the pretrained language
model and encoders with the conditionality prefix guiding the generation. We
apply MAnTiS to the task of product description generation, conditioning a
network on both product images and titles to generate descriptive text. We
demonstrate that MAnTiS outperforms strong baseline approaches on standard NLG
scoring metrics. Furthermore, qualitative assessments demonstrate that MAnTiS
can generate human quality descriptions consistent with given multimodal
inputs.",2021-09-02
Goal-driven text descriptions for images,2021-08-28 05:10:38+00:00,http://arxiv.org/abs/2108.12575v1,Ruotian Luo,"cs.CV, cs.CL",image2text,"A big part of achieving Artificial General Intelligence(AGI) is to build a
machine that can see and listen like humans. Much work has focused on designing
models for image classification, video classification, object detection, pose
estimation, speech recognition, etc., and has achieved significant progress in
recent years thanks to deep learning. However, understanding the world is not
enough. An AI agent also needs to know how to talk, especially how to
communicate with a human. While perception (vision, for example) is more common
across animal species, the use of complicated language is unique to humans and
is one of the most important aspects of intelligence.
  In this thesis, we focus on generating textual output given visual input. In
Chapter 3, we focus on generating the referring expression, a text description
for an object in the image so that a receiver can infer which object is being
described. We use a comprehension machine to directly guide the generated
referring expressions to be more discriminative. In Chapter 4, we introduce a
method that encourages discriminability in image caption generation. We show
that more discriminative captioning models generate more descriptive captions.
In Chapter 5, we study how training objectives and sampling methods affect the
models' ability to generate diverse captions. We find that a popular captioning
training strategy will be detrimental to the diversity of generated captions.
In Chapter 6, we propose a model that can control the length of generated
captions. By changing the desired length, one can influence the style and
descriptiveness of the captions. Finally, in Chapter 7, we rank/generate
informative image tags according to their information utility. The proposed
method better matches what humans think are the most important tags for the
images.",2021-08-28
Automatic Text Evaluation through the Lens of Wasserstein Barycenters,2021-08-27 19:08:52+00:00,http://arxiv.org/abs/2108.12463v2,"Pierre Colombo, Guillaume Staerman, Chloe Clavel, Pablo Piantanida","cs.CL, cs.AI",image2text,"A new metric \texttt{BaryScore} to evaluate text generation based on deep
contextualized embeddings e.g., BERT, Roberta, ELMo) is introduced. This metric
is motivated by a new framework relying on optimal transport tools, i.e.,
Wasserstein distance and barycenter. By modelling the layer output of deep
contextualized embeddings as a probability distribution rather than by a vector
embedding; this framework provides a natural way to aggregate the different
outputs through the Wasserstein space topology. In addition, it provides
theoretical grounds to our metric and offers an alternative to available
solutions e.g., MoverScore and BertScore). Numerical evaluation is performed on
four different tasks: machine translation, summarization, data2text generation
and image captioning. Our results show that \texttt{BaryScore} outperforms
other BERT based metrics and exhibits more consistent behaviour in particular
for text summarization.",2021-08-27
CGEMs: A Metric Model for Automatic Code Generation using GPT-3,2021-08-23 13:28:57+00:00,http://arxiv.org/abs/2108.10168v1,"Aishwarya Narasimhan, Krishna Prasad Agara Venkatesha Rao, Veena M B",cs.AI,image2text,"Today, AI technology is showing its strengths in almost every industry and
walks of life. From text generation, text summarization, chatbots, NLP is being
used widely. One such paradigm is automatic code generation. An AI could be
generating anything; hence the output space is unconstrained. A self-driving
car is driven for 100 million miles to validate its safety, but tests cannot be
written to monitor and cover an unconstrained space. One of the solutions to
validate AI-generated content is to constrain the problem and convert it from
abstract to realistic, and this can be accomplished by either validating the
unconstrained algorithm using theoretical proofs or by using Monte-Carlo
simulation methods. In this case, we use the latter approach to test/validate a
statistically significant number of samples. This hypothesis of validating the
AI-generated code is the main motive of this work and to know if AI-generated
code is reliable, a metric model CGEMs is proposed. This is an extremely
challenging task as programs can have different logic with different naming
conventions, but the metrics must capture the structure and logic of the
program. This is similar to the importance grammar carries in AI-based text
generation, Q&A, translations, etc. The various metrics that are garnered in
this work to support the evaluation of generated code are as follows:
Compilation, NL description to logic conversion, number of edits needed, some
of the commonly used static-code metrics and NLP metrics. These metrics are
applied to 80 codes generated using OpenAI's GPT-3. Post which a Neural network
is designed for binary classification (acceptable/not acceptable quality of the
generated code). The inputs to this network are the values of the features
obtained from the metrics. The model achieves a classification accuracy of
76.92% and an F1 score of 55.56%. XAI is augmented for model interpretability.",2021-08-23
Group-based Distinctive Image Captioning with Memory Attention,2021-08-20 12:46:36+00:00,http://arxiv.org/abs/2108.09151v1,"Jiuniu Wang, Wenjia Xu, Qingzhong Wang, Antoni B. Chan","cs.CV, cs.CL, cs.LG",image2text,"Describing images using natural language is widely known as image captioning,
which has made consistent progress due to the development of computer vision
and natural language generation techniques. Though conventional captioning
models achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and
SPICE, the ability of captions to distinguish the target image from other
similar images is under-explored. To generate distinctive captions, a few
pioneers employ contrastive learning or re-weighted the ground-truth captions,
which focuses on one single input image. However, the relationships between
objects in a similar image group (e.g., items or properties within the same
album or fine-grained events) are neglected. In this paper, we improve the
distinctiveness of image captions using a Group-based Distinctive Captioning
Model (GdisCap), which compares each image with other images in one similar
group and highlights the uniqueness of each image. In particular, we propose a
group-based memory attention (GMA) module, which stores object features that
are unique among the image group (i.e., with low similarity to objects in other
images). These unique object features are highlighted when generating captions,
resulting in more distinctive captions. Furthermore, the distinctive words in
the ground-truth captions are selected to supervise the language decoder and
GMA. Finally, we propose a new evaluation metric, distinctive word rate
(DisWordRate) to measure the distinctiveness of captions. Quantitative results
indicate that the proposed method significantly improves the distinctiveness of
several baseline models, and achieves the state-of-the-art performance on both
accuracy and distinctiveness. Results of a user study agree with the
quantitative evaluation and demonstrate the rationality of the new metric
DisWordRate.",2021-08-20
CIGLI: Conditional Image Generation from Language & Image,2021-08-20 00:58:42+00:00,http://arxiv.org/abs/2108.08955v1,"Xiaopeng Lu, Lynnette Ng, Jared Fernandez, Hao Zhu","cs.CV, cs.CL",image2text,"Multi-modal generation has been widely explored in recent years. Current
research directions involve generating text based on an image or vice versa. In
this paper, we propose a new task called CIGLI: Conditional Image Generation
from Language and Image. Instead of generating an image based on text as in
text-image generation, this task requires the generation of an image from a
textual description and an image prompt. We designed a new dataset to ensure
that the text description describes information from both images, and that
solely analyzing the description is insufficient to generate an image. We then
propose a novel language-image fusion model which improves the performance over
two established baseline methods, as evaluated by quantitative (automatic) and
qualitative (human) evaluations. The code and dataset is available at
https://github.com/vincentlux/CIGLI.",2021-08-20
"Table Caption Generation in Scholarly Documents Leveraging Pre-trained
  Language Models",2021-08-18 12:25:43+00:00,http://arxiv.org/abs/2108.08111v1,"Junjie H. Xu, Kohei Shinden, Makoto P. Kato",cs.CL,image2text,"This paper addresses the problem of generating table captions for scholarly
documents, which often require additional information outside the table. To
this end, we propose a method of retrieving relevant sentences from the paper
body, and feeding the table content as well as the retrieved sentences into
pre-trained language models (e.g. T5 and GPT-2) for generating table captions.
The contributions of this paper are: (1) discussion on the challenges in table
captioning for scholarly documents; (2) development of a dataset DocBank-TB,
which is publicly available; and (3) comparison of caption generation methods
for scholarly documents with different strategies to retrieve relevant
sentences from the paper body. Our experimental results showed that T5 is the
better generation model for this task, as it outperformed GPT-2 in BLEU and
METEOR implying that the generated text are clearer and more precise. Moreover,
inputting relevant sentences matching the row header or whole table is
effective.",2021-08-18
"Reusable Templates and Guides For Documenting Datasets and Models for
  Natural Language Processing and Generation: A Case Study of the HuggingFace
  and GEM Data and Model Cards",2021-08-16 23:15:09+00:00,http://arxiv.org/abs/2108.07374v1,"Angelina McMillan-Major, Salomey Osei, Juan Diego Rodriguez, Pawan Sasanka Ammanamanchi, Sebastian Gehrmann, Yacine Jernite","cs.DB, cs.CL",image2text,"Developing documentation guidelines and easy-to-use templates for datasets
and models is a challenging task, especially given the variety of backgrounds,
skills, and incentives of the people involved in the building of natural
language processing (NLP) tools. Nevertheless, the adoption of standard
documentation practices across the field of NLP promotes more accessible and
detailed descriptions of NLP datasets and models, while supporting researchers
and developers in reflecting on their work. To help with the standardization of
documentation, we present two case studies of efforts that aim to develop
reusable documentation templates -- the HuggingFace data card, a general
purpose card for datasets in NLP, and the GEM benchmark data and model cards
with a focus on natural language generation. We describe our process for
developing these templates, including the identification of relevant
stakeholder groups, the definition of a set of guiding principles, the use of
existing templates as our foundation, and iterative revisions based on
feedback.",2021-08-16
AutoChart: A Dataset for Chart-to-Text Generation Task,2021-08-16 05:01:46+00:00,http://arxiv.org/abs/2108.06897v1,"Jiawen Zhu, Jinye Ran, Roy Ka-wei Lee, Kenny Choo, Zhi Li","cs.CL, cs.AI, cs.MM",image2text,"The analytical description of charts is an exciting and important research
area with many applications in academia and industry. Yet, this challenging
task has received limited attention from the computational linguistics research
community. This paper proposes \textsf{AutoChart}, a large dataset for the
analytical description of charts, which aims to encourage more research into
this important area. Specifically, we offer a novel framework that generates
the charts and their analytical description automatically. We conducted
extensive human and machine evaluations on the generated charts and
descriptions and demonstrate that the generated texts are informative,
coherent, and relevant to the corresponding charts.",2021-08-16
"HiTab: A Hierarchical Table Dataset for Question Answering and Natural
  Language Generation",2021-08-15 10:14:21+00:00,http://arxiv.org/abs/2108.06712v1,"Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou, Dongmei Zhang","cs.CL, cs.IR",image2text,"Tables are often created with hierarchies, but existing works on table
reasoning mainly focus on flat tables and neglect hierarchical tables.
Hierarchical tables challenge existing methods by hierarchical indexing, as
well as implicit relationships of calculation and semantics. This work presents
HiTab, a free and open dataset for the research community to study question
answering (QA) and natural language generation (NLG) over hierarchical tables.
HiTab is a cross-domain dataset constructed from a wealth of statistical
reports and Wikipedia pages, and has unique characteristics: (1) nearly all
tables are hierarchical, and (2) both target sentences for NLG and questions
for QA are revised from high-quality descriptions in statistical reports that
are meaningful and diverse. (3) HiTab provides fine-grained annotations on both
entity and quantity alignment. Targeting hierarchical structure, we devise a
novel hierarchy-aware logical form for symbolic reasoning over tables, which
shows high effectiveness. Then given annotations of entity and quantity
alignment, we propose partially supervised training, which helps models to
largely reduce spurious predictions in the QA task. In the NLG task, we find
that entity and quantity alignment also helps NLG models to generate better
results in a conditional generation setting. Experiment results of
state-of-the-art baselines suggest that this dataset presents a strong
challenge and a valuable benchmark for future research.",2021-08-15
Generating Diverse Descriptions from Semantic Graphs,2021-08-12 11:00:09+00:00,http://arxiv.org/abs/2108.05659v2,"Jiuzhou Han, Daniel Beck, Trevor Cohn",cs.CL,image2text,"Text generation from semantic graphs is traditionally performed with
deterministic methods, which generate a unique description given an input
graph. However, the generation problem admits a range of acceptable textual
outputs, exhibiting lexical, syntactic and semantic variation. To address this
disconnect, we present two main contributions. First, we propose a stochastic
graph-to-text model, incorporating a latent variable in an encoder-decoder
model, and its use in an ensemble. Second, to assess the diversity of the
generated sentences, we propose a new automatic evaluation metric which jointly
evaluates output diversity and quality in a multi-reference setting. We
evaluate the models on WebNLG datasets in English and Russian, and show an
ensemble of stochastic models produces diverse sets of generated sentences,
while retaining similar quality to state-of-the-art models.",2021-08-12
ICECAP: Information Concentrated Entity-aware Image Captioning,2021-08-04 13:27:51+00:00,http://arxiv.org/abs/2108.02050v1,"Anwen Hu, Shizhe Chen, Qin Jin","cs.CV, cs.MM",image2text,"Most current image captioning systems focus on describing general image
content, and lack background knowledge to deeply understand the image, such as
exact named entities or concrete events. In this work, we focus on the
entity-aware news image captioning task which aims to generate informative
captions by leveraging the associated news articles to provide background
knowledge about the target image. However, due to the length of news articles,
previous works only employ news articles at the coarse article or sentence
level, which are not fine-grained enough to refine relevant events and choose
named entities accurately. To overcome these limitations, we propose an
Information Concentrated Entity-aware news image CAPtioning (ICECAP) model,
which progressively concentrates on relevant textual information within the
corresponding news article from the sentence level to the word level. Our model
first creates coarse concentration on relevant sentences using a cross-modality
retrieval model and then generates captions by further concentrating on
relevant words within the sentences. Extensive experiments on both BreakingNews
and GoodNews datasets demonstrate the effectiveness of our proposed method,
which outperforms other state-of-the-arts. The code of ICECAP is publicly
available at https://github.com/HAWLYQ/ICECAP.",2021-08-04
"Exploiting BERT For Multimodal Target Sentiment Classification Through
  Input Space Translation",2021-08-03 18:02:38+00:00,http://arxiv.org/abs/2108.01682v2,"Zaid Khan, Yun Fu","cs.CL, cs.CV",image2text,"Multimodal target/aspect sentiment classification combines multimodal
sentiment analysis and aspect/target sentiment classification. The goal of the
task is to combine vision and language to understand the sentiment towards a
target entity in a sentence. Twitter is an ideal setting for the task because
it is inherently multimodal, highly emotional, and affects real world events.
However, multimodal tweets are short and accompanied by complex, possibly
irrelevant images. We introduce a two-stream model that translates images in
input space using an object-aware transformer followed by a single-pass
non-autoregressive text generation approach. We then leverage the translation
to construct an auxiliary sentence that provides multimodal information to a
language model. Our approach increases the amount of text available to the
language model and distills the object-level information in complex images. We
achieve state-of-the-art performance on two multimodal Twitter datasets without
modifying the internals of the language model to accept multimodal data,
demonstrating the effectiveness of our translation. In addition, we explain a
failure mode of a popular approach for aspect sentiment analysis when applied
to tweets. Our code is available at
\textcolor{blue}{\url{https://github.com/codezakh/exploiting-BERT-thru-translation}}.",2021-08-03
Logic-Consistency Text Generation from Semantic Parses,2021-08-02 01:12:18+00:00,http://arxiv.org/abs/2108.00577v1,"Chang Shu, Yusen Zhang, Xiangyu Dong, Peng Shi, Tao Yu, Rui Zhang",cs.CL,image2text,"Text generation from semantic parses is to generate textual descriptions for
formal representation inputs such as logic forms and SQL queries. This is
challenging due to two reasons: (1) the complex and intensive inner logic with
the data scarcity constraint, (2) the lack of automatic evaluation metrics for
logic consistency. To address these two challenges, this paper first proposes
SNOWBALL, a framework for logic consistent text generation from semantic parses
that employs an iterative training procedure by recursively augmenting the
training set with quality control. Second, we propose a novel automatic metric,
BLEC, for evaluating the logical consistency between the semantic parses and
generated texts. The experimental results on two benchmark datasets, Logic2Text
and Spider, demonstrate the SNOWBALL framework enhances the logic consistency
on both BLEC and human evaluation. Furthermore, our statistical analysis
reveals that BLEC is more logically consistent with human evaluation than
general-purpose automatic metrics including BLEU, ROUGE and, BLEURT. Our data
and code are available at https://github.com/Ciaranshu/relogic.",2021-08-02
"Robust Learning for Text Classification with Multi-source Noise
  Simulation and Hard Example Mining",2021-07-15 04:39:22+00:00,http://arxiv.org/abs/2107.07113v1,"Guowei Xu, Wenbiao Ding, Weiping Fu, Zhongqin Wu, Zitao Liu","cs.CL, cs.AI",image2text,"Many real-world applications involve the use of Optical Character Recognition
(OCR) engines to transform handwritten images into transcripts on which
downstream Natural Language Processing (NLP) models are applied. In this
process, OCR engines may introduce errors and inputs to downstream NLP models
become noisy. Despite that pre-trained models achieve state-of-the-art
performance in many NLP benchmarks, we prove that they are not robust to noisy
texts generated by real OCR engines. This greatly limits the application of NLP
models in real-world scenarios. In order to improve model performance on noisy
OCR transcripts, it is natural to train the NLP model on labelled noisy texts.
However, in most cases there are only labelled clean texts. Since there is no
handwritten pictures corresponding to the text, it is impossible to directly
use the recognition model to obtain noisy labelled data. Human resources can be
employed to copy texts and take pictures, but it is extremely expensive
considering the size of data for model training. Consequently, we are
interested in making NLP models intrinsically robust to OCR errors in a low
resource manner. We propose a novel robust training framework which 1) employs
simple but effective methods to directly simulate natural OCR noises from clean
texts and 2) iteratively mines the hard examples from a large number of
simulated samples for optimal performance. 3) To make our model learn
noise-invariant representations, a stability loss is employed. Experiments on
three real-world datasets show that the proposed framework boosts the
robustness of pre-trained models by a large margin. We believe that this work
can greatly promote the application of NLP models in actual scenarios, although
the algorithm we use is simple and straightforward. We make our codes and three
datasets publicly
available\footnote{https://github.com/tal-ai/Robust-learning-MSSHEM}.",2021-07-15
From Show to Tell: A Survey on Image Captioning,2021-07-14 18:00:54+00:00,http://arxiv.org/abs/2107.06912v1,"Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli, Giuseppe Fiameni, Rita Cucchiara","cs.CV, cs.CL",image2text,"Connecting Vision and Language plays an essential role in Generative
Intelligence. For this reason, in the last few years, a large research effort
has been devoted to image captioning, i.e. the task of describing images with
syntactically and semantically meaningful sentences. Starting from 2015 the
task has generally been addressed with pipelines composed of a visual encoding
step and a language model for text generation. During these years, both
components have evolved considerably through the exploitation of object
regions, attributes, and relationships and the introduction of multi-modal
connections, fully-attentive approaches, and BERT-like early-fusion strategies.
However, regardless of the impressive results obtained, research in image
captioning has not reached a conclusive answer yet. This work aims at providing
a comprehensive overview and categorization of image captioning approaches,
from visual encoding and text generation to training strategies, used datasets,
and evaluation metrics. In this respect, we quantitatively compare many
relevant state-of-the-art approaches to identify the most impactful technical
innovations in image captioning architectures and training strategies.
Moreover, many variants of the problem and its open challenges are analyzed and
discussed. The final goal of this work is to serve as a tool for understanding
the existing state-of-the-art and highlighting the future directions for an
area of research where Computer Vision and Natural Language Processing can find
an optimal synergy.",2021-07-14
"Between Flexibility and Consistency: Joint Generation of Captions and
  Subtitles",2021-07-13 17:06:04+00:00,http://arxiv.org/abs/2107.06246v1,"Alina Karakanta, Marco Gaido, Matteo Negri, Marco Turchi",cs.CL,image2text,"Speech translation (ST) has lately received growing interest for the
generation of subtitles without the need for an intermediate source language
transcription and timing (i.e. captions). However, the joint generation of
source captions and target subtitles does not only bring potential output
quality advantages when the two decoding processes inform each other, but it is
also often required in multilingual scenarios. In this work, we focus on ST
models which generate consistent captions-subtitles in terms of structure and
lexical content. We further introduce new metrics for evaluating subtitling
consistency. Our findings show that joint decoding leads to increased
performance and consistency between the generated captions and subtitles while
still allowing for sufficient flexibility to produce subtitles conforming to
language-specific needs and norms.",2021-07-13
"Tortured phrases: A dubious writing style emerging in science. Evidence
  of critical issues affecting established journals",2021-07-12 20:47:08+00:00,http://arxiv.org/abs/2107.06751v1,"Guillaume Cabanac, Cyril LabbÃ©, Alexander Magazinov","cs.DL, cs.CL, cs.CY, cs.IR",image2text,"Probabilistic text generators have been used to produce fake scientific
papers for more than a decade. Such nonsensical papers are easily detected by
both human and machine. Now more complex AI-powered generation techniques
produce texts indistinguishable from that of humans and the generation of
scientific texts from a few keywords has been documented. Our study introduces
the concept of tortured phrases: unexpected weird phrases in lieu of
established ones, such as 'counterfeit consciousness' instead of 'artificial
intelligence.' We combed the literature for tortured phrases and study one
reputable journal where these concentrated en masse. Hypothesising the use of
advanced language models we ran a detector on the abstracts of recent articles
of this journal and on several control sets. The pairwise comparisons reveal a
concentration of abstracts flagged as 'synthetic' in the journal. We also
highlight irregularities in its operation, such as abrupt changes in editorial
timelines. We substantiate our call for investigation by analysing several
individual dubious articles, stressing questionable features: tortured writing
style, citation of non-existent literature, and unacknowledged image reuse.
Surprisingly, some websites offer to rewrite texts for free, generating
gobbledegook full of tortured phrases. We believe some authors used rewritten
texts to pad their manuscripts. We wish to raise the awareness on publications
containing such questionable AI-generated or rewritten texts that passed (poor)
peer review. Deception with synthetic texts threatens the integrity of the
scientific literature.",2021-07-12
Structured Denoising Diffusion Models in Discrete State-Spaces,2021-07-07 04:11:00+00:00,http://arxiv.org/abs/2107.03006v2,"Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, Rianne van den Berg","cs.LG, cs.AI, cs.CL, cs.CV",image2text,"Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown
impressive results on image and waveform generation in continuous state spaces.
Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs),
diffusion-like generative models for discrete data that generalize the
multinomial diffusion model of Hoogeboom et al. 2021, by going beyond
corruption processes with uniform transition probabilities. This includes
corruption with transition matrices that mimic Gaussian kernels in continuous
space, matrices based on nearest neighbors in embedding space, and matrices
that introduce absorbing states. The third allows us to draw a connection
between diffusion models and autoregressive and mask-based generative models.
We show that the choice of transition matrix is an important design decision
that leads to improved results in image and text domains. We also introduce a
new loss function that combines the variational lower bound with an auxiliary
cross entropy loss. For text, this model class achieves strong results on
character-level text generation while scaling to large vocabularies on LM1B. On
the image dataset CIFAR-10, our models approach the sample quality and exceed
the log-likelihood of the continuous-space DDPM model.",2021-07-07
"Don't Take It Literally: An Edit-Invariant Sequence Loss for Text
  Generation",2021-06-29 03:59:21+00:00,http://arxiv.org/abs/2106.15078v1,"Guangyi Liu, Zichao Yang, Tianhua Tao, Xiaodan Liang, Zhen Li, Bowen Zhou, Shuguang Cui, Zhiting Hu","cs.CL, cs.AI",image2text,"Neural text generation models are typically trained by maximizing
log-likelihood with the sequence cross entropy loss, which encourages an exact
token-by-token match between a target sequence with a generated sequence. Such
training objective is sub-optimal when the target sequence not perfect, e.g.,
when the target sequence is corrupted with noises, or when only weak sequence
supervision is available. To address this challenge, we propose a novel
Edit-Invariant Sequence Loss (EISL), which computes the matching loss of a
target n-gram with all n-grams in the generated sequence. EISL draws
inspirations from convolutional networks (ConvNets) which are shift-invariant
to images, hence is robust to the shift of n-grams to tolerate edits in the
target sequences. Moreover, the computation of EISL is essentially a
convolution operation with target n-grams as kernels, which is easy to
implement with existing libraries. To demonstrate the effectiveness of EISL, we
conduct experiments on three tasks: machine translation with noisy target
sequences, unsupervised text style transfer, and non-autoregressive machine
translation. Experimental results show our method significantly outperforms
cross entropy loss on these three tasks.",2021-06-29
"UMIC: An Unreferenced Metric for Image Captioning via Contrastive
  Learning",2021-06-26 13:27:14+00:00,http://arxiv.org/abs/2106.14019v1,"Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Trung Bui, Kyomin Jung","cs.CL, cs.CV",image2text,"Despite the success of various text generation metrics such as BERTScore, it
is still difficult to evaluate the image captions without enough reference
captions due to the diversity of the descriptions. In this paper, we introduce
a new metric UMIC, an Unreferenced Metric for Image Captioning which does not
require reference captions to evaluate image captions. Based on
Vision-and-Language BERT, we train UMIC to discriminate negative captions via
contrastive learning. Also, we observe critical problems of the previous
benchmark dataset (i.e., human annotations) on image captioning metric, and
introduce a new collection of human annotations on the generated captions. We
validate UMIC on four datasets, including our new dataset, and show that UMIC
has a higher correlation than all previous metrics that require multiple
references. We release the benchmark dataset and pre-trained models to compute
the UMIC.",2021-06-26
"ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural
  Language Generation",2021-06-10 17:59:52+00:00,http://arxiv.org/abs/2106.05970v1,"Wanrong Zhu, Xin Eric Wang, An Yan, Miguel Eckstein, William Yang Wang","cs.CL, cs.AI, cs.CV",image2text,"Automatic evaluations for natural language generation (NLG) conventionally
rely on token-level or embedding-level comparisons with the text references.
This is different from human language processing, for which visual imaginations
often improve comprehension. In this work, we propose ImaginE, an
imagination-based automatic evaluation metric for natural language generation.
With the help of CLIP and DALL-E, two cross-modal models pre-trained on
large-scale image-text pairs, we automatically generate an image as the
embodied imagination for the text snippet and compute the imagination
similarity using contextual embeddings. Experiments spanning several text
generation tasks demonstrate that adding imagination with our ImaginE displays
great potential in introducing multi-modal information into NLG evaluation, and
improves existing automatic metrics' correlations with human similarity
judgments in many circumstances.",2021-06-10
"Sketch and Refine: Towards Faithful and Informative Table-to-Text
  Generation",2021-05-31 08:18:13+00:00,http://arxiv.org/abs/2105.14778v1,"Peng Wang, Junyang Lin, An Yang, Chang Zhou, Yichang Zhang, Jingren Zhou, Hongxia Yang",cs.CL,image2text,"Table-to-text generation refers to generating a descriptive text from a
key-value table. Traditional autoregressive methods, though can generate text
with high fluency, suffer from low coverage and poor faithfulness problems. To
mitigate these problems, we propose a novel Skeleton-based two-stage method
that combines both Autoregressive and Non-Autoregressive generations (SANA).
Our approach includes: (1) skeleton generation with an autoregressive pointer
network to select key tokens from the source table; (2) edit-based
non-autoregressive generation model to produce texts via iterative insertion
and deletion operations. By integrating hard constraints from the skeleton, the
non-autoregressive model improves the generation's coverage over the source
table and thus enhances its faithfulness. We conduct automatic and human
evaluations on both WikiPerson and WikiBio datasets. Experimental results
demonstrate that our method outperforms the previous state-of-the-art methods
in both automatic and human evaluation, especially on coverage and
faithfulness. In particular, we achieve PARENT-T recall of 99.47 in WikiPerson,
improving over the existing best results by more than 10 points.",2021-05-31
"Dependent Multi-Task Learning with Causal Intervention for Image
  Captioning",2021-05-18 14:57:33+00:00,http://arxiv.org/abs/2105.08573v1,"Wenqing Chen, Jidong Tian, Caoyun Fan, Hao He, Yaohui Jin","cs.LG, cs.CV, eess.IV",image2text,"Recent work for image captioning mainly followed an extract-then-generate
paradigm, pre-extracting a sequence of object-based features and then
formulating image captioning as a single sequence-to-sequence task. Although
promising, we observed two problems in generated captions: 1) content
inconsistency where models would generate contradicting facts; 2) not
informative enough where models would miss parts of important information. From
a causal perspective, the reason is that models have captured spurious
statistical correlations between visual features and certain expressions (e.g.,
visual features of ""long hair"" and ""woman""). In this paper, we propose a
dependent multi-task learning framework with the causal intervention (DMTCI).
Firstly, we involve an intermediate task, bag-of-categories generation, before
the final task, image captioning. The intermediate task would help the model
better understand the visual features and thus alleviate the content
inconsistency problem. Secondly, we apply Pearl's do-calculus on the model,
cutting off the link between the visual features and possible confounders and
thus letting models focus on the causal visual features. Specifically, the
high-frequency concept set is considered as the proxy confounders where the
real confounders are inferred in the continuous space. Finally, we use a
multi-agent reinforcement learning (MARL) strategy to enable end-to-end
training and reduce the inter-task error accumulations. The extensive
experiments show that our model outperforms the baseline models and achieves
competitive performance with state-of-the-art models.",2021-05-18
Multi-Modal Image Captioning for the Visually Impaired,2021-05-17 18:35:24+00:00,http://arxiv.org/abs/2105.08106v1,"Hiba Ahsan, Nikita Bhalla, Daivat Bhatt, Kaivankumar Shah",cs.CL,image2text,"One of the ways blind people understand their surroundings is by clicking
images and relying on descriptions generated by image captioning systems.
Current work on captioning images for the visually impaired do not use the
textual data present in the image when generating captions. This problem is
critical as many visual scenes contain text. Moreover, up to 21% of the
questions asked by blind people about the images they click pertain to the text
present in them. In this work, we propose altering AoANet, a state-of-the-art
image captioning model, to leverage the text detected in the image as an input
feature. In addition, we use a pointer-generator mechanism to copy the detected
text to the caption when tokens need to be reproduced accurately. Our model
outperforms AoANet on the benchmark dataset VizWiz, giving a 35% and 16.2%
performance improvement on CIDEr and SPICE scores, respectively.",2021-05-17
Passage Retrieval for Outside-Knowledge Visual Question Answering,2021-05-09 13:27:22+00:00,http://arxiv.org/abs/2105.03938v1,"Chen Qu, Hamed Zamani, Liu Yang, W. Bruce Croft, Erik Learned-Miller",cs.IR,image2text,"In this work, we address multi-modal information needs that contain text
questions and images by focusing on passage retrieval for outside-knowledge
visual question answering. This task requires access to outside knowledge,
which in our case we define to be a large unstructured passage collection. We
first conduct sparse retrieval with BM25 and study expanding the question with
object names and image captions. We verify that visual clues play an important
role and captions tend to be more informative than object names in sparse
retrieval. We then construct a dual-encoder dense retriever, with the query
encoder being LXMERT, a multi-modal pre-trained transformer. We further show
that dense retrieval significantly outperforms sparse retrieval that uses
object expansion. Moreover, dense retrieval matches the performance of sparse
retrieval that leverages human-generated captions.",2021-05-09
"e-ViL: A Dataset and Benchmark for Natural Language Explanations in
  Vision-Language Tasks",2021-05-08 18:46:33+00:00,http://arxiv.org/abs/2105.03761v1,"Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep Akata, Thomas Lukasiewicz","cs.CV, cs.CL, cs.LG",image2text,"Recently, an increasing number of works have introduced models capable of
generating natural language explanations (NLEs) for their predictions on
vision-language (VL) tasks. Such models are appealing because they can provide
human-friendly and comprehensive explanations. However, there is still a lack
of unified evaluation approaches for the explanations generated by these
models. Moreover, there are currently only few datasets of NLEs for VL tasks.
In this work, we introduce e-ViL, a benchmark for explainable vision-language
tasks that establishes a unified evaluation framework and provides the first
comprehensive comparison of existing approaches that generate NLEs for VL
tasks. e-ViL spans four models and three datasets. Both automatic metrics and
human evaluation are used to assess model-generated explanations. We also
introduce e-SNLI-VE, the largest existing VL dataset with NLEs (over 430k
instances). Finally, we propose a new model that combines UNITER, which learns
joint embeddings of images and text, and GPT-2, a pre-trained language model
that is well-suited for text generation. It surpasses the previous
state-of-the-art by a large margin across all datasets.",2021-05-08
"Removing Word-Level Spurious Alignment between Images and
  Pseudo-Captions in Unsupervised Image Captioning",2021-04-28 16:36:52+00:00,http://arxiv.org/abs/2104.13872v1,"Ukyo Honda, Yoshitaka Ushiku, Atsushi Hashimoto, Taro Watanabe, Yuji Matsumoto","cs.CL, cs.CV",image2text,"Unsupervised image captioning is a challenging task that aims at generating
captions without the supervision of image-sentence pairs, but only with images
and sentences drawn from different sources and object labels detected from the
images. In previous work, pseudo-captions, i.e., sentences that contain the
detected object labels, were assigned to a given image. The focus of the
previous work was on the alignment of input images and pseudo-captions at the
sentence level. However, pseudo-captions contain many words that are irrelevant
to a given image. In this work, we investigate the effect of removing
mismatched words from image-sentence alignment to determine how they make this
task difficult. We propose a simple gating mechanism that is trained to align
image features with only the most reliable words in pseudo-captions: the
detected object labels. The experimental results show that our proposed method
outperforms the previous methods without introducing complex sentence-level
learning objectives. Combined with the sentence-level alignment method of
previous work, our method further improves its performance. These results
confirm the importance of careful alignment in word-level details.",2021-04-28
MusCaps: Generating Captions for Music Audio,2021-04-24 16:34:47+00:00,http://arxiv.org/abs/2104.11984v1,"Ilaria Manco, Emmanouil Benetos, Elio Quinton, Gyorgy Fazekas","cs.SD, cs.CL, cs.LG, eess.AS",image2text,"Content-based music information retrieval has seen rapid progress with the
adoption of deep learning. Current approaches to high-level music description
typically make use of classification models, such as in auto-tagging or genre
and mood classification. In this work, we propose to address music description
via audio captioning, defined as the task of generating a natural language
description of music audio content in a human-like manner. To this end, we
present the first music audio captioning model, MusCaps, consisting of an
encoder-decoder with temporal attention. Our method combines convolutional and
recurrent neural network architectures to jointly process audio-text inputs
through a multimodal encoder and leverages pre-training on audio data to obtain
representations that effectively capture and summarise musical features in the
input. Evaluation of the generated captions through automatic metrics shows
that our method outperforms a baseline designed for non-music audio captioning.
Through an ablation study, we unveil that this performance boost can be mainly
attributed to pre-training of the audio encoder, while other design choices -
modality fusion, decoding strategy and the use of attention - contribute only
marginally. Our model represents a shift away from classification-based music
description and combines tasks requiring both auditory and linguistic
understanding to bridge the semantic gap in music information retrieval.",2021-04-24
"Imaginative Walks: Generative Random Walk Deviation Loss for Improved
  Unseen Learning Representation",2021-04-20 04:34:28+00:00,http://arxiv.org/abs/2104.09757v1,"Mohamed Elhoseiny, Divyansh Jha, Kai Yi, Ivan Skorokhodov","cs.CV, cs.AI",image2text,"We propose a novel loss for generative models, dubbed as GRaWD (Generative
Random Walk Deviation), to improve learning representations of unexplored
visual spaces. Quality learning representation of unseen classes (or styles) is
crucial to facilitate novel image generation and better generative
understanding of unseen visual classes (a.k.a. Zero-Shot Learning, ZSL). By
generating representations of unseen classes from their semantic descriptions,
such as attributes or text, Generative ZSL aims at identifying unseen
categories discriminatively from seen ones. We define GRaWD by constructing a
dynamic graph, including the seen class/style centers and generated samples in
the current mini-batch. Our loss starts a random walk probability from each
center through visual generations produced from hallucinated unseen classes. As
a deviation signal, we encourage the random walk to eventually land after t
steps in a feature representation that is hard to classify to any of the seen
classes. We show that our loss can improve unseen class representation quality
on four text-based ZSL benchmarks on CUB and NABirds datasets and three
attribute-based ZSL benchmarks on AWA2, SUN, and aPY datasets. We also study
our loss's ability to produce meaningful novel visual art generations on
WikiArt dataset. Our experiments and human studies show that our loss can
improve StyleGAN1 and StyleGAN2 generation quality, creating novel art that is
significantly more preferred. Code will be made available.",2021-04-20
Learning to Reason for Text Generation from Scientific Tables,2021-04-16 18:01:36+00:00,http://arxiv.org/abs/2104.08296v1,"Nafise Sadat Moosavi, Andreas RÃ¼cklÃ©, Dan Roth, Iryna Gurevych",cs.CL,image2text,"In this paper, we introduce SciGen, a new challenge dataset for the task of
reasoning-aware data-to-text generation consisting of tables from scientific
articles and their corresponding descriptions. Describing scientific tables
goes beyond the surface realization of the table content and requires reasoning
over table values. The unique properties of SciGen are that (1) tables mostly
contain numerical values, and (2) the corresponding descriptions require
arithmetic reasoning. SciGen is therefore the first dataset that assesses the
arithmetic reasoning capabilities of generation models on complex input
structures, i.e., tables from scientific articles. We study the effectiveness
of state-of-the-art data-to-text generation models on SciGen and evaluate the
results using common metrics as well as human evaluation. Our results and
analyses show that (a) while humans like to reason for describing scientific
tables, the ability of state-of-the-art models is severely limited on this
task, (b) while adding more training data improves the results, it is not the
solution for reasoning-aware text generation, and (c) one of the main
bottlenecks for this task is the lack of proper automatic evaluation metrics.
The data, code, and annotations for human evaluation will be available at
https://github.com/UKPLab/SciGen. SciGen opens new avenues for future research
in reasoning-aware text generation and evaluation.",2021-04-16
IGA : An Intent-Guided Authoring Assistant,2021-04-14 17:32:21+00:00,http://arxiv.org/abs/2104.07000v1,"Simeng Sun, Wenlong Zhao, Varun Manjunatha, Rajiv Jain, Vlad Morariu, Franck Dernoncourt, Balaji Vasan Srinivasan, Mohit Iyyer",cs.CL,image2text,"While large-scale pretrained language models have significantly improved
writing assistance functionalities such as autocomplete, more complex and
controllable writing assistants have yet to be explored. We leverage advances
in language modeling to build an interactive writing assistant that generates
and rephrases text according to fine-grained author specifications. Users
provide input to our Intent-Guided Assistant (IGA) in the form of text
interspersed with tags that correspond to specific rhetorical directives (e.g.,
adding description or contrast, or rephrasing a particular sentence). We
fine-tune a language model on a dataset heuristically-labeled with author
intent, which allows IGA to fill in these tags with generated text that users
can subsequently edit to their liking. A series of automatic and crowdsourced
evaluations confirm the quality of IGA's generated outputs, while a small-scale
user study demonstrates author preference for IGA over baseline methods in a
creative writing task. We release our dataset, code, and demo to spur further
research into AI-assisted writing.",2021-04-14
"Automatic Generation of Descriptive Titles for Video Clips Using Deep
  Learning",2021-04-07 18:14:18+00:00,http://arxiv.org/abs/2104.03337v1,"Soheyla Amirian, Khaled Rasheed, Thiab R. Taha, Hamid R. Arabnia","cs.CV, cs.AI, cs.LG",image2text,"Over the last decade, the use of Deep Learning in many applications produced
results that are comparable to and in some cases surpassing human expert
performance. The application domains include diagnosing diseases, finance,
agriculture, search engines, robot vision, and many others. In this paper, we
are proposing an architecture that utilizes image/video captioning methods and
Natural Language Processing systems to generate a title and a concise abstract
for a video. Such a system can potentially be utilized in many application
domains, including, the cinema industry, video search engines, security
surveillance, video databases/warehouses, data centers, and others. The
proposed system functions and operates as followed: it reads a video;
representative image frames are identified and selected; the image frames are
captioned; NLP is applied to all generated captions together with text
summarization; and finally, a title and an abstract are generated for the
video. All functions are performed automatically. Preliminary results are
provided in this paper using publicly available datasets. This paper is not
concerned about the efficiency of the system at the execution time. We hope to
be able to address execution efficiency issues in our subsequent publications.",2021-04-07
"On Hallucination and Predictive Uncertainty in Conditional Language
  Generation",2021-03-28 00:32:27+00:00,http://arxiv.org/abs/2103.15025v1,"Yijun Xiao, William Yang Wang",cs.CL,image2text,"Despite improvements in performances on different natural language generation
tasks, deep neural models are prone to hallucinating facts that are incorrect
or nonexistent. Different hypotheses are proposed and examined separately for
different tasks, but no systematic explanations are available across these
tasks. In this study, we draw connections between hallucinations and predictive
uncertainty in conditional language generation. We investigate their
relationship in both image captioning and data-to-text generation and propose a
simple extension to beam search to reduce hallucination. Our analysis shows
that higher predictive uncertainty corresponds to a higher chance of
hallucination. Epistemic uncertainty is more indicative of hallucination than
aleatoric or total uncertainties. It helps to achieve better results of trading
performance in standard metric for less hallucination with the proposed beam
search variant.",2021-03-28
Relationship-based Neural Baby Talk,2021-03-08 15:51:24+00:00,http://arxiv.org/abs/2103.04846v1,"Fan Fu, Tingting Xie, Ioannis Patras, Sepehr Jalali","cs.CV, cs.AI",image2text,"Understanding interactions between objects in an image is an important
element for generating captions. In this paper, we propose a relationship-based
neural baby talk (R-NBT) model to comprehensively investigate several types of
pairwise object interactions by encoding each image via three different
relationship-based graph attention networks (GATs). We study three main
relationships: \textit{spatial relationships} to explore geometric
interactions, \textit{semantic relationships} to extract semantic interactions,
and \textit{implicit relationships} to capture hidden information that could
not be modelled explicitly as above. We construct three relationship graphs
with the objects in an image as nodes, and the mutual relationships of pairwise
objects as edges. By exploring features of neighbouring regions individually
via GATs, we integrate different types of relationships into visual features of
each node. Experiments on COCO dataset show that our proposed R-NBT model
outperforms state-of-the-art models trained on COCO dataset in three image
caption generation tasks.",2021-03-08
Controllable and Diverse Text Generation in E-commerce,2021-02-23 05:16:27+00:00,http://arxiv.org/abs/2102.11497v1,"Huajie Shao, Jun Wang, Haohong Lin, Xuezhou Zhang, Aston Zhang, Heng Ji, Tarek Abdelzaher",cs.LG,image2text,"In E-commerce, a key challenge in text generation is to find a good trade-off
between word diversity and accuracy (relevance) in order to make generated text
appear more natural and human-like. In order to improve the relevance of
generated results, conditional text generators were developed that use input
keywords or attributes to produce the corresponding text. Prior work, however,
do not finely control the diversity of automatically generated sentences. For
example, it does not control the order of keywords to put more relevant ones
first. Moreover, it does not explicitly control the balance between diversity
and accuracy. To remedy these problems, we propose a fine-grained controllable
generative model, called~\textit{Apex}, that uses an algorithm borrowed from
automatic control (namely, a variant of the \textit{proportional, integral, and
derivative (PID) controller}) to precisely manipulate the diversity/accuracy
trade-off of generated text. The algorithm is injected into a Conditional
Variational Autoencoder (CVAE), allowing \textit{Apex} to control both (i) the
order of keywords in the generated sentences (conditioned on the input keywords
and their order), and (ii) the trade-off between diversity and accuracy.
Evaluation results on real-world datasets show that the proposed method
outperforms existing generative models in terms of diversity and relevance.
Apex is currently deployed to generate production descriptions and item
recommendation reasons in Taobao owned by Alibaba, the largest E-commerce
platform in China. The A/B production test results show that our method
improves click-through rate (CTR) by 13.17\% compared to the existing method
for production descriptions. For item recommendation reason, it is able to
increase CTR by 6.89\% and 1.42\% compared to user reviews and top-K item
recommendation without reviews, respectively.",2021-02-23
Progressive Transformer-Based Generation of Radiology Reports,2021-02-19 07:42:13+00:00,http://arxiv.org/abs/2102.09777v1,"Farhad Nooralahzadeh, Nicolas Perez Gonzalez, Thomas Frauenfelder, Koji Fujimoto, Michael Krauthammer",cs.CL,image2text,"Inspired by Curriculum Learning, we propose a consecutive (i.e.
image-to-text-to-text) generation framework where we divide the problem of
radiology report generation into two steps. Contrary to generating the full
radiology report from the image at once, the model generates global concepts
from the image in the first step and then reforms them into finer and coherent
texts using transformer-based architecture. We follow the transformer-based
sequence-to-sequence paradigm at each step. We improve upon the
state-of-the-art on two benchmark datasets.",2021-02-19
Annotation Cleaning for the MSR-Video to Text Dataset,2021-02-12 11:14:56+00:00,http://arxiv.org/abs/2102.06448v1,"Haoran Chen, Jianmin Li, Simone Frintrop, Xiaolin Hu","cs.CV, cs.LG, 68T45, 68T50, I.2.10; I.2.7",image2text,"The video captioning task is to describe the video contents with natural
language by the machine. Many methods have been proposed for solving this task.
A large dataset called MSR Video to Text (MSR-VTT) is often used as the
benckmark dataset for testing the performance of the methods. However, we found
that the human annotations, i.e., the descriptions of video contents in the
dataset are quite noisy, e.g., there are many duplicate captions and many
captions contain grammatical problems. These problems may pose difficulties to
video captioning models for learning. We cleaned the MSR-VTT annotations by
removing these problems, then tested several typical video captioning models on
the cleaned dataset. Experimental results showed that data cleaning boosted the
performances of the models measured by popular quantitative metrics. We
recruited subjects to evaluate the results of a model trained on the original
and cleaned datasets. The human behavior experiment demonstrated that trained
on the cleaned dataset, the model generated captions that were more coherent
and more relevant to contents of the video clips. The cleaned dataset is
publicly available.",2021-02-12
SG2Caps: Revisiting Scene Graphs for Image Captioning,2021-02-09 18:00:53+00:00,http://arxiv.org/abs/2102.04990v1,"Subarna Tripathi, Kien Nguyen, Tanaya Guha, Bang Du, Truong Q. Nguyen","cs.CV, cs.CL",image2text,"The mainstream image captioning models rely on Convolutional Neural Network
(CNN) image features with an additional attention to salient regions and
objects to generate captions via recurrent models. Recently, scene graph
representations of images have been used to augment captioning models so as to
leverage their structural semantics, such as object entities, relationships and
attributes. Several studies have noted that naive use of scene graphs from a
black-box scene graph generator harms image caption-ing performance, and scene
graph-based captioning mod-els have to incur the overhead of explicit use of
image features to generate decent captions. Addressing these challenges, we
propose a framework, SG2Caps, that utilizes only the scene graph labels for
competitive image caption-ing performance. The basic idea is to close the
semantic gap between two scene graphs - one derived from the input image and
the other one from its caption. In order to achieve this, we leverage the
spatial location of objects and the Human-Object-Interaction (HOI) labels as an
additional HOI graph. Our framework outperforms existing scene graph-only
captioning models by a large margin (CIDEr score of 110 vs 71) indicating scene
graphs as a promising representation for image captioning. Direct utilization
of the scene graph labels avoids expensive graph convolutions over
high-dimensional CNN features resulting in 49%fewer trainable parameters.",2021-02-09
Controlling Hallucinations at Word Level in Data-to-Text Generation,2021-02-04 18:58:28+00:00,http://arxiv.org/abs/2102.02810v1,"ClÃ©ment Rebuffel, Marco Roberti, Laure Soulier, Geoffrey Scoutheeten, Rossella Cancelliere, Patrick Gallinari","cs.CL, cs.AI, cs.LG, cs.NE, 68T50 (Primary), 68T07 (Secondary), 68T05, I.2.6; I.2.7",image2text,"Data-to-Text Generation (DTG) is a subfield of Natural Language Generation
aiming at transcribing structured data in natural language descriptions. The
field has been recently boosted by the use of neural-based generators which
exhibit on one side great syntactic skills without the need of hand-crafted
pipelines; on the other side, the quality of the generated text reflects the
quality of the training data, which in realistic settings only offer
imperfectly aligned structure-text pairs. Consequently, state-of-art neural
models include misleading statements - usually called hallucinations - in their
outputs. The control of this phenomenon is today a major challenge for DTG, and
is the problem addressed in the paper.
  Previous work deal with this issue at the instance level: using an alignment
score for each table-reference pair. In contrast, we propose a finer-grained
approach, arguing that hallucinations should rather be treated at the word
level. Specifically, we propose a Multi-Branch Decoder which is able to
leverage word-level labels to learn the relevant parts of each training
instance. These labels are obtained following a simple and efficient scoring
procedure based on co-occurrence analysis and dependency parsing. Extensive
evaluations, via automated metrics and human judgment on the standard WikiBio
benchmark, show the accuracy of our alignment labels and the effectiveness of
the proposed Multi-Branch Decoder. Our model is able to reduce and control
hallucinations, while keeping fluency and coherence in generated texts. Further
experiments on a degraded version of ToTTo show that our model could be
successfully used on very noisy settings.",2021-02-04
Unifying Vision-and-Language Tasks via Text Generation,2021-02-04 17:59:30+00:00,http://arxiv.org/abs/2102.02779v1,"Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal","cs.CL, cs.AI, cs.CV, cs.LG",image2text,"Existing methods for vision-and-language learning typically require designing
task-specific architectures and objectives for each task. For example, a
multi-label answer classifier for visual question answering, a region scorer
for referring expression comprehension, and a language decoder for image
captioning, etc. To alleviate these hassles, in this work, we propose a unified
framework that learns different tasks in a single architecture with the same
language modeling objective, i.e., multimodal conditional text generation,
where our models learn to generate labels in text based on the visual and
textual inputs. On 7 popular vision-and-language benchmarks, including visual
question answering, referring expression comprehension, visual commonsense
reasoning, most of which have been previously modeled as discriminative tasks,
our generative approach (with a single unified architecture) reaches comparable
performance to recent task-specific state-of-the-art vision-and-language
models. Moreover, our generative approach shows better generalization ability
on answering questions that have rare answers. In addition, we show that our
framework allows multi-task learning in a single architecture with a single set
of parameters, which achieves similar performance to separately optimized
single-task models. Our code will be publicly available at:
https://github.com/j-min/VL-T5",2021-02-04
"The GEM Benchmark: Natural Language Generation, its Evaluation and
  Metrics",2021-02-02 18:42:05+00:00,http://arxiv.org/abs/2102.01672v2,"Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna Clinciu, Dipanjan Das, Kaustubh D. Dhole, Wanyu Du, Esin Durmus, OndÅ™ej DuÅ¡ek, Chris Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Rubungo Andre Niyongabo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, JoÃ£o Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, Jiawei Zhou","cs.CL, cs.AI, cs.LG",image2text,"We introduce GEM, a living benchmark for natural language Generation (NLG),
its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly
evolving ecosystem of automated metrics, datasets, and human evaluation
standards. However, due to this moving target, new models often still evaluate
on divergent anglo-centric corpora with well-established, but flawed, metrics.
This disconnect makes it challenging to identify the limitations of current
models and opportunities for progress. Addressing this limitation, GEM provides
an environment in which models can easily be applied to a wide set of corpora
and evaluation strategies can be tested. Regular updates to the benchmark will
help NLG research become more multilingual and evolve the challenge alongside
models.
  This paper serves as the description of the initial release for which we are
organizing a shared task at our ACL 2021 Workshop and to which we invite the
entire NLG community to participate.",2021-02-02
"Generating images from caption and vice versa via CLIP-Guided Generative
  Latent Space Search",2021-02-02 18:00:13+00:00,http://arxiv.org/abs/2102.01645v2,"Federico A. Galatolo, Mario G. C. A. Cimino, Gigliola Vaglini","cs.NE, cs.AI, cs.LG",image2text,"In this research work we present GLaSS, a novel zero-shot framework to
generate an image(or a caption) corresponding to a given caption(or image).
GLaSS is based on the CLIP neural network which given an image and a
descriptive caption provides similar embeddings. Differently, GLaSS takes a
caption (or an image) as an input, and generates the image (or the caption)
whose CLIP embedding is most similar to the input one. This optimal image (or
caption) is produced via a generative network after an exploration by a genetic
algorithm. Promising results are shown, based on the experimentation of the
image generators BigGAN and StyleGAN2, and of the text generator GPT2.",2021-02-02
"VX2TEXT: End-to-End Learning of Video-Based Text Generation From
  Multimodal Inputs",2021-01-28 15:22:36+00:00,http://arxiv.org/abs/2101.12059v2,"Xudong Lin, Gedas Bertasius, Jue Wang, Shih-Fu Chang, Devi Parikh, Lorenzo Torresani","cs.CV, cs.CL",image2text,"We present \textsc{Vx2Text}, a framework for text generation from multimodal
inputs consisting of video plus text, speech, or audio. In order to leverage
transformer networks, which have been shown to be effective at modeling
language, each modality is first converted into a set of language embeddings by
a learnable tokenizer. This allows our approach to perform multimodal fusion in
the language space, thus eliminating the need for ad-hoc cross-modal fusion
modules. To address the non-differentiability of tokenization on continuous
inputs (e.g., video or audio), we utilize a relaxation scheme that enables
end-to-end training. Furthermore, unlike prior encoder-only models, our network
includes an autoregressive decoder to generate open-ended text from the
multimodal embeddings fused by the language encoder. This renders our approach
fully generative and makes it directly applicable to different ""video+$x$ to
text"" problems without the need to design specialized network heads for each
task. The proposed framework is not only conceptually simple but also
remarkably effective: experiments demonstrate that our approach based on a
single architecture outperforms the state-of-the-art on three video-based
text-generation tasks -- captioning, question answering and audio-visual
scene-aware dialog.",2021-01-28
"GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial
  Networks",2021-01-25 09:42:58+00:00,http://arxiv.org/abs/2101.09978v2,"Tianming Zhao, Chunyang Chen, Yuanning Liu, Xiaodong Zhu","cs.HC, cs.CV, cs.LG, cs.SE",image2text,"Graphical User Interface (GUI) is ubiquitous in almost all modern desktop
software, mobile applications, and online websites. A good GUI design is
crucial to the success of the software in the market, but designing a good GUI
which requires much innovation and creativity is difficult even to well-trained
designers. Besides, the requirement of the rapid development of GUI design also
aggravates designers' working load. So, the availability of various automated
generated GUIs can help enhance the design personalization and specialization
as they can cater to the taste of different designers. To assist designers, we
develop a model GUIGAN to automatically generate GUI designs. Different from
conventional image generation models based on image pixels, our GUIGAN is to
reuse GUI components collected from existing mobile app GUIs for composing a
new design that is similar to natural-language generation. Our GUIGAN is based
on SeqGAN by modeling the GUI component style compatibility and GUI structure.
The evaluation demonstrates that our model significantly outperforms the best
of the baseline methods by 30.77% in Frechet Inception distance (FID) and
12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we
provide initial evidence of the usefulness of our approach for generating
acceptable brand new GUI designs.",2021-01-25
"Towards Understanding How Readers Integrate Charts and Captions: A Case
  Study with Line Charts",2021-01-20 18:11:35+00:00,http://arxiv.org/abs/2101.08235v1,"Dae Hyun Kim, Vidya Setlur, Maneesh Agrawala",cs.HC,image2text,"Charts often contain visually prominent features that draw attention to
aspects of the data and include text captions that emphasize aspects of the
data. Through a crowdsourced study, we explore how readers gather takeaways
when considering charts and captions together. We first ask participants to
mark visually prominent regions in a set of line charts. We then generate text
captions based on the prominent features and ask participants to report their
takeaways after observing chart-caption pairs. We find that when both the chart
and caption describe a high-prominence feature, readers treat the doubly
emphasized high-prominence feature as the takeaway; when the caption describes
a low-prominence chart feature, readers rely on the chart and report a
higher-prominence feature as the takeaway. We also find that external
information that provides context, helps further convey the caption's message
to the reader. We use these findings to provide guidelines for authoring
effective chart-caption pairs.",2021-01-20
Narration Generation for Cartoon Videos,2021-01-17 23:23:09+00:00,http://arxiv.org/abs/2101.06803v1,"Nikos Papasarantopoulos, Shay B. Cohen",cs.CL,image2text,"Research on text generation from multimodal inputs has largely focused on
static images, and less on video data. In this paper, we propose a new task,
narration generation, that is complementing videos with narration texts that
are to be interjected in several places. The narrations are part of the video
and contribute to the storyline unfolding in it. Moreover, they are
context-informed, since they include information appropriate for the timeframe
of video they cover, and also, do not need to include every detail shown in
input scenes, as a caption would. We collect a new dataset from the animated
television series Peppa Pig. Furthermore, we formalize the task of narration
generation as including two separate tasks, timing and content generation, and
present a set of models on the new task.",2021-01-17
Zero-shot Learning by Generating Task-specific Adapters,2021-01-02 10:50:23+00:00,http://arxiv.org/abs/2101.00420v1,"Qinyuan Ye, Xiang Ren","cs.CL, cs.LG",image2text,"Pre-trained text-to-text transformers achieve impressive performance across a
wide range of NLP tasks, and they naturally support zero-shot learning (ZSL) by
using the task description as prompt in the input. However, this approach has
potential limitations, as it learns from input-output pairs at instance level,
instead of learning to solve tasks at task level. Alternatively, applying
existing ZSL methods to text-to-text transformers is non-trivial due to their
text generation objective and huge size. To address these issues, we introduce
Hypter, a framework that improves zero-shot transferability by training a
hypernetwork to generate task-specific adapters from task descriptions. This
formulation enables learning at task level, and greatly reduces the number of
parameters by using light-weight adapters. Experiments on two datasets
demonstrate Hypter improves upon fine-tuning baselines.",2021-01-02
Neural Text Generation with Artificial Negative Examples,2020-12-28 07:25:10+00:00,http://arxiv.org/abs/2012.14124v1,"Keisuke Shirai, Kazuma Hashimoto, Akiko Eriguchi, Takashi Ninomiya, Shinsuke Mori","cs.CL, cs.AI",image2text,"Neural text generation models conditioning on given input (e.g. machine
translation and image captioning) are usually trained by maximum likelihood
estimation of target text. However, the trained models suffer from various
types of errors at inference time. In this paper, we propose to suppress an
arbitrary type of errors by training the text generation model in a
reinforcement learning framework, where we use a trainable reward function that
is capable of discriminating between references and sentences containing the
targeted type of errors. We create such negative examples by artificially
injecting the targeted errors to the references. In experiments, we focus on
two error types, repeated and dropped tokens in model-generated text. The
experimental results show that our method can suppress the generation errors
and achieve significant improvements on two machine translation and two image
captioning tasks.",2020-12-28
Few-Shot Text Generation with Pattern-Exploiting Training,2020-12-22 10:53:07+00:00,http://arxiv.org/abs/2012.11926v1,"Timo Schick, Hinrich SchÃ¼tze","cs.CL, cs.LG",image2text,"Providing pretrained language models with simple task descriptions or prompts
in natural language yields impressive few-shot results for a wide range of text
classification tasks when combined with gradient-based learning from examples.
In this paper, we show that the underlying idea can also be applied to text
generation tasks: We adapt Pattern-Exploiting Training (PET), a recently
proposed few-shot approach, for finetuning generative language models on text
generation tasks. On several text summarization and headline generation
datasets, our proposed variant of PET gives consistent improvements over a
strong baseline in few-shot settings.",2020-12-22
Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings,2020-12-14 10:59:59+00:00,http://arxiv.org/abs/2012.07412v2,"Qipeng Guo, Zhijing Jin, Ziyu Wang, Xipeng Qiu, Weinan Zhang, Jun Zhu, Zheng Zhang, David Wipf","cs.LG, cs.AI, cs.CL",image2text,"Cycle-consistent training is widely used for jointly learning a forward and
inverse mapping between two domains of interest without the cumbersome
requirement of collecting matched pairs within each domain. In this regard, the
implicit assumption is that there exists (at least approximately) a
ground-truth bijection such that a given input from either domain can be
accurately reconstructed from successive application of the respective
mappings. But in many applications no such bijection can be expected to exist
and large reconstruction errors can compromise the success of cycle-consistent
training. As one important instance of this limitation, we consider
practically-relevant situations where there exists a many-to-one or surjective
mapping between domains. To address this regime, we develop a conditional
variational autoencoder (CVAE) approach that can be viewed as converting
surjective mappings to implicit bijections whereby reconstruction errors in
both directions can be minimized, and as a natural byproduct, realistic output
diversity can be obtained in the one-to-many direction. As theoretical
motivation, we analyze a simplified scenario whereby minima of the proposed
CVAE-based energy function align with the recovery of ground-truth surjective
mappings. On the empirical side, we consider a synthetic image dataset with
known ground-truth, as well as a real-world application involving natural
language generation from knowledge graphs and vice versa, a prototypical
surjective case. For the latter, our CVAE pipeline can capture such many-to-one
mappings during cycle training while promoting textural diversity for
graph-to-text tasks. Our code is available at github.com/QipengGuo/CycleGT",2020-12-14
Video Generative Adversarial Networks: A Review,2020-11-04 12:16:05+00:00,http://arxiv.org/abs/2011.02250v1,"Nuha Aldausari, Arcot Sowmya, Nadine Marcus, Gelareh Mohammadi","cs.CV, cs.LG, eess.IV",image2text,"With the increasing interest in the content creation field in multiple
sectors such as media, education, and entertainment, there is an increasing
trend in the papers that uses AI algorithms to generate content such as images,
videos, audio, and text. Generative Adversarial Networks (GANs) in one of the
promising models that synthesizes data samples that are similar to real data
samples. While the variations of GANs models, in general, have been covered to
some extent in several survey papers, to the best of our knowledge, this is
among the first survey papers that reviews the state-of-the-art video GANs
models. This paper first categorized GANs review papers into general GANs
review papers, image GANs review papers, and special field GANs review papers
such as anomaly detection, medical imaging, or cybersecurity. The paper then
summarizes the main improvements in GANs frameworks that are not initially
developed for the video domain but have been adopted in multiple video GANs
variations. Then, a comprehensive review of video GANs models is provided under
two main divisions according to the presence or non-presence of a condition.
The conditional models then further grouped according to the type of condition
into audio, text, video, and image. The paper is concluded by highlighting the
main challenges and limitations of the current video GANs models. A
comprehensive list of datasets, applied loss functions, and evaluation metrics
is provided in the supplementary material.",2020-11-04
Personalized Multimodal Feedback Generation in Education,2020-10-31 05:26:49+00:00,http://arxiv.org/abs/2011.00192v1,"Haochen Liu, Zitao Liu, Zhongqin Wu, Jiliang Tang","cs.CL, cs.AI",image2text,"The automatic evaluation for school assignments is an important application
of AI in the education field. In this work, we focus on the task of
personalized multimodal feedback generation, which aims to generate
personalized feedback for various teachers to evaluate students' assignments
involving multimodal inputs such as images, audios, and texts. This task
involves the representation and fusion of multimodal information and natural
language generation, which presents the challenges from three aspects: 1) how
to encode and integrate multimodal inputs; 2) how to generate feedback specific
to each modality; and 3) how to realize personalized feedback generation. In
this paper, we propose a novel Personalized Multimodal Feedback Generation
Network (PMFGN) armed with a modality gate mechanism and a personalized bias
mechanism to address these challenges. The extensive experiments on real-world
K-12 education data show that our model significantly outperforms several
baselines by generating more accurate and diverse feedback. In addition,
detailed ablation experiments are conducted to deepen our understanding of the
proposed framework.",2020-10-31
Fusion Models for Improved Visual Captioning,2020-10-28 21:55:25+00:00,http://arxiv.org/abs/2010.15251v2,"Marimuthu Kalimuthu, Aditya Mogadala, Marius Mosbach, Dietrich Klakow","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Visual captioning aims to generate textual descriptions given images or
videos. Traditionally, image captioning models are trained on human annotated
datasets such as Flickr30k and MS-COCO, which are limited in size and
diversity. This limitation hinders the generalization capabilities of these
models while also rendering them liable to making mistakes. Language models
can, however, be trained on vast amounts of freely available unlabelled data
and have recently emerged as successful language encoders and coherent text
generators. Meanwhile, several unimodal and multimodal fusion techniques have
been proven to work well for natural language generation and automatic speech
recognition. Building on these recent developments, and with the aim of
improving the quality of generated captions, the contribution of our work in
this paper is two-fold: First, we propose a generic multimodal model fusion
framework for caption generation as well as emendation where we utilize
different fusion strategies to integrate a pretrained Auxiliary Language Model
(AuxLM) within the traditional encoder-decoder visual captioning frameworks.
Next, we employ the same fusion strategies to integrate a pretrained Masked
Language Model (MLM), namely BERT, with a visual captioning model, viz. Show,
Attend, and Tell, for emending both syntactic and semantic errors in captions.
Our caption emendation experiments on three benchmark image captioning
datasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the
baseline, indicating the usefulness of our proposed multimodal fusion
strategies. Further, we perform a preliminary qualitative analysis on the
emended captions and identify error categories based on the type of
corrections.",2020-10-28
Safe Handover in Mixed-Initiative Control for Cyber-Physical Systems,2020-10-21 12:59:32+00:00,http://arxiv.org/abs/2010.10967v1,"Frederik Wiehr, Anke Hirsch, Florian Daiber, Antonio Kruger, Alisa Kovtunova, Stefan Borgwardt, Ernie Chang, Vera Demberg, Marcel Steinmetz, Hoffmann Jorg",cs.HC,image2text,"For mixed-initiative control between cyber-physical systems (CPS) and its
users, it is still an open question how machines can safely hand over control
to humans. In this work, we propose a concept to provide technological support
that uses formal methods from AI -- description logic (DL) and automated
planning -- to predict more reliably when a hand-over is necessary, and to
increase the advance notice for handovers by planning ahead of runtime. We
combine this with methods from human-computer interaction (HCI) and natural
language generation (NLG) to develop solutions for safe and smooth handovers
and provide an example autonomous driving scenario. A study design is proposed
with the assessment of qualitative feedback, cognitive load and trust in
automation.",2020-10-21
"Improving Factual Completeness and Consistency of Image-to-Text
  Radiology Report Generation",2020-10-20 05:42:47+00:00,http://arxiv.org/abs/2010.10042v1,"Yasuhide Miura, Yuhao Zhang, Curtis P. Langlotz, Dan Jurafsky",cs.CL,image2text,"Neural image-to-text radiology report generation systems offer the potential
to accelerate clinical processes by saving radiologists from the repetitive
labor of drafting radiology reports and preventing medical errors. However,
existing report generation systems, despite achieving high performances on
natural language generation metrics such as CIDEr or BLEU, still suffer from
incomplete and inconsistent generations, rendering these systems unusable in
practice. In this work, we aim to overcome this problem by proposing two new
metrics that encourage the factual completeness and consistency of generated
radiology reports. The first metric, the Exact Entity Match score, evaluates a
generation by its coverage of radiology domain entities against the references.
The second metric, the Entailing Entity Match score, augments the first metric
by introducing a natural language inference model into the entity match process
to encourage consistent generations that can be entailed from the references.
To achieve this, we also developed an in-domain NLI model via weak supervision
to improve its performance on radiology text. We further propose a report
generation system that optimizes these two new metrics via reinforcement
learning. On two open radiology report datasets, our system not only achieves
the best performance on these two metrics compared to baselines, but also leads
to as much as +2.0 improvement on the F1 score of a clinical finding metric. We
show via analysis and examples that our system leads to generations that are
more complete and consistent compared to the baselines.",2020-10-20
"RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich
  Semantic Annotations for Task-Oriented Dialogue Modeling",2020-10-17 08:18:59+00:00,http://arxiv.org/abs/2010.08738v1,"Jun Quan, Shian Zhang, Qian Cao, Zizhong Li, Deyi Xiong",cs.CL,image2text,"In order to alleviate the shortage of multi-domain data and to capture
discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a
large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic
Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn
semantically annotated dialogues, with more than 150K utterances spanning over
12 domains, which is larger than all previous annotated H2H conversational
datasets. Both single- and multi-domain dialogues are constructed, accounting
for 65% and 35%, respectively. Each dialogue is labeled with comprehensive
dialogue annotations, including dialogue goal in the form of natural language
description, domain, dialogue states and acts at both the user and system side.
In addition to traditional dialogue annotations, we especially provide
linguistic annotations on discourse phenomena, e.g., ellipsis and coreference,
in dialogues, which are useful for dialogue coreference and ellipsis resolution
tasks. Apart from the fully annotated dataset, we also present a detailed
description of the data collection procedure, statistics and analysis of the
dataset. A series of benchmark models and results are reported, including
natural language understanding (intent detection & slot filling), dialogue
state tracking and dialogue context-to-text generation, as well as coreference
and ellipsis resolution, which facilitate the baseline comparison for future
research on this corpus.",2020-10-17
Dissecting the components and factors of Neural Text Generation,2020-10-14 17:54:42+00:00,http://arxiv.org/abs/2010.07279v1,"Khyathi Raghavi Chandu, Alan W Black",cs.CL,image2text,"Neural text generation metamorphosed into several critical natural language
applications ranging from text completion to free form narrative generation.
Generating natural language has fundamentally been a human attribute and the
advent of ubiquitous NLP applications and virtual agents marks the need to
impart this skill to machines. There has been a colossal research effort in
various frontiers of neural text generation including machine translation,
summarization, image captioning, storytelling etc., We believe that this is an
excellent juncture to retrospect on the directions of the field. Specifically,
this paper surveys the fundamental factors and components relaying task
agnostic impacts across various generation tasks such as storytelling,
summarization, translation etc., In specific, we present an abstraction of the
imperative techniques with respect to learning paradigms, pretraining, modeling
approaches, decoding and the key challenges. Thereby, we hope to deliver a
one-stop destination for researchers in the field to facilitate a perspective
on where to situate their work and how it impacts other closely related tasks.",2020-10-14
"Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on
  Chest X-rays",2020-10-06 04:18:18+00:00,http://arxiv.org/abs/2010.02467v1,"Jianmo Ni, Chun-Nan Hsu, Amilcare Gentili, Julian McAuley","cs.CV, cs.CL",image2text,"Automatic medical image report generation has drawn growing attention due to
its potential to alleviate radiologists' workload. Existing work on report
generation often trains encoder-decoder networks to generate complete reports.
However, such models are affected by data bias (e.g.~label imbalance) and face
common issues inherent in text generation models (e.g.~repetition). In this
work, we focus on reporting abnormal findings on radiology images; instead of
training on complete radiology reports, we propose a method to identify
abnormal findings from the reports in addition to grouping them with
unsupervised clustering and minimal rules. We formulate the task as cross-modal
retrieval and propose Conditional Visual-Semantic Embeddings to align images
and fine-grained abnormal findings in a joint embedding space. We demonstrate
that our method is able to retrieve abnormal findings and outperforms existing
generation models on both clinical correctness and text generation metrics.",2020-10-06
"Knowledge-Enhanced Personalized Review Generation with Capsule Graph
  Neural Network",2020-10-04 03:54:40+00:00,http://arxiv.org/abs/2010.01480v1,"Junyi Li, Siqing Li, Wayne Xin Zhao, Gaole He, Zhicheng Wei, Nicholas Jing Yuan, Ji-Rong Wen","cs.CL, cs.AI",image2text,"Personalized review generation (PRG) aims to automatically produce review
text reflecting user preference, which is a challenging natural language
generation task. Most of previous studies do not explicitly model factual
description of products, tending to generate uninformative content. Moreover,
they mainly focus on word-level generation, but cannot accurately reflect more
abstractive user preference in multiple aspects. To address the above issues,
we propose a novel knowledge-enhanced PRG model based on capsule graph neural
network~(Caps-GNN). We first construct a heterogeneous knowledge graph (HKG)
for utilizing rich item attributes. We adopt Caps-GNN to learn graph capsules
for encoding underlying characteristics from the HKG. Our generation process
contains two major steps, namely aspect sequence generation and sentence
generation. First, based on graph capsules, we adaptively learn aspect capsules
for inferring the aspect sequence. Then, conditioned on the inferred aspect
label, we design a graph-based copy mechanism to generate sentences by
incorporating related entities or words from HKG. To our knowledge, we are the
first to utilize knowledge graph for the PRG task. The incorporated KG
information is able to enhance user preference at both aspect and word levels.
Extensive experiments on three real-world datasets have demonstrated the
effectiveness of our model on the PRG task.",2020-10-04
