title,pubdate,id,authors,categories,search,abstract,displaydate
"On Hallucination and Predictive Uncertainty in Conditional Language
  Generation",2021-03-28 00:32:27+00:00,http://arxiv.org/abs/2103.15025v1,"Yijun Xiao, William Yang Wang",cs.CL,image2text,"Despite improvements in performances on different natural language generation
tasks, deep neural models are prone to hallucinating facts that are incorrect
or nonexistent. Different hypotheses are proposed and examined separately for
different tasks, but no systematic explanations are available across these
tasks. In this study, we draw connections between hallucinations and predictive
uncertainty in conditional language generation. We investigate their
relationship in both image captioning and data-to-text generation and propose a
simple extension to beam search to reduce hallucination. Our analysis shows
that higher predictive uncertainty corresponds to a higher chance of
hallucination. Epistemic uncertainty is more indicative of hallucination than
aleatoric or total uncertainties. It helps to achieve better results of trading
performance in standard metric for less hallucination with the proposed beam
search variant.",2021-03-28
Relationship-based Neural Baby Talk,2021-03-08 15:51:24+00:00,http://arxiv.org/abs/2103.04846v1,"Fan Fu, Tingting Xie, Ioannis Patras, Sepehr Jalali","cs.CV, cs.AI",image2text,"Understanding interactions between objects in an image is an important
element for generating captions. In this paper, we propose a relationship-based
neural baby talk (R-NBT) model to comprehensively investigate several types of
pairwise object interactions by encoding each image via three different
relationship-based graph attention networks (GATs). We study three main
relationships: \textit{spatial relationships} to explore geometric
interactions, \textit{semantic relationships} to extract semantic interactions,
and \textit{implicit relationships} to capture hidden information that could
not be modelled explicitly as above. We construct three relationship graphs
with the objects in an image as nodes, and the mutual relationships of pairwise
objects as edges. By exploring features of neighbouring regions individually
via GATs, we integrate different types of relationships into visual features of
each node. Experiments on COCO dataset show that our proposed R-NBT model
outperforms state-of-the-art models trained on COCO dataset in three image
caption generation tasks.",2021-03-08
Controllable and Diverse Text Generation in E-commerce,2021-02-23 05:16:27+00:00,http://arxiv.org/abs/2102.11497v1,"Huajie Shao, Jun Wang, Haohong Lin, Xuezhou Zhang, Aston Zhang, Heng Ji, Tarek Abdelzaher",cs.LG,image2text,"In E-commerce, a key challenge in text generation is to find a good trade-off
between word diversity and accuracy (relevance) in order to make generated text
appear more natural and human-like. In order to improve the relevance of
generated results, conditional text generators were developed that use input
keywords or attributes to produce the corresponding text. Prior work, however,
do not finely control the diversity of automatically generated sentences. For
example, it does not control the order of keywords to put more relevant ones
first. Moreover, it does not explicitly control the balance between diversity
and accuracy. To remedy these problems, we propose a fine-grained controllable
generative model, called~\textit{Apex}, that uses an algorithm borrowed from
automatic control (namely, a variant of the \textit{proportional, integral, and
derivative (PID) controller}) to precisely manipulate the diversity/accuracy
trade-off of generated text. The algorithm is injected into a Conditional
Variational Autoencoder (CVAE), allowing \textit{Apex} to control both (i) the
order of keywords in the generated sentences (conditioned on the input keywords
and their order), and (ii) the trade-off between diversity and accuracy.
Evaluation results on real-world datasets show that the proposed method
outperforms existing generative models in terms of diversity and relevance.
Apex is currently deployed to generate production descriptions and item
recommendation reasons in Taobao owned by Alibaba, the largest E-commerce
platform in China. The A/B production test results show that our method
improves click-through rate (CTR) by 13.17\% compared to the existing method
for production descriptions. For item recommendation reason, it is able to
increase CTR by 6.89\% and 1.42\% compared to user reviews and top-K item
recommendation without reviews, respectively.",2021-02-23
Progressive Transformer-Based Generation of Radiology Reports,2021-02-19 07:42:13+00:00,http://arxiv.org/abs/2102.09777v1,"Farhad Nooralahzadeh, Nicolas Perez Gonzalez, Thomas Frauenfelder, Koji Fujimoto, Michael Krauthammer",cs.CL,image2text,"Inspired by Curriculum Learning, we propose a consecutive (i.e.
image-to-text-to-text) generation framework where we divide the problem of
radiology report generation into two steps. Contrary to generating the full
radiology report from the image at once, the model generates global concepts
from the image in the first step and then reforms them into finer and coherent
texts using transformer-based architecture. We follow the transformer-based
sequence-to-sequence paradigm at each step. We improve upon the
state-of-the-art on two benchmark datasets.",2021-02-19
Annotation Cleaning for the MSR-Video to Text Dataset,2021-02-12 11:14:56+00:00,http://arxiv.org/abs/2102.06448v1,"Haoran Chen, Jianmin Li, Simone Frintrop, Xiaolin Hu","cs.CV, cs.LG, 68T45, 68T50, I.2.10; I.2.7",image2text,"The video captioning task is to describe the video contents with natural
language by the machine. Many methods have been proposed for solving this task.
A large dataset called MSR Video to Text (MSR-VTT) is often used as the
benckmark dataset for testing the performance of the methods. However, we found
that the human annotations, i.e., the descriptions of video contents in the
dataset are quite noisy, e.g., there are many duplicate captions and many
captions contain grammatical problems. These problems may pose difficulties to
video captioning models for learning. We cleaned the MSR-VTT annotations by
removing these problems, then tested several typical video captioning models on
the cleaned dataset. Experimental results showed that data cleaning boosted the
performances of the models measured by popular quantitative metrics. We
recruited subjects to evaluate the results of a model trained on the original
and cleaned datasets. The human behavior experiment demonstrated that trained
on the cleaned dataset, the model generated captions that were more coherent
and more relevant to contents of the video clips. The cleaned dataset is
publicly available.",2021-02-12
SG2Caps: Revisiting Scene Graphs for Image Captioning,2021-02-09 18:00:53+00:00,http://arxiv.org/abs/2102.04990v1,"Subarna Tripathi, Kien Nguyen, Tanaya Guha, Bang Du, Truong Q. Nguyen","cs.CV, cs.CL",image2text,"The mainstream image captioning models rely on Convolutional Neural Network
(CNN) image features with an additional attention to salient regions and
objects to generate captions via recurrent models. Recently, scene graph
representations of images have been used to augment captioning models so as to
leverage their structural semantics, such as object entities, relationships and
attributes. Several studies have noted that naive use of scene graphs from a
black-box scene graph generator harms image caption-ing performance, and scene
graph-based captioning mod-els have to incur the overhead of explicit use of
image features to generate decent captions. Addressing these challenges, we
propose a framework, SG2Caps, that utilizes only the scene graph labels for
competitive image caption-ing performance. The basic idea is to close the
semantic gap between two scene graphs - one derived from the input image and
the other one from its caption. In order to achieve this, we leverage the
spatial location of objects and the Human-Object-Interaction (HOI) labels as an
additional HOI graph. Our framework outperforms existing scene graph-only
captioning models by a large margin (CIDEr score of 110 vs 71) indicating scene
graphs as a promising representation for image captioning. Direct utilization
of the scene graph labels avoids expensive graph convolutions over
high-dimensional CNN features resulting in 49%fewer trainable parameters.",2021-02-09
Controlling Hallucinations at Word Level in Data-to-Text Generation,2021-02-04 18:58:28+00:00,http://arxiv.org/abs/2102.02810v1,"Cl√©ment Rebuffel, Marco Roberti, Laure Soulier, Geoffrey Scoutheeten, Rossella Cancelliere, Patrick Gallinari","cs.CL, cs.AI, cs.LG, cs.NE, 68T50 (Primary), 68T07 (Secondary), 68T05, I.2.6; I.2.7",image2text,"Data-to-Text Generation (DTG) is a subfield of Natural Language Generation
aiming at transcribing structured data in natural language descriptions. The
field has been recently boosted by the use of neural-based generators which
exhibit on one side great syntactic skills without the need of hand-crafted
pipelines; on the other side, the quality of the generated text reflects the
quality of the training data, which in realistic settings only offer
imperfectly aligned structure-text pairs. Consequently, state-of-art neural
models include misleading statements - usually called hallucinations - in their
outputs. The control of this phenomenon is today a major challenge for DTG, and
is the problem addressed in the paper.
  Previous work deal with this issue at the instance level: using an alignment
score for each table-reference pair. In contrast, we propose a finer-grained
approach, arguing that hallucinations should rather be treated at the word
level. Specifically, we propose a Multi-Branch Decoder which is able to
leverage word-level labels to learn the relevant parts of each training
instance. These labels are obtained following a simple and efficient scoring
procedure based on co-occurrence analysis and dependency parsing. Extensive
evaluations, via automated metrics and human judgment on the standard WikiBio
benchmark, show the accuracy of our alignment labels and the effectiveness of
the proposed Multi-Branch Decoder. Our model is able to reduce and control
hallucinations, while keeping fluency and coherence in generated texts. Further
experiments on a degraded version of ToTTo show that our model could be
successfully used on very noisy settings.",2021-02-04
Unifying Vision-and-Language Tasks via Text Generation,2021-02-04 17:59:30+00:00,http://arxiv.org/abs/2102.02779v1,"Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal","cs.CL, cs.AI, cs.CV, cs.LG",image2text,"Existing methods for vision-and-language learning typically require designing
task-specific architectures and objectives for each task. For example, a
multi-label answer classifier for visual question answering, a region scorer
for referring expression comprehension, and a language decoder for image
captioning, etc. To alleviate these hassles, in this work, we propose a unified
framework that learns different tasks in a single architecture with the same
language modeling objective, i.e., multimodal conditional text generation,
where our models learn to generate labels in text based on the visual and
textual inputs. On 7 popular vision-and-language benchmarks, including visual
question answering, referring expression comprehension, visual commonsense
reasoning, most of which have been previously modeled as discriminative tasks,
our generative approach (with a single unified architecture) reaches comparable
performance to recent task-specific state-of-the-art vision-and-language
models. Moreover, our generative approach shows better generalization ability
on answering questions that have rare answers. In addition, we show that our
framework allows multi-task learning in a single architecture with a single set
of parameters, which achieves similar performance to separately optimized
single-task models. Our code will be publicly available at:
https://github.com/j-min/VL-T5",2021-02-04
"The GEM Benchmark: Natural Language Generation, its Evaluation and
  Metrics",2021-02-02 18:42:05+00:00,http://arxiv.org/abs/2102.01672v2,"Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna Clinciu, Dipanjan Das, Kaustubh D. Dhole, Wanyu Du, Esin Durmus, Ond≈ôej Du≈°ek, Chris Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Rubungo Andre Niyongabo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, Jo√£o Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, Jiawei Zhou","cs.CL, cs.AI, cs.LG",image2text,"We introduce GEM, a living benchmark for natural language Generation (NLG),
its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly
evolving ecosystem of automated metrics, datasets, and human evaluation
standards. However, due to this moving target, new models often still evaluate
on divergent anglo-centric corpora with well-established, but flawed, metrics.
This disconnect makes it challenging to identify the limitations of current
models and opportunities for progress. Addressing this limitation, GEM provides
an environment in which models can easily be applied to a wide set of corpora
and evaluation strategies can be tested. Regular updates to the benchmark will
help NLG research become more multilingual and evolve the challenge alongside
models.
  This paper serves as the description of the initial release for which we are
organizing a shared task at our ACL 2021 Workshop and to which we invite the
entire NLG community to participate.",2021-02-02
"Generating images from caption and vice versa via CLIP-Guided Generative
  Latent Space Search",2021-02-02 18:00:13+00:00,http://arxiv.org/abs/2102.01645v2,"Federico A. Galatolo, Mario G. C. A. Cimino, Gigliola Vaglini","cs.NE, cs.AI, cs.LG",image2text,"In this research work we present GLaSS, a novel zero-shot framework to
generate an image(or a caption) corresponding to a given caption(or image).
GLaSS is based on the CLIP neural network which given an image and a
descriptive caption provides similar embeddings. Differently, GLaSS takes a
caption (or an image) as an input, and generates the image (or the caption)
whose CLIP embedding is most similar to the input one. This optimal image (or
caption) is produced via a generative network after an exploration by a genetic
algorithm. Promising results are shown, based on the experimentation of the
image generators BigGAN and StyleGAN2, and of the text generator GPT2.",2021-02-02
"VX2TEXT: End-to-End Learning of Video-Based Text Generation From
  Multimodal Inputs",2021-01-28 15:22:36+00:00,http://arxiv.org/abs/2101.12059v2,"Xudong Lin, Gedas Bertasius, Jue Wang, Shih-Fu Chang, Devi Parikh, Lorenzo Torresani","cs.CV, cs.CL",image2text,"We present \textsc{Vx2Text}, a framework for text generation from multimodal
inputs consisting of video plus text, speech, or audio. In order to leverage
transformer networks, which have been shown to be effective at modeling
language, each modality is first converted into a set of language embeddings by
a learnable tokenizer. This allows our approach to perform multimodal fusion in
the language space, thus eliminating the need for ad-hoc cross-modal fusion
modules. To address the non-differentiability of tokenization on continuous
inputs (e.g., video or audio), we utilize a relaxation scheme that enables
end-to-end training. Furthermore, unlike prior encoder-only models, our network
includes an autoregressive decoder to generate open-ended text from the
multimodal embeddings fused by the language encoder. This renders our approach
fully generative and makes it directly applicable to different ""video+$x$ to
text"" problems without the need to design specialized network heads for each
task. The proposed framework is not only conceptually simple but also
remarkably effective: experiments demonstrate that our approach based on a
single architecture outperforms the state-of-the-art on three video-based
text-generation tasks -- captioning, question answering and audio-visual
scene-aware dialog.",2021-01-28
"GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial
  Networks",2021-01-25 09:42:58+00:00,http://arxiv.org/abs/2101.09978v2,"Tianming Zhao, Chunyang Chen, Yuanning Liu, Xiaodong Zhu","cs.HC, cs.CV, cs.LG, cs.SE",image2text,"Graphical User Interface (GUI) is ubiquitous in almost all modern desktop
software, mobile applications, and online websites. A good GUI design is
crucial to the success of the software in the market, but designing a good GUI
which requires much innovation and creativity is difficult even to well-trained
designers. Besides, the requirement of the rapid development of GUI design also
aggravates designers' working load. So, the availability of various automated
generated GUIs can help enhance the design personalization and specialization
as they can cater to the taste of different designers. To assist designers, we
develop a model GUIGAN to automatically generate GUI designs. Different from
conventional image generation models based on image pixels, our GUIGAN is to
reuse GUI components collected from existing mobile app GUIs for composing a
new design that is similar to natural-language generation. Our GUIGAN is based
on SeqGAN by modeling the GUI component style compatibility and GUI structure.
The evaluation demonstrates that our model significantly outperforms the best
of the baseline methods by 30.77% in Frechet Inception distance (FID) and
12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we
provide initial evidence of the usefulness of our approach for generating
acceptable brand new GUI designs.",2021-01-25
"Towards Understanding How Readers Integrate Charts and Captions: A Case
  Study with Line Charts",2021-01-20 18:11:35+00:00,http://arxiv.org/abs/2101.08235v1,"Dae Hyun Kim, Vidya Setlur, Maneesh Agrawala",cs.HC,image2text,"Charts often contain visually prominent features that draw attention to
aspects of the data and include text captions that emphasize aspects of the
data. Through a crowdsourced study, we explore how readers gather takeaways
when considering charts and captions together. We first ask participants to
mark visually prominent regions in a set of line charts. We then generate text
captions based on the prominent features and ask participants to report their
takeaways after observing chart-caption pairs. We find that when both the chart
and caption describe a high-prominence feature, readers treat the doubly
emphasized high-prominence feature as the takeaway; when the caption describes
a low-prominence chart feature, readers rely on the chart and report a
higher-prominence feature as the takeaway. We also find that external
information that provides context, helps further convey the caption's message
to the reader. We use these findings to provide guidelines for authoring
effective chart-caption pairs.",2021-01-20
Narration Generation for Cartoon Videos,2021-01-17 23:23:09+00:00,http://arxiv.org/abs/2101.06803v1,"Nikos Papasarantopoulos, Shay B. Cohen",cs.CL,image2text,"Research on text generation from multimodal inputs has largely focused on
static images, and less on video data. In this paper, we propose a new task,
narration generation, that is complementing videos with narration texts that
are to be interjected in several places. The narrations are part of the video
and contribute to the storyline unfolding in it. Moreover, they are
context-informed, since they include information appropriate for the timeframe
of video they cover, and also, do not need to include every detail shown in
input scenes, as a caption would. We collect a new dataset from the animated
television series Peppa Pig. Furthermore, we formalize the task of narration
generation as including two separate tasks, timing and content generation, and
present a set of models on the new task.",2021-01-17
Zero-shot Learning by Generating Task-specific Adapters,2021-01-02 10:50:23+00:00,http://arxiv.org/abs/2101.00420v1,"Qinyuan Ye, Xiang Ren","cs.CL, cs.LG",image2text,"Pre-trained text-to-text transformers achieve impressive performance across a
wide range of NLP tasks, and they naturally support zero-shot learning (ZSL) by
using the task description as prompt in the input. However, this approach has
potential limitations, as it learns from input-output pairs at instance level,
instead of learning to solve tasks at task level. Alternatively, applying
existing ZSL methods to text-to-text transformers is non-trivial due to their
text generation objective and huge size. To address these issues, we introduce
Hypter, a framework that improves zero-shot transferability by training a
hypernetwork to generate task-specific adapters from task descriptions. This
formulation enables learning at task level, and greatly reduces the number of
parameters by using light-weight adapters. Experiments on two datasets
demonstrate Hypter improves upon fine-tuning baselines.",2021-01-02
Neural Text Generation with Artificial Negative Examples,2020-12-28 07:25:10+00:00,http://arxiv.org/abs/2012.14124v1,"Keisuke Shirai, Kazuma Hashimoto, Akiko Eriguchi, Takashi Ninomiya, Shinsuke Mori","cs.CL, cs.AI",image2text,"Neural text generation models conditioning on given input (e.g. machine
translation and image captioning) are usually trained by maximum likelihood
estimation of target text. However, the trained models suffer from various
types of errors at inference time. In this paper, we propose to suppress an
arbitrary type of errors by training the text generation model in a
reinforcement learning framework, where we use a trainable reward function that
is capable of discriminating between references and sentences containing the
targeted type of errors. We create such negative examples by artificially
injecting the targeted errors to the references. In experiments, we focus on
two error types, repeated and dropped tokens in model-generated text. The
experimental results show that our method can suppress the generation errors
and achieve significant improvements on two machine translation and two image
captioning tasks.",2020-12-28
Few-Shot Text Generation with Pattern-Exploiting Training,2020-12-22 10:53:07+00:00,http://arxiv.org/abs/2012.11926v1,"Timo Schick, Hinrich Sch√ºtze","cs.CL, cs.LG",image2text,"Providing pretrained language models with simple task descriptions or prompts
in natural language yields impressive few-shot results for a wide range of text
classification tasks when combined with gradient-based learning from examples.
In this paper, we show that the underlying idea can also be applied to text
generation tasks: We adapt Pattern-Exploiting Training (PET), a recently
proposed few-shot approach, for finetuning generative language models on text
generation tasks. On several text summarization and headline generation
datasets, our proposed variant of PET gives consistent improvements over a
strong baseline in few-shot settings.",2020-12-22
Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings,2020-12-14 10:59:59+00:00,http://arxiv.org/abs/2012.07412v2,"Qipeng Guo, Zhijing Jin, Ziyu Wang, Xipeng Qiu, Weinan Zhang, Jun Zhu, Zheng Zhang, David Wipf","cs.LG, cs.AI, cs.CL",image2text,"Cycle-consistent training is widely used for jointly learning a forward and
inverse mapping between two domains of interest without the cumbersome
requirement of collecting matched pairs within each domain. In this regard, the
implicit assumption is that there exists (at least approximately) a
ground-truth bijection such that a given input from either domain can be
accurately reconstructed from successive application of the respective
mappings. But in many applications no such bijection can be expected to exist
and large reconstruction errors can compromise the success of cycle-consistent
training. As one important instance of this limitation, we consider
practically-relevant situations where there exists a many-to-one or surjective
mapping between domains. To address this regime, we develop a conditional
variational autoencoder (CVAE) approach that can be viewed as converting
surjective mappings to implicit bijections whereby reconstruction errors in
both directions can be minimized, and as a natural byproduct, realistic output
diversity can be obtained in the one-to-many direction. As theoretical
motivation, we analyze a simplified scenario whereby minima of the proposed
CVAE-based energy function align with the recovery of ground-truth surjective
mappings. On the empirical side, we consider a synthetic image dataset with
known ground-truth, as well as a real-world application involving natural
language generation from knowledge graphs and vice versa, a prototypical
surjective case. For the latter, our CVAE pipeline can capture such many-to-one
mappings during cycle training while promoting textural diversity for
graph-to-text tasks. Our code is available at github.com/QipengGuo/CycleGT",2020-12-14
Video Generative Adversarial Networks: A Review,2020-11-04 12:16:05+00:00,http://arxiv.org/abs/2011.02250v1,"Nuha Aldausari, Arcot Sowmya, Nadine Marcus, Gelareh Mohammadi","cs.CV, cs.LG, eess.IV",image2text,"With the increasing interest in the content creation field in multiple
sectors such as media, education, and entertainment, there is an increasing
trend in the papers that uses AI algorithms to generate content such as images,
videos, audio, and text. Generative Adversarial Networks (GANs) in one of the
promising models that synthesizes data samples that are similar to real data
samples. While the variations of GANs models, in general, have been covered to
some extent in several survey papers, to the best of our knowledge, this is
among the first survey papers that reviews the state-of-the-art video GANs
models. This paper first categorized GANs review papers into general GANs
review papers, image GANs review papers, and special field GANs review papers
such as anomaly detection, medical imaging, or cybersecurity. The paper then
summarizes the main improvements in GANs frameworks that are not initially
developed for the video domain but have been adopted in multiple video GANs
variations. Then, a comprehensive review of video GANs models is provided under
two main divisions according to the presence or non-presence of a condition.
The conditional models then further grouped according to the type of condition
into audio, text, video, and image. The paper is concluded by highlighting the
main challenges and limitations of the current video GANs models. A
comprehensive list of datasets, applied loss functions, and evaluation metrics
is provided in the supplementary material.",2020-11-04
Personalized Multimodal Feedback Generation in Education,2020-10-31 05:26:49+00:00,http://arxiv.org/abs/2011.00192v1,"Haochen Liu, Zitao Liu, Zhongqin Wu, Jiliang Tang","cs.CL, cs.AI",image2text,"The automatic evaluation for school assignments is an important application
of AI in the education field. In this work, we focus on the task of
personalized multimodal feedback generation, which aims to generate
personalized feedback for various teachers to evaluate students' assignments
involving multimodal inputs such as images, audios, and texts. This task
involves the representation and fusion of multimodal information and natural
language generation, which presents the challenges from three aspects: 1) how
to encode and integrate multimodal inputs; 2) how to generate feedback specific
to each modality; and 3) how to realize personalized feedback generation. In
this paper, we propose a novel Personalized Multimodal Feedback Generation
Network (PMFGN) armed with a modality gate mechanism and a personalized bias
mechanism to address these challenges. The extensive experiments on real-world
K-12 education data show that our model significantly outperforms several
baselines by generating more accurate and diverse feedback. In addition,
detailed ablation experiments are conducted to deepen our understanding of the
proposed framework.",2020-10-31
Fusion Models for Improved Visual Captioning,2020-10-28 21:55:25+00:00,http://arxiv.org/abs/2010.15251v2,"Marimuthu Kalimuthu, Aditya Mogadala, Marius Mosbach, Dietrich Klakow","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Visual captioning aims to generate textual descriptions given images or
videos. Traditionally, image captioning models are trained on human annotated
datasets such as Flickr30k and MS-COCO, which are limited in size and
diversity. This limitation hinders the generalization capabilities of these
models while also rendering them liable to making mistakes. Language models
can, however, be trained on vast amounts of freely available unlabelled data
and have recently emerged as successful language encoders and coherent text
generators. Meanwhile, several unimodal and multimodal fusion techniques have
been proven to work well for natural language generation and automatic speech
recognition. Building on these recent developments, and with the aim of
improving the quality of generated captions, the contribution of our work in
this paper is two-fold: First, we propose a generic multimodal model fusion
framework for caption generation as well as emendation where we utilize
different fusion strategies to integrate a pretrained Auxiliary Language Model
(AuxLM) within the traditional encoder-decoder visual captioning frameworks.
Next, we employ the same fusion strategies to integrate a pretrained Masked
Language Model (MLM), namely BERT, with a visual captioning model, viz. Show,
Attend, and Tell, for emending both syntactic and semantic errors in captions.
Our caption emendation experiments on three benchmark image captioning
datasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the
baseline, indicating the usefulness of our proposed multimodal fusion
strategies. Further, we perform a preliminary qualitative analysis on the
emended captions and identify error categories based on the type of
corrections.",2020-10-28
Safe Handover in Mixed-Initiative Control for Cyber-Physical Systems,2020-10-21 12:59:32+00:00,http://arxiv.org/abs/2010.10967v1,"Frederik Wiehr, Anke Hirsch, Florian Daiber, Antonio Kruger, Alisa Kovtunova, Stefan Borgwardt, Ernie Chang, Vera Demberg, Marcel Steinmetz, Hoffmann Jorg",cs.HC,image2text,"For mixed-initiative control between cyber-physical systems (CPS) and its
users, it is still an open question how machines can safely hand over control
to humans. In this work, we propose a concept to provide technological support
that uses formal methods from AI -- description logic (DL) and automated
planning -- to predict more reliably when a hand-over is necessary, and to
increase the advance notice for handovers by planning ahead of runtime. We
combine this with methods from human-computer interaction (HCI) and natural
language generation (NLG) to develop solutions for safe and smooth handovers
and provide an example autonomous driving scenario. A study design is proposed
with the assessment of qualitative feedback, cognitive load and trust in
automation.",2020-10-21
"Improving Factual Completeness and Consistency of Image-to-Text
  Radiology Report Generation",2020-10-20 05:42:47+00:00,http://arxiv.org/abs/2010.10042v1,"Yasuhide Miura, Yuhao Zhang, Curtis P. Langlotz, Dan Jurafsky",cs.CL,image2text,"Neural image-to-text radiology report generation systems offer the potential
to accelerate clinical processes by saving radiologists from the repetitive
labor of drafting radiology reports and preventing medical errors. However,
existing report generation systems, despite achieving high performances on
natural language generation metrics such as CIDEr or BLEU, still suffer from
incomplete and inconsistent generations, rendering these systems unusable in
practice. In this work, we aim to overcome this problem by proposing two new
metrics that encourage the factual completeness and consistency of generated
radiology reports. The first metric, the Exact Entity Match score, evaluates a
generation by its coverage of radiology domain entities against the references.
The second metric, the Entailing Entity Match score, augments the first metric
by introducing a natural language inference model into the entity match process
to encourage consistent generations that can be entailed from the references.
To achieve this, we also developed an in-domain NLI model via weak supervision
to improve its performance on radiology text. We further propose a report
generation system that optimizes these two new metrics via reinforcement
learning. On two open radiology report datasets, our system not only achieves
the best performance on these two metrics compared to baselines, but also leads
to as much as +2.0 improvement on the F1 score of a clinical finding metric. We
show via analysis and examples that our system leads to generations that are
more complete and consistent compared to the baselines.",2020-10-20
"RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich
  Semantic Annotations for Task-Oriented Dialogue Modeling",2020-10-17 08:18:59+00:00,http://arxiv.org/abs/2010.08738v1,"Jun Quan, Shian Zhang, Qian Cao, Zizhong Li, Deyi Xiong",cs.CL,image2text,"In order to alleviate the shortage of multi-domain data and to capture
discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a
large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic
Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn
semantically annotated dialogues, with more than 150K utterances spanning over
12 domains, which is larger than all previous annotated H2H conversational
datasets. Both single- and multi-domain dialogues are constructed, accounting
for 65% and 35%, respectively. Each dialogue is labeled with comprehensive
dialogue annotations, including dialogue goal in the form of natural language
description, domain, dialogue states and acts at both the user and system side.
In addition to traditional dialogue annotations, we especially provide
linguistic annotations on discourse phenomena, e.g., ellipsis and coreference,
in dialogues, which are useful for dialogue coreference and ellipsis resolution
tasks. Apart from the fully annotated dataset, we also present a detailed
description of the data collection procedure, statistics and analysis of the
dataset. A series of benchmark models and results are reported, including
natural language understanding (intent detection & slot filling), dialogue
state tracking and dialogue context-to-text generation, as well as coreference
and ellipsis resolution, which facilitate the baseline comparison for future
research on this corpus.",2020-10-17
Dissecting the components and factors of Neural Text Generation,2020-10-14 17:54:42+00:00,http://arxiv.org/abs/2010.07279v1,"Khyathi Raghavi Chandu, Alan W Black",cs.CL,image2text,"Neural text generation metamorphosed into several critical natural language
applications ranging from text completion to free form narrative generation.
Generating natural language has fundamentally been a human attribute and the
advent of ubiquitous NLP applications and virtual agents marks the need to
impart this skill to machines. There has been a colossal research effort in
various frontiers of neural text generation including machine translation,
summarization, image captioning, storytelling etc., We believe that this is an
excellent juncture to retrospect on the directions of the field. Specifically,
this paper surveys the fundamental factors and components relaying task
agnostic impacts across various generation tasks such as storytelling,
summarization, translation etc., In specific, we present an abstraction of the
imperative techniques with respect to learning paradigms, pretraining, modeling
approaches, decoding and the key challenges. Thereby, we hope to deliver a
one-stop destination for researchers in the field to facilitate a perspective
on where to situate their work and how it impacts other closely related tasks.",2020-10-14
"Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on
  Chest X-rays",2020-10-06 04:18:18+00:00,http://arxiv.org/abs/2010.02467v1,"Jianmo Ni, Chun-Nan Hsu, Amilcare Gentili, Julian McAuley","cs.CV, cs.CL",image2text,"Automatic medical image report generation has drawn growing attention due to
its potential to alleviate radiologists' workload. Existing work on report
generation often trains encoder-decoder networks to generate complete reports.
However, such models are affected by data bias (e.g.~label imbalance) and face
common issues inherent in text generation models (e.g.~repetition). In this
work, we focus on reporting abnormal findings on radiology images; instead of
training on complete radiology reports, we propose a method to identify
abnormal findings from the reports in addition to grouping them with
unsupervised clustering and minimal rules. We formulate the task as cross-modal
retrieval and propose Conditional Visual-Semantic Embeddings to align images
and fine-grained abnormal findings in a joint embedding space. We demonstrate
that our method is able to retrieve abnormal findings and outperforms existing
generation models on both clinical correctness and text generation metrics.",2020-10-06
"Knowledge-Enhanced Personalized Review Generation with Capsule Graph
  Neural Network",2020-10-04 03:54:40+00:00,http://arxiv.org/abs/2010.01480v1,"Junyi Li, Siqing Li, Wayne Xin Zhao, Gaole He, Zhicheng Wei, Nicholas Jing Yuan, Ji-Rong Wen","cs.CL, cs.AI",image2text,"Personalized review generation (PRG) aims to automatically produce review
text reflecting user preference, which is a challenging natural language
generation task. Most of previous studies do not explicitly model factual
description of products, tending to generate uninformative content. Moreover,
they mainly focus on word-level generation, but cannot accurately reflect more
abstractive user preference in multiple aspects. To address the above issues,
we propose a novel knowledge-enhanced PRG model based on capsule graph neural
network~(Caps-GNN). We first construct a heterogeneous knowledge graph (HKG)
for utilizing rich item attributes. We adopt Caps-GNN to learn graph capsules
for encoding underlying characteristics from the HKG. Our generation process
contains two major steps, namely aspect sequence generation and sentence
generation. First, based on graph capsules, we adaptively learn aspect capsules
for inferring the aspect sequence. Then, conditioned on the inferred aspect
label, we design a graph-based copy mechanism to generate sentences by
incorporating related entities or words from HKG. To our knowledge, we are the
first to utilize knowledge graph for the PRG task. The incorporated KG
information is able to enhance user preference at both aspect and word levels.
Extensive experiments on three real-world datasets have demonstrated the
effectiveness of our model on the PRG task.",2020-10-04
