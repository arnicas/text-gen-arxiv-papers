title,pubdate,id,authors,categories,search,abstract,displaydate
"Woodpecker: Hallucination Correction for Multimodal Large Language
  Models",2023-10-24 17:58:07+00:00,http://arxiv.org/abs/2310.16045v1,"Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, Enhong Chen","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Hallucination is a big shadow hanging over the rapidly evolving Multimodal
Large Language Models (MLLMs), referring to the phenomenon that the generated
text is inconsistent with the image content. In order to mitigate
hallucinations, existing studies mainly resort to an instruction-tuning manner
that requires retraining the models with specific data. In this paper, we pave
a different way, introducing a training-free method named Woodpecker. Like a
woodpecker heals trees, it picks out and corrects hallucinations from the
generated text. Concretely, Woodpecker consists of five stages: key concept
extraction, question formulation, visual knowledge validation, visual claim
generation, and hallucination correction. Implemented in a post-remedy manner,
Woodpecker can easily serve different MLLMs, while being interpretable by
accessing intermediate outputs of the five stages. We evaluate Woodpecker both
quantitatively and qualitatively and show the huge potential of this new
paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement
in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released
at https://github.com/BradyFU/Woodpecker.",2023-10-24
GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions,2023-10-23 23:24:57+00:00,http://arxiv.org/abs/2310.15405v1,"Ting-Yao Hsu, Chieh-Yang Huang, Ryan Rossi, Sungchul Kim, C. Lee Giles, Ting-Hao K. Huang",cs.CL,image2text,"There is growing interest in systems that generate captions for scientific
figures. However, assessing these systems output poses a significant challenge.
Human evaluation requires academic expertise and is costly, while automatic
evaluation depends on often low-quality author-written captions. This paper
investigates using large language models (LLMs) as a cost-effective,
reference-free method for evaluating figure captions. We first constructed
SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600
scientific figure captions, both original and machine-made, for 600 arXiv
figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption
based on its potential to aid reader understanding, given relevant context such
as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot
evaluator, outperformed all other models and even surpassed assessments made by
Computer Science and Informatics undergraduates, achieving a Kendall
correlation score of 0.401 with Ph.D. students rankings",2023-10-23
"HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online
  Posts using Large Language Models",2023-10-21 12:18:29+00:00,http://arxiv.org/abs/2310.13985v1,"Vibhor Agarwal, Yu Chen, Nishanth Sastry",cs.CL,image2text,"Hate speech has become pervasive in today's digital age. Although there has
been considerable research to detect hate speech or generate counter speech to
combat hateful views, these approaches still cannot completely eliminate the
potential harmful societal consequences of hate speech -- hate speech, even
when detected, can often not be taken down or is often not taken down enough;
and hate speech unfortunately spreads quickly, often much faster than any
generated counter speech.
  This paper investigates a relatively new yet simple and effective approach of
suggesting a rephrasing of potential hate speech content even before the post
is made. We show that Large Language Models (LLMs) perform well on this task,
outperforming state-of-the-art baselines such as BART-Detox. We develop 4
different prompts based on task description, hate definition, few-shot
demonstrations and chain-of-thoughts for comprehensive experiments and conduct
experiments on open-source LLMs such as LLaMA-1, LLaMA-2 chat, Vicuna as well
as OpenAI's GPT-3.5. We propose various evaluation metrics to measure the
efficacy of the generated text and ensure the generated text has reduced hate
intensity without drastically changing the semantic meaning of the original
text.
  We find that LLMs with a few-shot demonstrations prompt work the best in
generating acceptable hate-rephrased text with semantic meaning similar to the
original text. Overall, we find that GPT-3.5 outperforms the baseline and
open-source models for all the different kinds of prompts. We also perform
human evaluations and interestingly, find that the rephrasings generated by
GPT-3.5 outperform even the human-generated ground-truth rephrasings in the
dataset. We also conduct detailed ablation studies to investigate why LLMs work
satisfactorily on this task and conduct a failure analysis to understand the
gaps.",2023-10-21
"RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question
  Answering",2023-10-19 19:32:27+00:00,http://arxiv.org/abs/2310.13120v1,"Yuduo Wang, Pedram Ghamisi","cs.CV, cs.LG",image2text,"In recent years, with the rapid advancement of transformer models,
transformer-based multimodal architectures have found wide application in
various downstream tasks, including but not limited to Image Captioning, Visual
Question Answering (VQA), and Image-Text Generation. However, contemporary
approaches to Remote Sensing (RS) VQA often involve resource-intensive
techniques, such as full fine-tuning of large models or the extraction of
image-text features from pre-trained multimodal models, followed by modality
fusion using decoders. These approaches demand significant computational
resources and time, and a considerable number of trainable parameters are
introduced. To address these challenges, we introduce a novel method known as
RSAdapter, which prioritizes runtime and parameter efficiency. RSAdapter
comprises two key components: the Parallel Adapter and an additional linear
transformation layer inserted after each fully connected (FC) layer within the
Adapter. This approach not only improves adaptation to pre-trained multimodal
models but also allows the parameters of the linear transformation layer to be
integrated into the preceding FC layers during inference, reducing inference
costs. To demonstrate the effectiveness of RSAdapter, we conduct an extensive
series of experiments using three distinct RS-VQA datasets and achieve
state-of-the-art results on all three datasets. The code for RSAdapter will be
available online at https://github.com/Y-D-Wang/RSAdapter.",2023-10-19
"MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and
  Uni-Modal Adapter",2023-10-19 14:52:58+00:00,http://arxiv.org/abs/2310.12798v1,"Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, Tat-Seng Chua","cs.CL, cs.MM",image2text,"Language Models (LMs) have demonstrated impressive molecule understanding
ability on various 1D text-related tasks. However, they inherently lack 2D
graph perception - a critical ability of human professionals in comprehending
molecules' topological structures. To bridge this gap, we propose MolCA:
Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal
Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and
graph-based molecular contents via the cross-modal projector. Specifically, the
cross-modal projector is implemented as a Q-Former to connect a graph encoder's
representation space and an LM's text space. Further, MolCA employs a uni-modal
adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks.
Unlike previous studies that couple an LM with a graph encoder via cross-modal
contrastive learning, MolCA retains the LM's ability of open-ended text
generation and augments it with 2D graph information. To showcase its
effectiveness, we extensively benchmark MolCA on tasks of molecule captioning,
IUPAC name prediction, and molecule-text retrieval, on which MolCA
significantly outperforms the baselines. Our codes and checkpoints can be found
at https://github.com/acharkq/MolCA.",2023-10-19
"Motion2Language, Unsupervised learning of synchronized semantic motion
  segmentation",2023-10-16 17:16:32+00:00,http://arxiv.org/abs/2310.10594v1,"Karim Radouane, Andon Tchechmedjiev, Sylvie Ranwez, Julien Lagarde","cs.CV, cs.CL",image2text,"In this paper, we investigate building a sequence to sequence architecture
for motion to language translation and synchronization. The aim is to translate
motion capture inputs into English natural-language descriptions, such that the
descriptions are generated synchronously with the actions performed, enabling
semantic segmentation as a byproduct, but without requiring synchronized
training data. We propose a new recurrent formulation of local attention that
is suited for synchronous/live text generation, as well as an improved motion
encoder architecture better suited to smaller data and for synchronous
generation. We evaluate both contributions in individual experiments, using the
standard BLEU4 metric, as well as a simple semantic equivalence measure, on the
KIT motion language dataset. In a follow-up experiment, we assess the quality
of the synchronization of generated text in our proposed approaches through
multiple evaluation metrics. We find that both contributions to the attention
mechanism and the encoder architecture additively improve the quality of
generated text (BLEU and semantic equivalence), but also of synchronization.
Our code will be made available at
\url{https://github.com/rd20karim/M2T-Segmentation/tree/main}",2023-10-16
"BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools
  for Video-based Texts Generation",2023-10-16 17:05:56+00:00,http://arxiv.org/abs/2310.10586v1,"Ji Qi, Kaixuan Ji, Jifan Yu, Duokang Wang, Bin Xu, Lei Hou, Juanzi Li","cs.CV, cs.CL",image2text,"Building models that generate textual responses to user instructions for
videos is a practical and challenging topic, as it requires both vision
understanding and knowledge reasoning. Compared to language and image
modalities, training efficiency remains a serious problem as existing studies
train models on massive sparse videos aligned with brief descriptions. In this
paper, we introduce BiLL-VTG, a fast adaptive framework that leverages large
language models (LLMs) to reasoning on videos based on essential lightweight
visual tools. Specifically, we reveal the key to response specific instructions
is the concentration on relevant video events, and utilize two visual tools of
structured scene graph generation and descriptive image caption generation to
gather and represent the events information. Thus, a LLM equipped with world
knowledge is adopted as the reasoning agent to achieve the response by
performing multiple reasoning steps on specified video events.To address the
difficulty of specifying events from agent, we further propose an
Instruction-oriented Video Events Recognition (InsOVER) algorithm based on the
efficient Hungarian matching to localize corresponding video events using
linguistic instructions, enabling LLMs to interact with long videos. Extensive
experiments on two typical video-based texts generations tasks show that our
tuning-free framework outperforms the pre-trained models including
Flamingo-80B, to achieve the state-of-the-art performance.",2023-10-16
"Prompting for Discovery: Flexible Sense-Making for AI Art-Making with
  Dreamsheets",2023-10-15 23:54:20+00:00,http://arxiv.org/abs/2310.09985v1,"Shm Garanganao Almeda, J. D. Zamfirescu-Pereira, Kyu Won Kim, Pradeep Mani Rathnam, Bjoern Hartmann",cs.HC,image2text,"Design space exploration (DSE) for Text-to-Image (TTI) models entails
navigating a vast, opaque space of possible image outputs, through a
commensurately vast input space of hyperparameters and prompt text. Minor
adjustments to prompt input can surface unexpectedly disparate images. How can
interfaces support end-users in reliably steering prompt-space explorations
towards interesting results? Our design probe, DreamSheets, supports
exploration strategies with LLM-based functions for assisted prompt
construction and simultaneous display of generated results, hosted in a
spreadsheet interface. The flexible layout and novel generative functions
enable experimentation with user-defined workflows. Two studies, a preliminary
lab study and a longitudinal study with five expert artists, revealed a set of
strategies participants use to tackle the challenges of TTI design space
exploration, and the interface features required to support them - like using
text-generation to define local ""axes"" of exploration. We distill these
insights into a UI mockup to guide future interfaces.",2023-10-15
VLIS: Unimodal Language Models Guide Multimodal Language Generation,2023-10-15 07:58:52+00:00,http://arxiv.org/abs/2310.09767v1,"Jiwan Chung, Youngjae Yu","cs.CL, cs.AI",image2text,"Multimodal language generation, which leverages the synergy of language and
vision, is a rapidly expanding field. However, existing vision-language models
face challenges in tasks that require complex linguistic understanding. To
address this issue, we introduce Visual-Language models as Importance Sampling
weights (VLIS), a novel framework that combines the visual conditioning
capability of vision-language models with the language understanding of
unimodal text-only language models without further training. It extracts
pointwise mutual information of each image and text from a visual-language
model and uses the value as an importance sampling weight to adjust the token
likelihood from a text-only model. VLIS improves vision-language models on
diverse tasks, including commonsense understanding (WHOOPS, OK-VQA, and
ScienceQA) and complex text generation (Concadia, Image Paragraph Captioning,
and ROCStories). Our results suggest that VLIS represents a promising new
direction for multimodal language generation.",2023-10-15
"GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language
  Models",2023-10-12 16:46:58+00:00,http://arxiv.org/abs/2310.08487v1,"Yuanchun Shen, Ruotong Liao, Zhen Han, Yunpu Ma, Volker Tresp",cs.CL,image2text,"While multi-modal models have successfully integrated information from image,
video, and audio modalities, integrating graph modality into large language
models (LLMs) remains unexplored. This discrepancy largely stems from the
inherent divergence between structured graph data and unstructured text data.
Incorporating graph knowledge provides a reliable source of information,
enabling potential solutions to address issues in text generation, e.g.,
hallucination, and lack of domain knowledge. To evaluate the integration of
graph knowledge into language models, a dedicated dataset is needed. However,
there is currently no benchmark dataset specifically designed for multimodal
graph-language models. To address this gap, we propose GraphextQA, a question
answering dataset with paired subgraphs, retrieved from Wikidata, to facilitate
the evaluation and future development of graph-language models. Additionally,
we introduce a baseline model called CrossGNN, which conditions answer
generation on the paired graphs by cross-attending question-aware graph
features at decoding. The proposed dataset is designed to evaluate
graph-language models' ability to understand graphs and make use of it for
answer generation. We perform experiments with language-only models and the
proposed graph-language model to validate the usefulness of the paired graphs
and to demonstrate the difficulty of the task.",2023-10-12
"CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large
  Language Models",2023-10-12 12:31:23+00:00,http://arxiv.org/abs/2310.08279v1,"Rui Yang, Li Fang, Yi Zhou","cs.CL, cs.AI",image2text,"Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce
and infer missing connections within knowledge graphs. Text-based approaches,
like SimKGC, have outperformed graph embedding methods, showcasing the promise
of inductive KGC. However, the efficacy of text-based methods hinges on the
quality of entity textual descriptions. In this paper, we identify the key
issue of whether large language models (LLMs) can generate effective text. To
mitigate hallucination in LLM-generated text in this paper, we introduce a
constraint-based prompt that utilizes the entity and its textual description as
contextual constraints to enhance data quality. Our Constrained-Prompt
Knowledge Graph Completion (CP-KGC) method demonstrates effective inference
under low resource computing conditions and surpasses prior results on the
WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC
tasks and provides new directions for future research.",2023-10-12
"Ziya-VL: Bilingual Large Vision-Language Model via Multi-Task
  Instruction Tuning",2023-10-12 09:39:17+00:00,http://arxiv.org/abs/2310.08166v1,"Junyu Lu, Dixiang Zhang, Xiaojun Wu, Xinyu Gao, Ruyi Gan, Jiaxing Zhang, Yan Song, Pingjian Zhang",cs.CL,image2text,"Recent advancements enlarge the capabilities of large language models (LLMs)
in zero-shot image-to-text generation and understanding by integrating
multi-modal inputs. However, such success is typically limited to English
scenarios due to the lack of large-scale and high-quality non-English
multi-modal resources, making it extremely difficult to establish competitive
counterparts in other languages. In this paper, we introduce the Ziya-VL
series, a set of bilingual large-scale vision-language models (LVLMs) designed
to incorporate visual semantics into LLM for multi-modal dialogue. Composed of
Ziya-VL-Base and Ziya-VL-Chat, our models adopt the Querying Transformer from
BLIP-2, further exploring the assistance of optimization schemes such as
instruction tuning, multi-stage training and low-rank adaptation module for
visual-language alignment. In addition, we stimulate the understanding ability
of GPT-4 in multi-modal scenarios, translating our gathered English image-text
datasets into Chinese and generating instruction-response through the
in-context learning method. The experiment results demonstrate that compared to
the existing LVLMs, Ziya-VL achieves competitive performance across a wide
range of English-only tasks including zero-shot image-text retrieval, image
captioning, and visual question answering. The evaluation leaderboard accessed
by GPT-4 also indicates that our models possess satisfactory image-text
understanding and generation capabilities in Chinese multi-modal scenario
dialogues. Code, demo and models are available at
~\url{https://huggingface.co/IDEA-CCNL/Ziya-BLIP2-14B-Visual-v1}.",2023-10-12
Multimodal Graph Learning for Generative Tasks,2023-10-11 13:25:03+00:00,http://arxiv.org/abs/2310.07478v2,"Minji Yoon, Jing Yu Koh, Bryan Hooi, Ruslan Salakhutdinov",cs.AI,image2text,"Multimodal learning combines multiple data modalities, broadening the types
and complexity of data our models can utilize: for example, from plain text to
image-caption pairs. Most multimodal learning algorithms focus on modeling
simple one-to-one pairs of data from two modalities, such as image-caption
pairs, or audio-text pairs. However, in most real-world settings, entities of
different modalities interact with each other in more complex and multifaceted
ways, going beyond one-to-one mappings. We propose to represent these complex
relationships as graphs, allowing us to capture data with any number of
modalities, and with complex relationships between modalities that can flexibly
vary from one sample to another. Toward this goal, we propose Multimodal Graph
Learning (MMGL), a general and systematic framework for capturing information
from multiple multimodal neighbors with relational structures among them. In
particular, we focus on MMGL for generative tasks, building upon pretrained
Language Models (LMs), aiming to augment their text generation with multimodal
neighbor contexts. We study three research questions raised by MMGL: (1) how
can we infuse multiple neighbor information into the pretrained LMs, while
avoiding scalability issues? (2) how can we infuse the graph structure
information among multimodal neighbors into the LMs? and (3) how can we
finetune the pretrained LMs to learn from the neighbor context in a
parameter-efficient manner? We conduct extensive experiments to answer these
three questions on MMGL and analyze the empirical results to pave the way for
future MMGL research.",2023-10-11
Video-CSR: Complex Video Digest Creation for Visual-Language Models,2023-10-08 08:02:43+00:00,http://arxiv.org/abs/2310.05060v1,"Tingkai Liu, Yunzhe Tao, Haogeng Liu, Qihang Fan, Ding Zhou, Huaibo Huang, Ran He, Hongxia Yang","cs.CV, cs.AI",image2text,"We present a novel task and human annotated dataset for evaluating the
ability for visual-language models to generate captions and summaries for
real-world video clips, which we call Video-CSR (Captioning, Summarization and
Retrieval). The dataset contains 4.8K YouTube video clips of 20-60 seconds in
duration and covers a wide range of topics and interests. Each video clip
corresponds to 5 independently annotated captions (1 sentence) and summaries
(3-10 sentences). Given any video selected from the dataset and its
corresponding ASR information, we evaluate visual-language models on either
caption or summary generation that is grounded in both the visual and auditory
content of the video. Additionally, models are also evaluated on caption- and
summary-based retrieval tasks, where the summary-based retrieval task requires
the identification of a target video given excerpts of a corresponding summary.
Given the novel nature of the paragraph-length video summarization task, we
perform extensive comparative analyses of different existing evaluation metrics
and their alignment with human preferences. Finally, we propose a foundation
model with competitive generation and retrieval capabilities that serves as a
baseline for the Video-CSR task. We aim for Video-CSR to serve as a useful
evaluation set in the age of large language models and complex multi-modal
tasks.",2023-10-08
"InstructProtein: Aligning Human and Protein Language via Knowledge
  Instruction",2023-10-05 02:45:39+00:00,http://arxiv.org/abs/2310.03269v1,"Zeyuan Wang, Qiang Zhang, Keyan Ding, Ming Qin, Xiang Zhuang, Xiaotong Li, Huajun Chen","q-bio.BM, cs.CL",image2text,"Large Language Models (LLMs) have revolutionized the field of natural
language processing, but they fall short in comprehending biological sequences
such as proteins. To address this challenge, we propose InstructProtein, an
innovative LLM that possesses bidirectional generation capabilities in both
human and protein languages: (i) taking a protein sequence as input to predict
its textual function description and (ii) using natural language to prompt
protein sequence generation. To achieve this, we first pre-train an LLM on both
protein and natural language corpora, enabling it to comprehend individual
languages. Then supervised instruction tuning is employed to facilitate the
alignment of these two distinct languages. Herein, we introduce a knowledge
graph-based instruction generation framework to construct a high-quality
instruction dataset, addressing annotation imbalance and instruction deficits
in existing protein-text corpus. In particular, the instructions inherit the
structural relations between proteins and function annotations in knowledge
graphs, which empowers our model to engage in the causal modeling of protein
functions, akin to the chain-of-thought processes in natural languages.
Extensive experiments on bidirectional protein-text generation tasks show that
InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,
InstructProtein serves as a pioneering step towards text-based protein function
prediction and sequence design, effectively bridging the gap between protein
and human language understanding.",2023-10-05
"Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image
  Captioning",2023-09-10 08:55:24+00:00,http://arxiv.org/abs/2309.04965v1,"Guisheng Liu, Yi Li, Zhengcong Fei, Haiyan Fu, Xiangyang Luo, Yanqing Guo","cs.CV, cs.AI, cs.CL",image2text,"While impressive performance has been achieved in image captioning, the
limited diversity of the generated captions and the large parameter scale
remain major barriers to the real-word application of these systems. In this
work, we propose a lightweight image captioning network in combination with
continuous diffusion, called Prefix-diffusion. To achieve diversity, we design
an efficient method that injects prefix image embeddings into the denoising
process of the diffusion model. In order to reduce trainable parameters, we
employ a pre-trained model to extract image features and further design an
extra mapping network. Prefix-diffusion is able to generate diverse captions
with relatively less parameters, while maintaining the fluency and relevance of
the captions benefiting from the generative capabilities of the diffusion
model. Our work paves the way for scaling up diffusion models for image
captioning, and achieves promising performance compared with recent approaches.",2023-09-10
Zero-Shot Audio Captioning via Audibility Guidance,2023-09-07 17:45:58+00:00,http://arxiv.org/abs/2309.03884v1,"Tal Shaharabany, Ariel Shaulov, Lior Wolf","cs.SD, cs.CL, eess.AS",image2text,"The task of audio captioning is similar in essence to tasks such as image and
video captioning. However, it has received much less attention. We propose
three desiderata for captioning audio -- (i) fluency of the generated text,
(ii) faithfulness of the generated text to the input audio, and the somewhat
related (iii) audibility, which is the quality of being able to be perceived
based only on audio. Our method is a zero-shot method, i.e., we do not learn to
perform captioning. Instead, captioning occurs as an inference process that
involves three networks that correspond to the three desired qualities: (i) A
Large Language Model, in our case, for reasons of convenience, GPT-2, (ii) A
model that provides a matching score between an audio file and a text, for
which we use a multimodal matching network called ImageBind, and (iii) A text
classifier, trained using a dataset we collected automatically by instructing
GPT-4 with prompts designed to direct the generation of both audible and
inaudible sentences. We present our results on the AudioCap dataset,
demonstrating that audibility guidance significantly enhances performance
compared to the baseline, which lacks this objective.",2023-09-07
"Parameter Efficient Audio Captioning With Faithful Guidance Using
  Audio-text Shared Latent Representation",2023-09-06 19:42:52+00:00,http://arxiv.org/abs/2309.03340v1,"Arvind Krishna Sridhar, Yinyi Guo, Erik Visser, Rehana Mahfuz","cs.CL, cs.MM, cs.SD",image2text,"There has been significant research on developing pretrained transformer
architectures for multimodal-to-text generation tasks. Albeit performance
improvements, such models are frequently overparameterized, hence suffer from
hallucination and large memory footprint making them challenging to deploy on
edge devices. In this paper, we address both these issues for the application
of automated audio captioning. First, we propose a data augmentation technique
for generating hallucinated audio captions and show that similarity based on an
audio-text shared latent space is suitable for detecting hallucination. Then,
we propose a parameter efficient inference time faithful decoding algorithm
that enables smaller audio captioning models with performance equivalent to
larger models trained with more data. During the beam decoding step, the
smaller model utilizes an audio-text shared latent representation to
semantically align the generated text with corresponding input audio. Faithful
guidance is introduced into the beam probability by incorporating the cosine
similarity between latent representation projections of greedy rolled out
intermediate beams and audio clip. We show the efficacy of our algorithm on
benchmark datasets and evaluate the proposed scheme against baselines using
conventional audio captioning and semantic similarity metrics while
illustrating tradeoffs between performance and complexity.",2023-09-06
"Generative AI-aided Joint Training-free Secure Semantic Communications
  via Multi-modal Prompts",2023-09-05 23:24:56+00:00,http://arxiv.org/abs/2309.02616v1,"Hongyang Du, Guangyuan Liu, Dusit Niyato, Jiayi Zhang, Jiawen Kang, Zehui Xiong, Bo Ai, Dong In Kim","eess.IV, cs.LG, cs.NI",image2text,"Semantic communication (SemCom) holds promise for reducing network resource
consumption while achieving the communications goal. However, the computational
overheads in jointly training semantic encoders and decoders-and the subsequent
deployment in network devices-are overlooked. Recent advances in Generative
artificial intelligence (GAI) offer a potential solution. The robust learning
abilities of GAI models indicate that semantic decoders can reconstruct source
messages using a limited amount of semantic information, e.g., prompts, without
joint training with the semantic encoder. A notable challenge, however, is the
instability introduced by GAI's diverse generation ability. This instability,
evident in outputs like text-generated images, limits the direct application of
GAI in scenarios demanding accurate message recovery, such as face image
transmission. To solve the above problems, this paper proposes a GAI-aided
SemCom system with multi-model prompts for accurate content decoding. Moreover,
in response to security concerns, we introduce the application of covert
communications aided by a friendly jammer. The system jointly optimizes the
diffusion step, jamming, and transmitting power with the aid of the generative
diffusion models, enabling successful and secure transmission of the source
messages.",2023-09-05
"Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction
  Tuning",2023-09-05 21:27:27+00:00,http://arxiv.org/abs/2309.02591v1,"Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, Armen Aghajanyan","cs.LG, cs.CL, cs.CV",image2text,"We present CM3Leon (pronounced ""Chameleon""), a retrieval-augmented,
token-based, decoder-only multi-modal language model capable of generating and
infilling both text and images. CM3Leon uses the CM3 multi-modal architecture
but additionally shows the extreme benefits of scaling up and tuning on more
diverse instruction-style data. It is the first multi-modal model trained with
a recipe adapted from text-only language models, including a large-scale
retrieval-augmented pre-training stage and a second multi-task supervised
fine-tuning (SFT) stage. It is also a general-purpose model that can do both
text-to-image and image-to-text generation, allowing us to introduce
self-contained contrastive decoding methods that produce high-quality outputs.
Extensive experiments demonstrate that this recipe is highly effective for
multi-modal models. CM3Leon achieves state-of-the-art performance in
text-to-image generation with 5x less training compute than comparable methods
(zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon can also demonstrate
unprecedented levels of controllability in tasks ranging from language-guided
image editing to image-controlled generation and segmentation.",2023-09-05
"Breaking Barriers to Creative Expression: Co-Designing and Implementing
  an Accessible Text-to-Image Interface",2023-09-05 17:31:10+00:00,http://arxiv.org/abs/2309.02402v1,"Atieh Taheri, Mohammad Izadi, Gururaj Shriram, Negar Rostamzadeh, Shaun Kane","cs.HC, J.5; J.6; I.2.7",image2text,"Text-to-image generation models have grown in popularity due to their ability
to produce high-quality images from a text prompt. One use for this technology
is to enable the creation of more accessible art creation software. In this
paper, we document the development of an alternative user interface that
reduces the typing effort needed to enter image prompts by providing
suggestions from a large language model, developed through iterative design and
testing within the project team. The results of this testing demonstrate how
generative text models can support the accessibility of text-to-image models,
enabling users with a range of abilities to create visual art.",2023-09-05
PromptTTS 2: Describing and Generating Voices with Text Prompt,2023-09-05 14:45:27+00:00,http://arxiv.org/abs/2309.02285v1,"Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao, Tao Qin, Jiang Bian","eess.AS, cs.CL, cs.LG, cs.SD",image2text,"Speech conveys more information than just text, as the same word can be
uttered in various voices to convey diverse information. Compared to
traditional text-to-speech (TTS) methods relying on speech prompts (reference
speech) for voice variability, using text prompts (descriptions) is more
user-friendly since speech prompts can be hard to find or may not exist at all.
TTS approaches based on the text prompt face two challenges: 1) the one-to-many
problem, where not all details about voice variability can be described in the
text prompt, and 2) the limited availability of text prompt datasets, where
vendors and large cost of data labeling are required to write text prompt for
speech. In this work, we introduce PromptTTS 2 to address these challenges with
a variation network to provide variability information of voice not captured by
text prompts, and a prompt generation pipeline to utilize the large language
models (LLM) to compose high quality text prompts. Specifically, the variation
network predicts the representation extracted from the reference speech (which
contains full information about voice) based on the text prompt representation.
For the prompt generation pipeline, it generates text prompts for speech with a
speech understanding model to recognize voice attributes (e.g., gender, speed)
from speech and a large language model to formulate text prompt based on the
recognition results. Experiments on a large-scale (44K hours) speech dataset
demonstrate that compared to the previous works, PromptTTS 2 generates voices
more consistent with text prompts and supports the sampling of diverse voice
variability, thereby offering users more choices on voice generation.
Additionally, the prompt generation pipeline produces high-quality prompts,
eliminating the large labeling cost. The demo page of PromptTTS 2 is available
online\footnote{https://speechresearch.github.io/prompttts2}.",2023-09-05
"Towards Vision-Language Mechanistic Interpretability: A Causal Tracing
  Tool for BLIP",2023-08-27 18:46:47+00:00,http://arxiv.org/abs/2308.14179v1,"Vedant Palit, Rohan Pandey, Aryaman Arora, Paul Pu Liang","cs.CL, cs.AI, cs.CV",image2text,"Mechanistic interpretability seeks to understand the neural mechanisms that
enable specific behaviors in Large Language Models (LLMs) by leveraging
causality-based methods. While these approaches have identified neural circuits
that copy spans of text, capture factual knowledge, and more, they remain
unusable for multimodal models since adapting these tools to the
vision-language domain requires considerable architectural changes. In this
work, we adapt a unimodal causal tracing tool to BLIP to enable the study of
the neural mechanisms underlying image-conditioned text generation. We
demonstrate our approach on a visual question answering dataset, highlighting
the causal relevance of later layer representations for all tokens.
Furthermore, we release our BLIP causal tracing tool as open source to enable
further experimentation in vision-language mechanistic interpretability by the
community. Our code is available at
https://github.com/vedantpalit/Towards-Vision-Language-Mechanistic-Interpretability.",2023-08-27
"GeoExplainer: A Visual Analytics Framework for Spatial Modeling
  Contextualization and Report Generation",2023-08-25 16:55:33+00:00,http://arxiv.org/abs/2308.13588v1,"Fan Lei, Yuxin Ma, Stewart Fotheringham, Elizabeth Mack, Ziqi Li, Mehak Sachdeva, Sarah Bardin, Ross Maciejewski","cs.HC, cs.LG",image2text,"Geographic regression models of various descriptions are often applied to
identify patterns and anomalies in the determinants of spatially distributed
observations. These types of analyses focus on answering why questions about
underlying spatial phenomena, e.g., why is crime higher in this locale, why do
children in one school district outperform those in another, etc.? Answers to
these questions require explanations of the model structure, the choice of
parameters, and contextualization of the findings with respect to their
geographic context. This is particularly true for local forms of regression
models which are focused on the role of locational context in determining human
behavior. In this paper, we present GeoExplainer, a visual analytics framework
designed to support analysts in creating explanative documentation that
summarizes and contextualizes their spatial analyses. As analysts create their
spatial models, our framework flags potential issues with model parameter
selections, utilizes template-based text generation to summarize model outputs,
and links with external knowledge repositories to provide annotations that help
to explain the model results. As analysts explore the model results, all
visualizations and annotations can be captured in an interactive report
generation widget. We demonstrate our framework using a case study modeling the
determinants of voting in the 2016 US Presidential Election.",2023-08-25
Manipulating Embeddings of Stable Diffusion Prompts,2023-08-23 10:59:41+00:00,http://arxiv.org/abs/2308.12059v1,"Niklas Deckers, Julia Peters, Martin Potthast","cs.CV, cs.LG",image2text,"Generative text-to-image models such as Stable Diffusion allow users to
generate images based on a textual description, the prompt. Changing the prompt
is still the primary means for the user to change a generated image as desired.
However, changing the image by reformulating the prompt remains a difficult
process of trial and error, which has led to the emergence of prompt
engineering as a new field of research. We propose and analyze methods to
change the embedding of a prompt directly instead of the prompt text. It allows
for more fine-grained and targeted control that takes into account user
intentions. Our approach treats the generative text-to-image model as a
continuous function and passes gradients between the image space and the prompt
embedding space. By addressing different user interaction problems, we can
apply this idea in three scenarios: (1) Optimization of a metric defined in
image space that could measure, for example, image style. (2) Assistance of
users in creative tasks by enabling them to navigate the image space along a
selection of directions of ""near"" prompt embeddings. (3) Changing the embedding
of the prompt to include information that the user has seen in a particular
seed but finds difficult to describe in the prompt. Our experiments demonstrate
the feasibility of the described methods.",2023-08-23
CgT-GAN: CLIP-guided Text GAN for Image Captioning,2023-08-23 10:25:37+00:00,http://arxiv.org/abs/2308.12045v1,"Jiarui Yu, Haoran Li, Yanbin Hao, Bin Zhu, Tong Xu, Xiangnan He","cs.CV, cs.AI, cs.CL, cs.MM",image2text,"The large-scale visual-language pre-trained model, Contrastive Language-Image
Pre-training (CLIP), has significantly improved image captioning for scenarios
without human-annotated image-caption pairs. Recent advanced CLIP-based image
captioning without human annotations follows a text-only training paradigm,
i.e., reconstructing text from shared embedding space. Nevertheless, these
approaches are limited by the training/inference gap or huge storage
requirements for text embeddings. Given that it is trivial to obtain images in
the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates
images into the training process to enable the model to ""see"" real visual
modality. Particularly, we use adversarial training to teach CgT-GAN to mimic
the phrases of an external text corpus and CLIP-based reward to provide
semantic guidance. The caption generator is jointly rewarded based on the
caption naturalness to human language calculated from the GAN's discriminator
and the semantic guidance reward computed by the CLIP-based reward module. In
addition to the cosine similarity as the semantic guidance reward (i.e.,
CLIP-cos), we further introduce a novel semantic guidance reward called
CLIP-agg, which aligns the generated caption with a weighted text embedding by
attentively aggregating the entire corpus. Experimental results on three
subtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms
state-of-the-art methods significantly across all metrics. Code is available at
https://github.com/Lihr747/CgtGAN.",2023-08-23
"Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal
  Embeddings",2023-08-22 21:57:22+00:00,http://arxiv.org/abs/2308.11804v1,"Eugene Bagdasaryan, Vitaly Shmatikov","cs.CR, cs.AI, cs.LG",image2text,"Multi-modal encoders map images, sounds, texts, videos, etc. into a single
embedding space, aligning representations across modalities (e.g., associate an
image of a dog with a barking sound). We show that multi-modal embeddings can
be vulnerable to an attack we call ""adversarial illusions."" Given an input in
any modality, an adversary can perturb it so as to make its embedding close to
that of an arbitrary, adversary-chosen input in another modality. Illusions
thus enable the adversary to align any image with any text, any text with any
sound, etc.
  Adversarial illusions exploit proximity in the embedding space and are thus
agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how
adversarially aligned inputs, generated without knowledge of specific
downstream tasks, mislead image generation, text generation, and zero-shot
classification.",2023-08-22
"Music Understanding LLaMA: Advancing Text-to-Music Generation with
  Question Answering and Captioning",2023-08-22 08:43:33+00:00,http://arxiv.org/abs/2308.11276v1,"Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, Ying Shan","cs.SD, cs.AI, cs.CL, cs.MM, eess.AS",image2text,"Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity
of large-scale publicly available music datasets with natural language
captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA),
capable of answering music-related questions and generating captions for music
files. Our model utilizes audio representations from a pretrained MERT model to
extract music features. However, obtaining a suitable dataset for training the
MU-LLaMA model remains challenging, as existing publicly accessible audio
question answering datasets lack the necessary depth for open-ended music
question answering. To fill this gap, we present a methodology for generating
question-answer pairs from existing audio captioning datasets and introduce the
MusicQA Dataset designed for answering open-ended music-related questions. The
experiments demonstrate that the proposed MU-LLaMA model, trained on our
designed MusicQA dataset, achieves outstanding performance in both music
question answering and music caption generation across various metrics,
outperforming current state-of-the-art (SOTA) models in both fields and
offering a promising advancement in the T2M-Gen research field.",2023-08-22
Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection,2023-08-22 01:55:03+00:00,http://arxiv.org/abs/2308.11119v1,Masato Tamura,"cs.CV, cs.LG",image2text,"This paper presents a novel method that leverages a visual-language model,
CLIP, as a data source for zero-shot anomaly detection. Tremendous efforts have
been put towards developing anomaly detectors due to their potential industrial
applications. Considering the difficulty in acquiring various anomalous samples
for training, most existing methods train models with only normal samples and
measure discrepancies from the distribution of normal samples during inference,
which requires training a model for each object category. The problem of this
inefficient training requirement has been tackled by designing a CLIP-based
anomaly detector that applies prompt-guided classification to each part of an
image in a sliding window manner. However, the method still suffers from the
labor of careful prompt ensembling with known object categories. To overcome
the issues above, we propose leveraging CLIP as a data source for training. Our
method generates text embeddings with the text encoder in CLIP with typical
prompts that include words of normal and anomaly. In addition to these words,
we insert several randomly generated words into prompts, which enables the
encoder to generate a diverse set of normal and anomalous samples. Using the
generated embeddings as training data, a feed-forward neural network learns to
extract features of normal and anomaly from CLIP's embeddings, and as a result,
a category-agnostic anomaly detector can be obtained without any training
images. Experimental results demonstrate that our method achieves
state-of-the-art performance without laborious prompt ensembling in zero-shot
setups.",2023-08-22
"VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity
  Control",2023-08-18 20:18:30+00:00,http://arxiv.org/abs/2308.09804v1,"Zi-Yuan Hu, Yanyang Li, Michael R. Lyu, Liwei Wang","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"As the model size of pre-trained language models (PLMs) grows rapidly, full
fine-tuning becomes prohibitively expensive for model training and storage. In
vision-and-language (VL), parameter-efficient tuning (PET) techniques are
proposed to integrate modular modifications (e.g., Adapter and LoRA) into
encoder-decoder PLMs. By tuning a small set of trainable parameters, these
techniques perform on par with full fine-tuning. However, excessive modular
modifications and neglecting the functionality gap between the encoders and
decoders can lead to performance degradation, while existing PET techniques
(e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a
Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose
effective control over modular modifications via a novel granularity-controlled
mechanism. Considering different granularity-controlled matrices generated by
this mechanism, a variety of model-agnostic VL-PET modules can be instantiated
from our framework for better efficiency and effectiveness trade-offs. We
further propose lightweight PET module designs to enhance VL alignment and
modeling for the encoders and maintain text generation for the decoders.
Extensive experiments conducted on four image-text tasks and four video-text
tasks demonstrate the efficiency, effectiveness and transferability of our
VL-PET framework. In particular, our VL-PET-large with lightweight PET module
designs significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37%
(7.03%) with BART-base (T5-base) on image-text tasks. Furthermore, we validate
the enhanced effect of employing our VL-PET designs on existing PET techniques,
enabling them to achieve significant performance improvements. Our code is
available at https://github.com/HenryHZY/VL-PET.",2023-08-18
Can Knowledge Graphs Simplify Text?,2023-08-14 07:20:49+00:00,http://arxiv.org/abs/2308.06975v2,"Anthony Colas, Haodi Ma, Xuanli He, Yang Bai, Daisy Zhe Wang",cs.CL,image2text,"Knowledge Graph (KG)-to-Text Generation has seen recent improvements in
generating fluent and informative sentences which describe a given KG. As KGs
are widespread across multiple domains and contain important entity-relation
information, and as text simplification aims to reduce the complexity of a text
while preserving the meaning of the original text, we propose KGSimple, a novel
approach to unsupervised text simplification which infuses KG-established
techniques in order to construct a simplified KG path and generate a concise
text which preserves the original input's meaning. Through an iterative and
sampling KG-first approach, our model is capable of simplifying text when
starting from a KG by learning to keep important information while harnessing
KG-to-text generation to output fluent and descriptive sentences. We evaluate
various settings of the KGSimple model on currently-available KG-to-text
datasets, demonstrating its effectiveness compared to unsupervised text
simplification models which start with a given complex text. Our code is
available on GitHub.",2023-08-14
Mirror Diffusion Models,2023-08-11 18:31:54+00:00,http://arxiv.org/abs/2308.06342v2,Jaesung Tae,cs.LG,image2text,"Diffusion models have successfully been applied to generative tasks in
various continuous domains. However, applying diffusion to discrete categorical
data remains a non-trivial task. Moreover, generation in continuous domains
often requires clipping in practice, which motivates the need for a theoretical
framework for adapting diffusion to constrained domains. Inspired by the mirror
Langevin algorithm for the constrained sampling problem, in this theoretical
report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the
context of simplex diffusion and propose natural extensions to popular domains
such as image and text generation.",2023-08-11
Generative Forests,2023-08-07 14:58:53+00:00,http://arxiv.org/abs/2308.03648v1,"Richard Nock, Mathieu Guillame-Bert","cs.LG, I.2.6",image2text,"Tabular data represents one of the most prevalent form of data. When it comes
to data generation, many approaches would learn a density for the data
generation process, but would not necessarily end up with a sampler, even less
so being exact with respect to the underlying density. A second issue is on
models: while complex modeling based on neural nets thrives in image or text
generation (etc.), less is known for powerful generative models on tabular
data. A third problem is the visible chasm on tabular data between training
algorithms for supervised learning with remarkable properties (e.g. boosting),
and a comparative lack of guarantees when it comes to data generation. In this
paper, we tackle the three problems, introducing new tree-based generative
models convenient for density modeling and tabular data generation that improve
on modeling capabilities of recent proposals, and a training algorithm which
simplifies the training setting of previous approaches and displays
boosting-compliant convergence. This algorithm has the convenient property to
rely on a supervised training scheme that can be implemented by a few tweaks to
the most popular induction scheme for decision tree induction with two classes.
Experiments are provided on missing data imputation and comparing generated
data to real data, displaying the quality of the results obtained by our
approach, in particular against state of the art.",2023-08-07
FAST: Font-Agnostic Scene Text Editing,2023-08-05 15:54:06+00:00,http://arxiv.org/abs/2308.02905v1,"Alloy Das, Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal, Michael Blumenstein","cs.CV, cs.MM",image2text,"Scene Text Editing (STE) is a challenging research problem, and it aims to
modify existing texts in an image while preserving the background and the font
style of the original text of the image. Due to its various real-life
applications, researchers have explored several approaches toward STE in recent
years. However, most of the existing STE methods show inferior editing
performance because of (1) complex image backgrounds, (2) various font styles,
and (3) varying word lengths within the text. To address such inferior editing
performance issues, in this paper, we propose a novel font-agnostic scene text
editing framework, named FAST, for simultaneously generating text in arbitrary
styles and locations while preserving a natural and realistic appearance
through combined mask generation and style transfer. The proposed approach
differs from the existing methods as they directly modify all image pixels.
Instead, the proposed method has introduced a filtering mechanism to remove
background distractions, allowing the network to focus solely on the text
regions where editing is required. Additionally, a text-style transfer module
has been designed to mitigate the challenges posed by varying word lengths.
Extensive experiments and ablations have been conducted, and the results
demonstrate that the proposed method outperforms the existing methods both
qualitatively and quantitatively.",2023-08-05
Guiding Image Captioning Models Toward More Specific Captions,2023-07-31 14:00:12+00:00,http://arxiv.org/abs/2307.16686v1,"Simon Kornblith, Lala Li, Zirui Wang, Thao Nguyen","cs.CV, cs.LG",image2text,"Image captioning is conventionally formulated as the task of generating
captions for images that match the distribution of reference image-caption
pairs. However, reference captions in standard captioning datasets are short
and may not uniquely identify the images they describe. These problems are
further exacerbated when models are trained directly on image-alt text pairs
collected from the internet. In this work, we show that it is possible to
generate more specific captions with minimal changes to the training process.
We implement classifier-free guidance for an autoregressive captioning model by
fine-tuning it to estimate both conditional and unconditional distributions
over captions. The guidance scale applied at decoding controls a trade-off
between maximizing $p(\mathrm{caption}|\mathrm{image})$ and
$p(\mathrm{image}|\mathrm{caption})$. Compared to standard greedy decoding,
decoding with a guidance scale of 2 substantially improves reference-free
metrics such as CLIPScore (0.808 vs. 0.775) and caption$\to$image retrieval
performance in the CLIP embedding space (recall@1 44.6% vs. 26.5%), but worsens
standard reference-based captioning metrics (e.g., CIDEr 78.6 vs 126.1). We
further explore the use of language models to guide the decoding process,
obtaining small improvements over the Pareto frontier of reference-free vs.
reference-based captioning metrics that arises from classifier-free guidance,
and substantially improving the quality of captions generated from a model
trained only on minimally curated web data.",2023-07-31
"Transferable Decoding with Visual Entities for Zero-Shot Image
  Captioning",2023-07-31 09:47:06+00:00,http://arxiv.org/abs/2307.16525v1,"Junjie Fei, Teng Wang, Jinrui Zhang, Zhenyu He, Chengjie Wang, Feng Zheng","cs.CV, cs.CL",image2text,"Image-to-text generation aims to describe images using natural language.
Recently, zero-shot image captioning based on pre-trained vision-language
models (VLMs) and large language models (LLMs) has made significant progress.
However, we have observed and empirically demonstrated that these methods are
susceptible to modality bias induced by LLMs and tend to generate descriptions
containing objects (entities) that do not actually exist in the image but
frequently appear during training (i.e., object hallucination). In this paper,
we propose ViECap, a transferable decoding model that leverages entity-aware
decoding to generate descriptions in both seen and unseen scenarios. ViECap
incorporates entity-aware hard prompts to guide LLMs' attention toward the
visual entities present in the image, enabling coherent caption generation
across diverse scenes. With entity-aware hard prompts, ViECap is capable of
maintaining performance when transferring from in-domain to out-of-domain
scenarios. Extensive experiments demonstrate that ViECap sets a new
state-of-the-art cross-domain (transferable) captioning and performs
competitively in-domain captioning compared to previous VLMs-based zero-shot
methods. Our code is available at: https://github.com/FeiElysia/ViECap",2023-07-31
"Visual Captioning at Will: Describing Images and Videos Guided by a Few
  Stylized Sentences",2023-07-31 04:26:01+00:00,http://arxiv.org/abs/2307.16399v1,"Dingyi Yang, Hongyu Chen, Xinglin Hou, Tiezheng Ge, Yuning Jiang, Qin Jin","cs.MM, cs.CV",image2text,"Stylized visual captioning aims to generate image or video descriptions with
specific styles, making them more attractive and emotionally appropriate. One
major challenge with this task is the lack of paired stylized captions for
visual content, so most existing works focus on unsupervised methods that do
not rely on parallel datasets. However, these approaches still require training
with sufficient examples that have style labels, and the generated captions are
limited to predefined styles. To address these limitations, we explore the
problem of Few-Shot Stylized Visual Captioning, which aims to generate captions
in any desired style, using only a few examples as guidance during inference,
without requiring further training. We propose a framework called FS-StyleCap
for this task, which utilizes a conditional encoder-decoder language model and
a visual projection module. Our two-step training scheme proceeds as follows:
first, we train a style extractor to generate style representations on an
unlabeled text-only corpus. Then, we freeze the extractor and enable our
decoder to generate stylized descriptions based on the extracted style vector
and projected visual content vectors. During inference, our model can generate
desired stylized captions by deriving the style representation from
user-supplied examples. Our automatic evaluation results for few-shot
sentimental visual captioning outperform state-of-the-art approaches and are
comparable to models that are fully trained on labeled style corpora. Human
evaluations further confirm our model s ability to handle multiple styles.",2023-07-31
"Learning Multi-modal Representations by Watching Hundreds of Surgical
  Video Lectures",2023-07-27 22:38:12+00:00,http://arxiv.org/abs/2307.15220v1,"Kun Yuan, Vinkle Srivastav, Tong Yu, Joel Lavanchy, Pietro Mascagni, Nassir Navab, Nicolas Padoy","cs.CV, cs.AI",image2text,"Recent advancements in surgical computer vision applications have been driven
by fully-supervised methods, primarily using only visual data. These methods
rely on manually annotated surgical videos to predict a fixed set of object
categories, limiting their generalizability to unseen surgical procedures and
downstream tasks. In this work, we put forward the idea that the surgical video
lectures available through open surgical e-learning platforms can provide
effective supervisory signals for multi-modal representation learning without
relying on manual annotations. We address the surgery-specific linguistic
challenges present in surgical video lectures by employing multiple
complementary automatic speech recognition systems to generate text
transcriptions. We then present a novel method, SurgVLP - Surgical Vision
Language Pre-training, for multi-modal representation learning. SurgVLP
constructs a new contrastive learning objective to align video clip embeddings
with the corresponding multiple text embeddings by bringing them together
within a joint latent space. To effectively show the representation capability
of the learned joint latent space, we introduce several vision-and-language
tasks for surgery, such as text-based video retrieval, temporal activity
grounding, and video captioning, as benchmarks for evaluation. We further
demonstrate that without using any labeled ground truth, our approach can be
employed for traditional vision-only surgical downstream tasks, such as
surgical tool, phase, and triplet recognition. The code will be made available
at https://github.com/CAMMA-public/SurgVLP",2023-07-27
"A Transformer-based Approach for Arabic Offline Handwritten Text
  Recognition",2023-07-27 17:51:52+00:00,http://arxiv.org/abs/2307.15045v1,"Saleh Momeni, Bagher BabaAli","cs.CV, cs.LG",image2text,"Handwriting recognition is a challenging and critical problem in the fields
of pattern recognition and machine learning, with applications spanning a wide
range of domains. In this paper, we focus on the specific issue of recognizing
offline Arabic handwritten text. Existing approaches typically utilize a
combination of convolutional neural networks for image feature extraction and
recurrent neural networks for temporal modeling, with connectionist temporal
classification used for text generation. However, these methods suffer from a
lack of parallelization due to the sequential nature of recurrent neural
networks. Furthermore, these models cannot account for linguistic rules,
necessitating the use of an external language model in the post-processing
stage to boost accuracy. To overcome these issues, we introduce two alternative
architectures, namely the Transformer Transducer and the standard
sequence-to-sequence Transformer, and compare their performance in terms of
accuracy and speed. Our approach can model language dependencies and relies
only on the attention mechanism, thereby making it more parallelizable and less
complex. We employ pre-trained Transformers for both image understanding and
language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that
our proposed method outperforms the current state-of-the-art approaches for
recognizing offline Arabic handwritten text.",2023-07-27
Evaluating Generative Models for Graph-to-Text Generation,2023-07-27 09:03:05+00:00,http://arxiv.org/abs/2307.14712v1,"Shuzhou Yuan, Michael Färber","cs.CL, cs.AI",image2text,"Large language models (LLMs) have been widely employed for graph-to-text
generation tasks. However, the process of finetuning LLMs requires significant
training resources and annotation work. In this paper, we explore the
capability of generative models to generate descriptive text from graph data in
a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two
graph-to-text datasets and compare their performance with that of finetuned LLM
models such as T5 and BART. Our results demonstrate that generative models are
capable of generating fluent and coherent text, achieving BLEU scores of 10.57
and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error
analysis reveals that generative models still struggle with understanding the
semantic relations between entities, and they also tend to generate text with
hallucinations or irrelevant information. As a part of error analysis, we
utilize BERT to detect machine-generated text and achieve high macro-F1 scores.
We have made the text generated by generative models publicly available.",2023-07-27
XDLM: Cross-lingual Diffusion Language Model for Machine Translation,2023-07-25 15:08:34+00:00,http://arxiv.org/abs/2307.13560v2,"Linyao Chen, Aosong Feng, Boming Yang, Zihui Li",cs.CL,image2text,"Recently, diffusion models have excelled in image generation tasks and have
also been applied to neural language processing (NLP) for controllable text
generation. However, the application of diffusion models in a cross-lingual
setting is less unexplored. Additionally, while pretraining with diffusion
models has been studied within a single language, the potential of
cross-lingual pretraining remains understudied. To address these gaps, we
propose XDLM, a novel Cross-lingual diffusion model for machine translation,
consisting of pretraining and fine-tuning stages. In the pretraining stage, we
propose TLDM, a new training objective for mastering the mapping between
different languages; in the fine-tuning stage, we build up the translation
system based on the pretrained model. We evaluate the result on several machine
translation benchmarks and outperformed both diffusion and Transformer
baselines.",2023-07-25
Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts,2023-07-21 15:49:59+00:00,http://arxiv.org/abs/2307.11661v1,"Mayug Maniparambil, Chris Vorster, Derek Molloy, Noel Murphy, Kevin McGuinness, Noel E. O'Connor","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have
revolutionized visual representation learning by providing good performance on
downstream datasets. VLMs are 0-shot adapted to a downstream dataset by
designing prompts that are relevant to the dataset. Such prompt engineering
makes use of domain expertise and a validation dataset. Meanwhile, recent
developments in generative pretrained models like GPT-4 mean they can be used
as advanced internet search tools. They can also be manipulated to provide
visual information in any structure. In this work, we show that GPT-4 can be
used to generate text that is visually descriptive and how this can be used to
adapt CLIP to downstream tasks. We show considerable improvements in 0-shot
transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD
(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.
We also design a simple few-shot adapter that learns to choose the best
possible sentences to construct generalizable classifiers that outperform the
recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized
fine-grained datasets. We will release the code, prompts, and auxiliary text
dataset upon acceptance.",2023-07-21
OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?,2023-07-21 14:58:44+00:00,http://arxiv.org/abs/2307.11636v1,"Runjia Li, Shuyang Sun, Mohamed Elhoseiny, Philip Torr","cs.CV, cs.CL",image2text,"This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scale
dataset for humour generation and understanding. Humour is an abstract,
subjective, and context-dependent cognitive construct involving several
cognitive factors, making it a challenging task to generate and interpret.
Hence, humour generation and understanding can serve as a new task for
evaluating the ability of deep-learning methods to process abstract and
subjective information. Due to the scarcity of data, humour-related generation
tasks such as captioning remain under-explored. To address this gap,
OxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores to
train a generalizable humour captioning model. Contrary to existing captioning
datasets, OxfordTVG-HIC features a wide range of emotional and semantic
diversity resulting in out-of-context examples that are particularly conducive
to generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive
content. We also show how OxfordTVG-HIC can be leveraged for evaluating the
humour of a generated text. Through explainability analysis of the trained
models, we identify the visual and linguistic cues influential for evoking
humour prediction (and generation). We observe qualitatively that these cues
are aligned with the benign violation theory of humour in cognitive psychology.",2023-07-21
"Generating Image-Specific Text Improves Fine-grained Image
  Classification",2023-07-21 02:47:18+00:00,http://arxiv.org/abs/2307.11315v1,"Emily Mu, Kathleen M. Lewis, Adrian V. Dalca, John Guttag","cs.CV, cs.CL",image2text,"Recent vision-language models outperform vision-only models on many image
classification tasks. However, because of the absence of paired text/image
descriptions, it remains difficult to fine-tune these models for fine-grained
image classification. In this work, we propose a method, GIST, for generating
image-specific fine-grained text descriptions from image-only datasets, and
show that these text descriptions can be used to improve classification. Key
parts of our method include 1. prompting a pretrained large language model with
domain-specific prompts to generate diverse fine-grained text descriptions for
each class and 2. using a pretrained vision-language model to match each image
to label-preserving text descriptions that capture relevant visual features in
the image. We demonstrate the utility of GIST by fine-tuning vision-language
models on the image-and-generated-text pairs to learn an aligned
vision-language representation space for improved classification. We evaluate
our learned representation space in full-shot and few-shot scenarios across
four diverse fine-grained classification datasets, each from a different
domain. Our method achieves an average improvement of $4.1\%$ in accuracy over
CLIP linear probes and an average of $1.1\%$ improvement in accuracy over the
previous state-of-the-art image-text classification method on the full-shot
datasets. Our method achieves similar improvements across few-shot regimes.
Code is available at https://github.com/emu1729/GIST.",2023-07-21
"FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with
  Human Feedback",2023-07-20 13:40:22+00:00,http://arxiv.org/abs/2307.10867v1,"Ashish Singh, Prateek Agarwal, Zixuan Huang, Arpita Singh, Tong Yu, Sungchul Kim, Victor Bursztyn, Nikos Vlassis, Ryan A. Rossi","cs.CL, cs.CV, cs.LG",image2text,"Captions are crucial for understanding scientific visualizations and
documents. Existing captioning methods for scientific figures rely on
figure-caption pairs extracted from documents for training, many of which fall
short with respect to metrics like helpfulness, explainability, and
visual-descriptiveness [15] leading to generated captions being misaligned with
reader preferences. To enable the generation of high-quality figure captions,
we introduce FigCaps-HF a new framework for figure-caption generation that can
incorporate domain expert feedback in generating captions optimized for reader
preferences. Our framework comprises of 1) an automatic method for evaluating
quality of figure-caption pairs, 2) a novel reinforcement learning with human
feedback (RLHF) method to optimize a generative figure-to-caption model for
reader preferences. We demonstrate the effectiveness of our simple learning
framework by improving performance over standard fine-tuning across different
types of models. In particular, when using BLIP as the base model, our RLHF
framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and
Meteor, respectively. Finally, we release a large-scale benchmark dataset with
human feedback on figure-caption pairs to enable further evaluation and
development of RLHF techniques for this problem.",2023-07-20
Improving Multimodal Datasets with Image Captioning,2023-07-19 17:47:12+00:00,http://arxiv.org/abs/2307.10350v1,"Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, Ludwig Schmidt","cs.LG, cs.CV",image2text,"Massive web datasets play a key role in the success of large vision-language
models like CLIP and Flamingo. However, the raw web data is noisy, and existing
filtering methods to reduce noise often come at the expense of data diversity.
Our work focuses on caption quality as one major source of noise, and studies
how generated captions can increase the utility of web-scraped datapoints with
nondescript text. Through exploring different mixing strategies for raw and
generated captions, we outperform the best filtering method proposed by the
DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a
candidate pool of 128M image-text pairs. Our best approach is also 2x better at
Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an
effective source of text supervision. In experimenting with different image
captioning models, we also demonstrate that the performance of a model on
standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable
indicator of the utility of the captions it generates for multimodal training.
Finally, our experiments with using generated captions at DataComp's large
scale (1.28B image-text pairs) offer insights into the limitations of synthetic
text, as well as the importance of image curation with increasing training data
quantity.",2023-07-19
"PromptMagician: Interactive Prompt Engineering for Text-to-Image
  Creation",2023-07-18 07:46:25+00:00,http://arxiv.org/abs/2307.09036v1,"Yingchaojie Feng, Xingbo Wang, Kam Kwai Wong, Sijia Wang, Yuhong Lu, Minfeng Zhu, Baicheng Wang, Wei Chen","cs.AI, cs.HC",image2text,"Generative text-to-image models have gained great popularity among the public
for their powerful capability to generate high-quality images based on natural
language prompts. However, developing effective prompts for desired images can
be challenging due to the complexity and ambiguity of natural language. This
research proposes PromptMagician, a visual analysis system that helps users
explore the image results and refine the input prompts. The backbone of our
system is a prompt recommendation model that takes user prompts as input,
retrieves similar prompt-image pairs from DiffusionDB, and identifies special
(important and relevant) prompt keywords. To facilitate interactive prompt
refinement, PromptMagician introduces a multi-level visualization for the
cross-modal embedding of the retrieved images and recommended keywords, and
supports users in specifying multiple criteria for personalized exploration.
Two usage scenarios, a user study, and expert interviews demonstrate the
effectiveness and usability of our system, suggesting it facilitates prompt
engineering and improves the creativity support of the generative text-to-image
model.",2023-07-18
Reading Radiology Imaging Like The Radiologist,2023-07-12 05:36:47+00:00,http://arxiv.org/abs/2307.05921v3,Yuhao Wang,"cs.CV, cs.AI",image2text,"Automated radiology report generation aims to generate radiology reports that
contain rich, fine-grained descriptions of radiology imaging. Compared with
image captioning in the natural image domain, medical images are very similar
to each other, with only minor differences in the occurrence of diseases. Given
the importance of these minor differences in the radiology report, it is
crucial to encourage the model to focus more on the subtle regions of disease
occurrence. Secondly, the problem of visual and textual data biases is serious.
Not only do normal cases make up the majority of the dataset, but sentences
describing areas with pathological changes also constitute only a small part of
the paragraph. Lastly, generating medical image reports involves the challenge
of long text generation, which requires more expertise and empirical training
in medical knowledge. As a result, the difficulty of generating such reports is
increased. To address these challenges, we propose a disease-oriented retrieval
framework that utilizes similar reports as prior knowledge references. We
design a factual consistency captioning generator to generate more accurate and
factually consistent disease descriptions. Our framework can find most similar
reports for a given disease from the CXR database by retrieving a
disease-oriented mask consisting of the position and morphological
characteristics. By referencing the disease-oriented similar report and the
visual features, the factual consistency model can generate a more accurate
radiology report.",2023-07-12
"Empirical Analysis of a Segmentation Foundation Model in Prostate
  Imaging",2023-07-06 20:00:52+00:00,http://arxiv.org/abs/2307.03266v1,"Heejong Kim, Victor Ion Butoi, Adrian V. Dalca, Mert R. Sabuncu","eess.IV, cs.CV, cs.LG",image2text,"Most state-of-the-art techniques for medical image segmentation rely on
deep-learning models. These models, however, are often trained on
narrowly-defined tasks in a supervised fashion, which requires expensive
labeled datasets. Recent advances in several machine learning domains, such as
natural language generation have demonstrated the feasibility and utility of
building foundation models that can be customized for various downstream tasks
with little to no labeled data. This likely represents a paradigm shift for
medical imaging, where we expect that foundation models may shape the future of
the field. In this paper, we consider a recently developed foundation model for
medical image segmentation, UniverSeg. We conduct an empirical evaluation study
in the context of prostate imaging and compare it against the conventional
approach of training a task-specific segmentation model. Our results and
discussion highlight several important factors that will likely be important in
the development and adoption of foundation models for medical image
segmentation.",2023-07-06
Vision Language Transformers: A Survey,2023-07-06 19:08:56+00:00,http://arxiv.org/abs/2307.03254v1,"Clayton Fields, Casey Kennington","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Vision language tasks, such as answering questions about or generating
captions that describe an image, are difficult tasks for computers to perform.
A relatively recent body of research has adapted the pretrained transformer
architecture introduced in \citet{vaswani2017attention} to vision language
modeling. Transformer models have greatly improved performance and versatility
over previous vision language models. They do so by pretraining models on a
large generic datasets and transferring their learning to new tasks with minor
changes in architecture and parameter values. This type of transfer learning
has become the standard modeling practice in both natural language processing
and computer vision. Vision language transformers offer the promise of
producing similar advancements in tasks which require both vision and language.
In this paper, we provide a broad synthesis of the currently available research
on vision language transformer models and offer some analysis of their
strengths, limitations and some open questions that remain.",2023-07-06
Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment,2023-07-05 23:01:26+00:00,http://arxiv.org/abs/2307.02682v2,"Yongrae Jo, Seongyun Lee, Aiden SJ Lee, Hyunji Lee, Hanseok Oh, Minjoon Seo","cs.CV, cs.CL",image2text,"Dense video captioning, a task of localizing meaningful moments and
generating relevant captions for videos, often requires a large, expensive
corpus of annotated video segments paired with text. In an effort to minimize
the annotation cost, we propose ZeroTA, a novel method for dense video
captioning in a zero-shot manner. Our method does not require any videos or
annotations for training; instead, it localizes and describes events within
each input video at test time by optimizing solely on the input. This is
accomplished by introducing a soft moment mask that represents a temporal
segment in the video and jointly optimizing it with the prefix parameters of a
language model. This joint optimization aligns a frozen language generation
model (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,
CLIP) by maximizing the matching score between the generated text and a moment
within the video. We also introduce a pairwise temporal IoU loss to let a set
of soft moment masks capture multiple distinct events within the video. Our
method effectively discovers diverse significant events within the video, with
the resulting captions appropriately describing these events. The empirical
results demonstrate that ZeroTA surpasses zero-shot baselines and even
outperforms the state-of-the-art few-shot method on the widely-used benchmark
ActivityNet Captions. Moreover, our method shows greater robustness compared to
supervised methods when evaluated in out-of-domain scenarios. This research
provides insight into the potential of aligning widely-used models, such as
language generation models and vision-language models, to unlock a new
capability: understanding temporal aspects of videos.",2023-07-05
"A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image
  Diagnosis",2023-07-05 01:45:19+00:00,http://arxiv.org/abs/2307.01981v1,"Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Xiaotang Gai, Yang Feng, Zuozhu Liu","eess.IV, cs.CV, cs.LG",image2text,"Zero-shot medical image classification is a critical process in real-world
scenarios where we have limited access to all possible diseases or large-scale
annotated data. It involves computing similarity scores between a query medical
image and possible disease categories to determine the diagnostic result.
Recent advances in pretrained vision-language models (VLMs) such as CLIP have
shown great performance for zero-shot natural image recognition and exhibit
benefits in medical applications. However, an explainable zero-shot medical
image recognition framework with promising performance is yet under
development. In this paper, we propose a novel CLIP-based zero-shot medical
image classification framework supplemented with ChatGPT for explainable
diagnosis, mimicking the diagnostic process performed by human experts. The key
idea is to query large language models (LLMs) with category names to
automatically generate additional cues and knowledge, such as disease symptoms
or descriptions other than a single category name, to help provide more
accurate and explainable diagnosis in CLIP. We further design specific prompts
to enhance the quality of generated texts by ChatGPT that describe visual
medical features. Extensive results on one private dataset and four public
datasets along with detailed analysis demonstrate the effectiveness and
explainability of our training-free zero-shot diagnosis pipeline, corroborating
the great potential of VLMs and LLMs for medical applications.",2023-07-05
"More for Less: Compact Convolutional Transformers Enable Robust Medical
  Image Classification with Limited Data",2023-07-01 03:31:30+00:00,http://arxiv.org/abs/2307.00213v1,Andrew Kean Gao,"cs.CV, cs.LG, I.4.9, I.2.10",image2text,"Transformers are very powerful tools for a variety of tasks across domains,
from text generation to image captioning. However, transformers require
substantial amounts of training data, which is often a challenge in biomedical
settings, where high quality labeled data can be challenging or expensive to
obtain. This study investigates the efficacy of Compact Convolutional
Transformers (CCT) for robust medical image classification with limited data,
addressing a key issue faced by conventional Vision Transformers - their
requirement for large datasets. A hybrid of transformers and convolutional
layers, CCTs demonstrate high accuracy on modestly sized datasets. We employed
a benchmark dataset of peripheral blood cell images of eight distinct cell
types, each represented by approximately 2,000 low-resolution (28x28x3 pixel)
samples. Despite the dataset size being smaller than those typically used with
Vision Transformers, we achieved a commendable classification accuracy of
92.49% and a micro-average ROC AUC of 0.9935. The CCT also learned quickly,
exceeding 80% validation accuracy after five epochs. Analysis of per-class
precision, recall, F1, and ROC showed that performance was strong across cell
types. Our findings underscore the robustness of CCTs, indicating their
potential as a solution to data scarcity issues prevalent in biomedical
imaging. We substantiate the applicability of CCTs in data-constrained areas
and encourage further work on CCTs.",2023-07-01
Concept-Oriented Deep Learning with Large Language Models,2023-06-29 16:47:11+00:00,http://arxiv.org/abs/2306.17089v1,Daniel T. Chang,"cs.LG, cs.CL",image2text,"Large Language Models (LLMs) have been successfully used in many
natural-language tasks and applications including text generation and AI
chatbots. They also are a promising new technology for concept-oriented deep
learning (CODL). However, the prerequisite is that LLMs understand concepts and
ensure conceptual consistency. We discuss these in this paper, as well as major
uses of LLMs for CODL including concept extraction from text, concept graph
extraction from text, and concept learning. Human knowledge consists of both
symbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only
LLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal
LLMs, on the other hand, are capable of representing the full range (conceptual
and sensory) of human knowledge. We discuss conceptual understanding in
visual-language LLMs, the most important multimodal LLMs, and major uses of
them for CODL including concept extraction from image, concept graph extraction
from image, and concept learning. While uses of LLMs for CODL are valuable
standalone, they are particularly valuable as part of LLM applications such as
AI chatbots.",2023-06-29
Joint Level Generation and Translation Using Gameplay Videos,2023-06-29 03:46:44+00:00,http://arxiv.org/abs/2306.16662v1,"Negar Mirgati, Matthew Guzdial","cs.CV, cs.LG",image2text,"Procedural Content Generation via Machine Learning (PCGML) faces a
significant hurdle that sets it apart from other fields, such as image or text
generation, which is limited annotated data. Many existing methods for
procedural level generation via machine learning require a secondary
representation besides level images. However, the current methods for obtaining
such representations are laborious and time-consuming, which contributes to
this problem. In this work, we aim to address this problem by utilizing
gameplay videos of two human-annotated games to develop a novel multi-tail
framework that learns to perform simultaneous level translation and generation.
The translation tail of our framework can convert gameplay video frames to an
equivalent secondary representation, while its generation tail can produce
novel level segments. Evaluation results and comparisons between our framework
and baselines suggest that combining the level generation and translation tasks
can lead to an overall improved performance regarding both tasks. This
represents a possible solution to limited annotated level data, and we
demonstrate the potential for future versions to generalize to unseen games.",2023-06-29
"ZeroGen: Zero-shot Multimodal Controllable Text Generation with Multiple
  Oracles",2023-06-29 03:22:43+00:00,http://arxiv.org/abs/2306.16649v1,"Haoqin Tu, Bowen Yang, Xianfeng Zhao",cs.CL,image2text,"Automatically generating textual content with desired attributes is an
ambitious task that people have pursued long. Existing works have made a series
of progress in incorporating unimodal controls into language models (LMs),
whereas how to generate controllable sentences with multimodal signals and high
efficiency remains an open question. To tackle the puzzle, we propose a new
paradigm of zero-shot controllable text generation with multimodal signals
(\textsc{ZeroGen}). Specifically, \textsc{ZeroGen} leverages controls of text
and image successively from token-level to sentence-level and maps them into a
unified probability space at decoding, which customizes the LM outputs by
weighted addition without extra training. To achieve better inter-modal
trade-offs, we further introduce an effective dynamic weighting mechanism to
regulate all control weights. Moreover, we conduct substantial experiments to
probe the relationship of being in-depth or in-width between signals from
distinct modalities. Encouraging empirical results on three downstream tasks
show that \textsc{ZeroGen} not only outperforms its counterparts on captioning
tasks by a large margin but also shows great potential in multimodal news
generation with a higher degree of control. Our code will be released at
https://github.com/ImKeTT/ZeroGen.",2023-06-29
"You Can Generate It Again: Data-to-text Generation with Verification and
  Correction Prompting",2023-06-28 05:34:25+00:00,http://arxiv.org/abs/2306.15933v1,"Xuan Ren, Lingqiao Liu","cs.CL, cs.AI, cs.LG",image2text,"Despite significant advancements in existing models, generating text
descriptions from structured data input, known as data-to-text generation,
remains a challenging task. In this paper, we propose a novel approach that
goes beyond traditional one-shot generation methods by introducing a multi-step
process consisting of generation, verification, and correction stages. Our
approach, VCP(Verification and Correction Prompting), begins with the model
generating an initial output. We then proceed to verify the correctness of
different aspects of the generated text. The observations from the verification
step are converted into a specialized error-indication prompt, which instructs
the model to regenerate the output while considering the identified errors. To
enhance the model's correction ability, we have developed a carefully designed
training procedure. This procedure enables the model to incorporate feedback
from the error-indication prompt, resulting in improved output generation.
Through experimental results, we demonstrate that our approach effectively
reduces slot error rates while maintaining the overall quality of the generated
text.",2023-06-28
FunQA: Towards Surprising Video Comprehension,2023-06-26 17:59:55+00:00,http://arxiv.org/abs/2306.14899v1,"Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, Ziwei Liu","cs.CV, cs.AI, cs.CL, cs.MM",image2text,"Surprising videos, e.g., funny clips, creative performances, or visual
illusions, attract significant attention. Enjoyment of these videos is not
simply a response to visual stimuli; rather, it hinges on the human capacity to
understand (and appreciate) commonsense violations depicted in these videos. We
introduce FunQA, a challenging video question answering (QA) dataset
specifically designed to evaluate and enhance the depth of video reasoning
based on counter-intuitive and fun videos. Unlike most video QA benchmarks
which focus on less surprising contexts, e.g., cooking or instructional videos,
FunQA covers three previously unexplored types of surprising videos: 1)
HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous
QA tasks designed to assess the model's capability in counter-intuitive
timestamp localization, detailed video description, and reasoning around
counter-intuitiveness. We also pose higher-level tasks, such as attributing a
fitting and vivid title to the video, and scoring the video creativity. In
total, the FunQA benchmark consists of 312K free-text QA pairs derived from
4.3K video clips, spanning a total of 24 video hours. Extensive experiments
with existing VideoQA models reveal significant performance gaps for the FunQA
videos across spatial-temporal reasoning, visual-centered reasoning, and
free-text generation.",2023-06-26
"Learning Descriptive Image Captioning via Semipermeable Maximum
  Likelihood Estimation",2023-06-23 12:03:07+00:00,http://arxiv.org/abs/2306.13460v1,"Zihao Yue, Anwen Hu, Liang Zhang, Qin Jin",cs.CL,image2text,"Image captioning aims to describe visual content in natural language. As 'a
picture is worth a thousand words', there could be various correct descriptions
for an image. However, with maximum likelihood estimation as the training
objective, the captioning model is penalized whenever its prediction mismatches
with the label. For instance, when the model predicts a word expressing richer
semantics than the label, it will be penalized and optimized to prefer more
concise expressions, referred to as conciseness optimization. In contrast,
predictions that are more concise than labels lead to richness optimization.
Such conflicting optimization directions could eventually result in the model
generating general descriptions. In this work, we introduce Semipermeable
MaxImum Likelihood Estimation (SMILE), which allows richness optimization while
blocking conciseness optimization, thus encouraging the model to generate
longer captions with more details. Extensive experiments on two mainstream
image captioning datasets MSCOCO and Flickr30K demonstrate that SMILE
significantly enhances the descriptiveness of generated captions. We further
provide in-depth investigations to facilitate a better understanding of how
SMILE works.",2023-06-23
"Improving Image Captioning Descriptiveness by Ranking and LLM-based
  Fusion",2023-06-20 15:13:02+00:00,http://arxiv.org/abs/2306.11593v1,"Simone Bianco, Luigi Celona, Marco Donzella, Paolo Napoletano","cs.CV, cs.AI, cs.CL, cs.DB, cs.LG",image2text,"State-of-The-Art (SoTA) image captioning models often rely on the Microsoft
COCO (MS-COCO) dataset for training. This dataset contains annotations provided
by human annotators, who typically produce captions averaging around ten
tokens. However, this constraint presents a challenge in effectively capturing
complex scenes and conveying detailed information. Furthermore, captioning
models tend to exhibit bias towards the ``average'' caption, which captures
only the more general aspects. What would happen if we were able to
automatically generate longer captions, thereby making them more detailed?
Would these captions, evaluated by humans, be more or less representative of
the image content compared to the original MS-COCO captions? In this paper, we
present a novel approach to address previous challenges by showcasing how
captions generated from different SoTA models can be effectively fused,
resulting in richer captions. Our proposed method leverages existing models
from the literature, eliminating the need for additional training. Instead, it
utilizes an image-text based metric to rank the captions generated by SoTA
models for a given image. Subsequently, the top two captions are fused using a
Large Language Model (LLM). Experimental results demonstrate the effectiveness
of our approach, as the captions generated by our model exhibit higher
consistency with human judgment when evaluated on the MS-COCO test set. By
combining the strengths of various SoTA models, our method enhances the quality
and appeal of image captions, bridging the gap between automated systems and
the rich, informative nature of human-generated descriptions. This advance
opens up new possibilities for generating captions that are more suitable for
the training of both vision-language and captioning models.",2023-06-20
"Energy-Based Cross Attention for Bayesian Context Update in
  Text-to-Image Diffusion Models",2023-06-16 14:30:41+00:00,http://arxiv.org/abs/2306.09869v2,"Geon Yeong Park, Jeongsol Kim, Beomsu Kim, Sang Wan Lee, Jong Chul Ye","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Despite the remarkable performance of text-to-image diffusion models in image
generation tasks, recent studies have raised the issue that generated images
sometimes cannot capture the intended semantic contents of the text prompts,
which phenomenon is often called semantic misalignment. To address this, here
we present a novel energy-based model (EBM) framework. Specifically, we first
formulate EBMs of latent image representations and text embeddings in each
cross-attention layer of the denoising autoencoder. Then, we obtain the
gradient of the log posterior of context vectors, which can be updated and
transferred to the subsequent cross-attention layer, thereby implicitly
minimizing a nested hierarchy of energy functions. Our latent EBMs further
allow zero-shot compositional generation as a linear combination of
cross-attention outputs from different contexts. Using extensive experiments,
we demonstrate that the proposed method is highly effective in handling various
image generation tasks, including multi-concept generation, text-guided image
inpainting, and real and synthetic image editing.",2023-06-16
"Human Preference Score v2: A Solid Benchmark for Evaluating Human
  Preferences of Text-to-Image Synthesis",2023-06-15 17:59:31+00:00,http://arxiv.org/abs/2306.09341v1,"Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, Hongsheng Li","cs.CV, cs.AI, cs.DB",image2text,"Recent text-to-image generative models can generate high-fidelity images from
text inputs, but the quality of these generated images cannot be accurately
evaluated by existing evaluation metrics. To address this issue, we introduce
Human Preference Dataset v2 (HPD v2), a large-scale dataset that captures human
preferences on images from a wide range of sources. HPD v2 comprises 798,090
human preference choices on 430,060 pairs of images, making it the largest
dataset of its kind. The text prompts and images are deliberately collected to
eliminate potential bias, which is a common issue in previous datasets. By
fine-tuning CLIP on HPD v2, we obtain Human Preference Score v2 (HPS v2), a
scoring model that can more accurately predict text-generated images' human
preferences. Our experiments demonstrate that HPS v2 generalizes better than
previous metrics across various image distributions and is responsive to
algorithmic improvements of text-to-image generative models, making it a
preferable evaluation metric for these models. We also investigate the design
of the evaluation prompts for text-to-image generative models, to make the
evaluation stable, fair and easy-to-use. Finally, we establish a benchmark for
text-to-image generative models using HPS v2, which includes a set of recent
text-to-image models from the academia, community and industry. The code and
dataset is / will be available at https://github.com/tgxs002/HPSv2.",2023-06-15
GBSD: Generative Bokeh with Stage Diffusion,2023-06-14 05:34:02+00:00,http://arxiv.org/abs/2306.08251v1,"Jieren Deng, Xin Zhou, Hao Tian, Zhihong Pan, Derek Aguiar","cs.CV, cs.AI",image2text,"The bokeh effect is an artistic technique that blurs out-of-focus areas in a
photograph and has gained interest due to recent developments in text-to-image
synthesis and the ubiquity of smart-phone cameras and photo-sharing apps. Prior
work on rendering bokeh effects have focused on post hoc image manipulation to
produce similar blurring effects in existing photographs using classical
computer graphics or neural rendering techniques, but have either depth
discontinuity artifacts or are restricted to reproducing bokeh effects that are
present in the training data. More recent diffusion based models can synthesize
images with an artistic style, but either require the generation of
high-dimensional masks, expensive fine-tuning, or affect global image
characteristics. In this paper, we present GBSD, the first generative
text-to-image model that synthesizes photorealistic images with a bokeh style.
Motivated by how image synthesis occurs progressively in diffusion models, our
approach combines latent diffusion models with a 2-stage conditioning algorithm
to render bokeh effects on semantically defined objects. Since we can focus the
effect on objects, this semantic bokeh effect is more versatile than classical
rendering techniques. We evaluate GBSD both quantitatively and qualitatively
and demonstrate its ability to be applied in both text-to-image and
image-to-image settings.",2023-06-14
I See Dead People: Gray-Box Adversarial Attack on Image-To-Text Models,2023-06-13 07:35:28+00:00,http://arxiv.org/abs/2306.07591v1,"Raz Lapid, Moshe Sipper","cs.CV, cs.NE",image2text,"Modern image-to-text systems typically adopt the encoder-decoder framework,
which comprises two main components: an image encoder, responsible for
extracting image features, and a transformer-based decoder, used for generating
captions. Taking inspiration from the analysis of neural networks' robustness
against adversarial perturbations, we propose a novel gray-box algorithm for
creating adversarial examples in image-to-text models. Unlike image
classification tasks that have a finite set of class labels, finding visually
similar adversarial examples in an image-to-text task poses greater challenges
because the captioning system allows for a virtually infinite space of possible
captions. In this paper, we present a gray-box adversarial attack on
image-to-text, both untargeted and targeted. We formulate the process of
discovering adversarial perturbations as an optimization problem that uses only
the image-encoder component, meaning the proposed attack is language-model
agnostic. Through experiments conducted on the ViT-GPT2 model, which is the
most-used image-to-text model in Hugging Face, and the Flickr30k dataset, we
demonstrate that our proposed attack successfully generates visually similar
adversarial examples, both with untargeted and targeted captions. Notably, our
attack operates in a gray-box manner, requiring no knowledge about the decoder
module. We also show that our attacks fool the popular open-source platform
Hugging Face.",2023-06-13
"Generative Text-Guided 3D Vision-Language Pretraining for Unified
  Medical Image Segmentation",2023-06-07 22:20:51+00:00,http://arxiv.org/abs/2306.04811v1,"Yinda Chen, Che Liu, Wei Huang, Sibo Cheng, Rossella Arcucci, Zhiwei Xiong","cs.CV, cs.AI",image2text,"Vision-Language Pretraining (VLP) has demonstrated remarkable capabilities in
learning visual representations from textual descriptions of images without
annotations. Yet, effective VLP demands large-scale image-text pairs, a
resource that suffers scarcity in the medical domain. Moreover, conventional
VLP is limited to 2D images while medical images encompass diverse modalities,
often in 3D, making the learning process more challenging. To address these
challenges, we present Generative Text-Guided 3D Vision-Language Pretraining
for Unified Medical Image Segmentation (GTGM), a framework that extends of VLP
to 3D medical images without relying on paired textual descriptions.
Specifically, GTGM utilizes large language models (LLM) to generate
medical-style text from 3D medical images. This synthetic text is then used to
supervise 3D visual representation learning. Furthermore, a negative-free
contrastive learning objective strategy is introduced to cultivate consistent
visual representations between augmented 3D medical image patches, which
effectively mitigates the biases associated with strict positive-negative
sample pairings. We evaluate GTGM on three imaging modalities - Computed
Tomography (CT), Magnetic Resonance Imaging (MRI), and electron microscopy (EM)
over 13 datasets. GTGM's superior performance across various medical image
segmentation tasks underscores its effectiveness and versatility, by enabling
VLP extension into 3D medical imagery while bypassing the need for paired text.",2023-06-07
On the Difference of BERT-style and CLIP-style Text Encoders,2023-06-06 13:41:09+00:00,http://arxiv.org/abs/2306.03678v1,"Zhihong Chen, Guiming Hardy Chen, Shizhe Diao, Xiang Wan, Benyou Wang",cs.CL,image2text,"Masked language modeling (MLM) has been one of the most popular pretraining
recipes in natural language processing, e.g., BERT, one of the representative
models. Recently, contrastive language-image pretraining (CLIP) has also
attracted attention, especially its vision models that achieve excellent
performance on a broad range of vision tasks. However, few studies are
dedicated to studying the text encoders learned by CLIP. In this paper, we
analyze the difference between BERT-style and CLIP-style text encoders from
three experiments: (i) general text understanding, (ii) vision-centric text
understanding, and (iii) text-to-image generation. Experimental analyses show
that although CLIP-style text encoders underperform BERT-style ones for general
text understanding tasks, they are equipped with a unique ability, i.e.,
synesthesia, for the cross-modal association, which is more similar to the
senses of humans.",2023-06-06
Putting Humans in the Image Captioning Loop,2023-06-06 07:50:46+00:00,http://arxiv.org/abs/2306.03476v1,"Aliki Anagnostopoulou, Mareike Hartmann, Daniel Sonntag","cs.CL, cs.CV",image2text,"Image Captioning (IC) models can highly benefit from human feedback in the
training process, especially in cases where data is limited. We present
work-in-progress on adapting an IC system to integrate human feedback, with the
goal to make it easily adaptable to user-specific data. Our approach builds on
a base IC model pre-trained on the MS COCO dataset, which generates captions
for unseen images. The user will then be able to offer feedback on the image
and the generated/predicted caption, which will be augmented to create
additional training instances for the adaptation of the model. The additional
instances are integrated into the model using step-wise updates, and a sparse
memory replay component is used to avoid catastrophic forgetting. We hope that
this approach, while leading to improved results, will also result in
customizable IC models.",2023-06-06
"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video
  Understanding",2023-06-05 13:17:27+00:00,http://arxiv.org/abs/2306.02858v2,"Hang Zhang, Xin Li, Lidong Bing","cs.CL, cs.CV, cs.SD, eess.AS",image2text,"We present Video-LLaMA, a multi-modal framework that empowers Large Language
Models (LLMs) with the capability of understanding both visual and auditory
content in the video. Video-LLaMA bootstraps cross-modal training from the
frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous
vision-LLMs that focus on static image comprehensions such as MiniGPT-4 and
LLaVA, Video-LLaMA mainly tackles two challenges in video understanding: (1)
capturing the temporal changes in visual scenes, (2) integrating audio-visual
signals. To counter the first challenge, we propose a Video Q-former to
assemble the pre-trained image encoder into our video encoder and introduce a
video-to-text generation task to learn video-language correspondence. For the
second challenge, we leverage ImageBind, a universal embedding model aligning
multiple modalities as the pre-trained audio encoder, and introduce an Audio
Q-former on top of ImageBind to learn reasonable auditory query embeddings for
the LLM module. To align the output of both visual & audio encoders with LLM's
embedding space, we train Video-LLaMA on massive video/image-caption pairs as
well as visual-instruction-tuning datasets of moderate amount but higher
quality. We found Video-LLaMA showcases the ability to perceive and comprehend
video content, generating meaningful responses that are grounded in the visual
and auditory information presented in the videos. This highlights the potential
of Video-LLaMA as a promising prototype for audio-visual AI assistants.",2023-06-05
"Identifying the style by a qualified reader on a short fragment of
  generated poetry",2023-06-05 10:55:15+00:00,http://arxiv.org/abs/2306.02771v1,Boris Orekhov,"cs.CL, cs.AI, cs.LG",image2text,"Style is an important concept in today's challenges in natural language
generating. After the success in the field of image style transfer, the task of
text style transfer became actual and attractive. Researchers are also
interested in the tasks of style reproducing in generation of the poetic text.
Evaluation of style reproducing in natural poetry generation remains a problem.
I used 3 character-based LSTM-models to work with style reproducing assessment.
All three models were trained on the corpus of texts by famous Russian-speaking
poets. Samples were shown to the assessors and 4 answer options were offered,
the style of which poet this sample reproduces. In addition, the assessors were
asked how well they were familiar with the work of the poet they had named.
Students studying history of literature were the assessors, 94 answers were
received. It has appeared that accuracy of definition of style increases if the
assessor can quote the poet by heart. Each model showed at least 0.7
macro-average accuracy. The experiment showed that it is better to involve a
professional rather than a naive reader in the evaluation of style in the tasks
of poetry generation, while lstm models are good at reproducing the style of
Russian poets even on a limited training corpus.",2023-06-05
Multilingual Conceptual Coverage in Text-to-Image Models,2023-06-02 17:59:09+00:00,http://arxiv.org/abs/2306.01735v1,"Michael Saxon, William Yang Wang","cs.CL, cs.AI, cs.CV, eess.IV",image2text,"We propose ""Conceptual Coverage Across Languages"" (CoCo-CroLa), a technique
for benchmarking the degree to which any generative text-to-image system
provides multilingual parity to its training language in terms of tangible
nouns. For each model we can assess ""conceptual coverage"" of a given target
language relative to a source language by comparing the population of images
generated for a series of tangible nouns in the source language to the
population of images generated for each noun under translation in the target
language. This technique allows us to estimate how well-suited a model is to a
target language as well as identify model-specific weaknesses, spurious
correlations, and biases without a-priori assumptions. We demonstrate how it
can be used to benchmark T2I models in terms of multilinguality, and how
despite its simplicity it is a good proxy for impressive generalization.",2023-06-02
"FlexRound: Learnable Rounding based on Element-wise Division for
  Post-Training Quantization",2023-06-01 03:31:12+00:00,http://arxiv.org/abs/2306.00317v1,"Jung Hyun Lee, Jeonghoon Kim, Se Jung Kwon, Dongsoo Lee","cs.LG, cs.AI",image2text,"Post-training quantization (PTQ) has been gaining popularity for the
deployment of deep neural networks on resource-limited devices since unlike
quantization-aware training, neither a full training dataset nor end-to-end
training is required at all. As PTQ schemes based on reconstructing each layer
or block output turn out to be effective to enhance quantized model
performance, recent works have developed algorithms to devise and learn a new
weight-rounding scheme so as to better reconstruct each layer or block output.
In this work, we propose a simple yet effective new weight-rounding mechanism
for PTQ, coined FlexRound, based on element-wise division instead of typical
element-wise addition such that FlexRound enables jointly learning a common
quantization grid size as well as a different scale for each pre-trained
weight. Thanks to the reciprocal rule of derivatives induced by element-wise
division, FlexRound is inherently able to exploit pre-trained weights when
updating their corresponding scales, and thus, flexibly quantize pre-trained
weights depending on their magnitudes. We empirically validate the efficacy of
FlexRound on a wide range of models and tasks. To the best of our knowledge,
our work is the first to carry out comprehensive experiments on not only image
classification and natural language understanding but also natural language
generation, assuming a per-tensor uniform PTQ setting. Moreover, we
demonstrate, for the first time, that large language models can be efficiently
quantized, with only a negligible impact on performance compared to
half-precision baselines, achieved by reconstructing the output in a
block-by-block manner.",2023-06-01
"CapText: Large Language Model-based Caption Generation From Image
  Context and Description",2023-06-01 02:40:44+00:00,http://arxiv.org/abs/2306.00301v2,"Shinjini Ghosh, Sagnik Anupam","cs.LG, cs.CL",image2text,"While deep-learning models have been shown to perform well on image-to-text
datasets, it is difficult to use them in practice for captioning images. This
is because captions traditionally tend to be context-dependent and offer
complementary information about an image, while models tend to produce
descriptions that describe the visual features of the image. Prior research in
caption generation has explored the use of models that generate captions when
provided with the images alongside their respective descriptions or contexts.
We propose and evaluate a new approach, which leverages existing large language
models to generate captions from textual descriptions and context alone,
without ever processing the image directly. We demonstrate that after
fine-tuning, our approach outperforms current state-of-the-art image-text
alignment models like OSCAR-VinVL on this task on the CIDEr metric.",2023-06-01
"LMCap: Few-shot Multilingual Image Captioning by Retrieval Augmented
  Language Model Prompting",2023-05-31 13:03:17+00:00,http://arxiv.org/abs/2305.19821v1,"Rita Ramos, Bruno Martins, Desmond Elliott","cs.CL, cs.CV",image2text,"Multilingual image captioning has recently been tackled by training with
large-scale machine translated data, which is an expensive, noisy, and
time-consuming process. Without requiring any multilingual caption data, we
propose LMCap, an image-blind few-shot multilingual captioning model that works
by prompting a language model with retrieved captions. Specifically, instead of
following the standard encoder-decoder paradigm, given an image, LMCap first
retrieves the captions of similar images using a multilingual CLIP encoder.
These captions are then combined into a prompt for an XGLM decoder, in order to
generate captions in the desired language. In other words, the generation model
does not directly process the image, instead processing retrieved captions.
Experiments on the XM3600 dataset of geographically diverse images show that
our model is competitive with fully-supervised multilingual captioning models,
without requiring any supervised training on any captioning data.",2023-05-31
"Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic
  Rewards",2023-05-31 06:59:21+00:00,http://arxiv.org/abs/2305.19599v2,"Guian Fang, Zutao Jiang, Jianhua Han, Guansong Lu, Hang Xu, Xiaodan Liang","cs.CV, cs.AI",image2text,"Recent advances in text-to-image diffusion models have achieved remarkable
success in generating high-quality, realistic images from given text prompts.
However, previous methods fail to perform accurate modality alignment between
text concepts and generated images due to the lack of fine-level semantic
guidance that successfully diagnoses the modality discrepancy. In this paper,
we propose FineRewards to improve the alignment between text and images in
text-to-image diffusion models by introducing two new fine-grained semantic
rewards: the caption reward and the Semantic Segment Anything (SAM) reward.
From the global semantic view, the caption reward generates a corresponding
detailed caption that depicts all important contents in the synthetic image via
a BLIP-2 model and then calculates the reward score by measuring the similarity
between the generated caption and the given prompt. From the local semantic
view, the SAM reward segments the generated images into local parts with
category labels, and scores the segmented parts by measuring the likelihood of
each category appearing in the prompted scene via a large language model, i.e.,
Vicuna-7B. Additionally, we adopt an assemble reward-ranked learning strategy
to enable the integration of multiple reward functions to jointly guide the
model training. Adapting results of text-to-image models on the MS-COCO
benchmark show that the proposed semantic reward outperforms other baseline
reward functions with a considerable margin on both visual quality and semantic
similarity with the input prompt. Moreover, by adopting the assemble
reward-ranked learning strategy, we further demonstrate that model performance
is further improved when adapting under the unifying of the proposed semantic
reward with the current image rewards.",2023-05-31
Fine-grained Text Style Transfer with Diffusion-Based Language Models,2023-05-31 02:51:26+00:00,http://arxiv.org/abs/2305.19512v1,"Yiwei Lyu, Tiange Luo, Jiacheng Shi, Todd C. Hollon, Honglak Lee","cs.CL, cs.AI, cs.LG",image2text,"Diffusion probabilistic models have shown great success in generating
high-quality images controllably, and researchers have tried to utilize this
controllability into text generation domain. Previous works on diffusion-based
language models have shown that they can be trained without external knowledge
(such as pre-trained weights) and still achieve stable performance and
controllability. In this paper, we trained a diffusion-based model on StylePTB
dataset, the standard benchmark for fine-grained text style transfers. The
tasks in StylePTB requires much more refined control over the output text
compared to tasks evaluated in previous works, and our model was able to
achieve state-of-the-art performance on StylePTB on both individual and
compositional transfers. Moreover, our model, trained on limited data from
StylePTB without external knowledge, outperforms previous works that utilized
pretrained weights, embeddings, and external grammar parsers, and this may
indicate that diffusion-based language models have great potential under
low-resource settings.",2023-05-31
Learning to Imagine: Visually-Augmented Natural Language Generation,2023-05-26 13:59:45+00:00,http://arxiv.org/abs/2305.16944v2,"Tianyi Tang, Yushuo Chen, Yifan Du, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen",cs.CL,image2text,"People often imagine relevant scenes to aid in the writing process. In this
work, we aim to utilize visual information for composition in the same manner
as humans. We propose a method, LIVE, that makes pre-trained language models
(PLMs) Learn to Imagine for Visuallyaugmented natural language gEneration.
First, we imagine the scene based on the text: we use a diffusion model to
synthesize high-quality images conditioned on the input texts. Second, we use
CLIP to determine whether the text can evoke the imagination in a posterior
way. Finally, our imagination is dynamic, and we conduct synthesis for each
sentence rather than generate only one image for an entire paragraph.
Technically, we propose a novel plug-and-play fusion layer to obtain
visually-augmented representations for each text. Our vision-text fusion layer
is compatible with Transformerbased architecture. We have conducted extensive
experiments on four generation tasks using BART and T5, and the automatic
results and human evaluation demonstrate the effectiveness of our proposed
method. We will release the code, model, and data at the link:
https://github.com/RUCAIBox/LIVE.",2023-05-26
"Not All Metrics Are Guilty: Improving NLG Evaluation with LLM
  Paraphrasing",2023-05-24 11:53:29+00:00,http://arxiv.org/abs/2305.15067v1,"Tianyi Tang, Hongyuan Lu, Yuchen Eleanor Jiang, Haoyang Huang, Dongdong Zhang, Wayne Xin Zhao, Furu Wei",cs.CL,image2text,"Most research about natural language generation (NLG) relies on evaluation
benchmarks with limited references for a sample, which may result in poor
correlations with human judgements. The underlying reason is that one semantic
meaning can actually be expressed in different forms, and the evaluation with a
single or few references may not accurately reflect the quality of the model's
hypotheses. To address this issue, this paper presents a novel method, named
Para-Ref, to enhance existing evaluation benchmarks by enriching the number of
references. We leverage large language models (LLMs) to paraphrase a single
reference into multiple high-quality ones in diverse expressions. Experimental
results on representative NLG tasks of machine translation, text summarization,
and image caption demonstrate that our method can effectively improve the
correlation with human evaluation for sixteen automatic evaluation metrics by
+7.82% in ratio. We release the code and data at
https://github.com/RUCAIBox/Para-Ref.",2023-05-24
"I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create
  Visual Metaphors",2023-05-24 05:01:10+00:00,http://arxiv.org/abs/2305.14724v1,"Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, Smaranda Muresan","cs.CL, cs.AI, cs.CV, cs.HC",image2text,"Visual metaphors are powerful rhetorical devices used to persuade or
communicate creative ideas through images. Similar to linguistic metaphors,
they convey meaning implicitly through symbolism and juxtaposition of the
symbols. We propose a new task of generating visual metaphors from linguistic
metaphors. This is a challenging task for diffusion-based text-to-image models,
such as DALL$\cdot$E 2, since it requires the ability to model implicit meaning
and compositionality. We propose to solve the task through the collaboration
between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3
(davinci-002) with Chain-of-Thought prompting generates text that represents a
visual elaboration of the linguistic metaphor containing the implicit meaning
and relevant objects, which is then used as input to the diffusion-based
text-to-image models.Using a human-AI collaboration framework, where humans
interact both with the LLM and the top-performing diffusion model, we create a
high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic
metaphors and their associated visual elaborations. Evaluation by professional
illustrators shows the promise of LLM-Diffusion Model collaboration for this
task.To evaluate the utility of our Human-AI collaboration framework and the
quality of our dataset, we perform both an intrinsic human-based evaluation and
an extrinsic evaluation using visual entailment as a downstream task.",2023-05-24
"Gender Biases in Automatic Evaluation Metrics: A Case Study on Image
  Captioning",2023-05-24 04:27:40+00:00,http://arxiv.org/abs/2305.14711v1,"Haoyi Qiu, Zi-Yi Dou, Tianlu Wang, Asli Celikyilmaz, Nanyun Peng",cs.CL,image2text,"Pretrained model-based evaluation metrics have demonstrated strong
performance with high correlations with human judgments in various natural
language generation tasks such as image captioning. Despite the impressive
results, their impact on fairness is under-explored -- it is widely
acknowledged that pretrained models can encode societal biases, and utilizing
them for evaluation purposes may inadvertently manifest and potentially amplify
biases. In this paper, we conduct a systematic study in gender biases of
model-based evaluation metrics with a focus on image captioning tasks.
Specifically, we first identify and quantify gender biases in different
evaluation metrics regarding profession, activity, and object concepts. Then,
we demonstrate the negative consequences of using these biased metrics, such as
favoring biased generation models in deployment and propagating the biases to
generation models through reinforcement learning. We also present a simple but
effective alternative to reduce gender biases by combining n-gram
matching-based and pretrained model-based evaluation metrics.",2023-05-24
"Process-To-Text: A Framework for the Quantitative Description of
  Processes in Natural Language",2023-05-23 13:14:34+00:00,http://arxiv.org/abs/2305.14044v1,"Yago Fontenla-Seco, Alberto Bugarín-Diz, Manuel Lama",cs.CL,image2text,"In this paper we present the Process-To-Text (P2T) framework for the
automatic generation of textual descriptive explanations of processes. P2T
integrates three AI paradigms: process mining for extracting temporal and
structural information from a process, fuzzy linguistic protoforms for
modelling uncertain terms, and natural language generation for building the
explanations. A real use-case in the cardiology domain is presented, showing
the potential of P2T for providing natural language explanations addressed to
specialists.",2023-05-23
"VNHSGE: VietNamese High School Graduation Examination Dataset for Large
  Language Models",2023-05-20 14:13:08+00:00,http://arxiv.org/abs/2305.12199v1,"Dao Xuan-Quy, Le Ngoc-Bich, Vo The-Duy, Phan Xuan-Dung, Ngo Bac-Bien, Nguyen Van-Tien, Nguyen Thi-My-Thanh, Nguyen Hong-Phuoc",cs.CL,image2text,"The VNHSGE (VietNamese High School Graduation Examination) dataset, developed
exclusively for evaluating large language models (LLMs), is introduced in this
article. The dataset, which covers nine subjects, was generated from the
Vietnamese National High School Graduation Examination and comparable tests.
300 literary essays have been included, and there are over 19,000
multiple-choice questions on a range of topics. The dataset assesses LLMs in
multitasking situations such as question answering, text generation, reading
comprehension, visual question answering, and more by including both textual
data and accompanying images. Using ChatGPT and BingChat, we evaluated LLMs on
the VNHSGE dataset and contrasted their performance with that of Vietnamese
students to see how well they performed. The results show that ChatGPT and
BingChat both perform at a human level in a number of areas, including
literature, English, history, geography, and civics education. They still have
space to grow, though, especially in the areas of mathematics, physics,
chemistry, and biology. The VNHSGE dataset seeks to provide an adequate
benchmark for assessing the abilities of LLMs with its wide-ranging coverage
and variety of activities. We intend to promote future developments in the
creation of LLMs by making this dataset available to the scientific community,
especially in resolving LLMs' limits in disciplines involving mathematics and
the natural sciences.",2023-05-20
STOAT: Structured Data to Analytical Text With Controls,2023-05-19 17:03:09+00:00,http://arxiv.org/abs/2305.11826v1,"Deepanway Ghosal, Preksha Nema, Aravindan Raghuveer","cs.CL, cs.AI",image2text,"Recent language models have made tremendous progress in the structured data
to text generation task. However, these models still give sub-optimal
performance where logical inference is required to generate the descriptions.
In this work, we specifically focus on analytical text generation from
structured data such as tables. Building on the taxonomy proposed in (Gupta et
al., 2020) we focus on controllable table to text generation for the following
reasoning categories: numerical reasoning, commonsense reasoning, temporal
reasoning, table knowledge, and entity knowledge. We propose STOAT model, which
is table and reasoning aware, with vector-quantization to infuse the given
reasoning categories in the output. We observe that our model provides 10.19%,
1.13% improvement on the PARENT metric in iToTTo and Infotabs for the
analytical sentence task. We also found that our model generates 15.3% more
faithful and analytical descriptions as compared to the baseline models in
human evaluation. We curate and release two reasoning category annotated
table-to-interesting text generation datasets based on the ToTTo (Parikh et
al., 2020) and InfoTabs datasets (Gupta et al.,2020).",2023-05-19
"Generating Visual Spatial Description via Holistic 3D Scene
  Understanding",2023-05-19 15:53:56+00:00,http://arxiv.org/abs/2305.11768v1,"Yu Zhao, Hao Fei, Wei Ji, Jianguo Wei, Meishan Zhang, Min Zhang, Tat-Seng Chua","cs.CV, cs.CL",image2text,"Visual spatial description (VSD) aims to generate texts that describe the
spatial relations of the given objects within images. Existing VSD work merely
models the 2D geometrical vision features, thus inevitably falling prey to the
problem of skewed spatial understanding of target objects. In this work, we
investigate the incorporation of 3D scene features for VSD. With an external 3D
scene extractor, we obtain the 3D objects and scene features for input images,
based on which we construct a target object-centered 3D spatial scene graph
(Go3D-S2G), such that we model the spatial semantics of target objects within
the holistic 3D scenes. Besides, we propose a scene subgraph selecting
mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the
diverse local structure features are navigated to yield spatially-diversified
text generation. Experimental results on two VSD datasets demonstrate that our
framework outperforms the baselines significantly, especially improving on the
cases with complex visual spatial relations. Meanwhile, our method can produce
more spatially-diversified generation. Code is available at
https://github.com/zhaoyucs/VSD.",2023-05-19
Brain Captioning: Decoding human brain activity into images and text,2023-05-19 09:57:19+00:00,http://arxiv.org/abs/2305.11560v1,"Matteo Ferrante, Furkan Ozcelik, Tommaso Boccato, Rufin VanRullen, Nicola Toschi","cs.CV, cs.AI",image2text,"Every day, the human brain processes an immense volume of visual information,
relying on intricate neural mechanisms to perceive and interpret these stimuli.
Recent breakthroughs in functional magnetic resonance imaging (fMRI) have
enabled scientists to extract visual information from human brain activity
patterns. In this study, we present an innovative method for decoding brain
activity into meaningful images and captions, with a specific focus on brain
captioning due to its enhanced flexibility as compared to brain decoding into
images. Our approach takes advantage of cutting-edge image captioning models
and incorporates a unique image reconstruction pipeline that utilizes latent
diffusion models and depth estimation. We utilized the Natural Scenes Dataset,
a comprehensive fMRI dataset from eight subjects who viewed images from the
COCO dataset. We employed the Generative Image-to-text Transformer (GIT) as our
backbone for captioning and propose a new image reconstruction pipeline based
on latent diffusion models. The method involves training regularized linear
regression models between brain activity and extracted features. Additionally,
we incorporated depth maps from the ControlNet model to further guide the
reconstruction process. We evaluate our methods using quantitative metrics for
both generated captions and images. Our brain captioning approach outperforms
existing methods, while our image reconstruction pipeline generates plausible
images with improved spatial relationships. In conclusion, we demonstrate
significant progress in brain decoding, showcasing the enormous potential of
integrating vision and language to better understand human cognition. Our
approach provides a flexible platform for future research, with potential
applications in various fields, including neural art, style transfer, and
portable devices.",2023-05-19
"Collaborative Generative AI: Integrating GPT-k for Efficient Editing in
  Text-to-Image Generation",2023-05-18 21:53:58+00:00,http://arxiv.org/abs/2305.11317v1,"Wanrong Zhu, Xinyi Wang, Yujie Lu, Tsu-Jui Fu, Xin Eric Wang, Miguel Eckstein, William Yang Wang",cs.CL,image2text,"The field of text-to-image (T2I) generation has garnered significant
attention both within the research community and among everyday users. Despite
the advancements of T2I models, a common issue encountered by users is the need
for repetitive editing of input prompts in order to receive a satisfactory
image, which is time-consuming and labor-intensive. Given the demonstrated text
generation power of large-scale language models, such as GPT-k, we investigate
the potential of utilizing such models to improve the prompt editing process
for T2I generation. We conduct a series of experiments to compare the common
edits made by humans and GPT-k, evaluate the performance of GPT-k in prompting
T2I, and examine factors that may influence this process. We found that GPT-k
models focus more on inserting modifiers while humans tend to replace words and
phrases, which includes changes to the subject matter. Experimental results
show that GPT-k are more effective in adjusting modifiers rather than
predicting spontaneous changes in the primary subject matters. Adopting the
edit suggested by GPT-k models may reduce the percentage of remaining edits by
20-30%.",2023-05-18
AIwriting: Relations Between Image Generation and Digital Writing,2023-05-18 09:23:05+00:00,http://arxiv.org/abs/2305.10834v1,"Scott Rettberg, Talan Memmott, Jill Walker Rettberg, Jason Nelson, Patrick Lichty","cs.AI, cs.CL, cs.HC, cs.MM, J.5",image2text,"During 2022, both transformer-based AI text generation sys-tems such as GPT-3
and AI text-to-image generation systems such as DALL-E 2 and Stable Diffusion
made exponential leaps forward and are unquestionably altering the fields of
digital art and electronic literature. In this panel a group of electronic
literature authors and theorists consider new oppor-tunities for human
creativity presented by these systems and present new works have produced
during the past year that specifically address these systems as environments
for literary expressions that are translated through iterative interlocutive
processes into visual representations. The premise that binds these
presentations is that these systems and the works gener-ated must be considered
from a literary perspective, as they originate in human writing. In works
ranging from a visual memoir of the personal experience of a health crisis, to
interac-tive web comics, to architectures based on abstract poetic language, to
political satire, four artists explore the capabili-ties of these writing
environments for new genres of literary artist practice, while a digital
culture theorist considers the origins and effects of the particular training
datasets of human language and images on which these new hybrid forms are
based.",2023-05-18
"ReGen: Zero-Shot Text Classification via Training Data Generation with
  Progressive Dense Retrieval",2023-05-18 04:30:09+00:00,http://arxiv.org/abs/2305.10703v1,"Yue Yu, Yuchen Zhuang, Rongzhi Zhang, Yu Meng, Jiaming Shen, Chao Zhang","cs.CL, cs.IR, cs.LG",image2text,"With the development of large language models (LLMs), zero-shot learning has
attracted much attention for various NLP tasks. Different from prior works that
generate training data with billion-scale natural language generation (NLG)
models, we propose a retrieval-enhanced framework to create training data from
a general-domain unlabeled corpus. To realize this, we first conduct
contrastive pretraining to learn an unsupervised dense retriever for extracting
the most relevant documents using class-descriptive verbalizers. We then
further propose two simple strategies, namely Verbalizer Augmentation with
Demonstrations and Self-consistency Guided Filtering to improve the topic
coverage of the dataset while removing noisy examples. Experiments on nine
datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines
and saves around 70% of the time compared to baselines using large NLG models.
Besides, REGEN can be naturally integrated with recently proposed large
language models to boost performance.",2023-05-18
What You See is What You Read? Improving Text-Image Alignment Evaluation,2023-05-17 17:43:38+00:00,http://arxiv.org/abs/2305.10400v1,"Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee Aharoni, Jonathan Herzig, Oran Lang, Eran Ofek, Idan Szpektor","cs.CL, cs.CV",image2text,"Automatically determining whether a text and a corresponding image are
semantically aligned is a significant challenge for vision-language models,
with applications in generative text-to-image and image-to-text tasks. In this
work, we study methods for automatic text-image alignment evaluation. We first
introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets
from both text-to-image and image-to-text generation tasks, with human
judgements for whether a given text-image pair is semantically aligned. We then
describe two automatic methods to determine alignment: the first involving a
pipeline based on question generation and visual question answering models, and
the second employing an end-to-end classification approach by finetuning
multimodal pretrained models. Both methods surpass prior approaches in various
text-image alignment tasks, with significant improvements in challenging cases
that involve complex composition or unnatural images. Finally, we demonstrate
how our approaches can localize specific misalignments between an image and a
given text, and how they can be used to automatically re-rank candidates in
text-to-image generation.",2023-05-17
Equivariant Few-Shot Learning from Pretrained Models,2023-05-17 02:20:34+00:00,http://arxiv.org/abs/2305.09900v1,"Sourya Basu, Pulkit Katdare, Prasanna Sattigeri, Vijil Chenthamarakshan, Katherine Driggs-Campbell, Payel Das, Lav R. Varshney","cs.LG, cs.AI, cs.CL, cs.CV",image2text,"Efficient transfer learning algorithms are key to the success of foundation
models on diverse downstream tasks even with limited data. Recent works of
\cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging
(\textit{equitune}) and optimization-based methods, respectively, over features
from group-transformed inputs to obtain equivariant outputs from
non-equivariant neural networks. While \cite{kaba2022equivariance} are only
concerned with training from scratch, we find that equitune performs poorly on
equivariant zero-shot tasks despite good finetuning results. We hypothesize
that this is because pretrained models provide better quality features for
certain transformations than others and simply averaging them is deleterious.
Hence, we propose $\lambda$-\textit{equitune} that averages the features using
\textit{importance weights}, $\lambda$s. These weights are learned directly
from the data using a small neural network, leading to excellent zero-shot and
finetuned results that outperform equitune. Further, we prove that
$\lambda$-equitune is equivariant and a universal approximator of equivariant
functions. Additionally, we show that the method of \cite{kaba2022equivariance}
used with appropriate loss functions, which we call \textit{equizero}, also
gives excellent zero-shot and finetuned performance. Both equitune and equizero
are special cases of $\lambda$-equitune. To show the simplicity and generality
of our method, we validate on a wide range of diverse applications and models
such as 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in
natural language generation (NLG), 4) compositional generalization in
languages, and 5) image classification using pretrained CNNs such as Resnet and
Alexnet.",2023-05-17
AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation,2023-05-16 15:10:22+00:00,http://arxiv.org/abs/2305.09515v2,"Tong Wu, Zhihao Fan, Xiao Liu, Yeyun Gong, Yelong Shen, Jian Jiao, Hai-Tao Zheng, Juntao Li, Zhongyu Wei, Jian Guo, Nan Duan, Weizhu Chen",cs.CL,image2text,"Diffusion models have gained significant attention in the realm of image
generation due to their exceptional performance. Their success has been
recently expanded to text generation via generating all tokens within a
sequence concurrently. However, natural language exhibits a far more pronounced
sequential dependency in comparison to images, and the majority of existing
language models are trained with a left-to-right auto-regressive approach. To
account for the inherent sequential characteristic of natural language, we
introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that
the generation of tokens on the right depends on the generated ones on the
left, a mechanism achieved through employing a dynamic number of denoising
steps that vary based on token position. This results in tokens on the left
undergoing fewer denoising steps than those on the right, thereby enabling them
to generate earlier and subsequently influence the generation of tokens on the
right. In a series of experiments on various text generation tasks, including
text summarization, machine translation, and common sense generation,
AR-Diffusion clearly demonstrated its superiority over existing diffusion
language models and that it can be $100\times\sim600\times$ faster when
achieving comparable results. Our code is available at
https://github.com/microsoft/ProphetNet/tree/master/AR-diffusion.",2023-05-16
Generative AI: Implications and Applications for Education,2023-05-12 16:52:38+00:00,http://arxiv.org/abs/2305.07605v2,"Anastasia Olga, Tzirides, Akash Saini, Gabriela Zapata, Duane Searsmith, Bill Cope, Mary Kalantzis, Vania Castro, Theodora Kourkoulou, John Jones, Rodrigo Abrantes da Silva, Jen Whiting, Nikoleta Polyxeni Kastania","cs.CY, cs.AI",image2text,"The launch of ChatGPT in November 2022 precipitated a panic among some
educators while prompting qualified enthusiasm from others. Under the umbrella
term Generative AI, ChatGPT is an example of a range of technologies for the
delivery of computer-generated text, image, and other digitized media. This
paper examines the implications for education of one generative AI technology,
chatbots responding from large language models, or C-LLM. It reports on an
application of a C-LLM to AI review and assessment of complex student work. In
a concluding discussion, the paper explores the intrinsic limits of generative
AI, bound as it is to language corpora and their textual representation through
binary notation. Within these limits, we suggest the range of emerging and
potential applications of Generative AI in education.",2023-05-12
Two-in-One: A Model Hijacking Attack Against Text Generation Models,2023-05-12 12:13:27+00:00,http://arxiv.org/abs/2305.07406v1,"Wai Man Si, Michael Backes, Yang Zhang, Ahmed Salem","cs.CR, cs.CL, cs.LG",image2text,"Machine learning has progressed significantly in various applications ranging
from face recognition to text generation. However, its success has been
accompanied by different attacks. Recently a new attack has been proposed which
raises both accountability and parasitic computing risks, namely the model
hijacking attack. Nevertheless, this attack has only focused on image
classification tasks. In this work, we broaden the scope of this attack to
include text generation and classification models, hence showing its broader
applicability. More concretely, we propose a new model hijacking attack, Ditto,
that can hijack different text classification tasks into multiple generation
ones, e.g., language translation, text summarization, and language modeling. We
use a range of text benchmark datasets such as SST-2, TweetEval, AGnews, QNLI,
and IMDB to evaluate the performance of our attacks. Our results show that by
using Ditto, an adversary can successfully hijack text generation models
without jeopardizing their utility.",2023-05-12
"Vision-Language Models in Remote Sensing: Current Progress and Future
  Trends",2023-05-09 19:17:07+00:00,http://arxiv.org/abs/2305.05726v1,"Congcong Wen, Yuan Hu, Xiang Li, Zhenghang Yuan, Xiao Xiang Zhu","cs.CV, cs.AI",image2text,"The remarkable achievements of ChatGPT and GPT-4 have sparked a wave of
interest and research in the field of large language models for Artificial
General Intelligence (AGI). These models provide us with intelligent solutions
that are more similar to human thinking, enabling us to use general artificial
intelligence to solve problems in various applications. However, in the field
of remote sensing, the scientific literature on the implementation of AGI
remains relatively scant. Existing AI-related research primarily focuses on
visual understanding tasks while neglecting the semantic understanding of the
objects and their relationships. This is where vision-language models excel, as
they enable reasoning about images and their associated textual descriptions,
allowing for a deeper understanding of the underlying semantics.
Vision-language models can go beyond recognizing the objects in an image and
can infer the relationships between them, as well as generate natural language
descriptions of the image. This makes them better suited for tasks that require
both visual and textual understanding, such as image captioning, text-based
image retrieval, and visual question answering. This paper provides a
comprehensive review of the research on vision-language models in remote
sensing, summarizing the latest progress, highlighting the current challenges,
and identifying potential research opportunities. Specifically, we review the
application of vision-language models in several mainstream remote sensing
tasks, including image captioning, text-based image generation, text-based
image retrieval, visual question answering, scene classification, semantic
segmentation, and object detection. For each task, we briefly describe the task
background and review some representative works. Finally, we summarize the
limitations of existing work and provide some possible directions for future
development.",2023-05-09
"UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in
  Vietnamese",2023-05-07 02:48:47+00:00,http://arxiv.org/abs/2305.04166v1,"Doanh C. Bui, Nghia Hieu Nguyen, Khang Nguyen","cs.CV, cs.CL",image2text,"Image Captioning is one of the vision-language tasks that still interest the
research community worldwide in the 2020s. MS-COCO Caption benchmark is
commonly used to evaluate the performance of advanced captioning models,
although it was published in 2015. Recent captioning models trained on the
MS-COCO Caption dataset only have good performance in language patterns of
English; they do not have such good performance in contexts captured in Vietnam
or fluently caption images using Vietnamese. To contribute to the low-resources
research community as in Vietnam, we introduce a novel image captioning dataset
in Vietnamese, the Open-domain Vietnamese Image Captioning dataset
(UIT-OpenViIC). The introduced dataset includes complex scenes captured in
Vietnam and manually annotated by Vietnamese under strict rules and
supervision. In this paper, we present in more detail the dataset creation
process. From preliminary analysis, we show that our dataset is challenging to
recent state-of-the-art (SOTA) Transformer-based baselines, which performed
well on the MS COCO dataset. Then, the modest results prove that UIT-OpenViIC
has room to grow, which can be one of the standard benchmarks in Vietnamese for
the research community to evaluate their captioning models. Furthermore, we
present a CAMO approach that effectively enhances the image representation
ability by a multi-level encoder output fusion mechanism, which helps improve
the quality of generated captions compared to previous captioning models.",2023-05-07
"Simulating H.P. Lovecraft horror literature with the ChatGPT large
  language model",2023-05-05 11:03:03+00:00,http://arxiv.org/abs/2305.03429v1,"Eduardo C. Garrido-Merchán, José Luis Arroyo-Barrigüete, Roberto Gozalo-Brihuela",cs.CL,image2text,"In this paper, we present a novel approach to simulating H.P. Lovecraft's
horror literature using the ChatGPT large language model, specifically the
GPT-4 architecture. Our study aims to generate text that emulates Lovecraft's
unique writing style and themes, while also examining the effectiveness of
prompt engineering techniques in guiding the model's output. To achieve this,
we curated a prompt containing several specialized literature references and
employed advanced prompt engineering methods. We conducted an empirical
evaluation of the generated text by administering a survey to a sample of
undergraduate students. Utilizing statistical hypothesis testing, we assessed
the students ability to distinguish between genuine Lovecraft works and those
generated by our model. Our findings demonstrate that the participants were
unable to reliably differentiate between the two, indicating the effectiveness
of the GPT-4 model and our prompt engineering techniques in emulating
Lovecraft's literary style. In addition to presenting the GPT model's
capabilities, this paper provides a comprehensive description of its underlying
architecture and offers a comparative analysis with related work that simulates
other notable authors and philosophers, such as Dennett. By exploring the
potential of large language models in the context of literary emulation, our
study contributes to the body of research on the applications and limitations
of these models in various creative domains.",2023-05-05
VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation,2023-05-04 23:27:21+00:00,http://arxiv.org/abs/2305.03204v1,"Xilun Chen, Lili Yu, Wenhan Xiong, Barlas Oğuz, Yashar Mehdad, Wen-tau Yih","cs.CV, cs.CL",image2text,"We propose a new two-stage pre-training framework for video-to-text
generation tasks such as video captioning and video question answering: A
generative encoder-decoder model is first jointly pre-trained on massive
image-text data to learn fundamental vision-language concepts, and then adapted
to video data in an intermediate video-text pre-training stage to learn
video-specific skills such as spatio-temporal reasoning. As a result, our
VideoOFA model achieves new state-of-the-art performance on four Video
Captioning benchmarks, beating prior art by an average of 9.7 points in CIDEr
score. It also outperforms existing models on two open-ended Video Question
Answering datasets, showcasing its generalization capability as a universal
video-to-text model.",2023-05-04
Image Captioners Sometimes Tell More Than Images They See,2023-05-04 15:32:41+00:00,http://arxiv.org/abs/2305.02932v1,"Honori Udo, Takafumi Koshinaka","cs.CV, cs.MM",image2text,"Image captioning, a.k.a. ""image-to-text,"" which generates descriptive text
from given images, has been rapidly developing throughout the era of deep
learning. To what extent is the information in the original image preserved in
the descriptive text generated by an image captioner? To answer that question,
we have performed experiments involving the classification of images from
descriptive text alone, without referring to the images at all, and compared
results with those from standard image-based classifiers. We have evaluate
several image captioning models with respect to a disaster image classification
task, CrisisNLP, and show that descriptive text classifiers can sometimes
achieve higher accuracy than standard image-based classifiers. Further, we show
that fusing an image-based classifier with a descriptive text classifier can
provide improvement in accuracy.",2023-05-04
"Governance of the AI, by the AI, and for the AI",2023-05-04 03:29:07+00:00,http://arxiv.org/abs/2305.03719v1,"Andrew W. Torrance, Bill Tomlinson","cs.CY, cs.AI",image2text,"Over the past half century, there have been several false dawns during which
the ""arrival"" of world-changing artificial intelligence (AI) has been heralded.
Tempting fate, the authors believe the age of AI has, indeed, finally arrived.
Powerful image generators, such as DALL-E2 and Midjourney have suddenly allowed
anyone with access the ability easily to create rich and complex art. In a
similar vein, text generators, such as GPT3.5 (including ChatGPT) and BLOOM,
allow users to compose detailed written descriptions of many topics of
interest. And, it is even possible now for a person without extensive expertise
in writing software to use AI to generate code capable of myriad applications.
While AI will continue to evolve and improve, probably at a rapid rate, the
current state of AI is already ushering in profound changes to many different
sectors of society. Every new technology challenges the ability of humanity to
govern it wisely. However, governance is usually viewed as both possible and
necessary due to the disruption new technology often poses to social
structures, industries, the environment, and other important human concerns. In
this article, we offer an analysis of a range of interactions between AI and
governance, with the hope that wise decisions may be made that maximize
benefits and minimize costs. The article addresses two main aspects of this
relationship: the governance of AI by humanity, and the governance of humanity
by AI. The approach we have taken is itself informed by AI, as this article was
written collaboratively by the authors and ChatGPT.",2023-05-04
Controlled Text Generation with Natural Language Instructions,2023-04-27 15:56:34+00:00,http://arxiv.org/abs/2304.14293v1,"Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, Mrinmaya Sachan","cs.CL, cs.AI, cs.LG",image2text,"Large language models generate fluent texts and can follow natural language
instructions to solve a wide range of tasks without task-specific training.
Nevertheless, it is notoriously difficult to control their generation to
satisfy the various constraints required by different applications. In this
work, we present InstructCTG, a controlled text generation framework that
incorporates different constraints by conditioning on natural language
descriptions and demonstrations of the constraints. In particular, we first
extract the underlying constraints of natural texts through a combination of
off-the-shelf NLP tools and simple heuristics. We then verbalize the
constraints into natural language instructions to form weakly supervised
training data. By prepending natural language descriptions of the constraints
and a few demonstrations, we fine-tune a pre-trained language model to
incorporate various types of constraints. Compared to existing search-based or
score-based methods, InstructCTG is more flexible to different constraint types
and has a much smaller impact on the generation quality and speed because it
does not modify the decoding procedure. Additionally, InstructCTG allows the
model to adapt to new constraints without re-training through the use of
few-shot task generalization and in-context learning abilities of
instruction-tuned language models.",2023-04-27
"From Association to Generation: Text-only Captioning by Unsupervised
  Cross-modal Mapping",2023-04-26 04:06:20+00:00,http://arxiv.org/abs/2304.13273v3,"Junyang Wang, Ming Yan, Yi Zhang, Jitao Sang","cs.CV, cs.CL, cs.LG",image2text,"With the development of Vision-Language Pre-training Models (VLPMs)
represented by CLIP and ALIGN, significant breakthroughs have been achieved for
association-based visual tasks such as image classification and image-text
retrieval by the zero-shot capability of CLIP without fine-tuning. However,
CLIP is hard to apply to generation-based tasks. This is due to the lack of
decoder architecture and pre-training tasks for generation. Although previous
works have created generation capacity for CLIP through additional language
models, a modality gap between the CLIP representations of different modalities
and the inability of CLIP to model the offset of this gap, which fails the
concept to transfer across modalities. To solve the problem, we try to map
images/videos to the language modality and generate captions from the language
modality. In this paper, we propose the K-nearest-neighbor Cross-modality
Mapping (Knight), a zero-shot method from association to generation. With
text-only unsupervised training, Knight achieves State-of-the-Art performance
in zero-shot methods for image captioning and video captioning. Our code is
available at https://github.com/junyangwang0410/Knight.",2023-04-26
RenderDiffusion: Text Generation as Image Generation,2023-04-25 02:14:44+00:00,http://arxiv.org/abs/2304.12519v1,"Junyi Li, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen","cs.CL, cs.CV, cs.LG",image2text,"Diffusion models have become a new generative paradigm for text generation.
Considering the discrete categorical nature of text, in this paper, we propose
\textsc{RenderDiffusion}, a novel diffusion approach for text generation via
text-guided image generation. Our key idea is to render the target text as a
\emph{glyph image} containing visual language content. In this way, conditional
text generation can be cast as a glyph image generation task, and it is then
natural to apply continuous diffusion models to discrete texts. Specially, we
utilize a cascaded architecture (\ie a base and a super-resolution diffusion
model) to generate high-fidelity glyph images, conditioned on the input text.
Furthermore, we design a text grounding module to transform and refine the
visual language content from generated glyph images into the final texts. In
experiments over four conditional text generation tasks and two classes of
metrics (\ie quality and diversity), \textsc{RenderDiffusion} can achieve
comparable or even better results than several baselines, including pretrained
language models. Our model also makes significant improvements compared to the
recent diffusion model.",2023-04-25
Token Imbalance Adaptation for Radiology Report Generation,2023-04-18 23:09:36+00:00,http://arxiv.org/abs/2304.09185v1,"Yuexin Wu, I-Chan Huang, Xiaolei Huang","cs.CL, cs.AI",image2text,"Imbalanced token distributions naturally exist in text documents, leading
neural language models to overfit on frequent tokens. The token imbalance may
dampen the robustness of radiology report generators, as complex medical terms
appear less frequently but reflect more medical information. In this study, we
demonstrate how current state-of-the-art models fail to generate infrequent
tokens on two standard benchmark datasets (IU X-RAY and MIMIC-CXR) of radiology
report generation. % However, no prior study has proposed methods to adapt
infrequent tokens for text generators feeding with medical images. To solve the
challenge, we propose the \textbf{T}oken \textbf{Im}balance Adapt\textbf{er}
(\textit{TIMER}), aiming to improve generation robustness on infrequent tokens.
The model automatically leverages token imbalance by an unlikelihood loss and
dynamically optimizes generation processes to augment infrequent tokens. We
compare our approach with multiple state-of-the-art methods on the two
benchmarks. Experiments demonstrate the effectiveness of our approach in
enhancing model robustness overall and infrequent tokens. Our ablation analysis
shows that our reinforcement learning method has a major effect in adapting
token imbalance for radiology report generation.",2023-04-18
"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and
  Dataset",2023-04-17 15:08:15+00:00,http://arxiv.org/abs/2304.08345v1,"Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang, Jing Liu","cs.LG, cs.CL, cs.CV, cs.MM, eess.AS",image2text,"In this paper, we propose a Vision-Audio-Language Omni-peRception pretraining
model (VALOR) for multi-modal understanding and generation. Different from
widely-studied vision-language pretraining models, VALOR jointly models
relationships of vision, audio and language in an end-to-end manner. It
contains three separate encoders for single modality representations, and a
decoder for multimodal conditional text generation. We design two pretext tasks
to pretrain VALOR model, including Multimodal Grouping Alignment (MGA) and
Multimodal Grouping Captioning (MGC). MGA projects vision, language and audio
to the same common space, building vision-language, audio-language and
audiovisual-language alignment simultaneously. MGC learns how to generate text
tokens in conditions of vision, audio or their both. To promote
vision-audio-language pretraining research, we construct a large-scale
high-quality tri-modality dataset named VALOR-1M, which contains 1M audiable
videos with human annotated audiovisual captions. Extensive experiments show
that VALOR can learn strong multimodal correlations and be generalized to
various downstream tasks (e.g., retrieval, captioning and question answering),
with different input modalities (e.g., vision-language, audio-language and
audiovisual-language). VALOR achieves new state-of-the-art performances on
series of public cross-modality benchmarks. Code and data are available at
project page https://casia-iva-group.github.io/projects/VALOR.",2023-04-17
Improving Diffusion Models for Scene Text Editing with Dual Encoders,2023-04-12 02:08:34+00:00,http://arxiv.org/abs/2304.05568v1,"Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian Price, Shiyu Chang","cs.CV, cs.AI",image2text,"Scene text editing is a challenging task that involves modifying or inserting
specified texts in an image while maintaining its natural and realistic
appearance. Most previous approaches to this task rely on style-transfer models
that crop out text regions and feed them into image transfer models, such as
GANs. However, these methods are limited in their ability to change text style
and are unable to insert texts into images. Recent advances in diffusion models
have shown promise in overcoming these limitations with text-conditional image
editing. However, our empirical analysis reveals that state-of-the-art
diffusion models struggle with rendering correct text and controlling text
style. To address these problems, we propose DIFFSTE to improve pre-trained
diffusion models with a dual encoder design, which includes a character encoder
for better text legibility and an instruction encoder for better style control.
An instruction tuning framework is introduced to train our model to learn the
mapping from the text instruction to the corresponding image with either the
specified style or the style of the surrounding texts in the background. Such a
training method further brings our method the zero-shot generalization ability
to the following three scenarios: generating text with unseen font variation,
e.g., italic and bold, mixing different fonts to construct a new font, and
using more relaxed forms of natural language as the instructions to guide the
generation task. We evaluate our approach on five datasets and demonstrate its
superior performance in terms of text correctness, image naturalness, and style
controllability. Our code is publicly available.
https://github.com/UCSB-NLP-Chang/DiffSTE",2023-04-12
"ImageCaptioner$^2$: Image Captioner for Image Captioning Bias
  Amplification Assessment",2023-04-10 21:40:46+00:00,http://arxiv.org/abs/2304.04874v1,"Eslam Mohamed Bakr, Pengzhan Sun, Li Erran Li, Mohamed Elhoseiny","cs.CV, cs.AI, cs.LG",image2text,"Most pre-trained learning systems are known to suffer from bias, which
typically emerges from the data, the model, or both. Measuring and quantifying
bias and its sources is a challenging task and has been extensively studied in
image captioning. Despite the significant effort in this direction, we observed
that existing metrics lack consistency in the inclusion of the visual signal.
In this paper, we introduce a new bias assessment metric, dubbed
$ImageCaptioner^2$, for image captioning. Instead of measuring the absolute
bias in the model or the data, $ImageCaptioner^2$ pay more attention to the
bias introduced by the model w.r.t the data bias, termed bias amplification.
Unlike the existing methods, which only evaluate the image captioning
algorithms based on the generated captions only, $ImageCaptioner^2$
incorporates the image while measuring the bias. In addition, we design a
formulation for measuring the bias of generated captions as prompt-based image
captioning instead of using language classifiers. Finally, we apply our
$ImageCaptioner^2$ metric across 11 different image captioning architectures on
three different datasets, i.e., MS-COCO caption dataset, Artemis V1, and
Artemis V2, and on three different protected attributes, i.e., gender, race,
and emotions. Consequently, we verify the effectiveness of our
$ImageCaptioner^2$ metric by proposing AnonymousBench, which is a novel human
evaluation paradigm for bias metrics. Our metric shows significant superiority
over the recent bias metric; LIC, in terms of human alignment, where the
correlation scores are 80% and 54% for our metric and LIC, respectively. The
code is available at https://eslambakr.github.io/imagecaptioner2.github.io/.",2023-04-10
Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions,2023-04-09 12:46:18+00:00,http://arxiv.org/abs/2304.04227v2,"Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, Mohamed Elhoseiny","cs.CV, cs.AI",image2text,"Video captioning aims to convey dynamic scenes from videos using natural
language, facilitating the understanding of spatiotemporal information within
our environment. Although there have been recent advances, generating detailed
and enriched video descriptions continues to be a substantial challenge. In
this work, we introduce Video ChatCaptioner, an innovative approach for
creating more comprehensive spatiotemporal video descriptions. Our method
employs a ChatGPT model as a controller, specifically designed to select frames
for posing video content-driven questions. Subsequently, a robust algorithm is
utilized to answer these visual queries. This question-answer framework
effectively uncovers intricate video details and shows promise as a method for
enhancing video content. Following multiple conversational rounds, ChatGPT can
summarize enriched video content based on previous conversations. We
qualitatively demonstrate that our Video ChatCaptioner can generate captions
containing more visual details about the videos. The code is publicly available
at https://github.com/Vision-CAIR/ChatCaptioner",2023-04-09
"Opinion Mining from YouTube Captions Using ChatGPT: A Case Study of
  Street Interviews Polling the 2023 Turkish Elections",2023-04-07 01:25:22+00:00,http://arxiv.org/abs/2304.03434v1,"Tuğrulcan Elmas, İlker Gül","cs.SI, cs.CY",image2text,"Opinion mining plays a critical role in understanding public sentiment and
preferences, particularly in the context of political elections. Traditional
polling methods, while useful, can be expensive and less scalable. Social media
offers an alternative source of data for opinion mining but presents challenges
such as noise, biases, and platform limitations in data collection. In this
paper, we propose a novel approach for opinion mining, utilizing YouTube's
auto-generated captions from public interviews as a data source, specifically
focusing on the 2023 Turkish elections as a case study. We introduce an opinion
mining framework using ChatGPT to mass-annotate voting intentions and
motivations that represent the stance and frames prior to the election. We
report that ChatGPT can predict the preferred candidate with 97\% accuracy and
identify the correct voting motivation out of 13 possible choices with 71\%
accuracy based on the data collected from 325 interviews. We conclude by
discussing the robustness of our approach, accounting for factors such as
captions quality, interview length, and channels. This new method will offer a
less noisy and cost-effective alternative for opinion mining using social media
data.",2023-04-07
DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model,2023-04-06 02:27:22+00:00,http://arxiv.org/abs/2304.02827v1,"Hoigi Seo, Hayeon Kim, Gwanghyun Kim, Se Young Chun","cs.CV, cs.AI",image2text,"The increasing demand for high-quality 3D content creation has motivated the
development of automated methods for creating 3D object models from a single
image and/or from a text prompt. However, the reconstructed 3D objects using
state-of-the-art image-to-3D methods still exhibit low correspondence to the
given image and low multi-view consistency. Recent state-of-the-art text-to-3D
methods are also limited, yielding 3D samples with low diversity per prompt
with long synthesis time. To address these challenges, we propose DITTO-NeRF, a
novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a
single image. Our DITTO-NeRF consists of constructing high-quality partial 3D
object for limited in-boundary (IB) angles using the given or text-generated 2D
image from the frontal view and then iteratively reconstructing the remaining
3D NeRF using inpainting latent diffusion model. We propose progressive 3D
object reconstruction schemes in terms of scales (low to high resolution),
angles (IB angles initially to outer-boundary (OB) later), and masks (object to
background boundary) in our DITTO-NeRF so that high-quality information on IB
can be propagated into OB. Our DITTO-NeRF outperforms state-of-the-art methods
in terms of fidelity and diversity qualitatively and quantitatively with much
faster training times than prior arts on image/text-to-3D such as DreamFusion,
and NeuralLift-360.",2023-04-06
"Scalable and Accurate Self-supervised Multimodal Representation Learning
  without Aligned Video and Text Data",2023-04-04 19:11:05+00:00,http://arxiv.org/abs/2304.02080v1,"Vladislav Lialin, Stephen Rawls, David Chan, Shalini Ghosh, Anna Rumshisky, Wael Hamza","cs.CV, cs.CL",image2text,"Scaling up weakly-supervised datasets has shown to be highly effective in the
image-text domain and has contributed to most of the recent state-of-the-art
computer vision and multimodal neural networks. However, existing large-scale
video-text datasets and mining techniques suffer from several limitations, such
as the scarcity of aligned data, the lack of diversity in the data, and the
difficulty of collecting aligned data. Currently popular video-text data mining
approach via automatic speech recognition (ASR) used in HowTo100M provides
low-quality captions that often do not refer to the video content. Other mining
approaches do not provide proper language descriptions (video tags) and are
biased toward short clips (alt text). In this work, we show how recent advances
in image captioning allow us to pre-train high-quality video models without any
parallel video-text data. We pre-train several video captioning models that are
based on an OPT language model and a TimeSformer visual backbone. We fine-tune
these networks on several video captioning datasets. First, we demonstrate that
image captioning pseudolabels work better for pre-training than the existing
HowTo100M ASR captions. Second, we show that pre-training on both images and
videos produces a significantly better network (+4 CIDER on MSR-VTT) than
pre-training on a single modality. Our methods are complementary to the
existing pre-training or data mining approaches and can be used in a variety of
settings. Given the efficacy of the pseudolabeling method, we are planning to
publicly release the generated captions.",2023-04-04
Cross-Domain Image Captioning with Discriminative Finetuning,2023-04-04 09:33:16+00:00,http://arxiv.org/abs/2304.01662v1,"Roberto Dessì, Michele Bevilacqua, Eleonora Gualdoni, Nathanael Carraz Rakotonirina, Francesca Franzon, Marco Baroni","cs.CV, cs.AI, cs.CL",image2text,"Neural captioners are typically trained to mimic human-generated references
without optimizing for any specific communication goal, leading to problems
such as the generation of vague captions. In this paper, we show that
fine-tuning an out-of-the-box neural captioner with a self-supervised
discriminative communication objective helps to recover a plain, visually
descriptive language that is more informative about image contents. Given a
target image, the system must learn to produce a description that enables an
out-of-the-box text-conditioned image retriever to identify such image among a
set of candidates. We experiment with the popular ClipCap captioner, also
replicating the main results with BLIP. In terms of similarity to ground-truth
human descriptions, the captions emerging from discriminative finetuning lag
slightly behind those generated by the non-finetuned model, when the latter is
trained and tested on the same caption dataset. However, when the model is used
without further tuning to generate captions for out-of-domain datasets, our
discriminatively-finetuned captioner generates descriptions that resemble human
references more than those produced by the same captioner without finetuning.
We further show that, on the Conceptual Captions dataset, discriminatively
finetuned captions are more helpful than either vanilla ClipCap captions or
ground-truth captions for human annotators tasked with an image discrimination
task.",2023-04-04
Can AI Put Gamma-Ray Astrophysicists Out of a Job?,2023-03-31 07:29:47+00:00,http://arxiv.org/abs/2303.17853v2,"Samuel T. Spencer, Vikas Joshi, Alison M. W. Mitchell","physics.pop-ph, astro-ph.HE, cs.CL",image2text,"In what will likely be a litany of generative-model-themed arXiv submissions
celebrating April the 1st, we evaluate the capacity of state-of-the-art
transformer models to create a paper detailing the detection of a Pulsar Wind
Nebula with a non-existent Imaging Atmospheric Cherenkov Telescope (IACT)
Array. We do this to evaluate the ability of such models to interpret
astronomical observations and sources based on language information alone, and
to assess potential means by which fraudulently generated scientific papers
could be identified during peer review (given that reliable generative model
watermarking has yet to be deployed for these tools). We conclude that our jobs
as astronomers are safe for the time being. From this point on, prompts given
to ChatGPT and Stable Diffusion are shown in orange, text generated by ChatGPT
is shown in black, whereas analysis by the (human) authors is in blue.",2023-03-31
Prefix tuning for automated audio captioning,2023-03-30 16:01:28+00:00,http://arxiv.org/abs/2303.17489v2,"Minkyu Kim, Kim Sung-Bin, Tae-Hyun Oh","eess.AS, cs.MM, cs.SD",image2text,"Audio captioning aims to generate text descriptions from environmental
sounds. One challenge of audio captioning is the difficulty of the
generalization due to the lack of audio-text paired training data. In this
work, we propose a simple yet effective method of dealing with small-scaled
datasets by leveraging a pre-trained language model. We keep the language model
frozen to maintain the expressivity for text generation, and we only learn to
extract global and temporal features from the input audio. To bridge a modality
gap between the audio features and the language model, we employ mapping
networks that translate audio features to the continuous vectors the language
model can understand, called prefixes. We evaluate our proposed method on the
Clotho and AudioCaps dataset and show our method outperforms prior arts in
diverse experimental settings.",2023-03-30
GPT is becoming a Turing machine: Here are some ways to program it,2023-03-25 00:43:41+00:00,http://arxiv.org/abs/2303.14310v1,"Ana Jojic, Zhen Wang, Nebojsa Jojic",cs.CL,image2text,"We demonstrate that, through appropriate prompting, GPT-3 family of models
can be triggered to perform iterative behaviours necessary to execute (rather
than just write or recall) programs that involve loops, including several
popular algorithms found in computer science curricula or software developer
interviews. We trigger execution and description of Iterations by Regimenting
Self-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong
repetitive structure in an example of an execution path of a target program for
one particular input, 2) Prompting with fragments of execution paths, and 3)
Explicitly forbidding (skipping) self-attention to parts of the generated text.
On a dynamic program execution, IRSA leads to larger accuracy gains than
replacing the model with the much more powerful GPT-4. IRSA has promising
applications in education, as the prompts and responses resemble student
assignments in data structures and algorithms classes. Our findings hold
implications for evaluating LLMs, which typically target the in-context
learning: We show that prompts that may not even cover one full task example
can trigger algorithmic behaviour, allowing solving problems previously thought
of as hard for LLMs, such as logical puzzles. Consequently, prompt design plays
an even more critical role in LLM performance than previously recognized.",2023-03-25
CoBIT: A Contrastive Bi-directional Image-Text Generation Model,2023-03-23 17:24:31+00:00,http://arxiv.org/abs/2303.13455v1,"Haoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, Jiahui Yu","cs.CV, cs.CL",image2text,"The field of vision and language has witnessed a proliferation of pre-trained
foundation models. Most existing methods are independently pre-trained with
contrastive objective like CLIP, image-to-text generative objective like PaLI,
or text-to-image generative objective like Parti. However, the three objectives
can be pre-trained on the same data, image-text pairs, and intuitively they
complement each other as contrasting provides global alignment capacity and
generation grants fine-grained understanding. In this work, we present a
Contrastive Bi-directional Image-Text generation model (CoBIT), which attempts
to unify the three pre-training objectives in one framework. Specifically,
CoBIT employs a novel unicoder-decoder structure, consisting of an image
unicoder, a text unicoder and a cross-modal decoder. The image/text unicoders
can switch between encoding and decoding in different tasks, enabling
flexibility and shared knowledge that benefits both image-to-text and
text-to-image generations. CoBIT achieves superior performance in image
understanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)
and text-based content creation, particularly in zero-shot scenarios. For
instance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in
zero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.",2023-03-23
Open-Vocabulary Object Detection using Pseudo Caption Labels,2023-03-23 05:10:22+00:00,http://arxiv.org/abs/2303.13040v1,"Han-Cheol Cho, Won Young Jhoo, Wooyoung Kang, Byungseok Roh","cs.CV, cs.AI",image2text,"Recent open-vocabulary detection methods aim to detect novel objects by
distilling knowledge from vision-language models (VLMs) trained on a vast
amount of image-text pairs. To improve the effectiveness of these methods,
researchers have utilized datasets with a large vocabulary that contains a
large number of object classes, under the assumption that such data will enable
models to extract comprehensive knowledge on the relationships between various
objects and better generalize to unseen object classes. In this study, we argue
that more fine-grained labels are necessary to extract richer knowledge about
novel objects, including object attributes and relationships, in addition to
their names. To address this challenge, we propose a simple and effective
method named Pseudo Caption Labeling (PCL), which utilizes an image captioning
model to generate captions that describe object instances from diverse
perspectives. The resulting pseudo caption labels offer dense samples for
knowledge distillation. On the LVIS benchmark, our best model trained on the
de-duplicated VisualGenome dataset achieves an AP of 34.5 and an APr of 30.6,
comparable to the state-of-the-art performance. PCL's simplicity and
flexibility are other notable features, as it is a straightforward
pre-processing technique that can be used with any image captioning model
without imposing any restrictions on model architecture or training process.",2023-03-23
HIVE: Harnessing Human Feedback for Instructional Visual Editing,2023-03-16 19:47:41+00:00,http://arxiv.org/abs/2303.09618v1,"Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu","cs.CV, cs.AI, cs.CL, cs.HC, cs.LG",image2text,"Incorporating human feedback has been shown to be crucial to align text
generated by large language models to human preferences. We hypothesize that
state-of-the-art instructional image editing models, where outputs are
generated based on an input image and an editing instruction, could similarly
benefit from human feedback, as their outputs may not adhere to the correct
instructions and preferences of users. In this paper, we present a novel
framework to harness human feedback for instructional visual editing (HIVE).
Specifically, we collect human feedback on the edited images and learn a reward
function to capture the underlying user preferences. We then introduce scalable
diffusion model fine-tuning methods that can incorporate human preferences
based on the estimated reward. Besides, to mitigate the bias brought by the
limitation of data, we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset to boost the
performance of instructional image editing. We conduct extensive empirical
experiments quantitatively and qualitatively, showing that HIVE is favored over
previous state-of-the-art instructional image editing approaches by a large
margin.",2023-03-16
Text-to-image Diffusion Model in Generative AI: A Survey,2023-03-14 13:49:54+00:00,http://arxiv.org/abs/2303.07909v1,"Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, In So Kweon","cs.CV, cs.AI, cs.LG",image2text,"This survey reviews text-to-image diffusion models in the context that
diffusion models have emerged to be popular for a wide range of generative
tasks. As a self-contained work, this survey starts with a brief introduction
of how a basic diffusion model works for image synthesis, followed by how
condition or guidance improves learning. Based on that, we present a review of
state-of-the-art methods on text-conditioned image synthesis, i.e.,
text-to-image. We further summarize applications beyond text-to-image
generation: text-guided creative generation and text-guided image editing.
Beyond the progress made so far, we discuss existing challenges and promising
future directions.",2023-03-14
Diffusion Models in NLP: A Survey,2023-03-14 01:53:49+00:00,http://arxiv.org/abs/2303.07576v1,"Yuansong Zhu, Yu Zhao","cs.CL, cs.AI",image2text,"Diffusion models have become a powerful family of deep generative models,
with record-breaking performance in many applications. This paper first gives
an overview and derivation of the basic theory of diffusion models, then
reviews the research results of diffusion models in the field of natural
language processing, from text generation, text-driven image generation and
other four aspects, and analyzes and summarizes the relevant literature
materials sorted out, and finally records the experience and feelings of this
topic literature review research.",2023-03-14
"ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and
  Multilingual Natural Language Generation",2023-03-11 17:14:33+00:00,http://arxiv.org/abs/2303.06458v1,"Bang Yang, Fenglin Liu, Yuexian Zou, Xian Wu, Yaowei Wang, David A. Clifton","cs.CL, cs.AI, cs.CV",image2text,"Natural Language Generation (NLG) accepts input data in the form of images,
videos, or text and generates corresponding natural language text as output.
Existing NLG methods mainly adopt a supervised approach and rely heavily on
coupled data-to-text pairs. However, for many targeted scenarios and for
non-English languages, sufficient quantities of labeled data are often not
available. To relax the dependency on labeled data of downstream tasks, we
propose an intuitive and effective zero-shot learning framework, ZeroNLG, which
can deal with multiple NLG tasks, including image-to-text (image captioning),
video-to-text (video captioning), and text-to-text (neural machine
translation), across English, Chinese, German, and French within a unified
framework. ZeroNLG does not require any labeled downstream pairs for training.
During training, ZeroNLG (i) projects different domains (across modalities and
languages) to corresponding coordinates in a shared common latent space; (ii)
bridges different domains by aligning their corresponding coordinates in this
space; and (iii) builds an unsupervised multilingual auto-encoder to learn to
generate text by reconstructing the input text given its coordinate in shared
latent space. Consequently, during inference, based on the data-to-text
pipeline, ZeroNLG can generate target sentences across different languages
given the coordinate of input data in the common space. Within this unified
framework, given visual (imaging or video) data as input, ZeroNLG can perform
zero-shot visual captioning; given textual sentences as input, ZeroNLG can
perform zero-shot machine translation. We present the results of extensive
experiments on twelve NLG tasks, showing that, without using any labeled
downstream pairs for training, ZeroNLG generates high-quality and believable
outputs and significantly outperforms existing zero-shot methods.",2023-03-11
"Describe me an Aucklet: Generating Grounded Perceptual Category
  Descriptions",2023-03-07 17:01:25+00:00,http://arxiv.org/abs/2303.04053v2,"Bill Noble, Nikolai Ilinykh",cs.CL,image2text,"Human language users can generate descriptions of perceptual concepts beyond
instance-level representations and also use such descriptions to learn
provisional class-level representations. However, the ability of computational
models to learn and operate with class representations is under-investigated in
the language-and-vision field. In this paper, we train separate neural networks
to generate and interpret class-level descriptions. We then use the zero-shot
classification performance of the interpretation model as a measure of
communicative success and class-level conceptual grounding. We investigate the
performance of prototype- and exemplar-based neural representations grounded
category description. Finally, we show that communicative success reveals
performance issues in the generation model that are not captured by traditional
intrinsic NLG evaluation metrics, and argue that these issues can be traced to
a failure to properly ground language in vision at the class level. We observe
that the interpretation model performs better with descriptions that are low in
diversity on the class level, possibly indicating a strong reliance on
frequently occurring features.",2023-03-07
"DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only
  Training",2023-03-06 11:02:47+00:00,http://arxiv.org/abs/2303.03032v1,"Wei Li, Linchao Zhu, Longyin Wen, Yi Yang","cs.CV, cs.AI, cs.CL",image2text,"Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong
zero-shot transfer capability in many discriminative tasks. Their adaptation to
zero-shot image-conditioned text generation tasks has drawn increasing
interest. Prior arts approach to zero-shot captioning by either utilizing the
existing large language models (e.g., GPT-2) or pre-training the
encoder-decoder network in an end-to-end manner. In this work, we propose a
simple framework, named DeCap, for zero-shot captioning. We introduce a
lightweight visual-aware language decoder. This decoder is both data-efficient
and computation-efficient: 1) it only requires the text data for training,
easing the burden on the collection of paired data. 2) it does not require
end-to-end training. When trained with text-only data, the decoder takes the
text embedding extracted from the off-the-shelf CLIP encoder as a prefix
embedding. The challenge is that the decoder is trained on the text corpus but
at the inference stage, it needs to generate captions based on visual inputs.
The modality gap issue is widely observed in multi-modal contrastive models
that prevents us from directly taking the visual embedding as the prefix
embedding. We propose a training-free mechanism to reduce the modality gap. We
project the visual embedding into the CLIP text embedding space, while the
projected embedding retains the information of the visual input. Taking the
projected embedding as the prefix embedding, the decoder generates high-quality
descriptions that match the visual input. The experiments show that DeCap
outperforms other zero-shot captioning methods and unpaired captioning methods
on the typical image captioning benchmarks, i.e., MSCOCO and NoCaps.",2023-03-06
Interactive Text Generation,2023-03-02 01:57:17+00:00,http://arxiv.org/abs/2303.00908v1,"Felix Faltings, Michel Galley, Baolin Peng, Kianté Brantley, Weixin Cai, Yizhe Zhang, Jianfeng Gao, Bill Dolan",cs.CL,image2text,"Users interact with text, image, code, or other editors on a daily basis.
However, machine learning models are rarely trained in the settings that
reflect the interactivity between users and their editor. This is
understandable as training AI models with real users is not only slow and
costly, but what these models learn may be specific to user interface design
choices. Unfortunately, this means most of the research on text, code, and
image generation has focused on non-interactive settings, whereby the model is
expected to get everything right without accounting for any input from a user
who may be willing to help.
  We introduce a new Interactive Text Generation task that allows training
generation models interactively without the costs of involving real users, by
using user simulators that provide edits that guide the model towards a given
target text. We train our interactive models using Imitation Learning, and our
experiments against competitive non-interactive generation models show that
models trained interactively are superior to their non-interactive
counterparts, even when all models are given the same budget of user inputs or
edits.",2023-03-02
Few-Shot Table-to-Text Generation with Prompt-based Adapter,2023-02-24 05:48:53+00:00,http://arxiv.org/abs/2302.12468v1,"Zhixin Guo, Minyxuan Yan, Jiexing Qi, Jianping Zhou, Ziwei He, Zhouhan Lin, Guanjie Zheng, Xinbing Wang",cs.CL,image2text,"Pre-trained language models (PLMs) have made remarkable progress in
table-to-text generation tasks. However, the topological gap between tabular
data and text and the lack of domain-specific knowledge make it difficult for
PLMs to produce faithful text, especially in real-world applications with
limited resources. In this paper, we mitigate the above challenges by
introducing a novel augmentation method: Prompt-based Adapter (PA), which
targets table-to-text generation under few-shot conditions. The core insight
design of the PA is to inject prompt templates for augmenting domain-specific
knowledge and table-related representations into the model for bridging the
structural gap between tabular data and descriptions through adapters. Such
prompt-based knowledge augmentation method brings at least two benefits: (1)
enables us to fully use the large amounts of unlabelled domain-specific
knowledge, which can alleviate the PLMs' inherent shortcomings of lacking
domain knowledge; (2) allows us to design different types of tasks supporting
the generative challenge. Extensive experiments and analyses are conducted on
three open-domain few-shot NLG datasets: Humans, Books, and Songs. Compared to
previous state-of-the-art approaches, our model achieves superior performance
in terms of both fluency and accuracy as judged by human and automatic
evaluations.",2023-02-24
Improved Training of Mixture-of-Experts Language GANs,2023-02-23 09:25:46+00:00,http://arxiv.org/abs/2302.11875v1,"Yekun Chai, Qiyue Yin, Junge Zhang",cs.CL,image2text,"Despite the dramatic success in image generation, Generative Adversarial
Networks (GANs) still face great challenges in synthesizing sequences of
discrete elements, in particular human language. The difficulty in generator
training arises from the limited representation capacity and uninformative
learning signals obtained from the discriminator. In this work, we (1) first
empirically show that the mixture-of-experts approach is able to enhance the
representation capacity of the generator for language GANs and (2) harness the
Feature Statistics Alignment (FSA) paradigm to render fine-grained learning
signals to advance the generator training. Specifically, FSA forces the mean
statistics of the distribution of fake data to approach that of real samples as
close as possible in the finite-dimensional feature space. Empirical study on
synthetic and real benchmarks shows the superior performance in quantitative
evaluation and demonstrates the effectiveness of our approach to adversarial
text generation.",2023-02-23
Improving User Controlled Table-To-Text Generation Robustness,2023-02-20 07:51:15+00:00,http://arxiv.org/abs/2302.09820v1,"Hanxu Hu, Yunqing Liu, Zhongyi Yu, Laura Perez-Beltrachini",cs.CL,image2text,"In this work we study user controlled table-to-text generation where users
explore the content in a table by selecting cells and reading a natural
language description thereof automatically produce by a natural language
generator. Such generation models usually learn from carefully selected cell
combinations (clean cell selections); however, in practice users may select
unexpected, redundant, or incoherent cell combinations (noisy cell selections).
In experiments, we find that models perform well on test sets coming from the
same distribution as the train data but their performance drops when evaluated
on realistic noisy user inputs. We propose a fine-tuning regime with additional
user-simulated noisy cell selections. Models fine-tuned with the proposed
regime gain 4.85 BLEU points on user noisy test cases and 1.4 on clean test
cases; and achieve comparable state-of-the-art performance on the ToTTo
dataset.",2023-02-20
Large Scale Multi-Lingual Multi-Modal Summarization Dataset,2023-02-13 18:00:23+00:00,http://arxiv.org/abs/2302.06560v1,"Yash Verma, Anubhav Jangra, Raghvendra Kumar, Sriparna Saha","cs.CL, cs.MM",image2text,"Significant developments in techniques such as encoder-decoder models have
enabled us to represent information comprising multiple modalities. This
information can further enhance many downstream tasks in the field of
information retrieval and natural language processing; however, improvements in
multi-modal techniques and their performance evaluation require large-scale
multi-modal data which offers sufficient diversity. Multi-lingual modeling for
a variety of tasks like multi-modal summarization, text generation, and
translation leverages information derived from high-quality multi-lingual
annotated data. In this work, we present the current largest multi-lingual
multi-modal summarization dataset (M3LS), and it consists of over a million
instances of document-image pairs along with a professionally annotated
multi-modal summary for each pair. It is derived from news articles published
by British Broadcasting Corporation(BBC) over a decade and spans 20 languages,
targeting diversity across five language roots, it is also the largest
summarization dataset for 13 languages and consists of cross-lingual
summarization data for 2 languages. We formally define the multi-lingual
multi-modal summarization task utilizing our dataset and report baseline scores
from various state-of-the-art summarization techniques in a multi-lingual
setting. We also compare it with many similar datasets to analyze the
uniqueness and difficulty of M3LS.",2023-02-13
Plan-then-Seam: Towards Efficient Table-to-Text Generation,2023-02-10 09:43:15+00:00,http://arxiv.org/abs/2302.05138v1,"Liang Li, Ruiying Geng, Chengyang Fang, Bing Li, Can Ma, Binhua Li, Yongbin Li",cs.CL,image2text,"Table-to-text generation aims at automatically generating text to help people
conveniently obtain salient information in tables. Recent works explicitly
decompose the generation process into content planning and surface generation
stages, employing two autoregressive networks for them respectively. However,
they are computationally expensive due to the non-parallelizable nature of
autoregressive decoding and the redundant parameters of two networks. In this
paper, we propose the first totally non-autoregressive table-to-text model
(Plan-then-Seam, PTS) that produces its outputs in parallel with one single
network. PTS firstly writes and calibrates one plan of the content to be
generated with a novel rethinking pointer predictor, and then takes the plan as
the context for seaming to decode the description. These two steps share
parameters and perform iteratively to capture token inter-dependency while
keeping parallel decoding. Experiments on two public benchmarks show that PTS
achieves 3.0~5.6 times speedup for inference time, reducing 50% parameters,
while maintaining as least comparable performance against strong two-stage
table-to-text competitors.",2023-02-10
"Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot
  Image Captioning",2023-02-09 18:57:56+00:00,http://arxiv.org/abs/2302.04858v1,"Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhiding Yu, Shiyi Lan, Bo Li, Ming-Yu Liu, Yuke Zhu, Mohammad Shoeybi, Bryan Catanzaro, Chaowei Xiao, Anima Anandkumar","cs.CV, cs.AI, cs.CL, cs.IR, cs.LG",image2text,"Augmenting pretrained language models (LMs) with a vision encoder (e.g.,
Flamingo) has obtained state-of-the-art results in image-to-text generation.
However, these models store all the knowledge within their parameters, thus
often requiring enormous model parameters to model the abundant visual concepts
and very rich textual descriptions. Additionally, they are inefficient in
incorporating new data, requiring a computational-expensive fine-tuning
process. In this work, we introduce a Retrieval-augmented Visual Language
Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant
knowledge from the external database for zero and in-context few-shot
image-to-text generations. By storing certain knowledge explicitly in the
external database, our approach reduces the number of model parameters and can
easily accommodate new data during evaluation by simply updating the database.
We also construct an interleaved image and text data that facilitates
in-context few-shot learning capabilities. We demonstrate that Re-ViLM
significantly boosts performance for image-to-text generation tasks, especially
for zero-shot and few-shot generation in out-of-domain settings with 4 times
less parameters compared with baseline methods.",2023-02-09
"Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge
  Memorization",2023-02-09 03:04:11+00:00,http://arxiv.org/abs/2302.04415v1,"Zhixin Guo, Minyxuan Yan, Jiexing Qi, Jianping Zhou, Ziwei He, Zhouhan Lin, Guanjie Zheng, Xinbing Wang","cs.CL, cs.AI",image2text,"Pre-trained language models (PLM) have achieved remarkable advancement in
table-to-text generation tasks. However, the lack of labeled domain-specific
knowledge and the topology gap between tabular data and text make it difficult
for PLMs to yield faithful text. Low-resource generation likewise faces unique
challenges in this domain. Inspired by how humans descript tabular data with
prior knowledge, we suggest a new framework: PromptMize, which targets
table-to-text generation under few-shot settings. The design of our framework
consists of two aspects: a prompt planner and a knowledge adapter. The prompt
planner aims to generate a prompt signal that provides instance guidance for
PLMs to bridge the topology gap between tabular data and text. Moreover, the
knowledge adapter memorizes domain-specific knowledge from the unlabelled
corpus to supply essential information during generation. Extensive experiments
and analyses are investigated on three open domain few-shot NLG datasets:
human, song, and book. Compared with previous state-of-the-art approaches, our
model achieves remarkable performance in generating quality as judged by human
and automatic evaluations.",2023-02-09
Adversarial Prompting for Black Box Foundation Models,2023-02-08 18:07:31+00:00,http://arxiv.org/abs/2302.04237v1,"Natalie Maus, Patrick Chao, Eric Wong, Jacob Gardner",cs.LG,image2text,"Prompting interfaces allow users to quickly adjust the output of generative
models in both vision and language. However, small changes and design choices
in the prompt can lead to significant differences in the output. In this work,
we develop a black-box framework for generating adversarial prompts for
unstructured image and text generation. These prompts, which can be standalone
or prepended to benign prompts, induce specific behaviors into the generative
process, such as generating images of a particular object or biasing the
frequency of specific letters in the generated text.",2023-02-08
GPTScore: Evaluate as You Desire,2023-02-08 16:17:29+00:00,http://arxiv.org/abs/2302.04166v1,"Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu",cs.CL,image2text,"Generative Artificial Intelligence (AI) has enabled the development of
sophisticated models that are capable of producing high-caliber text, images,
and other outputs through the utilization of large pre-trained models.
Nevertheless, assessing the quality of the generation is an even more arduous
task than the generation itself, and this issue has not been given adequate
consideration recently. This paper proposes a novel evaluation framework,
GPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction)
from generative pre-trained models to score generated texts. Experimental
results on four text generation tasks, 22 evaluation aspects, and corresponding
37 datasets demonstrate that this approach can effectively allow us to achieve
what one desires to evaluate for texts simply by natural language instructions.
This nature helps us overcome several long-standing challenges in text
evaluation--how to achieve customized, multi-faceted evaluation without the
need for annotated samples. We make our code publicly available at
https://github.com/jinlanfu/GPTScore.",2023-02-08
Grounding Language Models to Images for Multimodal Generation,2023-01-31 18:33:44+00:00,http://arxiv.org/abs/2301.13823v1,"Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried","cs.CL, cs.AI, cs.CV, cs.LG",image2text,"We propose an efficient method to ground pretrained text-only language models
to the visual domain, enabling them to process and generate arbitrarily
interleaved image-and-text data. Our method leverages the abilities of language
models learnt from large scale text-only pretraining, such as in-context
learning and free-form text generation. We keep the language model frozen, and
finetune input and output linear layers to enable cross-modality interactions.
This allows our model to process arbitrarily interleaved image-and-text inputs,
and generate free-form text interleaved with retrieved images. We achieve
strong zero-shot performance on grounded tasks such as contextual image
retrieval and multimodal dialogue, and showcase compelling interactive
abilities. Our approach works with any off-the-shelf language model and paves
the way towards an effective, general solution for leveraging pretrained
language models in visually grounded settings.",2023-01-31
Semi-Parametric Video-Grounded Text Generation,2023-01-27 03:00:43+00:00,http://arxiv.org/abs/2301.11507v1,"Sungdong Kim, Jin-Hwa Kim, Jiyoung Lee, Minjoon Seo","cs.CV, cs.CL, cs.LG",image2text,"Efficient video-language modeling should consider the computational cost
because of a large, sometimes intractable, number of video frames. Parametric
approaches such as the attention mechanism may not be ideal since its
computational cost quadratically increases as the video length increases.
Rather, previous studies have relied on offline feature extraction or frame
sampling to represent the video efficiently, focusing on cross-modal modeling
in short video clips. In this paper, we propose a semi-parametric
video-grounded text generation model, SeViT, a novel perspective on scalable
video-language modeling toward long untrimmed videos. Treating a video as an
external data store, SeViT includes a non-parametric frame retriever to select
a few query-relevant frames from the data store for a given query and a
parametric generator to effectively aggregate the frames with the query via
late fusion methods. Experimental results demonstrate our method has a
significant advantage in longer videos and causal video understanding.
Moreover, our model achieves the new state of the art on four video-language
datasets, iVQA (+4.8), Next-QA (+6.9), and Activitynet-QA (+4.8) in accuracy,
and MSRVTT-Caption (+3.6) in CIDEr.",2023-01-27
Explaining Visual Biases as Words by Generating Captions,2023-01-26 13:58:46+00:00,http://arxiv.org/abs/2301.11104v1,"Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, Jinwoo Shin","cs.LG, cs.CV",image2text,"We aim to diagnose the potential biases in image classifiers. To this end,
prior works manually labeled biased attributes or visualized biased features,
which need high annotation costs or are often ambiguous to interpret. Instead,
we leverage two types (generative and discriminative) of pre-trained
vision-language models to describe the visual bias as a word. Specifically, we
propose bias-to-text (B2T), which generates captions of the mispredicted images
using a pre-trained captioning model to extract the common keywords that may
describe visual biases. Then, we categorize the bias type as spurious
correlation or majority bias by checking if it is specific or agnostic to the
class, based on the similarity of class-wise mispredicted images and the
keyword upon a pre-trained vision-language joint embedding space, e.g., CLIP.
We demonstrate that the proposed simple and intuitive scheme can recover
well-known gender and background biases, and discover novel ones in real-world
datasets. Moreover, we utilize B2T to compare the classifiers using different
architectures or training methods. Finally, we show that one can obtain
debiased classifiers using the B2T bias keywords and CLIP, in both zero-shot
and full-shot manners, without using any human annotation on the bias.",2023-01-26
MTTN: Multi-Pair Text to Text Narratives for Prompt Generation,2023-01-21 06:55:44+00:00,http://arxiv.org/abs/2301.10172v1,"Archan Ghosh, Debgandhar Ghosh, Madhurima Maji, Suchinta Chanda, Kalporup Goswami","cs.CL, cs.LG",image2text,"The explosive popularity of diffusion models[ 1][ 2][ 3 ] has provided a huge
stage for further development in generative-text modelling. As prompt based
models are very nuanced, such that a carefully generated prompt can produce
truely breath taking images, on the contrary producing powerful or even
meaningful prompt is a hit or a miss. To lavish on this we have introduced a
large scale derived and synthesized dataset built with on real prompts and
indexed with popular image-text datasets like MS-COCO[4 ], Flickr[ 5], etc. We
have also introduced staging for these sentences that sequentially reduce the
context and increase the complexity, that will further strengthen the output
because of the complex annotations that are being created. MTTN consists of
over 2.4M sentences that are divided over 5 stages creating a combination
amounting to over 12M pairs, along with a vocab size of consisting more than
300 thousands unique words that creates an abundance of variations. The
original 2.4M million pairs are broken down in such a manner that it produces a
true scenario of internet lingo that is used globally thereby heightening the
robustness of the dataset, and any model trained on it.",2023-01-21
Regeneration Learning: A Learning Paradigm for Data Generation,2023-01-21 01:33:34+00:00,http://arxiv.org/abs/2301.08846v1,"Xu Tan, Tao Qin, Jiang Bian, Tie-Yan Liu, Yoshua Bengio","cs.LG, cs.AI, cs.CL, cs.CV, eess.AS",image2text,"Machine learning methods for conditional data generation usually build a
mapping from source conditional data X to target data Y. The target Y (e.g.,
text, speech, music, image, video) is usually high-dimensional and complex, and
contains information that does not exist in source data, which hinders
effective and efficient learning on the source-target mapping. In this paper,
we present a learning paradigm called regeneration learning for data
generation, which first generates Y' (an abstraction/representation of Y) from
X and then generates Y from Y'. During training, Y' is obtained from Y through
either handcrafted rules or self-supervised learning and is used to learn
X-->Y' and Y'-->Y. Regeneration learning extends the concept of representation
learning to data generation tasks, and can be regarded as a counterpart of
traditional representation learning, since 1) regeneration learning handles the
abstraction (Y') of the target data Y for data generation while traditional
representation learning handles the abstraction (X') of source data X for data
understanding; 2) both the processes of Y'-->Y in regeneration learning and
X-->X' in representation learning can be learned in a self-supervised way
(e.g., pre-training); 3) both the mappings from X to Y' in regeneration
learning and from X' to Y in representation learning are simpler than the
direct mapping from X to Y. We show that regeneration learning can be a
widely-used paradigm for data generation (e.g., text generation, speech
recognition, speech synthesis, music composition, image generation, and video
generation) and can provide valuable insights into developing data generation
methods.",2023-01-21
Universal Multimodal Representation for Language Understanding,2023-01-09 13:54:11+00:00,http://arxiv.org/abs/2301.03344v1,"Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao Li, Hai Zhao","cs.CL, cs.AI, cs.CV",image2text,"Representation learning is the foundation of natural language processing
(NLP). This work presents new methods to employ visual information as assistant
signals to general NLP tasks. For each sentence, we first retrieve a flexible
number of images either from a light topic-image lookup table extracted over
the existing sentence-image pairs or a shared cross-modal embedding space that
is pre-trained on out-of-shelf text-image pairs. Then, the text and images are
encoded by a Transformer encoder and convolutional neural network,
respectively. The two sequences of representations are further fused by an
attention layer for the interaction of the two modalities. In this study, the
retrieval process is controllable and flexible. The universal visual
representation overcomes the lack of large-scale bilingual sentence-image
pairs. Our method can be easily applied to text-only tasks without manually
annotated multimodal parallel corpora. We apply the proposed method to a wide
range of natural language generation and understanding tasks, including neural
machine translation, natural language inference, and semantic similarity.
Experimental results show that our method is generally effective for different
tasks and languages. Analysis indicates that the visual signals enrich textual
representations of content words, provide fine-grained grounding information
about the relationship between concepts and events, and potentially conduce to
disambiguation.",2023-01-09
"An Image captioning algorithm based on the Hybrid Deep Learning
  Technique (CNN+GRU)",2023-01-06 10:00:06+00:00,http://arxiv.org/abs/2301.02440v1,"Rana Adnan Ahmad, Muhammad Azhar, Hina Sattar","cs.CV, cs.AI",image2text,"Image captioning by the encoder-decoder framework has shown tremendous
advancement in the last decade where CNN is mainly used as encoder and LSTM is
used as a decoder. Despite such an impressive achievement in terms of accuracy
in simple images, it lacks in terms of time complexity and space complexity
efficiency. In addition to this, in case of complex images with a lot of
information and objects, the performance of this CNN-LSTM pair downgraded
exponentially due to the lack of semantic understanding of the scenes presented
in the images. Thus, to take these issues into consideration, we present
CNN-GRU encoder decode framework for caption-to-image reconstructor to handle
the semantic context into consideration as well as the time complexity. By
taking the hidden states of the decoder into consideration, the input image and
its similar semantic representations is reconstructed and reconstruction scores
from a semantic reconstructor are used in conjunction with likelihood during
model training to assess the quality of the generated caption. As a result, the
decoder receives improved semantic information, enhancing the caption
production process. During model testing, combining the reconstruction score
and the log-likelihood is also feasible to choose the most appropriate caption.
The suggested model outperforms the state-of-the-art LSTM-A5 model for picture
captioning in terms of time complexity and accuracy.",2023-01-06
"Towards Table-to-Text Generation with Pretrained Language Model: A Table
  Structure Understanding and Text Deliberating Approach",2023-01-05 14:03:26+00:00,http://arxiv.org/abs/2301.02071v1,"Miao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Jingbo Zhou, Dejing Dou, Hui Xiong","cs.CL, cs.AI",image2text,"Although remarkable progress on the neural table-to-text methods has been
made, the generalization issues hinder the applicability of these models due to
the limited source tables. Large-scale pretrained language models sound like a
promising solution to tackle such issues. However, how to effectively bridge
the gap between the structured table and the text input by fully leveraging
table information to fuel the pretrained model is still not well explored.
Besides, another challenge of integrating the deliberation mechanism into the
text-to-text pretrained model for solving the table-to-text task remains seldom
studied. In this paper, to implement the table-to-text generation with
pretrained language model, we propose a table structure understanding and text
deliberating approach, namely TASD. Specifically, we devise a three-layered
multi-head attention network to realize the table-structure-aware text
generation model with the help of the pretrained language model. Furthermore, a
multi-pass decoder framework is adopted to enhance the capability of polishing
generated text for table descriptions. The empirical studies, as well as human
evaluation, on two public datasets, validate that our approach can generate
faithful and fluent descriptive texts for different types of tables.",2023-01-05
eVAE: Evolutionary Variational Autoencoder,2023-01-01 23:54:35+00:00,http://arxiv.org/abs/2301.00011v1,"Zhangkai Wu, Longbing Cao, Lei Qi","cs.NE, cs.LG",image2text,"The surrogate loss of variational autoencoders (VAEs) poses various
challenges to their training, inducing the imbalance between task fitting and
representation inference. To avert this, the existing strategies for VAEs focus
on adjusting the tradeoff by introducing hyperparameters, deriving a tighter
bound under some mild assumptions, or decomposing the loss components per
certain neural settings. VAEs still suffer from uncertain tradeoff learning.We
propose a novel evolutionary variational autoencoder (eVAE) building on the
variational information bottleneck (VIB) theory and integrative evolutionary
neural learning. eVAE integrates a variational genetic algorithm into VAE with
variational evolutionary operators including variational mutation, crossover,
and evolution. Its inner-outer-joint training mechanism synergistically and
dynamically generates and updates the uncertain tradeoff learning in the
evidence lower bound (ELBO) without additional constraints. Apart from learning
a lossy compression and representation of data under the VIB assumption, eVAE
presents an evolutionary paradigm to tune critical factors of VAEs and deep
neural networks and addresses the premature convergence and random search
problem by integrating evolutionary optimization into deep learning.
Experiments show that eVAE addresses the KL-vanishing problem for text
generation with low reconstruction loss, generates all disentangled factors
with sharp images, and improves the image generation quality,respectively. eVAE
achieves better reconstruction loss, disentanglement, and generation-inference
balance than its competitors.",2023-01-01
MAUVE Scores for Generative Models: Theory and Practice,2022-12-30 07:37:40+00:00,http://arxiv.org/abs/2212.14578v1,"Krishna Pillutla, Lang Liu, John Thickstun, Sean Welleck, Swabha Swayamdipta, Rowan Zellers, Sewoong Oh, Yejin Choi, Zaid Harchaoui","cs.LG, cs.AI, cs.CL",image2text,"Generative AI has matured to a point where large-scale models can generate
text that seems indistinguishable from human-written text and remarkably
photorealistic images. Automatically measuring how close the distribution of
generated data is to the target real data distribution is a key step in
diagnosing existing models and developing better models. We present MAUVE, a
family of comparison measures between pairs of distributions such as those
encountered in the generative modeling of text or images. These scores are
statistical summaries of divergence frontiers capturing two types of errors in
generative modeling. We explore four approaches to statistically estimate these
scores: vector quantization, non-parametric estimation, classifier-based
estimation, and parametric Gaussian approximations. We provide statistical
bounds for the vector quantization approach. Empirically, we find that the
proposed scores paired with a range of $f$-divergences and statistical
estimation methods can quantify the gaps between the distributions of
human-written text and those of modern neural language models by correlating
with human judgments and identifying known properties of the generated texts.
We conclude the paper by demonstrating its applications to other AI domains and
discussing practical recommendations.",2022-12-30
"Noise-aware Learning from Web-crawled Image-Text Data for Image
  Captioning",2022-12-27 17:33:40+00:00,http://arxiv.org/abs/2212.13563v1,"Wooyoung Kang, Jonghwan Mun, Sungjun Lee, Byungseok Roh","cs.CV, cs.AI",image2text,"Image captioning is one of the straightforward tasks that can take advantage
of large-scale web-crawled data which provides rich knowledge about the visual
world for a captioning model. However, since web-crawled data contains
image-text pairs that are aligned at different levels, the inherent noises
(e.g., misaligned pairs) make it difficult to learn a precise captioning model.
While the filtering strategy can effectively remove noisy data, however, it
leads to a decrease in learnable knowledge and sometimes brings about a new
problem of data deficiency. To take the best of both worlds, we propose a
noise-aware learning framework, which learns rich knowledge from the whole
web-crawled data while being less affected by the noises. This is achieved by
the proposed quality controllable model, which is learned using alignment
levels of the image-text pairs as an additional control signal during training.
The alignment-conditioned training allows the model to generate high-quality
captions of well-aligned by simply setting the control signal to desired
alignment level at inference time. Through in-depth analysis, we show that our
controllable captioning model is effective in handling noise. In addition, with
two tasks of zero-shot captioning and text-to-image retrieval using generated
captions (i.e., self-retrieval), we also demonstrate our model can produce
high-quality captions in terms of descriptiveness and distinctiveness. Code is
available at \url{https://github.com/kakaobrain/noc}.",2022-12-27
"On Realization of Intelligent Decision-Making in the Real World: A
  Foundation Decision Model Perspective",2022-12-24 06:16:45+00:00,http://arxiv.org/abs/2212.12669v1,"Ying Wen, Ziyu Wan, Ming Zhou, Shufang Hou, Zhe Cao, Chenyang Le, Jingxiao Chen, Zheng Tian, Weinan Zhang, Jun Wang","cs.AI, cs.LG",image2text,"Our situated environment is full of uncertainty and highly dynamic, thus
hindering the widespread adoption of machine-led Intelligent Decision-Making
(IDM) in real world scenarios. This means IDM should have the capability of
continuously learning new skills and efficiently generalizing across wider
applications. IDM benefits from any new approaches and theoretical
breakthroughs that exhibit Artificial General Intelligence (AGI) breaking the
barriers between tasks and applications. Recent research has well-examined
neural architecture, Transformer, as a backbone foundation model and its
generalization to various tasks, including computer vision, natural language
processing, and reinforcement learning. We therefore argue that a foundation
decision model (FDM) can be established by formulating various decision-making
tasks as a sequence decoding task using the Transformer architecture; this
would be a promising solution to advance the applications of IDM in more
complex real world tasks. In this paper, we elaborate on how a foundation
decision model improves the efficiency and generalization of IDM. We also
discuss potential applications of a FDM in multi-agent game AI, production
scheduling, and robotics tasks. Finally, through a case study, we demonstrate
our realization of the FDM, DigitalBrain (DB1) with 1.2 billion parameters,
which achieves human-level performance over 453 tasks, including text
generation, images caption, video games playing, robotic control, and traveling
salesman problems. As a foundation decision model, DB1 would be a baby step
towards more autonomous and efficient real world IDM applications.",2022-12-24
Do DALL-E and Flamingo Understand Each Other?,2022-12-23 10:46:56+00:00,http://arxiv.org/abs/2212.12249v1,"Hang Li, Jindong Gu, Rajat Koner, Sahand Sharifzadeh, Volker Tresp","cs.CV, cs.LG",image2text,"A major goal of multimodal research is to improve machine understanding of
images and text. Tasks include image captioning, text-to-image generation, and
vision-language representation learning. So far, research has focused on the
relationships between images and text. For example, captioning models attempt
to understand the semantics of images which are then transformed into text. An
important question is: which annotation reflects best a deep understanding of
image content? Similarly, given a text, what is the best image that can present
the semantics of the text? In this work, we argue that the best text or caption
for a given image is the text which would generate the image which is the most
similar to that image. Likewise, the best image for a given text is the image
that results in the caption which is best aligned with the original text. To
this end, we propose a unified framework that includes both a text-to-image
generative model and an image-to-text generative model. Extensive experiments
validate our approach.",2022-12-23
A survey on text generation using generative adversarial networks,2022-12-20 17:54:08+00:00,http://arxiv.org/abs/2212.11119v1,"Gustavo Henrique de Rosa, João Paulo Papa","cs.CL, cs.AI, cs.LG",image2text,"This work presents a thorough review concerning recent studies and text
generation advancements using Generative Adversarial Networks. The usage of
adversarial learning for text generation is promising as it provides
alternatives to generate the so-called ""natural"" language. Nevertheless,
adversarial text generation is not a simple task as its foremost architecture,
the Generative Adversarial Networks, were designed to cope with continuous
information (image) instead of discrete data (text). Thus, most works are based
on three possible options, i.e., Gumbel-Softmax differentiation, Reinforcement
Learning, and modified training objectives. All alternatives are reviewed in
this survey as they present the most recent approaches for generating text
using adversarial-based techniques. The selected works were taken from renowned
databases, such as Science Direct, IEEEXplore, Springer, Association for
Computing Machinery, and arXiv, whereas each selected work has been critically
analyzed and assessed to present its objective, methodology, and experimental
results.",2022-12-20
SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers,2022-12-20 15:16:24+00:00,http://arxiv.org/abs/2212.10325v1,"Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang",cs.CL,image2text,"Diffusion model, a new generative modelling paradigm, has achieved great
success in image, audio, and video generation. However, considering the
discrete categorical nature of text, it is not trivial to extend continuous
diffusion models to natural language, and text diffusion models are less
studied. Sequence-to-sequence text generation is one of the essential natural
language processing topics. In this work, we apply diffusion models to approach
sequence-to-sequence text generation, and explore whether the superiority
generation performance of diffusion model can transfer to natural language
domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence
generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to
model denoising function. In order to improve generation quality, SeqDiffuSeq
combines the self-conditioning technique and a newly proposed adaptive noise
schedule technique. The adaptive noise schedule has the difficulty of denoising
evenly distributed across time steps, and considers exclusive noise schedules
for tokens at different positional order. Experiment results illustrate the
good performance on sequence-to-sequence generation in terms of text quality
and inference time.",2022-12-20
"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",2022-12-19 18:57:05+00:00,http://arxiv.org/abs/2212.09741v2,"Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu",cs.CL,image2text,"We introduce INSTRUCTOR, a new method for computing text embeddings given
task instructions: every text input is embedded together with instructions
explaining the use case (e.g., task and domain descriptions). Unlike encoders
from prior work that are more specialized, INSTRUCTOR is a single embedder that
can generate text embeddings tailored to different downstream tasks and
domains, without any further training. We first annotate instructions for 330
diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive
loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are
unseen during training), ranging from classification and information retrieval
to semantic textual similarity and text generation evaluation. INSTRUCTOR,
while having an order of magnitude fewer parameters than the previous best
model, achieves state-of-the-art performance, with an average improvement of
3.4% compared to the previous best results on the 70 diverse datasets. Our
analysis suggests that INSTRUCTOR is robust to changes in instructions, and
that instruction finetuning mitigates the challenge of training a single model
on diverse datasets. Our model, code, and data are available at
https://instructor-embedding.github.io.",2022-12-19
"Switching to Discriminative Image Captioning by Relieving a Bottleneck
  of Reinforcement Learning",2022-12-06 18:55:20+00:00,http://arxiv.org/abs/2212.03230v1,"Ukyo Honda, Taro Watanabe, Yuji Matsumoto","cs.CV, cs.CL",image2text,"Discriminativeness is a desirable feature of image captions: captions should
describe the characteristic details of input images. However, recent
high-performing captioning models, which are trained with reinforcement
learning (RL), tend to generate overly generic captions despite their high
performance in various other criteria. First, we investigate the cause of the
unexpectedly low discriminativeness and show that RL has a deeply rooted side
effect of limiting the output words to high-frequency words. The limited
vocabulary is a severe bottleneck for discriminativeness as it is difficult for
a model to describe the details beyond its vocabulary. Then, based on this
identification of the bottleneck, we drastically recast discriminative image
captioning as a much simpler task of encouraging low-frequency word generation.
Hinted by long-tail classification and debiasing methods, we propose methods
that easily switch off-the-shelf RL models to discriminativeness-aware models
with only a single-epoch fine-tuning on the part of the parameters. Extensive
experiments demonstrate that our methods significantly enhance the
discriminativeness of off-the-shelf RL models and even outperform previous
discriminativeness-aware methods with much smaller computational costs.
Detailed analysis and human evaluation also verify that our methods boost the
discriminativeness without sacrificing the overall quality of captions.",2022-12-06
Towards Generating Diverse Audio Captions via Adversarial Training,2022-12-05 05:06:19+00:00,http://arxiv.org/abs/2212.02033v1,"Xinhao Mei, Xubo Liu, Jianyuan Sun, Mark D. Plumbley, Wenwu Wang","eess.AS, cs.AI, cs.MM, cs.SD",image2text,"Automated audio captioning is a cross-modal translation task for describing
the content of audio clips with natural language sentences. This task has
attracted increasing attention and substantial progress has been made in recent
years. Captions generated by existing models are generally faithful to the
content of audio clips, however, these machine-generated captions are often
deterministic (e.g., generating a fixed caption for a given audio clip), simple
(e.g., using common words and simple grammar), and generic (e.g., generating
the same caption for similar audio clips). When people are asked to describe
the content of an audio clip, different people tend to focus on different sound
events and describe an audio clip diversely from various aspects using distinct
words and grammar. We believe that an audio captioning system should have the
ability to generate diverse captions, either for a fixed audio clip, or across
similar audio clips. To this end, we propose an adversarial training framework
based on a conditional generative adversarial network (C-GAN) to improve
diversity of audio captioning systems. A caption generator and two hybrid
discriminators compete and are learned jointly, where the caption generator can
be any standard encoder-decoder captioning model used to generate captions, and
the hybrid discriminators assess the generated captions from different
criteria, such as their naturalness and semantics. We conduct experiments on
the Clotho dataset. The results show that our proposed model can generate
captions with better diversity as compared to state-of-the-art methods.",2022-12-05
Grounded Keys-to-Text Generation: Towards Factual Open-Ended Generation,2022-12-04 23:59:41+00:00,http://arxiv.org/abs/2212.01956v1,"Faeze Brahman, Baolin Peng, Michel Galley, Sudha Rao, Bill Dolan, Snigdha Chaturvedi, Jianfeng Gao",cs.CL,image2text,"Large pre-trained language models have recently enabled open-ended generation
frameworks (e.g., prompt-to-text NLG) to tackle a variety of tasks going beyond
the traditional data-to-text generation. While this framework is more general,
it is under-specified and often leads to a lack of controllability restricting
their real-world usage. We propose a new grounded keys-to-text generation task:
the task is to generate a factual description about an entity given a set of
guiding keys, and grounding passages. To address this task, we introduce a new
dataset, called EntDeGen. Inspired by recent QA-based evaluation measures, we
propose an automatic metric, MAFE, for factual correctness of generated
descriptions. Our EntDescriptor model is equipped with strong rankers to fetch
helpful passages and generate entity descriptions. Experimental result shows a
good correlation (60.14) between our proposed metric and human judgments of
factuality. Our rankers significantly improved the factual correctness of
generated descriptions (15.95% and 34.51% relative gains in recall and
precision). Finally, our ablation study highlights the benefit of combining
keys and groundings.",2022-12-04
"Learning Automata-Based Task Knowledge Representation from Large-Scale
  Generative Language Models",2022-12-04 22:34:16+00:00,http://arxiv.org/abs/2212.01944v1,"Yunhao Yang, Jean-Raphaël Gaglione, Ufuk Topcu","cs.FL, cs.CL",image2text,"Automata-based representations play an important role in control and planning
in sequential decision-making, but obtaining high-level task knowledge for
building automata is often difficult. Although large-scale generative language
models (GLMs) can help automatically distill task knowledge, the textual
outputs from GLMs are not directly utilizable in sequential decision-making. We
resolve this problem by proposing a novel algorithm named GLM2FSA, which
obtains high-level task knowledge, represented in a finite state automaton
(FSA), from a given brief description of the task goal. GLM2FSA sends queries
to a GLM for task knowledge in textual form and then builds a FSA to represent
the textual knowledge. This algorithm fills the gap between text and
automata-based representations, and the constructed FSA can be directly
utilized in sequential decision-making. We provide examples to demonstrate how
GLM2FSA constructs FSAs to represent knowledge encoded in the texts generated
by the large-scale GLMs.",2022-12-04
3D-TOGO: Towards Text-Guided Cross-Category 3D Object Generation,2022-12-02 11:31:49+00:00,http://arxiv.org/abs/2212.01103v1,"Zutao Jiang, Guangsong Lu, Xiaodan Liang, Jihua Zhu, Wei Zhang, Xiaojun Chang, Hang Xu","cs.CV, cs.AI",image2text,"Text-guided 3D object generation aims to generate 3D objects described by
user-defined captions, which paves a flexible way to visualize what we
imagined. Although some works have been devoted to solving this challenging
task, these works either utilize some explicit 3D representations (e.g., mesh),
which lack texture and require post-processing for rendering photo-realistic
views; or require individual time-consuming optimization for every single case.
Here, we make the first attempt to achieve generic text-guided cross-category
3D object generation via a new 3D-TOGO model, which integrates a text-to-views
generation module and a views-to-3D generation module. The text-to-views
generation module is designed to generate different views of the target 3D
object given an input caption. prior-guidance, caption-guidance and view
contrastive learning are proposed for achieving better view-consistency and
caption similarity. Meanwhile, a pixelNeRF model is adopted for the views-to-3D
generation module to obtain the implicit 3D neural representation from the
previously-generated views. Our 3D-TOGO model generates 3D objects in the form
of the neural radiance field with good texture and requires no time-cost
optimization for every single caption. Besides, 3D-TOGO can control the
category, color and shape of generated 3D objects with the input caption.
Extensive experiments on the largest 3D object dataset (i.e., ABO) are
conducted to verify that 3D-TOGO can better generate high-quality 3D objects
according to the input captions across 98 different categories, in terms of
PSNR, SSIM, LPIPS and CLIP-score, compared with text-NeRF and Dreamfields.",2022-12-02
"On the Importance of Image Encoding in Automated Chest X-Ray Report
  Generation",2022-11-24 08:02:52+00:00,http://arxiv.org/abs/2211.13465v1,"Otabek Nazarov, Mohammad Yaqub, Karthik Nandakumar","cs.CV, cs.AI",image2text,"Chest X-ray is one of the most popular medical imaging modalities due to its
accessibility and effectiveness. However, there is a chronic shortage of
well-trained radiologists who can interpret these images and diagnose the
patient's condition. Therefore, automated radiology report generation can be a
very helpful tool in clinical practice. A typical report generation workflow
consists of two main steps: (i) encoding the image into a latent space and (ii)
generating the text of the report based on the latent image embedding. Many
existing report generation techniques use a standard convolutional neural
network (CNN) architecture for image encoding followed by a Transformer-based
decoder for medical text generation. In most cases, CNN and the decoder are
trained jointly in an end-to-end fashion. In this work, we primarily focus on
understanding the relative importance of encoder and decoder components.
Towards this end, we analyze four different image encoding approaches: direct,
fine-grained, CLIP-based, and Cluster-CLIP-based encodings in conjunction with
three different decoders on the large-scale MIMIC-CXR dataset. Among these
encoders, the cluster CLIP visual encoder is a novel approach that aims to
generate more discriminative and explainable representations. CLIP-based
encoders produce comparable results to traditional CNN-based encoders in terms
of NLP metrics, while fine-grained encoding outperforms all other encoders both
in terms of NLP and clinical accuracy metrics, thereby validating the
importance of image encoder to effectively extract semantic information. GitHub
repository: https://github.com/mudabek/encoding-cxr-report-gen",2022-11-24
Retrieval-Augmented Multimodal Language Modeling,2022-11-22 20:26:44+00:00,http://arxiv.org/abs/2211.12561v1,"Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, Wen-tau Yih","cs.CV, cs.CL, cs.LG",image2text,"Recent multimodal models such as DALL-E and CM3 have achieved remarkable
progress in text-to-image and image-to-text generation. However, these models
store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the
model parameters, requiring increasingly larger models and training data to
capture more knowledge. To integrate knowledge in a more scalable and modular
way, we propose a retrieval-augmented multimodal model, which enables a base
multimodal model (generator) to refer to relevant knowledge fetched by a
retriever from external memory (e.g., multimodal documents on the web).
Specifically, we implement a retriever using the pretrained CLIP model and a
generator using the CM3 Transformer architecture, and train this model using
the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3),
is the first multimodal model that can retrieve and generate mixtures of text
and images. We show that RA-CM3 significantly outperforms baseline multimodal
models such as DALL-E and CM3 on both image and caption generation tasks (12
FID and 17 CIDEr improvements on MS-COCO), while requiring much less compute
for training (<30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel
capabilities such as knowledge-intensive image generation and multimodal
in-context learning.",2022-11-22
"Aligning Source Visual and Target Language Domains for Unpaired Video
  Captioning",2022-11-22 10:26:26+00:00,http://arxiv.org/abs/2211.12148v1,"Fenglin Liu, Xian Wu, Chenyu You, Shen Ge, Yuexian Zou, Xu Sun","cs.CV, cs.LG",image2text,"Training supervised video captioning model requires coupled video-caption
pairs. However, for many targeted languages, sufficient paired data are not
available. To this end, we introduce the unpaired video captioning task aiming
to train models without coupled video-caption pairs in target language. To
solve the task, a natural choice is to employ a two-step pipeline system: first
utilizing video-to-pivot captioning model to generate captions in pivot
language and then utilizing pivot-to-target translation model to translate the
pivot captions to the target language. However, in such a pipeline system, 1)
visual information cannot reach the translation model, generating visual
irrelevant target captions; 2) the errors in the generated pivot captions will
be propagated to the translation model, resulting in disfluent target captions.
To address these problems, we propose the Unpaired Video Captioning with Visual
Injection system (UVC-VI). UVC-VI first introduces the Visual Injection Module
(VIM), which aligns source visual and target language domains to inject the
source visual information into the target language domain. Meanwhile, VIM
directly connects the encoder of the video-to-pivot model and the decoder of
the pivot-to-target model, allowing end-to-end inference by completely skipping
the generation of pivot captions. To enhance the cross-modality injection of
the VIM, UVC-VI further introduces a pluggable video encoder, i.e., Multimodal
Collaborative Encoder (MCE). The experiments show that UVC-VI outperforms
pipeline systems and exceeds several supervised systems. Furthermore, equipping
existing supervised systems with our MCE can achieve 4% and 7% relative margins
on the CIDEr scores to current state-of-the-art models on the benchmark MSVD
and MSR-VTT datasets, respectively.",2022-11-22
"How to Describe Images in a More Funny Way? Towards a Modular Approach
  to Cross-Modal Sarcasm Generation",2022-11-20 14:38:24+00:00,http://arxiv.org/abs/2211.10992v1,"Jie Ruan, Yue Wu, Xiaojun Wan, Yuesheng Zhu","cs.CV, cs.CL",image2text,"Sarcasm generation has been investigated in previous studies by considering
it as a text-to-text generation problem, i.e., generating a sarcastic sentence
for an input sentence. In this paper, we study a new problem of cross-modal
sarcasm generation (CMSG), i.e., generating a sarcastic description for a given
image. CMSG is challenging as models need to satisfy the characteristics of
sarcasm, as well as the correlation between different modalities. In addition,
there should be some inconsistency between the two modalities, which requires
imagination. Moreover, high-quality training data is insufficient. To address
these problems, we take a step toward generating sarcastic descriptions from
images without paired training data and propose an
Extraction-Generation-Ranking based Modular method (EGRM) for cross-model
sarcasm generation. Specifically, EGRM first extracts diverse information from
an image at different levels and uses the obtained image tags, sentimental
descriptive caption, and commonsense-based consequence to generate candidate
sarcastic texts. Then, a comprehensive ranking algorithm, which considers
image-text relation, sarcasticness, and grammaticality, is proposed to select a
final text from the candidate texts. Human evaluation at five criteria on a
total of 1200 generated image-text pairs from eight systems and auxiliary
automatic evaluation show the superiority of our method.",2022-11-20
"Feedback is Needed for Retakes: An Explainable Poor Image Notification
  Framework for the Visually Impaired",2022-11-17 09:22:28+00:00,http://arxiv.org/abs/2211.09427v1,"Kazuya Ohata, Shunsuke Kitada, Hitoshi Iyatomi","cs.CV, cs.AI, cs.CL, cs.HC, cs.LG",image2text,"We propose a simple yet effective image captioning framework that can
determine the quality of an image and notify the user of the reasons for any
flaws in the image. Our framework first determines the quality of images and
then generates captions using only those images that are determined to be of
high quality. The user is notified by the flaws feature to retake if image
quality is low, and this cycle is repeated until the input image is deemed to
be of high quality. As a component of the framework, we trained and evaluated a
low-quality image detection model that simultaneously learns difficulty in
recognizing images and individual flaws, and we demonstrated that our proposal
can explain the reasons for flaws with a sufficient score. We also evaluated a
dataset with low-quality images removed by our framework and found improved
values for all four common metrics (e.g., BLEU-4, METEOR, ROUGE-L, CIDEr),
confirming an improvement in general-purpose image captioning capability. Our
framework would assist the visually impaired, who have difficulty judging image
quality.",2022-11-17
PromptCap: Prompt-Guided Task-Aware Image Captioning,2022-11-15 19:07:53+00:00,http://arxiv.org/abs/2211.09699v1,"Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A. Smith, Jiebo Luo","cs.CV, cs.CL",image2text,"Image captioning aims to describe an image with a natural language sentence,
allowing powerful language models to understand images. The framework of
combining image captioning with language models has been successful on various
vision-language tasks. However, an image contains much more information than a
single sentence, leading to underspecification of which visual entities should
be described in the caption sentence. For example, when performing visual
questioning answering (VQA), generic image captions often miss visual details
that are essential for the language model to answer correctly. To address this
challenge, we propose PromptCap, a captioning model that takes a
natural-language prompt to control the contents of the generated caption. The
prompt contains a question that the caption should help to answer, and also
supports taking auxiliary text inputs such as scene text within the image
itself. To finetune a general image caption model for prompt-guided captioning,
we propose a pipeline to synthesize and filter training examples with GPT-3 and
existing VQA datasets. For evaluation, we start with an existing pipeline in
which a language model is prompted with image captions to carry out VQA. With
the same language model, a higher QA accuracy shows that our generated captions
are more relevant to the question prompts. PromptCap outperforms generic
captions by a large margin on a variety of VQA tasks and achieves the
state-of-the-art accuracy of 58.8 % on OK-VQA and 58.0 % on A-OKVQA. Zero-shot
experiments on WebQA show that PromptCap generalizes well to unseen domains.",2022-11-15
"CCPrompt: Counterfactual Contrastive Prompt-Tuning for Many-Class
  Classification",2022-11-11 03:45:59+00:00,http://arxiv.org/abs/2211.05987v1,"Yang Li, Canran Xu, Tao Shen, Jing Jiang, Guodong Long",cs.CL,image2text,"With the success of the prompt-tuning paradigm in Natural Language Processing
(NLP), various prompt templates have been proposed to further stimulate
specific knowledge for serving downstream tasks, e.g., machine translation,
text generation, relation extraction, and so on. Existing prompt templates are
mainly shared among all training samples with the information of task
description. However, training samples are quite diverse. The sharing task
description is unable to stimulate the unique task-related information in each
training sample, especially for tasks with the finite-label space. To exploit
the unique task-related information, we imitate the human decision process
which aims to find the contrastive attributes between the objective factual and
their potential counterfactuals. Thus, we propose the \textbf{C}ounterfactual
\textbf{C}ontrastive \textbf{Prompt}-Tuning (CCPrompt) approach for many-class
classification, e.g., relation classification, topic classification, and entity
typing. Compared with simple classification tasks, these tasks have more
complex finite-label spaces and are more rigorous for prompts. First of all, we
prune the finite label space to construct fact-counterfactual pairs. Then, we
exploit the contrastive attributes by projecting training instances onto every
fact-counterfactual pair. We further set up global prototypes corresponding
with all contrastive attributes for selecting valid contrastive attributes as
additional tokens in the prompt template. Finally, a simple Siamese
representation learning is employed to enhance the robustness of the model. We
conduct experiments on relation classification, topic classification, and
entity typing tasks in both fully supervised setting and few-shot setting. The
results indicate that our model outperforms former baselines.",2022-11-11
Self-conditioned Embedding Diffusion for Text Generation,2022-11-08 13:30:27+00:00,http://arxiv.org/abs/2211.04236v1,"Robin Strudel, Corentin Tallec, Florent Altché, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, Rémi Leblond","cs.CL, cs.LG",image2text,"Can continuous diffusion models bring the same performance breakthrough on
natural language they did for image generation? To circumvent the discrete
nature of text data, we can simply project tokens in a continuous space of
embeddings, as is standard in language modeling. We propose Self-conditioned
Embedding Diffusion, a continuous diffusion mechanism that operates on token
embeddings and allows to learn flexible and scalable diffusion models for both
conditional and unconditional text generation. Through qualitative and
quantitative evaluation, we show that our text diffusion models generate
samples comparable with those produced by standard autoregressive language
models - while being in theory more efficient on accelerator hardware at
inference time. Our work paves the way for scaling up diffusion models for
text, similarly to autoregressive models, and for improving performance with
recent refinements to continuous diffusion.",2022-11-08
Semantic Metadata Extraction from Dense Video Captioning,2022-11-05 22:06:50+00:00,http://arxiv.org/abs/2211.02982v1,"Johannes Scherer, Ansgar Scherp, Deepayan Bhowmik","cs.CV, cs.CL",image2text,"Annotation of multimedia data by humans is time-consuming and costly, while
reliable automatic generation of semantic metadata is a major challenge. We
propose a framework to extract semantic metadata from automatically generated
video captions. As metadata, we consider entities, the entities' properties,
relations between entities, and the video category. We employ two
state-of-the-art dense video captioning models with masked transformer (MT) and
parallel decoding (PVDC) to generate captions for videos of the ActivityNet
Captions dataset. Our experiments show that it is possible to extract entities,
their properties, relations between entities, and the video category from the
generated captions. We observe that the quality of the extracted information is
mainly influenced by the quality of the event localization in the video as well
as the performance of the event caption generation.",2022-11-05
"eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert
  Denoisers",2022-11-02 17:43:04+00:00,http://arxiv.org/abs/2211.01324v3,"Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu","cs.CV, cs.LG",image2text,"Large-scale diffusion-based generative models have led to breakthroughs in
text-conditioned high-resolution image synthesis. Starting from random noise,
such text-to-image diffusion models gradually synthesize images in an iterative
fashion while conditioning on text prompts. We find that their synthesis
behavior qualitatively changes throughout this process: Early in sampling,
generation strongly relies on the text prompt to generate text-aligned content,
while later, the text conditioning is almost entirely ignored. This suggests
that sharing model parameters throughout the entire generation process may not
be ideal. Therefore, in contrast to existing works, we propose to train an
ensemble of text-to-image diffusion models specialized for different synthesis
stages. To maintain training efficiency, we initially train a single model,
which is then split into specialized models that are trained for the specific
stages of the iterative generation process. Our ensemble of diffusion models,
called eDiff-I, results in improved text alignment while maintaining the same
inference computation cost and preserving high visual quality, outperforming
previous large-scale text-to-image diffusion models on the standard benchmark.
In addition, we train our model to exploit a variety of embeddings for
conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We
show that these different embeddings lead to different behaviors. Notably, the
CLIP image embedding allows an intuitive way of transferring the style of a
reference image to the target text-to-image output. Lastly, we show a technique
that enables eDiff-I's ""paint-with-words"" capability. A user can select the
word in the input text and paint it in a canvas to control the output, which is
very handy for crafting the desired image in mind. The project page is
available at https://deepimagination.cc/eDiff-I/",2022-11-02
CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation,2022-11-02 01:40:18+00:00,http://arxiv.org/abs/2211.00818v1,"Yihong Dong, Ge Li","cs.SE, cs.AI",image2text,"General-purpose code generation aims to automatically convert the natural
language (NL) description to code snippets in a general-purpose programming
language (GPL) like Python. Intrinsically, code generation is a special type of
text generation that generates well-formed text, i.e., code. However, existing
sequence-to-sequence (Seq2Seq) approaches generate the GPL code neglecting the
grammar rules. To this end, in this paper, we make the first attempt to
consider grammatical Seq2Seq models for general-purpose code generation and
propose CODEP, a grammatical Seq2Seq code generation framework equipped with a
Pushdown automaton (PDA) module. In the training stage, CODEP additionally
incorporates the state representation and the state prediction task, which
leverages PDA states to help CODEP comprehend the parsing process of the PDA
module. In the inference stage, CODEP generates well-formed code with the PDA
module and the joint prediction of PDA states. Furthermore, the PDA module can
be directly applied to Seq2Seq models without training to ensure the
grammatical correctness of the generated code. To evaluate the effectiveness of
our proposed method, we construct the DPA for the most popular GPL Python and
conduct extensive experiments on four benchmark datasets. The experimental
results demonstrate the superiority of CODEP compared to the state-of-the-art
approaches without pre-training, and the DPA module also achieves significant
improvements on the pre-trained models.",2022-11-02
"SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for
  Text Generation and Modular Control",2022-10-31 16:02:00+00:00,http://arxiv.org/abs/2210.17432v1,"Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov","cs.CL, cs.LG",image2text,"Despite the growing success of diffusion models in continuous-valued domains
(e.g., images), diffusion-based language models on discrete text have yet to
match autoregressive language models on text generation benchmarks. In this
work, we present SSD-LM -- a diffusion language model with two key design
choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of
text, allowing for flexible output length at decoding time while enabling local
bidirectional context updates. Second, it is simplex-based, performing
diffusion on the natural vocabulary space rather than a learned latent space,
allowing us to incorporate classifier guidance and modular control without any
adaptation of off-the-shelf classifiers. We evaluate SSD-LM on unconstrained as
well as controlled text generation benchmarks, and show that it matches or
outperforms strong autoregressive GPT-2 baselines across standard quality and
diversity metrics.",2022-10-31
Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention,2022-10-28 22:45:41+00:00,http://arxiv.org/abs/2210.16428v1,"Xubo Liu, Qiushi Huang, Xinhao Mei, Haohe Liu, Qiuqiang Kong, Jianyuan Sun, Shengchen Li, Tom Ko, Yu Zhang, Lilian H. Tang, Mark D. Plumbley, Volkan Kılıç, Wenwu Wang","eess.AS, cs.AI, cs.MM, cs.SD",image2text,"Audio captioning is the task of generating captions that describe the content
of audio clips. In the real world, many objects produce similar sounds. It is
difficult to identify these auditory ambiguous sound events with access to
audio information only. How to accurately recognize ambiguous sounds is a major
challenge for audio captioning systems. In this work, inspired by the
audio-visual multi-modal perception of human beings, we propose visually-aware
audio captioning, which makes use of visual information to help the recognition
of ambiguous sounding objects. Specifically, we introduce an off-the-shelf
visual encoder to process the video inputs, and incorporate the extracted
visual features into an audio captioning system. Furthermore, to better exploit
complementary contexts from redundant audio-visual streams, we propose an
audio-visual attention mechanism that integrates audio and visual information
adaptively according to their confidence levels. Experimental results on
AudioCaps, the largest publicly available audio captioning dataset, show that
the proposed method achieves significant improvement over a strong baseline
audio captioning system and is on par with the state-of-the-art result.",2022-10-28
"Improving the Factual Correctness of Radiology Report Generation with
  Semantic Rewards",2022-10-21 18:27:45+00:00,http://arxiv.org/abs/2210.12186v1,"Jean-Benoit Delbrouck, Pierre Chambon, Christian Bluethgen, Emily Tsai, Omar Almusa, Curtis P. Langlotz","cs.CL, cs.AI",image2text,"Neural image-to-text radiology report generation systems offer the potential
to improve radiology reporting by reducing the repetitive process of report
drafting and identifying possible medical errors. These systems have achieved
promising performance as measured by widely used NLG metrics such as BLEU and
CIDEr. However, the current systems face important limitations. First, they
present an increased complexity in architecture that offers only marginal
improvements on NLG metrics. Secondly, these systems that achieve high
performance on these metrics are not always factually complete or consistent
due to both inadequate training and evaluation. Recent studies have shown the
systems can be substantially improved by using new methods encouraging 1) the
generation of domain entities consistent with the reference and 2) describing
these entities in inferentially consistent ways. So far, these methods rely on
weakly-supervised approaches (rule-based) and named entity recognition systems
that are not specific to the chest X-ray domain. To overcome this limitation,
we propose a new method, the RadGraph reward, to further improve the factual
completeness and correctness of generated radiology reports. More precisely, we
leverage the RadGraph dataset containing annotated chest X-ray reports with
entities and relations between entities. On two open radiology report datasets,
our system substantially improves the scores up to 14.2% and 25.3% on metrics
evaluating the factual correctness and completeness of reports.",2022-10-21
Image Semantic Relation Generation,2022-10-19 16:15:19+00:00,http://arxiv.org/abs/2210.11253v1,Mingzhe Du,"cs.CV, cs.CL",image2text,"Scene graphs provide structured semantic understanding beyond images. For
downstream tasks, such as image retrieval, visual question answering, visual
relationship detection, and even autonomous vehicle technology, scene graphs
can not only distil complex image information but also correct the bias of
visual models using semantic-level relations, which has broad application
prospects. However, the heavy labour cost of constructing graph annotations may
hinder the application of PSG in practical scenarios. Inspired by the
observation that people usually identify the subject and object first and then
determine the relationship between them, we proposed to decouple the scene
graphs generation task into two sub-tasks: 1) an image segmentation task to
pick up the qualified objects. 2) a restricted auto-regressive text generation
task to generate the relation between given objects. Therefore, in this work,
we introduce image semantic relation generation (ISRG), a simple but effective
image-to-text model, which achieved 31 points on the OpenPSG dataset and
outperforms strong baselines respectively by 16 points (ResNet-50) and 5 points
(CLIP).",2022-10-19
"BioGPT: Generative Pre-trained Transformer for Biomedical Text
  Generation and Mining",2022-10-19 07:17:39+00:00,http://arxiv.org/abs/2210.10341v1,"Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu","cs.CL, cs.AI",image2text,"Pre-trained language models have attracted increasing attention in the
biomedical domain, inspired by their great success in the general natural
language domain. Among the two main branches of pre-trained language models in
the general language domain, i.e., BERT (and its variants) and GPT (and its
variants), the first one has been extensively studied in the biomedical domain,
such as BioBERT and PubMedBERT. While they have achieved great success on a
variety of discriminative downstream biomedical tasks, the lack of generation
ability constrains their application scope. In this paper, we propose BioGPT, a
domain-specific generative Transformer language model pre-trained on large
scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and
demonstrate that our model outperforms previous models on most tasks.
Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI
end-to-end relation extraction tasks respectively, and 78.2% accuracy on
PubMedQA, creating a new record. Our case study on text generation further
demonstrates the advantage of BioGPT on biomedical literature to generate
fluent descriptions for biomedical terms. Code is available at
https://github.com/microsoft/BioGPT.",2022-10-19
"Probing Cross-modal Semantics Alignment Capability from the Textual
  Perspective",2022-10-18 02:55:58+00:00,http://arxiv.org/abs/2210.09550v1,"Zheng Ma, Shi Zong, Mianzhi Pan, Jianbing Zhang, Shujian Huang, Xinyu Dai, Jiajun Chen",cs.CL,image2text,"In recent years, vision and language pre-training (VLP) models have advanced
the state-of-the-art results in a variety of cross-modal downstream tasks.
Aligning cross-modal semantics is claimed to be one of the essential
capabilities of VLP models. However, it still remains unclear about the inner
working mechanism of alignment in VLP models. In this paper, we propose a new
probing method that is based on image captioning to first empirically study the
cross-modal semantics alignment of VLP models. Our probing method is built upon
the fact that given an image-caption pair, the VLP models will give a score,
indicating how well two modalities are aligned; maximizing such scores will
generate sentences that VLP models believe are of good alignment. Analyzing
these sentences thus will reveal in what way different modalities are aligned
and how well these alignments are in VLP models. We apply our probing method to
five popular VLP models, including UNITER, ROSITA, ViLBERT, CLIP, and LXMERT,
and provide a comprehensive analysis of the generated captions guided by these
models. Our results show that VLP models (1) focus more on just aligning
objects with visual words, while neglecting global semantics; (2) prefer fixed
sentence patterns, thus ignoring more important textual information including
fluency and grammar; and (3) deem the captions with more visual words are
better aligned with images. These findings indicate that VLP models still have
weaknesses in cross-modal semantics alignment and we hope this work will draw
researchers' attention to such problems when designing a new VLP model.",2022-10-18
"UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation
  Model on a Single Image",2022-10-17 23:46:05+00:00,http://arxiv.org/abs/2210.09477v3,"Dani Valevski, Matan Kalman, Yossi Matias, Yaniv Leviathan","cs.CV, cs.GR, cs.LG",image2text,"We present UniTune, a simple and novel method for general text-driven image
editing. UniTune gets as input an arbitrary image and a textual edit
description, and carries out the edit while maintaining high semantic and
visual fidelity to the input image. UniTune uses text, an intuitive interface
for art-direction, and does not require additional inputs, like masks or
sketches. At the core of our method is the observation that with the right
choice of parameters, we can fine-tune a large text-to-image diffusion model on
a single image, encouraging the model to maintain fidelity to the input image
while still allowing expressive manipulations. We used Imagen as our
text-to-image model, but we expect UniTune to work with other large-scale
models as well. We test our method in a range of different use cases, and
demonstrate its wide applicability.",2022-10-17
Social Biases in Automatic Evaluation Metrics for NLG,2022-10-17 08:55:26+00:00,http://arxiv.org/abs/2210.08859v1,"Mingqi Gao, Xiaojun Wan","cs.CL, cs.AI",image2text,"Many studies have revealed that word embeddings, language models, and models
for specific downstream tasks in NLP are prone to social biases, especially
gender bias. Recently these techniques have been gradually applied to automatic
evaluation metrics for text generation. In the paper, we propose an evaluation
method based on Word Embeddings Association Test (WEAT) and Sentence Embeddings
Association Test (SEAT) to quantify social biases in evaluation metrics and
discover that social biases are also widely present in some model-based
automatic evaluation metrics. Moreover, we construct gender-swapped
meta-evaluation datasets to explore the potential impact of gender bias in
image caption and text summarization tasks. Results show that given
gender-neutral references in the evaluation, model-based evaluation metrics may
show a preference for the male hypothesis, and the performance of them, i.e.
the correlation between evaluation metrics and human judgments, usually has
more significant variation after gender swapping.",2022-10-17
"Plausible May Not Be Faithful: Probing Object Hallucination in
  Vision-Language Pre-training",2022-10-14 10:27:22+00:00,http://arxiv.org/abs/2210.07688v1,"Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, Pascale Fung","cs.CL, cs.CV",image2text,"Large-scale vision-language pre-trained (VLP) models are prone to hallucinate
non-existent visual objects when generating text based on visual information.
In this paper, we exhaustively probe the object hallucination problem from
three aspects. First, we examine various state-of-the-art VLP models, showing
that models achieving better scores on standard metrics(e.g., BLEU-4, CIDEr)
could hallucinate objects more frequently. Second, we investigate how different
types of visual features in VLP influence hallucination, including
region-based, grid-based, and patch-based. Surprisingly, we find that
patch-based features perform the best and smaller patch resolution yields a
non-trivial reduction in object hallucination. Third, we decouple various VLP
objectives and demonstrate their effectiveness in alleviating object
hallucination. Based on that, we propose a new pre-training loss, object masked
language modeling, to further reduce object hallucination. We evaluate models
on both COCO (in-domain) and NoCaps (out-of-domain) datasets with our improved
CHAIR metric. Furthermore, we investigate the effects of various text decoding
strategies and image augmentation methods on object hallucination.",2022-10-14
Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models,2022-10-13 08:45:23+00:00,http://arxiv.org/abs/2210.06475v1,"Sourya Basu, Prasanna Sattigeri, Karthikeyan Natesan Ramamurthy, Vijil Chenthamarakshan, Kush R. Varshney, Lav R. Varshney, Payel Das","cs.LG, cs.CL",image2text,"We introduce equi-tuning, a novel fine-tuning method that transforms
(potentially non-equivariant) pretrained models into group equivariant models
while incurring minimum $L_2$ loss between the feature representations of the
pretrained and the equivariant models. Large pretrained models can be
equi-tuned for different groups to satisfy the needs of various downstream
tasks. Equi-tuned models benefit from both group equivariance as an inductive
bias and semantic priors from pretrained models. We provide applications of
equi-tuning on three different tasks: image classification, compositional
generalization in language, and fairness in natural language generation (NLG).
We also provide a novel group-theoretic definition for fairness in NLG. The
effectiveness of this definition is shown by testing it against a standard
empirical method of fairness in NLG. We provide experimental results for
equi-tuning using a variety of pretrained models: Alexnet, Resnet, VGG, and
Densenet for image classification; RNNs, GRUs, and LSTMs for compositional
generalization; and GPT2 for fairness in NLG. We test these models on benchmark
datasets across all considered tasks to show the generality and effectiveness
of the proposed method.",2022-10-13
"Not All Errors are Equal: Learning Text Generation Metrics using
  Stratified Error Synthesis",2022-10-10 22:30:26+00:00,http://arxiv.org/abs/2210.05035v1,"Wenda Xu, Yilin Tuan, Yujie Lu, Michael Saxon, Lei Li, William Yang Wang","cs.CL, cs.AI",image2text,"Is it possible to build a general and automatic natural language generation
(NLG) evaluation metric? Existing learned metrics either perform
unsatisfactorily or are restricted to tasks where large human rating data is
already available. We introduce SESCORE, a model-based metric that is highly
correlated with human judgements without requiring human annotation, by
utilizing a novel, iterative error synthesis and severity scoring pipeline.
This pipeline applies a series of plausible errors to raw text and assigns
severity labels by simulating human judgements with entailment. We evaluate
SESCORE against existing metrics by comparing how their scores correlate with
human ratings. SESCORE outperforms all prior unsupervised metrics on multiple
diverse NLG tasks including machine translation, image captioning, and WebNLG
text generation. For WMT 20/21 En-De and Zh-En, SESCORE improve the average
Kendall correlation with human judgement from 0.154 to 0.195. SESCORE even
achieves comparable performance to the best supervised metric COMET, despite
receiving no human-annotated training data.",2022-10-10
CLIP-Diffusion-LM: Apply Diffusion Model on Image Captioning,2022-10-10 10:55:53+00:00,http://arxiv.org/abs/2210.04559v1,Shitong Xu,"cs.CV, cs.LG",image2text,"Image captioning task has been extensively researched by previous work.
However, limited experiments focus on generating captions based on
non-autoregressive text decoder. Inspired by the recent success of the
denoising diffusion model on image synthesis tasks, we apply denoising
diffusion probabilistic models to text generation in image captioning tasks. We
show that our CLIP-Diffusion-LM is capable of generating image captions using
significantly fewer inference steps than autoregressive models. On the Flickr8k
dataset, the model achieves 0.1876 BLEU-4 score. By training on the combined
Flickr8k and Flickr30k dataset, our model achieves 0.2470 BLEU-4 score. Our
code is available at https://github.com/xu-shitong/diffusion-image-captioning.",2022-10-10
ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models,2022-10-09 19:17:43+00:00,http://arxiv.org/abs/2210.04325v2,"Jiannan Xiang, Zhengzhong Liu, Yucheng Zhou, Eric P. Xing, Zhiting Hu",cs.CL,image2text,"Data-to-text generation is challenging due to the great variety of the input
data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse
predicates). Recent end-to-end neural methods thus require substantial training
examples to learn to disambiguate and describe the data. Yet, real-world
data-to-text problems often suffer from various data-scarce issues: one may
have access to only a handful of or no training examples, and/or have to rely
on examples in a different domain or schema. To fill this gap, we propose
Any-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse
settings by making efficient use of any given (or no) examples. ASDOT consists
of two steps, data disambiguation and sentence fusion, both of which are
amenable to be solved with off-the-shelf pretrained language models (LMs) with
optional finetuning. In the data disambiguation stage, we employ the prompted
GPT-3 model to understand possibly ambiguous triples from the input data and
convert each into a short sentence with reduced ambiguity. The sentence fusion
stage then uses an LM like T5 to fuse all the resulting sentences into a
coherent paragraph as the final description. We evaluate extensively on various
datasets in different scenarios, including the zero-/few-/full-shot settings,
and generalization to unseen predicates and out-of-domain data. Experimental
results show that ASDOT consistently achieves significant improvement over
baselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot
setting.",2022-10-09
"Visualize Before You Write: Imagination-Guided Open-Ended Text
  Generation",2022-10-07 18:01:09+00:00,http://arxiv.org/abs/2210.03765v1,"Wanrong Zhu, An Yan, Yujie Lu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, William Yang Wang","cs.CL, cs.AI",image2text,"Recent advances in text-to-image synthesis make it possible to visualize
machine imaginations for a given context. On the other hand, when generating
text, human writers are gifted at creative visualization, which enhances their
writings by forming imaginations as blueprints before putting down the stories
in words. Inspired by such a cognitive process, we ask the natural question of
whether we can endow machines with the same ability to utilize visual
information and construct a general picture of the context to guide text
generation. In this work, we propose iNLG that uses machine-generated images to
guide language models (LM) in open-ended text generation. The experiments and
analyses demonstrate the effectiveness of iNLG on open-ended text generation
tasks, including text completion, story generation, and concept-to-text
generation in few-shot scenarios. Both automatic metrics and human evaluations
verify that the text snippets generated by our iNLG are coherent and
informative while displaying minor degeneration.",2022-10-07
"Unsupervised Neural Stylistic Text Generation using Transfer learning
  and Adapters",2022-10-07 00:09:22+00:00,http://arxiv.org/abs/2210.03264v1,"Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah, Dan Roth",cs.CL,image2text,"Research has shown that personality is a key driver to improve engagement and
user experience in conversational systems. Conversational agents should also
maintain a consistent persona to have an engaging conversation with a user.
However, text generation datasets are often crowd sourced and thereby have an
averaging effect where the style of the generation model is an average style of
all the crowd workers that have contributed to the dataset. While one can
collect persona-specific datasets for each task, it would be an expensive and
time consuming annotation effort. In this work, we propose a novel transfer
learning framework which updates only $0.3\%$ of model parameters to learn
style specific attributes for response generation. For the purpose of this
study, we tackle the problem of stylistic story ending generation using the ROC
stories Corpus. We learn style specific attributes from the
PERSONALITY-CAPTIONS dataset. Through extensive experiments and evaluation
metrics we show that our novel training procedure can improve the style
generation by 200 over Encoder-Decoder baselines while maintaining on-par
content relevance metrics with",2022-10-07
"Co-Writing Screenplays and Theatre Scripts with Language Models: An
  Evaluation by Industry Professionals",2022-09-29 17:26:22+00:00,http://arxiv.org/abs/2209.14958v1,"Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, Richard Evans","cs.HC, cs.CL",image2text,"Language models are increasingly attracting interest from writers. However,
such models lack long-range semantic coherence, limiting their usefulness for
longform creative writing. We address this limitation by applying language
models hierarchically, in a system we call Dramatron. By building structural
context via prompt chaining, Dramatron can generate coherent scripts and
screenplays complete with title, characters, story beats, location
descriptions, and dialogue. We illustrate Dramatron's usefulness as an
interactive co-creative system with a user study of 15 theatre and film
industry professionals. Participants co-wrote theatre scripts and screenplays
with Dramatron and engaged in open-ended interviews. We report critical
reflections both from our interviewees and from independent reviewers who
watched stagings of the works to illustrate how both Dramatron and hierarchical
text generation could be useful for human-machine co-creativity. Finally, we
discuss the suitability of Dramatron for co-creativity, ethical considerations
-- including plagiarism and bias -- and participatory models for the design and
deployment of such tools.",2022-09-29
XF2T: Cross-lingual Fact-to-Text Generation for Low-Resource Languages,2022-09-22 18:01:27+00:00,http://arxiv.org/abs/2209.11252v1,"Shivprasad Sagare, Tushar Abhishek, Bhavyajeet Singh, Anubhav Sharma, Manish Gupta, Vasudeva Varma",cs.CL,image2text,"Multiple business scenarios require an automated generation of descriptive
human-readable text from structured input data. Hence, fact-to-text generation
systems have been developed for various downstream tasks like generating soccer
reports, weather and financial reports, medical reports, person biographies,
etc. Unfortunately, previous work on fact-to-text (F2T) generation has focused
primarily on English mainly due to the high availability of relevant datasets.
Only recently, the problem of cross-lingual fact-to-text (XF2T) was proposed
for generation across multiple languages alongwith a dataset, XALIGN for eight
languages. However, there has been no rigorous work on the actual XF2T
generation problem. We extend XALIGN dataset with annotated data for four more
languages: Punjabi, Malayalam, Assamese and Oriya. We conduct an extensive
study using popular Transformer-based text generation models on our extended
multi-lingual dataset, which we call XALIGNV2. Further, we investigate the
performance of different text generation strategies: multiple variations of
pretraining, fact-aware embeddings and structure-aware input encoding. Our
extensive experiments show that a multi-lingual mT5 model which uses fact-aware
embeddings with structure-aware input encoding leads to best results on average
across the twelve languages. We make our code, dataset and model publicly
available, and hope that this will help advance further research in this
critical area.",2022-09-22
Distribution Aware Metrics for Conditional Natural Language Generation,2022-09-15 17:58:13+00:00,http://arxiv.org/abs/2209.07518v1,"David M Chan, Yiming Ni, Austin Myers, Sudheendra Vijayanarasimhan, David A Ross, John Canny","cs.CL, cs.AI, cs.CV, cs.LG",image2text,"Traditional automated metrics for evaluating conditional natural language
generation use pairwise comparisons between a single generated text and the
best-matching gold-standard ground truth text. When multiple ground truths are
available, scores are aggregated using an average or max operation across
references. While this approach works well when diversity in the ground truth
data (i.e. dispersion of the distribution of conditional texts) can be ascribed
to noise, such as in automated speech recognition, it does not allow for robust
evaluation in the case where diversity in the ground truths represents signal
for the model. In this work we argue that existing metrics are not appropriate
for domains such as visual description or summarization where ground truths are
semantically diverse, and where the diversity in those captions captures useful
additional information about the context. We propose a novel paradigm for
multi-candidate evaluation of conditional language generation models, and a new
family of metrics that compare the distributions of reference and
model-generated caption sets using small sample sets of each. We demonstrate
the utility of our approach with a case study in visual description: where we
show that existing models optimize for single-description quality over
diversity, and gain some insights into how sampling methods and temperature
impact description quality and diversity.",2022-09-15
PaLI: A Jointly-Scaled Multilingual Language-Image Model,2022-09-14 17:24:07+00:00,http://arxiv.org/abs/2209.06794v2,"Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut","cs.CV, cs.CL",image2text,"Effective scaling and a flexible task interface enable large language models
to excel at many tasks. PaLI (Pathways Language and Image model) extends this
approach to the joint modeling of language and vision. PaLI generates text
based on visual and textual inputs, and with this interface performs many
vision, language, and multimodal tasks, in many languages. To train PaLI, we
make use of large pretrained encoder-decoder language models and Vision
Transformers (ViTs). This allows us to capitalize on their existing
capabilities and leverage the substantial cost of training them. We find that
joint scaling of the vision and language components is important. Since
existing Transformers for language are much larger than their vision
counterparts, we train the largest ViT to date (ViT-e) to quantify the benefits
from even larger-capacity vision models. To train PaLI, we create a large
multilingual mix of pretraining tasks, based on a new image-text training set
containing 10B images and texts in over 100 languages. PaLI achieves
state-of-the-art in multiple vision and language tasks (such as captioning,
visual question-answering, scene-text understanding), while retaining a simple,
modular, and scalable design.",2022-09-14
"Visual Recipe Flow: A Dataset for Learning Visual State Changes of
  Objects with Recipe Flows",2022-09-13 09:38:32+00:00,http://arxiv.org/abs/2209.05840v1,"Keisuke Shirai, Atsushi Hashimoto, Taichi Nishimura, Hirotaka Kameko, Shuhei Kurita, Yoshitaka Ushiku, Shinsuke Mori","cs.CL, cs.AI",image2text,"We present a new multimodal dataset called Visual Recipe Flow, which enables
us to learn each cooking action result in a recipe text. The dataset consists
of object state changes and the workflow of the recipe text. The state change
is represented as an image pair, while the workflow is represented as a recipe
flow graph (r-FG). The image pairs are grounded in the r-FG, which provides the
cross-modal relation. With our dataset, one can try a range of applications,
from multimodal commonsense reasoning and procedural text generation.",2022-09-13
"Bridging Music and Text with Crowdsourced Music Comments: A
  Sequence-to-Sequence Framework for Thematic Music Comments Generation",2022-09-05 14:51:51+00:00,http://arxiv.org/abs/2209.01996v1,"Peining Zhang, Junliang Guo, Linli Xu, Mu You, Junming Yin","cs.SD, cs.CL, eess.AS",image2text,"We consider a novel task of automatically generating text descriptions of
music. Compared with other well-established text generation tasks such as image
caption, the scarcity of well-paired music and text datasets makes it a much
more challenging task. In this paper, we exploit the crowd-sourced music
comments to construct a new dataset and propose a sequence-to-sequence model to
generate text descriptions of music. More concretely, we use the dilated
convolutional layer as the basic component of the encoder and a memory based
recurrent neural network as the decoder. To enhance the authenticity and
thematicity of generated texts, we further propose to fine-tune the model with
a discriminator as well as a novel topic evaluator. To measure the quality of
generated texts, we also propose two new evaluation metrics, which are more
aligned with human evaluation than traditional metrics such as BLEU.
Experimental results verify that our model is capable of generating fluent and
meaningful comments while containing thematic and content information of the
original music.",2022-09-05
"Every picture tells a story: Image-grounded controllable stylistic story
  generation",2022-09-04 15:07:53+00:00,http://arxiv.org/abs/2209.01638v1,"Holy Lovenia, Bryan Wilie, Romain Barraud, Samuel Cahyawijaya, Willy Chung, Pascale Fung",cs.CL,image2text,"Generating a short story out of an image is arduous. Unlike image captioning,
story generation from an image poses multiple challenges: preserving the story
coherence, appropriately assessing the quality of the story, steering the
generated story into a certain style, and addressing the scarcity of
image-story pair reference datasets limiting supervision during training. In
this work, we introduce Plug-and-Play Story Teller (PPST) and improve
image-to-story generation by: 1) alleviating the data scarcity problem by
incorporating large pre-trained models, namely CLIP and GPT-2, to facilitate a
fluent image-to-text generation with minimal supervision, and 2) enabling a
more style-relevant generation by incorporating stylistic adapters to control
the story generation. We conduct image-to-story generation experiments with
non-styled, romance-styled, and action-styled PPST approaches and compare our
generated stories with those of previous work over three aspects, i.e., story
coherence, image-story relevance, and style fitness, using both automatic and
human evaluation. The results show that PPST improves story coherence and has
better image-story relevance, but has yet to be adequately stylistic.",2022-09-04
Understanding Attention for Vision-and-Language Tasks,2022-08-17 06:45:07+00:00,http://arxiv.org/abs/2208.08104v1,"Feiqi Cao, Soyeon Caren Han, Siqu Long, Changwei Xu, Josiah Poon","cs.CV, cs.CL",image2text,"Attention mechanism has been used as an important component across
Vision-and-Language(VL) tasks in order to bridge the semantic gap between
visual and textual features. While attention has been widely used in VL tasks,
it has not been examined the capability of different attention alignment
calculation in bridging the semantic gap between visual and textual clues. In
this research, we conduct a comprehensive analysis on understanding the role of
attention alignment by looking into the attention score calculation methods and
check how it actually represents the visual region's and textual token's
significance for the global assessment. We also analyse the conditions which
attention score calculation mechanism would be more (or less) interpretable,
and which may impact the model performance on three different VL tasks,
including visual question answering, text-to-image generation, text-and-image
matching (both sentence and image retrieval). Our analysis is the first of its
kind and provides useful insights of the importance of each attention alignment
score calculation when applied at the training phase of VL tasks, commonly
ignored in attention-based cross modal models, and/or pretrained models.",2022-08-17
ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model,2022-07-19 17:59:01+00:00,http://arxiv.org/abs/2207.09446v1,"Rao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, Srinath Sridhar","cs.CV, cs.AI",image2text,"We present ShapeCrafter, a neural network for recursive text-conditioned 3D
shape generation. Existing methods to generate text-conditioned 3D shapes
consume an entire text prompt to generate a 3D shape in a single step. However,
humans tend to describe shapes recursively-we may start with an initial
description and progressively add details based on intermediate results. To
capture this recursive process, we introduce a method to generate a 3D shape
distribution, conditioned on an initial phrase, that gradually evolves as more
phrases are added. Since existing datasets are insufficient for training this
approach, we present Text2Shape++, a large dataset of 369K shape-text pairs
that supports recursive shape generation. To capture local details that are
often used to refine shape descriptions, we build on top of vector-quantized
deep implicit functions that generate a distribution of high-quality shapes.
Results show that our method can generate shapes consistent with text
descriptions, and shapes evolve gradually as more phrases are added. Our method
supports shape editing, extrapolation, and can enable new applications in
human-machine collaboration for creative design.",2022-07-19
"A Baseline for Detecting Out-of-Distribution Examples in Image
  Captioning",2022-07-12 09:29:57+00:00,http://arxiv.org/abs/2207.05418v1,"Gabi Shalev, Gal-Lev Shalev, Joseph Keshet","cs.CV, cs.LG",image2text,"Image captioning research achieved breakthroughs in recent years by
developing neural models that can generate diverse and high-quality
descriptions for images drawn from the same distribution as training images.
However, when facing out-of-distribution (OOD) images, such as corrupted
images, or images containing unknown objects, the models fail in generating
relevant captions.
  In this paper, we consider the problem of OOD detection in image captioning.
We formulate the problem and suggest an evaluation setup for assessing the
model's performance on the task. Then, we analyze and show the effectiveness of
the caption's likelihood score at detecting and rejecting OOD images, which
implies that the relatedness between the input image and the generated caption
is encapsulated within the score.",2022-07-12
Towards Multimodal Vision-Language Models Generating Non-Generic Text,2022-07-09 01:56:35+00:00,http://arxiv.org/abs/2207.04174v1,"Wes Robbins, Zanyar Zohourianshahzadi, Jugal Kalita","cs.CV, cs.AI",image2text,"Vision-language models can assess visual context in an image and generate
descriptive text. While the generated text may be accurate and syntactically
correct, it is often overly general. To address this, recent work has used
optical character recognition to supplement visual information with text
extracted from an image. In this work, we contend that vision-language models
can benefit from additional information that can be extracted from an image,
but are not used by current models. We modify previous multimodal frameworks to
accept relevant information from any number of auxiliary classifiers. In
particular, we focus on person names as an additional set of tokens and create
a novel image-caption dataset to facilitate captioning with person names. The
dataset, Politicians and Athletes in Captions (PAC), consists of captioned
images of well-known people in context. By fine-tuning pretrained models with
this dataset, we demonstrate a model that can naturally integrate facial
recognition tokens into generated text by training on limited data. For the PAC
dataset, we provide a discussion on collection and baseline benchmark scores.",2022-07-09
Dual-Stream Transformer for Generic Event Boundary Captioning,2022-07-07 01:47:19+00:00,http://arxiv.org/abs/2207.03038v1,"Xin Gu, Hanhua Ye, Guang Chen, Yufei Wang, Libo Zhang, Longyin Wen","cs.CV, cs.CL",image2text,"This paper describes our champion solution for the CVPR2022 Generic Event
Boundary Captioning (GEBC) competition. GEBC requires the captioning model to
have a comprehension of instantaneous status changes around the given video
boundary, which makes it much more challenging than conventional video
captioning task. In this paper, a Dual-Stream Transformer with improvements on
both video content encoding and captions generation is proposed: (1) We utilize
three pre-trained models to extract the video features from different
granularities. Moreover, we exploit the types of boundary as hints to help the
model generate captions. (2) We particularly design an model, termed as
Dual-Stream Transformer, to learn discriminative representations for boundary
captioning. (3) Towards generating content-relevant and human-like captions, we
improve the description quality by designing a word-level ensemble strategy.
The promising results on the GEBC test split demonstrate the efficacy of our
proposed model.",2022-07-07
"Syntax Controlled Knowledge Graph-to-Text Generation with Order and
  Semantic Consistency",2022-07-02 02:42:14+00:00,http://arxiv.org/abs/2207.00719v1,"Jin Liu, Chongfeng Fan, Fengyu Zhou, Huijuan Xu",cs.AI,image2text,"The knowledge graph (KG) stores a large amount of structural knowledge, while
it is not easy for direct human understanding. Knowledge graph-to-text
(KG-to-text) generation aims to generate easy-to-understand sentences from the
KG, and at the same time, maintains semantic consistency between generated
sentences and the KG. Existing KG-to-text generation methods phrase this task
as a sequence-to-sequence generation task with linearized KG as input and
consider the consistency issue of the generated texts and KG through a simple
selection between decoded sentence word and KG node word at each time step.
However, the linearized KG order is commonly obtained through a heuristic
search without data-driven optimization. In this paper, we optimize the
knowledge description order prediction under the order supervision extracted
from the caption and further enhance the consistency of the generated sentences
and KG through syntactic and semantic regularization. We incorporate the
Part-of-Speech (POS) syntactic tags to constrain the positions to copy words
from the KG and employ a semantic context scoring function to evaluate the
semantic fitness for each word in its local context when decoding each word in
the generated sentence. Extensive experiments are conducted on two datasets,
WebNLG and DART, and achieve state-of-the-art performances.",2022-07-02
Automatic Controllable Product Copywriting for E-Commerce,2022-06-21 04:18:52+00:00,http://arxiv.org/abs/2206.10103v1,"Xiaojie Guo, Qingkai Zeng, Meng Jiang, Yun Xiao, Bo Long, Lingfei Wu","cs.AI, cs.LG",image2text,"Automatic product description generation for e-commerce has witnessed
significant advancement in the past decade. Product copywriting aims to attract
users' interest and improve user experience by highlighting product
characteristics with textual descriptions. As the services provided by
e-commerce platforms become diverse, it is necessary to adapt the patterns of
automatically-generated descriptions dynamically. In this paper, we report our
experience in deploying an E-commerce Prefix-based Controllable Copywriting
Generation (EPCCG) system into the JD.com e-commerce product recommendation
platform. The development of the system contains two main components: 1)
copywriting aspect extraction; 2) weakly supervised aspect labeling; 3) text
generation with a prefix-based language model; 4) copywriting quality control.
We conduct experiments to validate the effectiveness of the proposed EPCCG. In
addition, we introduce the deployed architecture which cooperates with the
EPCCG into the real-time JD.com e-commerce recommendation platform and the
significant payoff since deployment.",2022-06-21
"niksss at HinglishEval: Language-agnostic BERT-based Contextual
  Embeddings with Catboost for Quality Evaluation of the Low-Resource
  Synthetically Generated Code-Mixed Hinglish Text",2022-06-17 17:36:03+00:00,http://arxiv.org/abs/2206.08910v1,Nikhil Singh,cs.CL,image2text,"This paper describes the system description for the HinglishEval challenge at
INLG 2022. The goal of this task was to investigate the factors influencing the
quality of the code-mixed text generation system. The task was divided into two
subtasks, quality rating prediction and annotators disagreement prediction of
the synthetic Hinglish dataset. We attempted to solve these tasks using
sentence-level embeddings, which are obtained from mean pooling the
contextualized word embeddings for all input tokens in our text. We
experimented with various classifiers on top of the embeddings produced for
respective tasks. Our best-performing system ranked 1st on subtask B and 3rd on
subtask A.",2022-06-17
Prefix Language Models are Unified Modal Learners,2022-06-15 17:49:38+00:00,http://arxiv.org/abs/2206.07699v1,"Shizhe Diao, Wangchunshu Zhou, Xinsong Zhang, Jiawei Wang","cs.CV, cs.CL, cs.LG",image2text,"With the success of vision-language pre-training, we have witnessed the
state-of-the-art has been pushed on multi-modal understanding and generation.
However, the current pre-training paradigm is either incapable of targeting all
modalities at once (e.g., text generation and image generation), or requires
multi-fold well-designed tasks which significantly limits the scalability. We
demonstrate that a unified modal model could be learned with a prefix language
modeling objective upon text and image sequences. Thanks to the simple but
powerful pre-training paradigm, our proposed model, DaVinci, is simple to
train, scalable to huge data, and adaptable to a variety of downstream tasks
across modalities (language / vision / vision+language), types (understanding /
generation) and settings (e.g., zero-shot, fine-tuning, linear evaluation) with
a single unified architecture. DaVinci achieves the competitive performance on
a wide range of 26 understanding / generation tasks, and outperforms previous
unified vision-language models on most tasks, including ImageNet classification
(+1.6%), VQAv2 (+1.4%), COCO caption generation (BLEU@4 +1.1%, CIDEr +1.5%) and
COCO image generation (IS +0.9%, FID -1.0%), at the comparable model and data
scale. Furthermore, we offer a well-defined benchmark for future research by
reporting the performance on different scales of the pre-training dataset on a
heterogeneous and wide distribution coverage. Our results establish new,
stronger baselines for future comparisons at different data scales and shed
light on the difficulties of comparing VLP models more generally.",2022-06-15
Exploring industrial safety knowledge via Zipf law,2022-05-25 10:22:14+00:00,http://arxiv.org/abs/2205.12636v1,"Zhenhua Wang, Ming Ren, Dong Gao, Zhuang Li",cs.CL,image2text,"The hazard and operability analysis (HAZOP) report contains precious
industrial safety knowledge (ISK) with expert experience and process nature,
which is of great significance to the development of industrial intelligence.
Subject to the attributes of ISK, existing researches mine them through
sequence labeling in deep learning. Yet, there are two thorny issues: (1)
Uneven distribution of ISK and (2) Consistent importance of ISK: for safety
review. In this study, we propose a novel generative mining strategy called
CRGM to explore ISK. Inspired Zipf law in linguistics, CRGM consists of
common-rare discriminator, induction-extension generator and ISK extractor.
Firstly, the common-rare discriminator divides HAZOP descriptions into common
words and rare words, and obtains the common description and the rare
description, where the latter contains more industrial substances. Then, they
are operated by the induction-extension generator in the way of deep text
generation, the common description is induced and the rare description is
extended, the material knowledge and the equipment knowledge can be enriched.
Finally, the ISK extractor processes the material knowledge and equipment
knowledge from the generated description through the rule template method, the
additional ISK is regarded as the supplement of the training set to train the
proposed sequence labeling model. We conduct multiple evaluation experiments on
two industrial safety datasets. The results show that CRGM has promising and
gratifying aptitudes, greatly improves the performance of the model, and is
efficient and generalized. Our sequence labeling model also shows the expected
performance, which is better than the existing research. Our research provides
a new perspective for exploring ISK, we hope it can contribute support for the
intelligent progress of industrial safety.",2022-05-25
"The Dialog Must Go On: Improving Visual Dialog via Generative
  Self-Training",2022-05-25 05:40:00+00:00,http://arxiv.org/abs/2205.12502v1,"Gi-Cheon Kang, Sungdong Kim, Jin-Hwa Kim, Donghyun Kwak, Byoung-Tak Zhang","cs.CV, cs.CL, cs.LG",image2text,"Visual dialog (VisDial) is a task of answering a sequence of questions
grounded in an image, using the dialog history as context. Prior work has
trained the dialog agents solely on VisDial data via supervised learning or
leveraged pre-training on related vision-and-language datasets. This paper
presents a semi-supervised learning approach for visually-grounded dialog,
called Generative Self-Training (GST), to leverage unlabeled images on the Web.
Specifically, GST first retrieves in-domain images through out-of-distribution
detection and generates synthetic dialogs regarding the images via multimodal
conditional text generation. GST then trains a dialog agent on the synthetic
and the original VisDial data. As a result, GST scales the amount of training
data up to an order of magnitude that of VisDial (1.2M to 12.9M QA data). For
robust training of the generated dialogs, we also propose perplexity-based data
selection and multimodal consistency regularization. Evaluation on VisDial v1.0
and v0.9 datasets shows that GST achieves new state-of-the-art results on both
datasets. We further observe strong performance gains in the low-data regime
(up to 9.35 absolute points on NDCG).",2022-05-25
"Rethinking Evaluation Practices in Visual Question Answering: A Case
  Study on Out-of-Distribution Generalization",2022-05-24 16:44:45+00:00,http://arxiv.org/abs/2205.12191v1,"Aishwarya Agrawal, Ivana Kajić, Emanuele Bugliarello, Elnaz Davoodi, Anita Gergely, Phil Blunsom, Aida Nematzadeh","cs.CL, cs.AI, cs.CV, cs.LG",image2text,"Vision-and-language (V&L) models pretrained on large-scale multimodal data
have demonstrated strong performance on various tasks such as image captioning
and visual question answering (VQA). The quality of such models is commonly
assessed by measuring their performance on unseen data that typically comes
from the same distribution as the training data. However, we observe that these
models exhibit poor out-of-distribution (OOD) generalization on the task of
VQA. To better understand the underlying causes of poor generalization, we
comprehensively investigate performance of two pretrained V&L models under
different settings (i.e. classification and open-ended text generation) by
conducting cross-dataset evaluations. We find that these models tend to learn
to solve the benchmark, rather than learning the high-level skills required by
the VQA task. We also argue that in most cases generative models are less
susceptible to shifts in data distribution, while frequently performing better
on our tested benchmarks. Moreover, we find that multimodal pretraining
improves OOD performance in most settings. Finally, we revisit assumptions
underlying the use of automatic VQA evaluation metrics, and empirically show
that their stringent nature repeatedly penalizes models for correct responses.",2022-05-24
"On Advances in Text Generation from Images Beyond Captioning: A Case
  Study in Self-Rationalization",2022-05-24 00:52:40+00:00,http://arxiv.org/abs/2205.11686v1,"Shruti Palaskar, Akshita Bhagia, Yonatan Bisk, Florian Metze, Alan W Black, Ana Marasovic","cs.CL, cs.CV",image2text,"Integrating vision and language has gained notable attention following the
success of pretrained language models. Despite that, a fraction of emerging
multimodal models is suitable for text generation conditioned on images. This
minority is typically developed and evaluated for image captioning, a text
generation task conditioned solely on images with the goal to describe what is
explicitly visible in an image. In this paper, we take a step back and ask: How
do these models work for more complex generative tasks, conditioned on both
text and images? Are models based on joint multimodal pretraining, visually
adapted pretrained language models, or models that combine these two
approaches, more promising for such tasks? We address these questions in the
context of self-rationalization (jointly generating task labels/answers and
free-text explanations) of three tasks: (i) visual question answering in VQA-X,
(ii) visual commonsense reasoning in VCR, and (iii) visual-textual entailment
in E-SNLI-VE. We show that recent advances in each modality, CLIP image
representations and scaling of language models, do not consistently improve
multimodal self-rationalization of tasks with multimodal inputs. We also
observe that no model type works universally the best across tasks/datasets and
finetuning data sizes. Our findings call for a backbone modelling approach that
can be built on to advance text generation from images and text beyond image
captioning.",2022-05-24
What Makes Data-to-Text Generation Hard for Pretrained Language Models?,2022-05-23 17:58:39+00:00,http://arxiv.org/abs/2205.11505v1,"Moniba Keymanesh, Adrian Benton, Mark Dredze","cs.CL, cs.AI, cs.IR, cs.LG",image2text,"Expressing natural language descriptions of structured facts or relations --
data-to-text generation (D2T) -- increases the accessibility of structured
knowledge repositories. Previous work shows that pre-trained language
models(PLMs) perform remarkably well on this task after fine-tuning on a
significant amount of task-specific training data. On the other hand, while
auto-regressive PLMs can generalize from a few task examples, their efficacy at
D2T is largely unexplored. Furthermore, we have an incomplete understanding of
the limits of PLMs on D2T.
  In this work, we conduct an empirical study of both fine-tuned and
auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their
performance as a function of the amount of task-specific data and how these
data are incorporated into the models: zero and few-shot learning, and
fine-tuning of model weights. In addition, we probe the limits of PLMs by
measuring performance on subsets of the evaluation data: novel predicates and
abstractive test examples. To improve the performance on these subsets, we
investigate two techniques: providing predicate descriptions in the context and
re-ranking generated candidates by information reflected in the source.
Finally, we conduct a human evaluation of model errors and show that D2T
generation tasks would benefit from datasets with more careful manual curation.",2022-05-23
"Language Models with Image Descriptors are Strong Few-Shot
  Video-Language Learners",2022-05-22 05:18:27+00:00,http://arxiv.org/abs/2205.10747v2,"Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal, Heng Ji","cs.CV, cs.AI",image2text,"The goal of this work is to build flexible video-language models that can
generalize to various video-to-text tasks from few examples, such as
domain-specific captioning, question answering, and future event prediction.
Existing few-shot video-language learners focus exclusively on the encoder,
resulting in the absence of a video-to-text decoder to handle generative tasks.
Video captioners have been pretrained on large-scale video-language datasets,
but they rely heavily on finetuning and lack the ability to generate text for
unseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language
Learner via Image and Language models, which demonstrates strong performance on
few-shot video-to-text tasks without the necessity of pretraining or finetuning
on any video datasets. We use the image-language models to translate the video
content into frame captions, object, attribute, and event phrases, and compose
them into a temporal structure template. We then instruct a language model,
with a prompt containing a few in-context examples, to generate a target output
from the composed content. The flexibility of prompting allows the model to
capture any form of text input, such as automatic speech recognition (ASR)
transcripts. Our experiments demonstrate the power of language models in
understanding videos on a wide variety of video-language tasks, including video
captioning, video question answering, video caption retrieval, and video future
event prediction. Especially, on video future event prediction, our few-shot
model significantly outperforms state-of-the-art supervised models trained on
large-scale video datasets. Code and resources are publicly available for
research purposes at https://github.com/MikeWangWZHL/VidIL .",2022-05-22
"Context Matters for Image Descriptions for Accessibility: Challenges for
  Referenceless Evaluation Metrics",2022-05-21 17:35:26+00:00,http://arxiv.org/abs/2205.10646v1,"Elisa Kreiss, Cynthia Bennett, Shayan Hooshmand, Eric Zelikman, Meredith Ringel Morris, Christopher Potts",cs.CL,image2text,"Few images on the Web receive alt-text descriptions that would make them
accessible to blind and low vision (BLV) users. Image-based NLG systems have
progressed to the point where they can begin to address this persistent
societal problem, but these systems will not be fully successful unless we
evaluate them on metrics that guide their development correctly. Here, we argue
against current referenceless metrics -- those that don't rely on
human-generated ground-truth descriptions -- on the grounds that they do not
align with the needs of BLV users. The fundamental shortcoming of these metrics
is that they cannot take context into account, whereas contextual information
is highly valued by BLV users. To substantiate these claims, we present a study
with BLV participants who rated descriptions along a variety of dimensions. An
in-depth analysis reveals that the lack of context-awareness makes current
referenceless metrics inadequate for advancing image accessibility, requiring a
rethinking of referenceless evaluation metrics for image-based NLG systems.",2022-05-21
"It Isn't Sh!tposting, It's My CAT Posting",2022-05-18 04:11:55+00:00,http://arxiv.org/abs/2205.08710v1,"Parthsarthi Rawat, Sayan Das, Jorge Aguirre, Akhil Daphara","cs.CV, cs.AI, cs.LG",image2text,"In this paper, we describe a novel architecture which can generate hilarious
captions for a given input image. The architecture is split into two halves,
i.e. image captioning and hilarious text conversion. The architecture starts
with a pre-trained CNN model, VGG16 in this implementation, and applies
attention LSTM on it to generate normal caption. These normal captions then are
fed forward to our hilarious text conversion transformer which converts this
text into something hilarious while maintaining the context of the input image.
The architecture can also be split into two halves and only the seq2seq
transformer can be used to generate hilarious caption by inputting a
sentence.This paper aims to help everyday user to be more lazy and hilarious at
the same time by generating captions using CATNet.",2022-05-18
"Breaking with Fixed Set Pathology Recognition through Report-Guided
  Contrastive Training",2022-05-14 21:44:05+00:00,http://arxiv.org/abs/2205.07139v1,"Constantin Seibold, Simon Reiß, M. Saquib Sarfraz, Rainer Stiefelhagen, Jens Kleesiek","cs.CV, cs.LG",image2text,"When reading images, radiologists generate text reports describing the
findings therein. Current state-of-the-art computer-aided diagnosis tools
utilize a fixed set of predefined categories automatically extracted from these
medical reports for training. This form of supervision limits the potential
usage of models as they are unable to pick up on anomalies outside of their
predefined set, thus, making it a necessity to retrain the classifier with
additional data when faced with novel classes. In contrast, we investigate
direct text supervision to break away from this closed set assumption. By doing
so, we avoid noisy label extraction via text classifiers and incorporate more
contextual information.
  We employ a contrastive global-local dual-encoder architecture to learn
concepts directly from unstructured medical reports while maintaining its
ability to perform free form classification.
  We investigate relevant properties of open set recognition for radiological
data and propose a method to employ currently weakly annotated data into
training.
  We evaluate our approach on the large-scale chest X-Ray datasets MIMIC-CXR,
CheXpert, and ChestX-Ray14 for disease classification. We show that despite
using unstructured medical report supervision, we perform on par with direct
label supervision through a sophisticated inference setting.",2022-05-14
"Robust (Controlled) Table-to-Text Generation with Structure-Aware
  Equivariance Learning",2022-05-08 23:37:27+00:00,http://arxiv.org/abs/2205.03972v1,"Fei Wang, Zhewei Xu, Pedro Szekely, Muhao Chen","cs.CL, cs.AI, cs.LG",image2text,"Controlled table-to-text generation seeks to generate natural language
descriptions for highlighted subparts of a table. Previous SOTA systems still
employ a sequence-to-sequence generation method, which merely captures the
table as a linear structure and is brittle when table layouts change. We seek
to go beyond this paradigm by (1) effectively expressing the relations of
content pieces in the table, and (2) making our model robust to
content-invariant structural transformations. Accordingly, we propose an
equivariance learning framework, which encodes tables with a structure-aware
self-attention mechanism. This prunes the full self-attention structure into an
order-invariant graph attention that captures the connected graph structure of
cells belonging to the same row or column, and it differentiates between
relevant cells and irrelevant cells from the structural perspective. Our
framework also modifies the positional encoding mechanism to preserve the
relative position of tokens in the same cell but enforce position invariance
among different cells. Our technology is free to be plugged into existing
table-to-text generation models, and has improved T5-based models to offer
better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo,
we preserve promising performance, while previous SOTA systems, even with
transformation-based data augmentation, have seen significant performance
drops. Our code is available at https://github.com/luka-group/Lattice.",2022-05-08
RoViST:Learning Robust Metrics for Visual Storytelling,2022-05-08 03:51:22+00:00,http://arxiv.org/abs/2205.03774v1,"Eileen Wang, Caren Han, Josiah Poon","cs.CV, cs.AI",image2text,"Visual storytelling (VST) is the task of generating a story paragraph that
describes a given image sequence. Most existing storytelling approaches have
evaluated their models using traditional natural language generation metrics
like BLEU or CIDEr. However, such metrics based on n-gram matching tend to have
poor correlation with human evaluation scores and do not explicitly consider
other criteria necessary for storytelling such as sentence structure or topic
coherence. Moreover, a single score is not enough to assess a story as it does
not inform us about what specific errors were made by the model. In this paper,
we propose 3 evaluation metrics sets that analyses which aspects we would look
for in a good story: 1) visual grounding, 2) coherence, and 3) non-redundancy.
We measure the reliability of our metric sets by analysing its correlation with
human judgement scores on a sample of machine stories obtained from 4
state-of-the-arts models trained on the Visual Storytelling Dataset (VIST). Our
metric sets outperforms other metrics on human correlation, and could be served
as a learning based evaluation metric set that is complementary to existing
rule-based metrics.",2022-05-08
"Attract me to Buy: Advertisement Copywriting Generation with Multimodal
  Multi-structured Information",2022-05-07 03:33:00+00:00,http://arxiv.org/abs/2205.03534v1,"Zhipeng Zhang, Xinglin Hou, Kai Niu, Zhongzhen Huang, Tiezheng Ge, Yuning Jiang, Qi Wu, Peng Wang","cs.CL, cs.CV, cs.MM",image2text,"Recently, online shopping has gradually become a common way of shopping for
people all over the world. Wonderful merchandise advertisements often attract
more people to buy. These advertisements properly integrate multimodal
multi-structured information of commodities, such as visual spatial information
and fine-grained structure information. However, traditional multimodal text
generation focuses on the conventional description of what existed and
happened, which does not match the requirement of advertisement copywriting in
the real world. Because advertisement copywriting has a vivid language style
and higher requirements of faithfulness. Unfortunately, there is a lack of
reusable evaluation frameworks and a scarcity of datasets. Therefore, we
present a dataset, E-MMAD (e-commercial multimodal multi-structured
advertisement copywriting), which requires, and supports much more detailed
information in text generation. Noticeably, it is one of the largest video
captioning datasets in this field. Accordingly, we propose a baseline method
and faithfulness evaluation metric on the strength of structured information
reasoning to solve the demand in reality on this dataset. It surpasses the
previous methods by a large margin on all metrics. The dataset and method are
coming soon on \url{https://e-mmad.github.io/e-mmad.net/index.html}.",2022-05-07
Language Models Can See: Plugging Visual Controls in Text Generation,2022-05-05 13:56:18+00:00,http://arxiv.org/abs/2205.02655v1,"Yixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani Yogatama, Yan Wang, Lingpeng Kong, Nigel Collier","cs.CV, cs.CL",image2text,"Generative language models (LMs) such as GPT-2/3 can be prompted to generate
text with remarkable quality. While they are designed for text-prompted
generation, it remains an open question how the generation process could be
guided by modalities beyond text such as images. In this work, we propose a
training-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP),
for plugging in visual controls in the generation process and enabling LMs to
perform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC
is a simple yet efficient plug-and-play framework, which directly combines an
off-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP)
for image-grounded text generation. During decoding, MAGIC influences the
generation of the LM by introducing a CLIP-induced score, called magic score,
which regularizes the generated result to be semantically related to a given
image while being coherent to the previously generated context. Notably, the
proposed decoding scheme does not involve any gradient update operation,
therefore being computationally efficient. On the challenging task of zero-shot
image captioning, MAGIC outperforms the state-of-the-art method by notable
margins with a nearly 27 times decoding speedup. MAGIC is a flexible framework
and is theoretically compatible with any text generation tasks that incorporate
image grounding. In the experiments, we showcase that it is also capable of
performing visually grounded story generation given both an image and a text
prompt.",2022-05-05
Diverse Image Captioning with Grounded Style,2022-05-03 22:57:59+00:00,http://arxiv.org/abs/2205.01813v1,"Franz Klein, Shweta Mahajan, Stefan Roth","cs.CV, cs.LG",image2text,"Stylized image captioning as presented in prior work aims to generate
captions that reflect characteristics beyond a factual description of the scene
composition, such as sentiments. Such prior work relies on given sentiment
identifiers, which are used to express a certain global style in the caption,
e.g. positive or negative, however without taking into account the stylistic
content of the visual scene. To address this shortcoming, we first analyze the
limitations of current stylized captioning datasets and propose COCO
attribute-based augmentations to obtain varied stylized captions from COCO
annotations. Furthermore, we encode the stylized information in the latent
space of a Variational Autoencoder; specifically, we leverage extracted image
attributes to explicitly structure its sequential latent space according to
different localized style characteristics. Our experiments on the Senticap and
COCO datasets show the ability of our approach to generate accurate captions
with diversity in styles that are grounded in the image.",2022-05-03
Cross-modal Memory Networks for Radiology Report Generation,2022-04-28 02:32:53+00:00,http://arxiv.org/abs/2204.13258v1,"Zhihong Chen, Yaling Shen, Yan Song, Xiang Wan",cs.CL,image2text,"Medical imaging plays a significant role in clinical practice of medical
diagnosis, where the text reports of the images are essential in understanding
them and facilitating later treatments. By generating the reports
automatically, it is beneficial to help lighten the burden of radiologists and
significantly promote clinical automation, which already attracts much
attention in applying artificial intelligence to medical domain. Previous
studies mainly follow the encoder-decoder paradigm and focus on the aspect of
text generation, with few studies considering the importance of cross-modal
mappings and explicitly exploit such mappings to facilitate radiology report
generation. In this paper, we propose a cross-modal memory networks (CMN) to
enhance the encoder-decoder framework for radiology report generation, where a
shared memory is designed to record the alignment between images and texts so
as to facilitate the interaction and generation across modalities. Experimental
results illustrate the effectiveness of our proposed model, where
state-of-the-art performance is achieved on two widely used benchmark datasets,
i.e., IU X-Ray and MIMIC-CXR. Further analyses also prove that our model is
able to better align information from radiology images and texts so as to help
generating more accurate reports in terms of clinical indicators.",2022-04-28
"Recovering Patient Journeys: A Corpus of Biomedical Entities and
  Relations on Twitter (BEAR)",2022-04-21 08:18:44+00:00,http://arxiv.org/abs/2204.09952v1,"Amelie Wührl, Roman Klinger","cs.CL, cs.IR",image2text,"Text mining and information extraction for the medical domain has focused on
scientific text generated by researchers. However, their direct access to
individual patient experiences or patient-doctor interactions can be limited.
Information provided on social media, e.g., by patients and their relatives,
complements the knowledge in scientific text. It reflects the patient's journey
and their subjective perspective on the process of developing symptoms, being
diagnosed and offered a treatment, being cured or learning to live with a
medical condition. The value of this type of data is therefore twofold:
Firstly, it offers direct access to people's perspectives. Secondly, it might
cover information that is not available elsewhere, including self-treatment or
self-diagnoses. Named entity recognition and relation extraction are methods to
structure information that is available in unstructured text. However, existing
medical social media corpora focused on a comparably small set of entities and
relations and particular domains, rather than putting the patient into the
center of analyses. With this paper we contribute a corpus with a rich set of
annotation layers following the motivation to uncover and model patients'
journeys and experiences in more detail. We label 14 entity classes (incl.
environmental factors, diagnostics, biochemical processes, patients'
quality-of-life descriptions, pathogens, medical conditions, and treatments)
and 20 relation classes (e.g., prevents, influences, interactions, causes) most
of which have not been considered before for social media data. The publicly
available dataset consists of 2,100 tweets with approx. 6,000 entity and 3,000
relation annotations. In a corpus analysis we find that over 80 % of documents
contain relevant entities. Over 50 % of tweets express relations which we
consider essential for uncovering patients' narratives about their journeys.",2022-04-21
"Evaluating Mixed-initiative Conversational Search Systems via User
  Simulation",2022-04-17 16:27:33+00:00,http://arxiv.org/abs/2204.08046v1,"Ivan Sekulić, Mohammad Aliannejadi, Fabio Crestani","cs.CL, cs.IR",image2text,"Clarifying the underlying user information need by asking clarifying
questions is an important feature of modern conversational search system.
However, evaluation of such systems through answering prompted clarifying
questions requires significant human effort, which can be time-consuming and
expensive. In this paper, we propose a conversational User Simulator, called
USi, for automatic evaluation of such conversational search systems. Given a
description of an information need, USi is capable of automatically answering
clarifying questions about the topic throughout the search session. Through a
set of experiments, including automated natural language generation metrics and
crowdsourcing studies, we show that responses generated by USi are both inline
with the underlying information need and comparable to human-generated answers.
Moreover, we make the first steps towards multi-turn interactions, where
conversational search systems asks multiple questions to the (simulated) user
with a goal of clarifying the user need. To this end, we expand on currently
available datasets for studying clarifying questions, i.e., Qulac and ClariQ,
by performing a crowdsourcing-based multi-turn data acquisition. We show that
our generative, GPT2-based model, is capable of providing accurate and natural
answers to unseen clarifying questions in the single-turn setting and discuss
capabilities of our model in the multi-turn setting. We provide the code, data,
and the pre-trained model to be used for further research on the topic.",2022-04-17
"Regularization-based Pruning of Irrelevant Weights in Deep Neural
  Architectures",2022-04-11 09:44:16+00:00,http://arxiv.org/abs/2204.04977v1,"Giovanni Bonetta, Matteo Ribero, Rossella Cancelliere","cs.CL, cs.AI",image2text,"Deep neural networks exploiting millions of parameters are nowadays the norm
in deep learning applications. This is a potential issue because of the great
amount of computational resources needed for training, and of the possible loss
of generalization performance of overparametrized networks. We propose in this
paper a method for learning sparse neural topologies via a regularization
technique which identifies non relevant weights and selectively shrinks their
norm, while performing a classic update for relevant ones. This technique,
which is an improvement of classical weight decay, is based on the definition
of a regularization term which can be added to any loss functional regardless
of its form, resulting in a unified general framework exploitable in many
different contexts. The actual elimination of parameters identified as
irrelevant is handled by an iterative pruning algorithm. We tested the proposed
technique on different image classification and Natural language generation
tasks, obtaining results on par or better then competitors in terms of sparsity
and metrics, while achieving strong models compression.",2022-04-11
"Explaining Deep Convolutional Neural Networks via Latent Visual-Semantic
  Filter Attention",2022-04-10 04:57:56+00:00,http://arxiv.org/abs/2204.04601v1,"Yu Yang, Seungbae Kim, Jungseock Joo","cs.CV, cs.AI, cs.LG",image2text,"Interpretability is an important property for visual models as it helps
researchers and users understand the internal mechanism of a complex model.
However, generating semantic explanations about the learned representation is
challenging without direct supervision to produce such explanations. We propose
a general framework, Latent Visual Semantic Explainer (LaViSE), to teach any
existing convolutional neural network to generate text descriptions about its
own latent representations at the filter level. Our method constructs a mapping
between the visual and semantic spaces using generic image datasets, using
images and category names. It then transfers the mapping to the target domain
which does not have semantic labels. The proposed framework employs a modular
structure and enables to analyze any trained network whether or not its
original training data is available. We show that our method can generate novel
descriptions for learned filters beyond the set of categories defined in the
training dataset and perform an extensive evaluation on multiple datasets. We
also demonstrate a novel application of our method for unsupervised dataset
bias analysis which allows us to automatically discover hidden biases in
datasets or compare different subsets without using additional labels. The
dataset and code are made public to facilitate further research.",2022-04-10
On Distinctive Image Captioning via Comparing and Reweighting,2022-04-08 08:59:23+00:00,http://arxiv.org/abs/2204.03938v1,"Jiuniu Wang, Wenjia Xu, Qingzhong Wang, Antoni B. Chan","cs.CV, cs.AI",image2text,"Recent image captioning models are achieving impressive results based on
popular metrics, i.e., BLEU, CIDEr, and SPICE. However, focusing on the most
popular metrics that only consider the overlap between the generated captions
and human annotation could result in using common words and phrases, which
lacks distinctiveness, i.e., many similar images have the same caption. In this
paper, we aim to improve the distinctiveness of image captions via comparing
and reweighting with a set of similar images. First, we propose a
distinctiveness metric -- between-set CIDEr (CIDErBtw) to evaluate the
distinctiveness of a caption with respect to those of similar images. Our
metric reveals that the human annotations of each image in the MSCOCO dataset
are not equivalent based on distinctiveness; however, previous works normally
treat the human annotations equally during training, which could be a reason
for generating less distinctive captions. In contrast, we reweight each
ground-truth caption according to its distinctiveness during training. We
further integrate a long-tailed weight strategy to highlight the rare words
that contain more information, and captions from the similar image set are
sampled as negative examples to encourage the generated sentence to be unique.
Finally, extensive experiments are conducted, showing that our proposed
approach significantly improves both distinctiveness (as measured by CIDErBtw
and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide
variety of image captioning baselines. These results are further confirmed
through a user study.",2022-04-08
CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations,2022-04-05 17:38:04+00:00,http://arxiv.org/abs/2204.02380v1,"Leonard Salewski, A. Sophia Koepke, Hendrik P. A. Lensch, Zeynep Akata","cs.CV, cs.CL",image2text,"Providing explanations in the context of Visual Question Answering (VQA)
presents a fundamental problem in machine learning. To obtain detailed insights
into the process of generating natural language explanations for VQA, we
introduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with
natural language explanations. For each image-question pair in the CLEVR
dataset, CLEVR-X contains multiple structured textual explanations which are
derived from the original scene graphs. By construction, the CLEVR-X
explanations are correct and describe the reasoning and visual information that
is necessary to answer a given question. We conducted a user study to confirm
that the ground-truth explanations in our proposed dataset are indeed complete
and relevant. We present baseline results for generating natural language
explanations in the context of VQA using two state-of-the-art frameworks on the
CLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation
generation quality for different question and answer types. Additionally, we
study the influence of using different numbers of ground-truth explanations on
the convergence of natural language generation (NLG) metrics. The CLEVR-X
dataset is publicly available at
\url{https://explainableml.github.io/CLEVR-X/}.",2022-04-05
Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language,2022-04-01 17:43:13+00:00,http://arxiv.org/abs/2204.00598v1,"Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Pete Florence","cs.CV, cs.AI, cs.CL, cs.LG",image2text,"Large foundation models can exhibit unique capabilities depending on the
domain of data they are trained on. While these domains are generic, they may
only barely overlap. For example, visual-language models (VLMs) are trained on
Internet-scale image captions, but large language models (LMs) are further
trained on Internet-scale text with no images (e.g. from spreadsheets, to SAT
questions). As a result, these models store different forms of commonsense
knowledge across different domains. In this work, we show that this model
diversity is symbiotic, and can be leveraged to build AI systems with
structured Socratic dialogue -- in which new multimodal tasks are formulated as
a guided language-based exchange between different pre-existing foundation
models, without additional finetuning. In the context of egocentric perception,
we present a case study of Socratic Models (SMs) that can provide meaningful
results for complex tasks such as generating free-form answers to contextual
questions about egocentric video, by formulating video Q&A as short story Q&A,
i.e. summarizing the video into a short story, then answering questions about
it. Additionally, SMs can generate captions for Internet images, and are
competitive with state-of-the-art on zero-shot video-to-text retrieval with
42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate how to compose foundation models
zero-shot to capture new multimodal functionalities, without domain-specific
data collection. Prototypes are available at socraticmodels.github.io.",2022-04-01
Neural Pipeline for Zero-Shot Data-to-Text Generation,2022-03-30 13:14:35+00:00,http://arxiv.org/abs/2203.16279v1,"Zdeněk Kasner, Ondřej Dušek",cs.CL,image2text,"In data-to-text (D2T) generation, training on in-domain data leads to
overfitting to the data representation and repeating training data noise. We
examine how to avoid finetuning pretrained language models (PLMs) on D2T
generation datasets while still taking advantage of surface realization
capabilities of PLMs. Inspired by pipeline approaches, we propose to generate
text by transforming single-item descriptions with a sequence of modules
trained on general-domain text-based operations: ordering, aggregation, and
paragraph compression. We train PLMs for performing these operations on a
synthetic corpus WikiFluent which we build from English Wikipedia. Our
experiments on two major triple-to-text datasets -- WebNLG and E2E -- show that
our approach enables D2T generation from RDF triples in zero-shot settings.",2022-03-30
"GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate
  Degradation of Artificial Neural Language Models",2022-03-25 00:25:42+00:00,http://arxiv.org/abs/2203.13397v1,"Changye Li, David Knopman, Weizhe Xu, Trevor Cohen, Serguei Pakhomov",cs.CL,image2text,"Deep learning (DL) techniques involving fine-tuning large numbers of model
parameters have delivered impressive performance on the task of discriminating
between language produced by cognitively healthy individuals, and those with
Alzheimer's disease (AD). However, questions remain about their ability to
generalize beyond the small reference sets that are publicly available for
research. As an alternative to fitting model parameters directly, we propose a
novel method by which a Transformer DL model (GPT-2) pre-trained on general
English text is paired with an artificially degraded version of itself (GPT-D),
to compute the ratio between these two models' \textit{perplexities} on
language from cognitively healthy and impaired individuals. This technique
approaches state-of-the-art performance on text data from a widely used ""Cookie
Theft"" picture description task, and unlike established alternatives also
generalizes well to spontaneous conversations. Furthermore, GPT-D generates
text with characteristics known to be associated with AD, demonstrating the
induction of dementia-related linguistic anomalies. Our study is a step toward
better understanding of the relationships between the inner workings of
generative neural language models, the language that they produce, and the
deleterious effects of dementia on human speech and language characteristics.",2022-03-25
Chart-to-Text: A Large-Scale Benchmark for Chart Summarization,2022-03-12 17:01:38+00:00,http://arxiv.org/abs/2203.06486v1,"Shankar Kanthara, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, Shafiq Joty",cs.CL,image2text,"Charts are commonly used for exploring data and communicating insights.
Generating natural language summaries from charts can be very helpful for
people in inferring key insights that would otherwise require a lot of
cognitive and perceptual efforts. We present Chart-to-text, a large-scale
benchmark with two datasets and a total of 44,096 charts covering a wide range
of topics and chart types. We explain the dataset construction process and
analyze the datasets. We also introduce a number of state-of-the-art neural
models as baselines that utilize image captioning and data-to-text generation
techniques to tackle two problem variations: one assumes the underlying data
table of the chart is available while the other needs to extract data from
chart images. Our analysis with automatic and human evaluation shows that while
our best models usually generate fluent summaries and yield reasonable BLEU
scores, they also suffer from hallucinations and factual errors as well as
difficulties in correctly explaining complex patterns and trends in charts.",2022-03-12
Compilable Neural Code Generation with Compiler Feedback,2022-03-10 03:15:17+00:00,http://arxiv.org/abs/2203.05132v1,"Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, Qun Liu","cs.CL, cs.AI, cs.PL",image2text,"Automatically generating compilable programs with (or without) natural
language descriptions has always been a touchstone problem for computational
linguistics and automated software engineering. Existing deep-learning
approaches model code generation as text generation, either constrained by
grammar structures in decoder, or driven by pre-trained language models on
large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of
them account for compilability of the generated programs. To improve
compilability of the generated programs, this paper proposes COMPCODER, a
three-stage pipeline utilizing compiler feedback for compilable code
generation, including language model fine-tuning, compilability reinforcement,
and compilability discrimination. Comprehensive experiments on two code
generation tasks demonstrate the effectiveness of our proposed approach,
improving the success rate of compilation from 44.18 to 89.18 in code
completion on average and from 70.3 to 96.2 in text-to-code generation,
respectively, when comparing with the state-of-the-art CodeGPT.",2022-03-10
"How to Fill the Optimum Set? Population Gradient Descent with Harmless
  Diversity",2022-02-16 23:40:18+00:00,http://arxiv.org/abs/2202.08376v1,"Chengyue Gong, Lemeng Wu, Qiang Liu","cs.LG, cs.CV",image2text,"Although traditional optimization methods focus on finding a single optimal
solution, most objective functions in modern machine learning problems,
especially those in deep learning, often have multiple or infinite numbers of
optima. Therefore, it is useful to consider the problem of finding a set of
diverse points in the optimum set of an objective function. In this work, we
frame this problem as a bi-level optimization problem of maximizing a diversity
score inside the optimum set of the main loss function, and solve it with a
simple population gradient descent framework that iteratively updates the
points to maximize the diversity score in a fashion that does not hurt the
optimization of the main loss. We demonstrate that our method can efficiently
generate diverse solutions on a variety of applications, including
text-to-image generation, text-to-mesh generation, molecular conformation
generation and ensemble neural network training.",2022-02-16
"Deep soccer captioning with transformer: dataset, semantics-related
  losses, and multi-level evaluation",2022-02-11 16:04:03+00:00,http://arxiv.org/abs/2202.05728v1,"Ahmad Hammoudeh, Bastein Vanderplaetse, Stéphane Dupont","cs.CV, cs.AI",image2text,"This work aims at generating captions for soccer videos using deep learning.
In this context, this paper introduces a dataset, model, and triple-level
evaluation. The dataset consists of 22k caption-clip pairs and three visual
features (images, optical flow, inpainting) for ~500 hours of \emph{SoccerNet}
videos. The model is divided into three parts: a transformer learns language,
ConvNets learn vision, and a fusion of linguistic and visual features generates
captions. The paper suggests evaluating generated captions at three levels:
syntax (the commonly used evaluation metrics such as BLEU-score and CIDEr),
meaning (the quality of descriptions for a domain expert), and corpus (the
diversity of generated captions). The paper shows that the diversity of
generated captions has improved (from 0.07 reaching 0.18) with
semantics-related losses that prioritize selected words. Semantics-related
losses and the utilization of more visual features (optical flow, inpainting)
improved the normalized captioning score by 28\%. The web page of this work:
https://sites.google.com/view/soccercaptioning}{https://sites.google.com/view/soccercaptioning",2022-02-11
"Unifying Architectures, Tasks, and Modalities Through a Simple
  Sequence-to-Sequence Learning Framework",2022-02-07 10:38:21+00:00,http://arxiv.org/abs/2202.03052v1,"Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang","cs.CV, cs.CL",image2text,"In this work, we pursue a unified paradigm for multimodal pretraining to
break the scaffolds of complex task/modality-specific customization. We propose
OFA, a unified multimodal pretrained model that unifies modalities (i.e.,
cross-modality, vision, language) and tasks (e.g., image generation, visual
grounding, image captioning, image classification, text generation, etc.) to a
simple sequence-to-sequence learning framework based on the encoder-decoder
architecture. OFA performs pretraining and finetuning with task instructions
and introduces no extra task-specific layers for finetuning. Experimental
results show that OFA achieves new state-of-the-arts on a series of multimodal
tasks, including image captioning (COCO test CIDEr: 149.6), text-to-image
generation (COCO test FID: 10.5), VQA (test-std acc.: 80.02), SNLI-VE (test
acc.: 90.20), and referring expression comprehension (RefCOCO / RefCOCO+ /
RefCOCOg test acc.: 92.93 / 90.10 / 85.20). Through extensive analyses, we
demonstrate that OFA reaches comparable performance with uni-modal pretrained
models (e.g., BERT, MAE, MoCo v3, SimCLR v2, etc.) in uni-modal tasks,
including NLU, NLG, and image classification, and it effectively transfers to
unseen tasks and domains. Code shall be released soon at
http://github.com/OFA-Sys/OFA",2022-02-07
"XAlign: Cross-lingual Fact-to-Text Alignment and Generation for
  Low-Resource Languages",2022-02-01 09:41:59+00:00,http://arxiv.org/abs/2202.00291v1,"Tushar Abhishek, Shivprasad Sagare, Bhavyajeet Singh, Anubhav Sharma, Manish Gupta, Vasudeva Varma",cs.CL,image2text,"Multiple critical scenarios (like Wikipedia text generation given English
Infoboxes) need automated generation of descriptive text in low resource (LR)
languages from English fact triples. Previous work has focused on English
fact-to-text (F2T) generation. To the best of our knowledge, there has been no
previous attempt on cross-lingual alignment or generation for LR languages.
Building an effective cross-lingual F2T (XF2T) system requires alignment
between English structured facts and LR sentences. We propose two unsupervised
methods for cross-lingual alignment. We contribute XALIGN, an XF2T dataset with
0.45M pairs across 8 languages, of which 5402 pairs have been manually
annotated. We also train strong baseline XF2T generation models on the XAlign
dataset.",2022-02-01
"BERTHA: Video Captioning Evaluation Via Transfer-Learned Human
  Assessment",2022-01-25 11:29:58+00:00,http://arxiv.org/abs/2201.10243v1,"Luis Lebron, Yvette Graham, Kevin McGuinness, Konstantinos Kouramas, Noel E. O'Connor","cs.CV, cs.LG",image2text,"Evaluating video captioning systems is a challenging task as there are
multiple factors to consider; for instance: the fluency of the caption,
multiple actions happening in a single scene, and the human bias of what is
considered important. Most metrics try to measure how similar the system
generated captions are to a single or a set of human-annotated captions. This
paper presents a new method based on a deep learning model to evaluate these
systems. The model is based on BERT, which is a language model that has been
shown to work well in multiple NLP tasks. The aim is for the model to learn to
perform an evaluation similar to that of a human. To do so, we use a dataset
that contains human evaluations of system generated captions. The dataset
consists of the human judgments of the captions produce by the system
participating in various years of the TRECVid video to text task. These
annotations will be made publicly available. BERTHA obtain favourable results,
outperforming the commonly used metrics in some setups.",2022-01-25
Pre-Trained Language Transformers are Universal Image Classifiers,2022-01-25 08:56:14+00:00,http://arxiv.org/abs/2201.10182v1,"Rahul Goel, Modar Sulaiman, Kimia Noorbakhsh, Mahdi Sharifi, Rajesh Sharma, Pooyan Jamshidi, Kallol Roy","cs.CV, cs.AI",image2text,"Facial images disclose many hidden personal traits such as age, gender, race,
health, emotion, and psychology. Understanding these traits will help to
classify the people in different attributes. In this paper, we have presented a
novel method for classifying images using a pretrained transformer model. We
apply the pretrained transformer for the binary classification of facial images
in criminal and non-criminal classes. The pretrained transformer of GPT-2 is
trained to generate text and then fine-tuned to classify facial images. During
the finetuning process with images, most of the layers of GT-2 are frozen
during backpropagation and the model is frozen pretrained transformer (FPT).
The FPT acts as a universal image classifier, and this paper shows the
application of FPT on facial images. We also use our FPT on encrypted images
for classification. Our FPT shows high accuracy on both raw facial images and
encrypted images. We hypothesize the meta-learning capacity FPT gained because
of its large size and trained on a large size with theory and experiments. The
GPT-2 trained to generate a single word token at a time, through the
autoregressive process, forced to heavy-tail distribution. Then the FPT uses
the heavy-tail property as its meta-learning capacity for classifying images.
Our work shows one way to avoid bias during the machine classification of
images.The FPT encodes worldly knowledge because of the pretraining of one
text, which it uses during the classification. The statistical error of
classification is reduced because of the added context gained from the text.Our
paper shows the ethical dimension of using encrypted data for
classification.Criminal images are sensitive to share across the boundary but
encrypted largely evades ethical concern.FPT showing good classification
accuracy on encrypted images shows promise for further research on
privacy-preserving machine learning.",2022-01-25
An Integrated Approach for Video Captioning and Applications,2022-01-23 01:06:00+00:00,http://arxiv.org/abs/2201.09153v1,"Soheyla Amirian, Thiab R. Taha, Khaled Rasheed, Hamid R. Arabnia","cs.CV, cs.AI",image2text,"Physical computing infrastructure, data gathering, and algorithms have
recently had significant advances to extract information from images and
videos. The growth has been especially outstanding in image captioning and
video captioning. However, most of the advancements in video captioning still
take place in short videos. In this research, we caption longer videos only by
using the keyframes, which are a small subset of the total video frames.
Instead of processing thousands of frames, only a few frames are processed
depending on the number of keyframes. There is a trade-off between the
computation of many frames and the speed of the captioning process. The
approach in this research is to allow the user to specify the trade-off between
execution time and accuracy. In addition, we argue that linking images, videos,
and natural language offers many practical benefits and immediate practical
applications. From the modeling perspective, instead of designing and staging
explicit algorithms to process videos and generate captions in complex
processing pipelines, our contribution lies in designing hybrid deep learning
architectures to apply in long videos by captioning video keyframes. We
consider the technology and the methodology that we have developed as steps
toward the applications discussed in this research.",2022-01-23
"Inferring Commonsense Explanations as Prompts for Future Event
  Generation",2022-01-18 16:21:23+00:00,http://arxiv.org/abs/2201.07099v1,"Li Lin, Yixin Cao, Lifu Huang, Shuang Li, Xuming Hu, Lijie Wen, Jianmin Wang","cs.CL, cs.LG, I.2.7; I.2.4",image2text,"Future Event Generation aims to generate fluent and reasonable future event
descriptions given preceding events. It requires not only fluent text
generation but also commonsense reasoning to maintain the coherence of the
entire event story. However, existing FEG methods are easily trapped into
repeated or general events without imposing any logical constraint to the
generation process. In this paper, we propose a novel explainable FEG framework
that consists of a commonsense inference model (IM) and an event generation
model (GM). The IM, which is pre-trained on a commonsense knowledge graph
ATOMIC, learns to interpret the preceding events and conducts commonsense
reasoning to reveal the characters psychology such as intent, reaction, and
needs as latent variables. GM further takes the commonsense knowledge as
prompts to guide and enforce the generation of logistically coherent future
events. As unique merit, the commonsense prompts can be further decoded into
textual descriptions, yielding explanations for the future event. Automatic and
human evaluation demonstrate that our approach can generate more coherent,
specific, and logical future events than the strong baselines.",2022-01-18
Local Information Assisted Attention-free Decoder for Audio Captioning,2022-01-10 08:55:52+00:00,http://arxiv.org/abs/2201.03217v1,"Feiyang Xiao, Jian Guan, Qiaoxi Zhu, Haiyan Lan, Wenwu Wang","cs.SD, cs.LG, eess.AS",image2text,"Automated audio captioning (AAC) aims to describe audio data with captions
using natural language. Most existing AAC methods adopt an encoder-decoder
structure, where the attention based mechanism is a popular choice in the
decoder (e.g., Transformer decoder) for predicting captions from audio
features. Such attention based decoders can capture the global information from
the audio features, however, their ability in extracting local information can
be limited, which may lead to degraded quality in the generated captions. In
this paper, we present an AAC method with an attention-free decoder, where an
encoder based on PANNs is employed for audio feature extraction, and the
attention-free decoder is designed to introduce local information. The proposed
method enables the effective use of both global and local information from
audio signals. Experiments show that our method outperforms the
state-of-the-art methods with the standard attention based decoder in Task 6 of
the DCASE 2021 Challenge.",2022-01-10
Self-Training Vision Language BERTs with a Unified Conditional Model,2022-01-06 11:00:52+00:00,http://arxiv.org/abs/2201.02010v1,"Xiaofeng Yang, Fengmao Lv, Fayao Liu, Guosheng Lin","cs.CV, cs.CL",image2text,"Natural language BERTs are trained with language corpus in a self-supervised
manner. Unlike natural language BERTs, vision language BERTs need paired data
to train, which restricts the scale of VL-BERT pretraining. We propose a
self-training approach that allows training VL-BERTs from unlabeled image data.
The proposed method starts with our unified conditional model -- a vision
language BERT model that can perform zero-shot conditional generation. Given
different conditions, the unified conditional model can generate captions,
dense captions, and even questions. We use the labeled image data to train a
teacher model and use the trained model to generate pseudo captions on
unlabeled image data. We then combine the labeled data and pseudo labeled data
to train a student model. The process is iterated by putting the student model
as a new teacher. By using the proposed self-training approach and only 300k
unlabeled extra data, we are able to get competitive or even better
performances compared to the models of similar model size trained with 3
million extra image data.",2022-01-06
Compact Bidirectional Transformer for Image Captioning,2022-01-06 09:23:18+00:00,http://arxiv.org/abs/2201.01984v1,"Yuanen Zhou, Zhenzhen Hu, Daqing Liu, Huixia Ben, Meng Wang","cs.CV, cs.CL",image2text,"Most current image captioning models typically generate captions from left to
right. This unidirectional property makes them can only leverage past context
but not future context. Though recent refinement-based models can exploit both
past and future context by generating a new caption in the second stage based
on pre-retrieved or pre-generated captions in the first stage, the decoder of
these models generally consists of two networks~(i.e. a retriever or captioner
in the first stage and a refiner in the second stage), which can only be
executed sequentially. In this paper, we introduce a Compact Bidirectional
Transformer model for image captioning that can leverage bidirectional context
implicitly and explicitly while the decoder can be executed parallelly.
Specifically, it is implemented by tightly coupling left-to-right(L2R) and
right-to-left(R2L) flows into a single compact model~(i.e. implicitly) and
optionally allowing interaction of the two flows(i.e. explicitly), while the
final caption is chosen from either L2R or R2L flow in a sentence-level
ensemble manner. We conduct extensive ablation studies on the MSCOCO benchmark
and find that the compact architecture, which serves as a regularization for
implicitly exploiting bidirectional context, and the sentence-level ensemble
play more important roles than the explicit interaction mechanism. By combining
with word-level ensemble seamlessly, the effect of the sentence-level ensemble
is further enlarged. We further extend the conventional one-flow self-critical
training to the two-flows version under this architecture and achieve new
state-of-the-art results in comparison with non-vision-language-pretraining
models. Source code is available at
{\color{magenta}\url{https://github.com/YuanEZhou/CBTrans}}.",2022-01-06
"StyleM: Stylized Metrics for Image Captioning Built with Contrastive
  N-grams",2022-01-04 04:44:05+00:00,http://arxiv.org/abs/2201.00975v1,"Chengxi Li, Brent Harrison","cs.CV, cs.AI, cs.CL",image2text,"In this paper, we build two automatic evaluation metrics for evaluating the
association between a machine-generated caption and a ground truth stylized
caption: OnlyStyle and StyleCIDEr.",2022-01-04
"ERNIE-ViLG: Unified Generative Pre-training for Bidirectional
  Vision-Language Generation",2021-12-31 03:53:33+00:00,http://arxiv.org/abs/2112.15283v1,"Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang","cs.CV, cs.CL",image2text,"Conventional methods for the image-text generation tasks mainly tackle the
naturally bidirectional generation tasks separately, focusing on designing
task-specific frameworks to improve the quality and fidelity of the generated
samples. Recently, Vision-Language Pre-training models have greatly improved
the performance of the image-to-text generation tasks, but large-scale
pre-training models for text-to-image synthesis task are still under-developed.
In this paper, we propose ERNIE-ViLG, a unified generative pre-training
framework for bidirectional image-text generation with transformer model. Based
on the image quantization models, we formulate both image generation and text
generation as autoregressive generative tasks conditioned on the text/image
input. The bidirectional image-text generative modeling eases the semantic
alignments across vision and language. For the text-to-image generation
process, we further propose an end-to-end training method to jointly learn the
visual sequence generator and the image reconstructor. To explore the landscape
of large-scale pre-training for bidirectional text-image generation, we train a
10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million
(Chinese) image-text pairs which achieves state-of-the-art performance for both
text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for
text-to-image synthesis and best results on COCO-CN and AIC-ICC for image
captioning.",2021-12-31
"Radiology Report Generation with a Learned Knowledge Base and
  Multi-modal Alignment",2021-12-30 10:43:56+00:00,http://arxiv.org/abs/2112.15011v1,"Shuxin Yang, Xian Wu, Shen Ge, Xingwang Wu, S. Kevin Zhou, Li Xiao","eess.IV, cs.CL, cs.CV",image2text,"In clinics, a radiology report is crucial for guiding a patient's treatment.
Unfortunately, report writing imposes a heavy burden on radiologists. To
effectively reduce such a burden, we hereby present an automatic, multi-modal
approach for report generation from chest x-ray. Our approach, motivated by the
observation that the descriptions in radiology reports are highly correlated
with the x-ray images, features two distinct modules: (i) Learned knowledge
base. To absorb the knowledge embedded in the above-mentioned correlation, we
automatically build a knowledge base based on textual embedding. (ii)
Multi-modal alignment. To promote the semantic alignment among reports, disease
labels and images, we explicitly utilize textual embedding to guide the
learning of the visual feature space. We evaluate the performance of the
proposed model using metrics from both natural language generation and clinic
efficacy on the public IU and MIMIC-CXR datasets. Our ablation study shows that
each module contributes to improving the quality of generated reports.
Furthermore, with the aid of both modules, our approach clearly outperforms
state-of-the-art methods.",2021-12-30
Automatic Product Copywriting for E-Commerce,2021-12-15 19:06:31+00:00,http://arxiv.org/abs/2112.11915v1,"Xueying Zhang, Yanyan Zou, Hainan Zhang, Jing Zhou, Shiliang Diao, Jiajia Chen, Zhuoye Ding, Zhen He, Xueqi He, Yun Xiao, Bo Long, Han Yu, Lingfei Wu","cs.CL, cs.AI",image2text,"Product copywriting is a critical component of e-commerce recommendation
platforms. It aims to attract users' interest and improve user experience by
highlighting product characteristics with textual descriptions. In this paper,
we report our experience deploying the proposed Automatic Product Copywriting
Generation (APCG) system into the JD.com e-commerce product recommendation
platform. It consists of two main components: 1) natural language generation,
which is built from a transformer-pointer network and a pre-trained
sequence-to-sequence model based on millions of training data from our in-house
platform; and 2) copywriting quality control, which is based on both automatic
evaluation and human screening. For selected domains, the models are trained
and updated daily with the updated training data. In addition, the model is
also used as a real-time writing assistant tool on our live broadcast platform.
The APCG system has been deployed in JD.com since Feb 2021. By Sep 2021, it has
generated 2.53 million product descriptions, and improved the overall averaged
click-through rate (CTR) and the Conversion Rate (CVR) by 4.22% and 3.61%,
compared to baselines, respectively on a year-on-year basis. The accumulated
Gross Merchandise Volume (GMV) made by our system is improved by 213.42%,
compared to the number in Feb 2021.",2021-12-15
Contextualized Scene Imagination for Generative Commonsense Reasoning,2021-12-12 20:38:08+00:00,http://arxiv.org/abs/2112.06318v1,"PeiFeng Wang, Jonathan Zamora, Junfeng Liu, Filip Ilievski, Muhao Chen, Xiang Ren",cs.CL,image2text,"Humans use natural language to compose common concepts from their environment
into plausible, day-to-day scene descriptions. However, such generative
commonsense reasoning (GCSR) skills are lacking in state-of-the-art text
generation methods. Descriptive sentences about arbitrary concepts generated by
neural text generation models (e.g., pre-trained text-to-text Transformers) are
often grammatically fluent but may not correspond to human common sense,
largely due to their lack of mechanisms to capture concept relations, to
identify implicit concepts, and to perform generalizable reasoning about unseen
concept compositions. In this paper, we propose an Imagine-and-Verbalize (I&V)
method, which learns to imagine a relational scene knowledge graph (SKG) with
relations between the input concepts, and leverage the SKG as a constraint when
generating a plausible scene description. We collect and harmonize a set of
knowledge resources from different domains and modalities, providing a rich
auxiliary supervision signal for I&V. The experiments demonstrate the
effectiveness of I&V in improving language models on both concept-to-sentence
and concept-to-story generation tasks, while enabling the model to learn well
from fewer task examples and generate SKGs that make common sense to human
annotators.",2021-12-12
"Improving Logical-Level Natural Language Generation with
  Topic-Conditioned Data Augmentation and Logical Form Generation",2021-12-12 13:50:18+00:00,http://arxiv.org/abs/2112.06240v1,"Ao Liu, Congjian Luo, Naoaki Okazaki",cs.CL,image2text,"Logical Natural Language Generation, i.e., generating textual descriptions
that can be logically entailed by a structured table, has been a challenge due
to the low fidelity of the generation. \citet{chen2020logic2text} have
addressed this problem by annotating interim logical programs to control the
generation contents and semantics, and presented the task of table-aware
logical form to text (Logic2text) generation. However, although table instances
are abundant in the real world, logical forms paired with textual descriptions
require costly human annotation work, which limits the performance of neural
models. To mitigate this, we propose topic-conditioned data augmentation
(TopicDA), which utilizes GPT-2 to generate unpaired logical forms and textual
descriptions directly from tables. We further introduce logical form generation
(LG), a dual task of Logic2text that requires generating a valid logical form
based on a text description of a table. We also propose a semi-supervised
learning approach to jointly train a Logic2text and an LG model with both
labeled and augmented data. The two models benefit from each other by providing
extra supervision signals through back-translation. Experimental results on the
Logic2text dataset and the LG task demonstrate that our approach can
effectively utilize the augmented data and outperform supervised baselines by a
substantial margin.",2021-12-12
Show and Write: Entity-aware News Generation with Image Information,2021-12-11 05:32:09+00:00,http://arxiv.org/abs/2112.05917v1,"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer",cs.CL,image2text,"Automatically writing long articles is a complex and challenging language
generation task. Prior work has primarily focused on generating these articles
using human-written prompt to provide some topical context and some metadata
about the article. That said, for many applications, such as generating news
stories, these articles are often paired with images and their captions or
alt-text, which in turn are based on real-world events and may reference many
different named entities that are difficult to be correctly recognized and
predicted by language models. To address these two problems, this paper
introduces an Entity-aware News Generation method with Image iNformation,
Engin, to incorporate news image information into language models. Engin
produces news articles conditioned on both metadata and information such as
captions and named entities extracted from images. We also propose an
Entity-aware mechanism to help our model better recognize and predict the
entity names in news. We perform experiments on two public large-scale news
datasets, GoodNews and VisualNews. Quantitative results show that our approach
improves article perplexity by 4-5 points over the base models. Qualitative
results demonstrate the text generated by Engin is more consistent with news
images. We also perform article quality annotation experiment on the generated
articles to validate that our model produces higher-quality articles. Finally,
we investigate the effect Engin has on methods that automatically detect
machine-generated articles.",2021-12-11
"Unified Multimodal Pre-training and Prompt-based Tuning for
  Vision-Language Understanding and Generation",2021-12-10 14:59:06+00:00,http://arxiv.org/abs/2112.05587v2,"Tianyi Liu, Zuxuan Wu, Wenhan Xiong, Jingjing Chen, Yu-Gang Jiang","cs.CV, cs.CL, cs.LG",image2text,"Most existing vision-language pre-training methods focus on understanding
tasks and use BERT-like objectives (masked language modeling and image-text
matching) during pretraining. Although they perform well in many understanding
downstream tasks, e.g., visual question answering, image-text retrieval and
visual entailment, they do not possess the ability to generate. To tackle this
problem, we propose Unified multimodal pre-training for both Vision-Language
understanding and generation (UniVL). The proposed UniVL is capable of handling
both understanding tasks and generative tasks. We augment existing pretraining
paradigms that only use random masks with causal masks, i.e., triangular masks
that mask out future tokens, such that the pre-trained models can have
autoregressive generation abilities by design. We formulate several previous
understanding tasks as a text generation task and propose to use prompt-based
method for fine-tuning on different downstream tasks. Our experiments show that
there is a trade-off between understanding tasks and generation tasks while
using the same model, and a feasible way to improve both tasks is to use more
data. Our UniVL framework attains comparable performance to recent
vision-language pre-training methods on both understanding tasks and generation
tasks. Moreover, we demostrate that prompt-based finetuning is more
data-efficient - it outperforms discriminative methods in few-shot scenarios.",2021-12-10
Self-Supervised Image-to-Text and Text-to-Image Synthesis,2021-12-09 13:54:56+00:00,http://arxiv.org/abs/2112.04928v1,"Anindya Sundar Das, Sriparna Saha","cs.CV, cs.CL, cs.LG",image2text,"A comprehensive understanding of vision and language and their interrelation
are crucial to realize the underlying similarities and differences between
these modalities and to learn more generalized, meaningful representations. In
recent years, most of the works related to Text-to-Image synthesis and
Image-to-Text generation, focused on supervised generative deep architectures
to solve the problems, where very little interest was placed on learning the
similarities between the embedding spaces across modalities. In this paper, we
propose a novel self-supervised deep learning based approach towards learning
the cross-modal embedding spaces; for both image to text and text to image
generations. In our approach, we first obtain dense vector representations of
images using StackGAN-based autoencoder model and also dense vector
representations on sentence-level utilizing LSTM based text-autoencoder; then
we study the mapping from embedding space of one modality to embedding space of
the other modality utilizing GAN and maximum mean discrepancy based generative
networks. We, also demonstrate that our model learns to generate textual
description from image data as well as images from textual data both
qualitatively and quantitatively.",2021-12-09
"Search and Learn: Improving Semantic Coverage for Data-to-Text
  Generation",2021-12-06 03:51:56+00:00,http://arxiv.org/abs/2112.02770v1,"Shailza Jolly, Zi Xuan Zhang, Andreas Dengel, Lili Mou",cs.CL,image2text,"Data-to-text generation systems aim to generate text descriptions based on
input data (often represented in the tabular form). A typical system uses huge
training samples for learning the correspondence between tables and texts.
However, large training sets are expensive to obtain, limiting the
applicability of these approaches in real-world scenarios. In this work, we
focus on few-shot data-to-text generation. We observe that, while fine-tuned
pretrained language models may generate plausible sentences, they suffer from
the low semantic coverage problem in the few-shot setting. In other words,
important input slots tend to be missing in the generated text. To this end, we
propose a search-and-learning approach that leverages pretrained language
models but inserts the missing slots to improve the semantic coverage. We
further fine-tune our system based on the search results to smooth out the
search noise, yielding better-quality text and improving inference efficiency
to a large extent. Experiments show that our model achieves high performance on
E2E and WikiBio datasets. Especially, we cover 98.35% of input slots on E2E,
largely alleviating the low coverage problem.",2021-12-06
"Protecting Intellectual Property of Language Generation APIs with
  Lexical Watermark",2021-12-05 22:54:54+00:00,http://arxiv.org/abs/2112.02701v1,"Xuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu, Chenguang Wang","cs.CR, cs.CL",image2text,"Nowadays, due to the breakthrough in natural language generation (NLG),
including machine translation, document summarization, image captioning, etc
NLG models have been encapsulated in cloud APIs to serve over half a billion
people worldwide and process over one hundred billion word generations per day.
Thus, NLG APIs have already become essential profitable services in many
commercial companies. Due to the substantial financial and intellectual
investments, service providers adopt a pay-as-you-use policy to promote
sustainable market growth. However, recent works have shown that cloud
platforms suffer from financial losses imposed by model extraction attacks,
which aim to imitate the functionality and utility of the victim services, thus
violating the intellectual property (IP) of cloud APIs. This work targets at
protecting IP of NLG APIs by identifying the attackers who have utilized
watermarked responses from the victim NLG APIs. However, most existing
watermarking techniques are not directly amenable for IP protection of NLG
APIs. To bridge this gap, we first present a novel watermarking method for text
generation APIs by conducting lexical modification to the original outputs.
Compared with the competitive baselines, our watermark approach achieves better
identifiable performance in terms of p-value, with fewer semantic losses. In
addition, our watermarks are more understandable and intuitive to humans than
the baselines. Finally, the empirical studies show our approach is also
applicable to queries from different domains, and is effective on the attacker
trained on a mixture of the corpus which includes less than 10\% watermarked
samples.",2021-12-05
"Representation Learning for Conversational Data using Discourse Mutual
  Information Maximization",2021-12-04 13:17:07+00:00,http://arxiv.org/abs/2112.05787v1,"Bishal Santra, Sumegh Roychowdhury, Aishik Mandal, Vasu Gurram, Atharva Naik, Manish Gupta, Pawan Goyal",cs.CL,image2text,"Although many pretrained models exist for text or images, there have been
relatively fewer attempts to train representations specifically for dialog
understanding. Prior works usually relied on finetuned representations based on
generic text representation models like BERT or GPT-2. But, existing
pretraining objectives do not take the structural information of text into
consideration. Although generative dialog models can learn structural features
too, we argue that the structure-unaware word-by-word generation is not
suitable for effective conversation modeling. We empirically demonstrate that
such representations do not perform consistently across various dialog
understanding tasks. Hence, we propose a structure-aware Mutual Information
based loss-function DMI (Discourse Mutual Information) for training
dialog-representation models, that additionally captures the inherent
uncertainty in response prediction. Extensive evaluation on nine diverse dialog
modeling tasks shows that our proposed DMI-based models outperform strong
baselines by significant margins, even with small-scale pretraining. Our models
show the most promising performance on the dialog evaluation task
DailyDialog++, in both random and adversarial negative scenarios.",2021-12-04
"LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with
  Self-training",2021-12-02 16:49:41+00:00,http://arxiv.org/abs/2112.01404v1,"Ningyu Zhang, Hongbin Ye, Jiacheng Yang, Shumin Deng, Chuanqi Tan, Mosha Chen, Songfang Huang, Fei Huang, Huajun Chen","cs.CL, cs.AI",image2text,"Natural language generation from structured data mainly focuses on
surface-level descriptions, suffering from uncontrollable content selection and
low fidelity. Previous works leverage logical forms to facilitate logical
knowledge-conditioned text generation. Though achieving remarkable progress,
they are data-hungry, which makes the adoption for real-world applications
challenging with limited data. To this end, this paper proposes a unified
framework for logical knowledge-conditioned text generation in the few-shot
setting. With only a few seeds logical forms (e.g., 20/100 shot), our approach
leverages self-training and samples pseudo logical forms based on content and
structure consistency. Experimental results demonstrate that our approach can
obtain better few-shot performance than baselines.",2021-12-02
"Translation-equivariant Image Quantizer for Bi-directional Image-Text
  Generation",2021-12-01 10:08:24+00:00,http://arxiv.org/abs/2112.00384v1,"Woncheol Shin, Gyubok Lee, Jiyoung Lee, Joonseok Lee, Edward Choi","cs.CV, cs.CL, cs.LG",image2text,"Recently, vector-quantized image modeling has demonstrated impressive
performance on generation tasks such as text-to-image generation. However, we
discover that the current image quantizers do not satisfy translation
equivariance in the quantized space due to aliasing, degrading performance in
the downstream text-to-image generation and image-to-text generation, even in
simple experimental setups. Instead of focusing on anti-aliasing, we take a
direct approach to encourage translation equivariance in the quantized space.
In particular, we explore a desirable property of image quantizers, called
'Translation Equivariance in the Quantized Space' and propose a simple but
effective way to achieve translation equivariance by regularizing orthogonality
in the codebook embedding vectors. Using this method, we improve accuracy by
+22% in text-to-image generation and +26% in image-to-text generation,
outperforming the VQGAN.",2021-12-01
Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic,2021-11-29 11:01:49+00:00,http://arxiv.org/abs/2111.14447v1,"Yoad Tewel, Yoav Shalev, Idan Schwartz, Lior Wolf","cs.CV, cs.AI, cs.CL",image2text,"Recent text-to-image matching models apply contrastive learning to large
corpora of uncurated pairs of images and sentences. While such models can
provide a powerful score for matching and subsequent zero-shot tasks, they are
not capable of generating caption given an image. In this work, we repurpose
such models to generate a descriptive text given an image at inference time,
without any further training or tuning step. This is done by combining the
visual-semantic model with a large language model, benefiting from the
knowledge in both web-scale models. The resulting captions are much less
restrictive than those obtained by supervised captioning methods. Moreover, as
a zero-shot learning method, it is extremely flexible and we demonstrate its
ability to perform image arithmetic in which the inputs can be either images or
text and the output is a sentence. This enables novel high-level vision
capabilities such as comparing two images or solving visual analogy tests.",2021-11-29
LAFITE: Towards Language-Free Training for Text-to-Image Generation,2021-11-27 01:54:45+00:00,http://arxiv.org/abs/2111.13792v1,"Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, Tong Sun","cs.CV, cs.LG",image2text,"One of the major challenges in training text-to-image generation models is
the need of a large number of high-quality image-text pairs. While image
samples are often easily accessible, the associated text descriptions typically
require careful human captioning, which is particularly time- and
cost-consuming. In this paper, we propose the first work to train text-to-image
generation models without any text data. Our method leverages the well-aligned
multi-modal semantic space of the powerful pre-trained CLIP model: the
requirement of text-conditioning is seamlessly alleviated via generating text
features from image features. Extensive experiments are conducted to illustrate
the effectiveness of the proposed method. We obtain state-of-the-art results in
the standard text-to-image generation tasks. Importantly, the proposed
language-free model outperforms most existing models trained with full
image-text pairs. Furthermore, our method can be applied in fine-tuning
pre-trained models, which saves both training time and cost in training
text-to-image generation models. Our pre-trained model obtains competitive
results in zero-shot text-to-image generation on the MS-COCO dataset, yet with
around only 1% of the model size and training data size relative to the
recently proposed large DALL-E model.",2021-11-27
"Octree Transformer: Autoregressive 3D Shape Generation on Hierarchically
  Structured Sequences",2021-11-24 13:17:16+00:00,http://arxiv.org/abs/2111.12480v1,"Moritz Ibing, Gregor Kobsik, Leif Kobbelt","cs.CV, cs.GR, cs.LG",image2text,"Autoregressive models have proven to be very powerful in NLP text generation
tasks and lately have gained popularity for image generation as well. However,
they have seen limited use for the synthesis of 3D shapes so far. This is
mainly due to the lack of a straightforward way to linearize 3D data as well as
to scaling problems with the length of the resulting sequences when describing
complex shapes. In this work we address both of these problems. We use octrees
as a compact hierarchical shape representation that can be sequentialized by
traversal ordering. Moreover, we introduce an adaptive compression scheme, that
significantly reduces sequence lengths and thus enables their effective
generation with a transformer, while still allowing fully autoregressive
sampling and parallel training. We demonstrate the performance of our model by
comparing against the state-of-the-art in shape generation.",2021-11-24
NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,2021-11-24 11:02:12+00:00,http://arxiv.org/abs/2111.12417v1,"Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan","cs.CV, cs.AI",image2text,"This paper presents a unified multimodal pre-trained model called N\""UWA that
can generate new or manipulate existing visual data (i.e., images and videos)
for various visual synthesis tasks. To cover language, image, and video at the
same time for different scenarios, a 3D transformer encoder-decoder framework
is designed, which can not only deal with videos as 3D data but also adapt to
texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA)
mechanism is also proposed to consider the nature of the visual data and reduce
the computational complexity. We evaluate N\""UWA on 8 downstream tasks.
Compared to several strong baselines, N\""UWA achieves state-of-the-art results
on text-to-image generation, text-to-video generation, video prediction, etc.
Furthermore, it also shows surprisingly good zero-shot capabilities on
text-guided image and video manipulation tasks. Project repo is
https://github.com/microsoft/NUWA.",2021-11-24
Scaling Up Vision-Language Pre-training for Image Captioning,2021-11-24 02:30:22+00:00,http://arxiv.org/abs/2111.12233v1,"Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, Lijuan Wang","cs.CV, cs.CL",image2text,"In recent years, we have witnessed significant performance boost in the image
captioning task based on vision-language pre-training (VLP). Scale is believed
to be an important factor for this advance. However, most existing work only
focuses on pre-training transformers with moderate sizes (e.g., 12 or 24
layers) on roughly 4 million images. In this paper, we present LEMON, a
LargE-scale iMage captiONer, and provide the first empirical study on the
scaling behavior of VLP for image captioning. We use the state-of-the-art VinVL
model as our reference model, which consists of an image feature extractor and
a transformer model, and scale the transformer both up and down, with model
sizes ranging from 13 to 675 million parameters. In terms of data, we conduct
experiments with up to 200 million image-text pairs which are automatically
collected from web based on the alt attribute of the image (dubbed as ALT200M).
Extensive analysis helps to characterize the performance trend as the model
size and the pre-training data size increase. We also compare different
training recipes, especially for training on large-scale noisy data. As a
result, LEMON achieves new state of the arts on several major image captioning
benchmarks, including COCO Caption, nocaps, and Conceptual Captions. We also
show LEMON can generate captions with long-tail visual concepts when used in a
zero-shot manner.",2021-11-24
L-Verse: Bidirectional Generation Between Image and Text,2021-11-22 11:48:26+00:00,http://arxiv.org/abs/2111.11133v3,"Taehoon Kim, Gwangmo Song, Sihaeng Lee, Sangyun Kim, Yewon Seo, Soonyoung Lee, Seung Hwan Kim, Honglak Lee, Kyunghoon Bae","cs.CV, cs.CL, cs.LG",image2text,"Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalabilty. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for text-to-image and image-to-text
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation tasks without any finetuning or extra
object detection frameworks. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial results of bidirectional vision-language representation learning on
general domain. Codes available at: https://github.com/tgisaturday/L-Verse",2021-11-22
