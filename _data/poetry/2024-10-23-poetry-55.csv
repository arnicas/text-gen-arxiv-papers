title,pubdate,id,authors,categories,search,abstract,displaydate
"Benchmarking Foundation Models on Exceptional Cases: Dataset Creation
  and Validation",2024-10-23 16:24:23+00:00,http://arxiv.org/abs/2410.18001v1,"Suho Kang, Jungyang Park, Joonseo Ha, SoMin Kim, JinHyeong Kim, Subeen Park, Kyungwoo Song",cs.AI,poetry,"Foundation models (FMs) have achieved significant success across various
tasks, leading to research on benchmarks for reasoning abilities. However,
there is a lack of studies on FMs performance in exceptional scenarios, which
we define as out-of-distribution (OOD) reasoning tasks. This paper is the first
to address these cases, developing a novel dataset for evaluation of FMs across
multiple modalities, including graphic novels, calligraphy, news articles, and
lyrics. It includes tasks for instance classification, character recognition,
token prediction, and text generation. The paper also proposes prompt
engineering techniques like Chain-of-Thought (CoT) and CoT+Few-Shot to enhance
performance. Validation of FMs using various methods revealed improvements. The
code repository is accessible at:
https://github.com/MLAI-Yonsei/ExceptionalBenchmark",2024-10-23
Does ChatGPT Have a Poetic Style?,2024-10-20 06:01:34+00:00,http://arxiv.org/abs/2410.15299v2,"Melanie Walsh, Anna Preus, Elizabeth Gronski",cs.CL,poetry,"Generating poetry has become a popular application of LLMs, perhaps
especially of OpenAI's widely-used chatbot ChatGPT. What kind of poet is
ChatGPT? Does ChatGPT have its own poetic style? Can it successfully produce
poems in different styles? To answer these questions, we prompt the GPT-3.5 and
GPT-4 models to generate English-language poems in 24 different poetic forms
and styles, about 40 different subjects, and in response to 3 different writing
prompt templates. We then analyze the resulting 5.7k poems, comparing them to a
sample of 3.7k poems from the Poetry Foundation and the Academy of American
Poets. We find that the GPT models, especially GPT-4, can successfully produce
poems in a range of both common and uncommon English-language forms in
superficial yet noteworthy ways, such as by producing poems of appropriate
lengths for sonnets (14 lines), villanelles (19 lines), and sestinas (39
lines). But the GPT models also exhibit their own distinct stylistic
tendencies, both within and outside of these specific forms. Our results show
that GPT poetry is much more constrained and uniform than human poetry, showing
a strong penchant for rhyme, quatrains (4-line stanzas), iambic meter,
first-person plural perspectives (we, us, our), and specific vocabulary like
""heart,"" ""embrace,"" ""echo,"" and ""whisper.""",2024-10-20
LLM-based multi-agent poetry generation in non-cooperative environments,2024-09-05 16:12:29+00:00,http://arxiv.org/abs/2409.03659v2,"Ran Zhang, Steffen Eger",cs.CL,poetry,"Despite substantial progress of large language models (LLMs) for automatic
poetry generation, the generated poetry lacks diversity while the training
process differs greatly from human learning. Under the rationale that the
learning process of the poetry generation systems should be more human-like and
their output more diverse and novel, we introduce a framework based on social
learning where we emphasize non-cooperative interactions besides cooperative
interactions to encourage diversity. Our experiments are the first attempt at
LLM-based multi-agent systems in non-cooperative environments for poetry
generation employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED
agents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems shows
that our framework benefits the poetry generation process for TRAINING-BASED
agents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity
and a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams.
The generated poetry from TRAINING-BASED agents also exhibits group divergence
in terms of lexicons, styles and semantics. PROMPTING-BASED agents in our
framework also benefit from non-cooperative environments and a more diverse
ensemble of models with non-homogeneous agents has the potential to further
enhance diversity, with an increase of 7.0-17.5 pp according to our
experiments. However, PROMPTING-BASED agents show a decrease in lexical
diversity over time and do not exhibit the group-based divergence intended in
the social network. Our paper argues for a paradigm shift in creative tasks
such as automatic poetry generation to include social learning processes (via
LLM-based agent modeling) similar to human interaction.",2024-09-05
Rhyme-aware Chinese lyric generator based on GPT,2024-08-19 16:17:20+00:00,http://arxiv.org/abs/2408.10130v1,"Yixiao Yuan, Yangchen Huang, Yu Ma, Xinjin Li, Zhenglin Li, Yiming Shi, Huapeng Zhou","cs.CL, cs.AI",poetry,"Neural language representation models such as GPT, pre-trained on large-scale
corpora, can effectively capture rich semantic patterns from plain text and be
fine-tuned to consistently improve natural language generation performance.
However, existing pre-trained language models used to generate lyrics rarely
consider rhyme information, which is crucial in lyrics. Using a pre-trained
model directly results in poor performance. To enhance the rhyming quality of
generated lyrics, we incorporate integrated rhyme information into our model,
thereby improving lyric generation performance.",2024-08-19
"Improving Structural Diversity of Blackbox LLMs via
  Chain-of-Specification Prompting",2024-08-12 14:34:06+00:00,http://arxiv.org/abs/2408.06186v1,"Halley Young, Yimeng Zeng, Jacob Gardner, Osbert Bastani","cs.CL, cs.LG",poetry,"The capability to generate diverse text is a key challenge facing large
language models (LLMs). Thus far, diversity has been studied via metrics such
as $n$-gram diversity or diversity of BERT embeddings. However, for these kinds
of diversity, the user has little control over the dimensions along which
diversity is considered. For example, in the poetry domain, one might desire
diversity in terms of rhyme and meter, whereas in the code domain, one might
desire diversity in terms of the kinds of expressions used to solve a problem.
We propose a diversity metric called structural diversity, where the user
provides a mapping from generated text to features capturing the kinds of
diversity that they care about. In addition, we propose a novel strategy called
chain-of-specification (CoS) prompting for improving diversity by first having
the LLM generate a specification encoding one instance of structural features,
and then prompting the LLM to generate text that satisfies these features;
notably, our strategy works with blackbox LLMs. In our experiments, we show
that for structural diversity in the poetry and code domains, CoS significantly
improves diversity compared to several baselines.",2024-08-12
Evaluating Diversity in Automatic Poetry Generation,2024-06-21 16:03:21+00:00,http://arxiv.org/abs/2406.15267v1,"Yanran Chen, Hannes Gröner, Sina Zarrieß, Steffen Eger",cs.CL,poetry,"Natural Language Generation (NLG), and more generally generative AI, are
among the currently most impactful research fields. Creative NLG, such as
automatic poetry generation, is a fascinating niche in this area. While most
previous research has focused on forms of the Turing test when evaluating
automatic poetry generation - can humans distinguish between automatic and
human generated poetry - we evaluate the diversity of automatically generated
poetry, by comparing distributions of generated poetry to distributions of
human poetry along structural, lexical, semantic and stylistic dimensions,
assessing different model types (word vs. character-level, general purpose LLMs
vs. poetry-specific models), including the very recent LLaMA3, and types of
fine-tuning (conditioned vs. unconditioned). We find that current automatic
poetry systems are considerably underdiverse along multiple dimensions - they
often do not rhyme sufficiently, are semantically too uniform and even do not
match the length distribution of human poetry. Our experiments reveal, however,
that style-conditioning and character-level modeling clearly increases
diversity across virtually all dimensions we explore. Our identified
limitations may serve as the basis for more genuinely diverse future poetry
generation models.",2024-06-21
GPT Czech Poet: Generation of Czech Poetic Strophes with Language Models,2024-06-18 06:19:45+00:00,http://arxiv.org/abs/2407.12790v1,"Michal Chudoba, Rudolf Rosa",cs.CL,poetry,"High-quality automated poetry generation systems are currently only available
for a small subset of languages. We introduce a new model for generating poetry
in Czech language, based on fine-tuning a pre-trained Large Language Model. We
demonstrate that guiding the generation process by explicitly specifying
strophe parameters within the poem text strongly improves the effectiveness of
the model. We also find that appropriate tokenization is crucial, showing that
tokenization methods based on syllables or individual characters instead of
subwords prove superior in generating poetic strophes. We further enhance the
results by introducing \textit{Forced~generation}, adding explicit
specifications of meter and verse parameters at inference time based on the
already generated text. We evaluate a range of setups, showing that our
proposed approach achieves high accuracies in rhyming and metric aspects of
formal quality of the generated poems.",2024-06-18
MUGC: Machine Generated versus User Generated Content Detection,2024-03-28 07:33:53+00:00,http://arxiv.org/abs/2403.19725v1,"Yaqi Xie, Anjali Rawal, Yujing Cen, Dixuan Zhao, Sunil K Narang, Shanu Sushmita","cs.CL, cs.AI, cs.LG",poetry,"As advanced modern systems like deep neural networks (DNNs) and generative AI
continue to enhance their capabilities in producing convincing and realistic
content, the need to distinguish between user-generated and machine generated
content is becoming increasingly evident. In this research, we undertake a
comparative evaluation of eight traditional machine-learning algorithms to
distinguish between machine-generated and human-generated data across three
diverse datasets: Poems, Abstracts, and Essays. Our results indicate that
traditional methods demonstrate a high level of accuracy in identifying
machine-generated data, reflecting the documented effectiveness of popular
pre-trained models like RoBERT. We note that machine-generated texts tend to be
shorter and exhibit less word variety compared to human-generated content.
While specific domain-related keywords commonly utilized by humans, albeit
disregarded by current LLMs (Large Language Models), may contribute to this
high detection accuracy, we show that deeper word representations like word2vec
can capture subtle semantic variances. Furthermore, readability, bias, moral,
and affect comparisons reveal a discernible contrast between machine-generated
and human generated content. There are variations in expression styles and
potentially underlying biases in the data sources (human and
machine-generated). This study provides valuable insights into the advancing
capacities and challenges associated with machine-generated content across
various domains.",2024-03-28
"CharPoet: A Chinese Classical Poetry Generation System Based on
  Token-free LLM",2024-01-07 15:00:36+00:00,http://arxiv.org/abs/2401.03512v3,"Chengyue Yu, Lei Zang, Jiaotuan Wang, Chenyi Zhuang, Jinjie Gu","cs.CL, cs.AI, cs.LG",poetry,"Automatic Chinese classical poetry generation has attracted much research
interest, but achieving effective control over format and content
simultaneously remains challenging. Traditional systems usually accept keywords
as user inputs, resulting in limited control over content. Large language
models (LLMs) improve content control by allowing unrestricted user
instructions, but the token-by-token generation process frequently makes format
errors. Motivated by this, we propose CharPoet, a Chinese classical poetry
generation system based on token-free LLM, which provides effective control
over both format and content. Our token-free architecture generates in a
character-by-character manner, enabling precise control over the number of
characters. Pruned from existing token-based LLMs, CharPoet inherits their
pretrained capabilities and can generate poetry following instructions like
""Write me a poem for my mother's birthday."" CharPoet achieves format accuracy
above 0.96, outperforming Jiuge-GPT-2 (0.91) and GPT-4 (0.38). In terms of
content quality, CharPoet surpasses traditional systems including Jiuge, and is
comparable to other LLMs. Our system is open source and available at
https://modelscope.cn/models/CharPoet/CharPoet. A video demonstration of
CharPoet is available at https://youtu.be/voZ25qEp3Dc.",2024-01-07
"TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and
  Advanced Decoding Techniques",2023-12-04 18:52:26+00:00,http://arxiv.org/abs/2312.02125v2,"Amir Panahandeh, Hanie Asemi, Esmaeil Nourani","cs.CL, cs.AI, cs.LG",poetry,"Recent advances in language models (LMs), have demonstrated significant
efficacy in tasks related to the arts and humanities. While LMs have exhibited
exceptional performance across a wide range of natural language processing
tasks, there are notable challenges associated with their utilization on small
datasets and their ability to replicate more creative human capacities. In this
study, we aim to address these challenges by training a Persian classical
poetry generation model using a transformer architecture on a specialized
dataset with no pretraining. Additionally, we propose a novel decoding method
to enhance coherence and meaningfulness in the generated poetry, effectively
managing the tradeoff between diversity and quality. Furthermore, the results
of our training approach and the proposed decoding method are evaluated through
comprehensive set of automatic and human evaluations and showed its superior
capability to generate coherent and meaningful poetry in compare to other
decoding methods and an existing Persian large language model (LLM).",2023-12-04
Inspo: Writing Stories with a Flock of AIs and Humans,2023-11-28 05:00:18+00:00,http://arxiv.org/abs/2311.16521v1,"Chieh-Yang Huang, Sanjana Gautam, Shannon McClellan Brooks, Ya-Fang Lin, Ting-Hao 'Kenneth' Huang",cs.HC,poetry,"Large Language Models (LLMs) have advanced automated writing assistance,
enabling complex tasks like co-writing novels and poems. However, real-world
writing typically requires various support and collaboration across stages and
scenarios. Existing research mainly examines how writers utilize single text
generators, neglecting this broader context. This paper introduces Inspo, a
web-based editor that incorporates various text generators and online crowd
workers. Through a three-phase user study, we examine writers' interactions
with Inspo for novel writing. Quantitative analyses of writing logs highlight
changes in participants' writing progress and the influence of various
text-generation models. Complementing this with qualitative insights from
semi-structured interviews, we illustrate participants' perceptions of these
models and the crowd. Based on the findings, we provide design recommendations
for the next generation of intelligent writing tools and discuss the potential
sociocultural implications of integrating AI and human input in the writing
process.",2023-11-28
Erato: Automatizing Poetry Evaluation,2023-10-31 10:06:37+00:00,http://arxiv.org/abs/2310.20326v1,"Manex Agirrezabal, Hugo Gonçalo Oliveira, Aitor Ormazabal",cs.CL,poetry,"We present Erato, a framework designed to facilitate the automated evaluation
of poetry, including that generated by poetry generation systems. Our framework
employs a diverse set of features, and we offer a brief overview of Erato's
capabilities and its potential for expansion. Using Erato, we compare and
contrast human-authored poetry with automatically-generated poetry,
demonstrating its effectiveness in identifying key differences. Our
implementation code and software are freely available under the GNU GPLv3
license.",2023-10-31
Urdu Poetry Generated by Using Deep Learning Techniques,2023-09-25 15:44:24+00:00,http://arxiv.org/abs/2309.14233v1,"Muhammad Shoaib Farooq, Ali Abbas","cs.CL, cs.LG",poetry,"This study provides Urdu poetry generated using different deep-learning
techniques and algorithms. The data was collected through the Rekhta website,
containing 1341 text files with several couplets. The data on poetry was not
from any specific genre or poet. Instead, it was a collection of mixed Urdu
poems and Ghazals. Different deep learning techniques, such as the model
applied Long Short-term Memory Networks (LSTM) and Gated Recurrent Unit (GRU),
have been used. Natural Language Processing (NLP) may be used in machine
learning to understand, analyze, and generate a language humans may use and
understand. Much work has been done on generating poetry for different
languages using different techniques. The collection and use of data were also
different for different researchers. The primary purpose of this project is to
provide a model that generates Urdu poems by using data completely, not by
sampling data. Also, this may generate poems in pure Urdu, not Roman Urdu, as
in the base paper. The results have shown good accuracy in the poems generated
by the model.",2023-09-25
"Studying the impacts of pre-training using ChatGPT-generated text on
  downstream tasks",2023-09-02 12:56:15+00:00,http://arxiv.org/abs/2309.05668v1,Sarthak Anand,"cs.CL, cs.AI",poetry,"In recent times, significant advancements have been witnessed in the field of
language models, particularly with the emergence of Large Language Models
(LLMs) that are trained on vast amounts of data extracted from internet
archives. These LLMs, such as ChatGPT, have become widely accessible, allowing
users to generate text for various purposes including articles, essays, jokes,
and poetry. Given that LLMs are trained on a diverse range of text sources,
encompassing platforms like Reddit and Twitter, it is foreseeable that future
training datasets will also incorporate text generated by previous iterations
of the models themselves. In light of this development, our research aims to
investigate the influence of artificial text in the pre-training phase of
language models. Specifically, we conducted a comparative analysis between a
language model, RoBERTa, pre-trained using CNN/DailyMail news articles, and
ChatGPT, which employed the same articles for its training and evaluated their
performance on three downstream tasks as well as their potential gender bias,
using sentiment analysis as a metric. Through a series of experiments, we
demonstrate that the utilization of artificial text during pre-training does
not have a significant impact on either the performance of the models in
downstream tasks or their gender bias. In conclusion, our findings suggest that
the inclusion of text generated by LLMs in their own pre-training process does
not yield substantial effects on the subsequent performance of the models in
downstream tasks or their potential gender bias.",2023-09-02
Sudowoodo: a Chinese Lyric Imitation System with Source Lyrics,2023-08-09 02:12:04+00:00,http://arxiv.org/abs/2308.04665v1,"Yongzhu Chang, Rongsheng Zhang, Lin Jiang, Qihang Chen, Le Zhang, Jiashu Pu",cs.CL,poetry,"Lyrics generation is a well-known application in natural language generation
research, with several previous studies focusing on generating accurate lyrics
using precise control such as keywords, rhymes, etc. However, lyrics imitation,
which involves writing new lyrics by imitating the style and content of the
source lyrics, remains a challenging task due to the lack of a parallel corpus.
In this paper, we introduce \textbf{\textit{Sudowoodo}}, a Chinese lyrics
imitation system that can generate new lyrics based on the text of source
lyrics. To address the issue of lacking a parallel training corpus for lyrics
imitation, we propose a novel framework to construct a parallel corpus based on
a keyword-based lyrics model from source lyrics. Then the pairs \textit{(new
lyrics, source lyrics)} are used to train the lyrics imitation model. During
the inference process, we utilize a post-processing module to filter and rank
the generated lyrics, selecting the highest-quality ones. We incorporated audio
information and aligned the lyrics with the audio to form the songs as a bonus.
The human evaluation results show that our framework can perform better lyric
imitation. Meanwhile, the \textit{Sudowoodo} system and demo video of the
system is available at
\href{https://Sudowoodo.apps-hp.danlu.netease.com/}{Sudowoodo} and
\href{https://youtu.be/u5BBT_j1L5M}{https://youtu.be/u5BBT\_j1L5M}.",2023-08-09
"The Imitation Game: Detecting Human and AI-Generated Texts in the Era of
  Large Language Models",2023-07-22 21:00:14+00:00,http://arxiv.org/abs/2307.12166v1,"Kadhim Hayawi, Sakib Shahriar, Sujith Samuel Mathew","cs.CL, cs.AI",poetry,"The potential of artificial intelligence (AI)-based large language models
(LLMs) holds considerable promise in revolutionizing education, research, and
practice. However, distinguishing between human-written and AI-generated text
has become a significant task. This paper presents a comparative study,
introducing a novel dataset of human-written and LLM-generated texts in
different genres: essays, stories, poetry, and Python code. We employ several
machine learning models to classify the texts. Results demonstrate the efficacy
of these models in discerning between human and AI-generated text, despite the
dataset's limited sample size. However, the task becomes more challenging when
classifying GPT-generated text, particularly in story writing. The results
indicate that the models exhibit superior performance in binary classification
tasks, such as distinguishing human-generated text from a specific LLM,
compared to the more complex multiclass tasks that involve discerning among
human-generated and multiple LLMs. Our findings provide insightful implications
for AI text detection while our dataset paves the way for future research in
this evolving area.",2023-07-22
"PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in
  Poetry Generation",2023-06-14 11:57:31+00:00,http://arxiv.org/abs/2306.08456v1,"Zhiyuan Hu, Chumin Liu, Yue Feng, Bryan Hooi",cs.CL,poetry,"Poetry generation is a typical and popular task in natural language
generation. While prior works have shown success in controlling either semantic
or metrical aspects of poetry generation, there are still challenges in
addressing both perspectives simultaneously. In this paper, we employ the
Diffusion model to generate poetry in Sonnet and SongCi in Chinese for the
first time to tackle such challenges. Different from autoregressive generation,
our PoetryDiffusion model, based on Diffusion model, generates the complete
sentence or poetry by taking into account the whole sentence information,
resulting in improved semantic expression. Additionally, we incorporate a novel
metrical controller to manipulate and evaluate metrics (format and rhythm). The
denoising process in PoetryDiffusion allows for gradual enhancement of
semantics and flexible integration of the metrical controller. Experimental
results on two datasets demonstrate that our model outperforms existing models
in terms of semantic, metrical and overall performance.",2023-06-14
"Identifying the style by a qualified reader on a short fragment of
  generated poetry",2023-06-05 10:55:15+00:00,http://arxiv.org/abs/2306.02771v1,Boris Orekhov,"cs.CL, cs.AI, cs.LG",poetry,"Style is an important concept in today's challenges in natural language
generating. After the success in the field of image style transfer, the task of
text style transfer became actual and attractive. Researchers are also
interested in the tasks of style reproducing in generation of the poetic text.
Evaluation of style reproducing in natural poetry generation remains a problem.
I used 3 character-based LSTM-models to work with style reproducing assessment.
All three models were trained on the corpus of texts by famous Russian-speaking
poets. Samples were shown to the assessors and 4 answer options were offered,
the style of which poet this sample reproduces. In addition, the assessors were
asked how well they were familiar with the work of the poet they had named.
Students studying history of literature were the assessors, 94 answers were
received. It has appeared that accuracy of definition of style increases if the
assessor can quote the poet by heart. Each model showed at least 0.7
macro-average accuracy. The experiment showed that it is better to involve a
professional rather than a naive reader in the evaluation of style in the tasks
of poetry generation, while lstm models are good at reproducing the style of
Russian poets even on a limited training corpus.",2023-06-05
Unsupervised Melody-to-Lyric Generation,2023-05-30 17:20:25+00:00,http://arxiv.org/abs/2305.19228v1,"Yufei Tian, Anjali Narayan-Chen, Shereen Oraby, Alessandra Cervone, Gunnar Sigurdsson, Chenyang Tao, Wenbo Zhao, Tagyoung Chung, Jing Huang, Nanyun Peng","cs.CL, cs.AI, cs.SD, eess.AS",poetry,"Automatic melody-to-lyric generation is a task in which song lyrics are
generated to go with a given melody. It is of significant practical interest
and more challenging than unconstrained lyric generation as the music imposes
additional constraints onto the lyrics. The training data is limited as most
songs are copyrighted, resulting in models that underfit the complicated
cross-modal relationship between melody and lyrics. In this work, we propose a
method for generating high-quality lyrics without training on any aligned
melody-lyric data. Specifically, we design a hierarchical lyric generation
framework that first generates a song outline and second the complete lyrics.
The framework enables disentanglement of training (based purely on text) from
inference (melody-guided text generation) to circumvent the shortage of
parallel data.
  We leverage the segmentation and rhythm alignment between melody and lyrics
to compile the given melody into decoding constraints as guidance during
inference. The two-step hierarchical design also enables content control via
the lyric outline, a much-desired feature for democratizing collaborative song
creation. Experimental results show that our model can generate high-quality
lyrics that are more on-topic, singable, intelligible, and coherent than strong
baselines, for example SongMASS, a SOTA model trained on a parallel dataset,
with a 24% relative overall quality improvement based on human ratings. O",2023-05-30
Creative Data Generation: A Review Focusing on Text and Poetry,2023-05-15 09:50:15+00:00,http://arxiv.org/abs/2305.08493v1,"Mohamad Elzohbi, Richard Zhao",cs.CL,poetry,"The rapid advancement in machine learning has led to a surge in automatic
data generation, making it increasingly challenging to differentiate between
naturally or human-generated data and machine-generated data. Despite these
advancements, the generation of creative data remains a challenge. This paper
aims to investigate and comprehend the essence of creativity, both in general
and within the context of natural language generation. We review various
approaches to creative writing devices and tasks, with a specific focus on the
generation of poetry. We aim to shed light on the challenges and opportunities
in the field of creative data generation.",2023-05-15
Unsupervised Melody-Guided Lyrics Generation,2023-05-12 20:57:20+00:00,http://arxiv.org/abs/2305.07760v1,"Yufei Tian, Anjali Narayan-Chen, Shereen Oraby, Alessandra Cervone, Gunnar Sigurdsson, Chenyang Tao, Wenbo Zhao, Tagyoung Chung, Jing Huang, Nanyun Peng","cs.AI, cs.CL, cs.MM",poetry,"Automatic song writing is a topic of significant practical interest. However,
its research is largely hindered by the lack of training data due to copyright
concerns and challenged by its creative nature. Most noticeably, prior works
often fall short of modeling the cross-modal correlation between melody and
lyrics due to limited parallel data, hence generating lyrics that are less
singable. Existing works also lack effective mechanisms for content control, a
much desired feature for democratizing song creation for people with limited
music background. In this work, we propose to generate pleasantly listenable
lyrics without training on melody-lyric aligned data. Instead, we design a
hierarchical lyric generation framework that disentangles training (based
purely on text) from inference (melody-guided text generation). At inference
time, we leverage the crucial alignments between melody and lyrics and compile
the given melody into constraints to guide the generation process. Evaluation
results show that our model can generate high-quality lyrics that are more
singable, intelligible, coherent, and in rhyme than strong baselines including
those supervised on parallel data.",2023-05-12
Bits of Grass: Does GPT already know how to write like Whitman?,2023-05-10 09:02:34+00:00,http://arxiv.org/abs/2305.11064v1,"Piotr Sawicki, Marek Grzes, Fabricio Goes, Dan Brown, Max Peeperkorn, Aisha Khatun",cs.CL,poetry,"This study examines the ability of GPT-3.5, GPT-3.5-turbo (ChatGPT) and GPT-4
models to generate poems in the style of specific authors using zero-shot and
many-shot prompts (which use the maximum context length of 8192 tokens). We
assess the performance of models that are not fine-tuned for generating poetry
in the style of specific authors, via automated evaluation. Our findings
indicate that without fine-tuning, even when provided with the maximum number
of 17 poem examples (8192 tokens) in the prompt, these models do not generate
poetry in the desired style.",2023-05-10
Message Ritual: A Posthuman Account of Living with Lamp,2023-01-26 05:53:51+00:00,http://arxiv.org/abs/2301.10947v1,"Nina Rajcic, Jon McCormack","cs.HC, cs.AI, cs.NE, I.2; J.5; H.5",poetry,"As we become increasingly entangled with digital technologies, the boundary
between human and machine is progressively blurring. Adopting a performative,
posthumanist perspective resolves this ambiguity by proposing that such
boundaries are not predetermined, rather they are enacted within a certain
material configuration. Using this approach, dubbed `Entanglement HCI', this
paper presents \emph{Message Ritual} -- a novel, integrated AI system that
encourages the re-framing of memory through machine generated poetics. Embodied
within a domestic table lamp, the system listens in on conversations occurring
within the home, drawing out key topics and phrases of the day and
reconstituting them through machine generated poetry, delivered to household
members via SMS upon waking each morning. Participants across four households
were asked to live with the lamp over a two week period. We present a
diffractive analysis exploring how the lamp \emph{becomes with} participants
and discuss the implications of this method for future HCI research.",2023-01-26
Modern French Poetry Generation with RoBERTa and GPT-2,2022-12-06 12:10:14+00:00,http://arxiv.org/abs/2212.02911v1,"Mika Hämäläinen, Khalid Alnajjar, Thierry Poibeau",cs.CL,poetry,"We present a novel neural model for modern poetry generation in French. The
model consists of two pretrained neural models that are fine-tuned for the poem
generation task. The encoder of the model is a RoBERTa based one while the
decoder is based on GPT-2. This way the model can benefit from the superior
natural language understanding performance of RoBERTa and the good natural
language generation performance of GPT-2. Our evaluation shows that the model
can create French poetry successfully. On a 5 point scale, the lowest score of
3.57 was given by human judges to typicality and emotionality of the output
poetry while the best score of 3.79 was given to understandability.",2022-12-06
Generation of Chinese classical poetry based on pre-trained model,2022-11-04 16:05:31+00:00,http://arxiv.org/abs/2211.02541v1,"Ziyao Wang, Lujin Guan, Guanyu Liu","cs.CL, cs.AI, J.5; I.2.7",poetry,"In order to test whether artificial intelligence can create qualified
classical poetry like humans, the author proposes a study of Chinese classical
poetry generation based on a pre-trained model. This paper mainly tries to use
BART and other pre training models, proposes FS2TEXT and RR2TEXT to generate
metrical poetry text and even specific style poetry text, and solves the
problem that the user's writing intention gradually reduces the relevance of
the generated poetry text.
  In order to test the model's results, the authors selected ancient poets, by
combining it with BART's poetic model work, developed a set of AI poetry Turing
problems, it was reviewed by a group of poets and poetry writing researchers.
There were more than 600 participants, and the final results showed that,
high-level poetry lovers can't distinguish between AI activity and human
activity, this indicates that the author's working methods are not
significantly different from human activities. The model of poetry generation
studied by the author generalizes works that cannot be distinguished from those
of advanced scholars.
  The number of modern Chinese poets has reached 5 million. However, many
modern Chinese poets lack language ability and skills as a result of their
childhood learning. However, many modern poets have no creative inspiration,
and the author's model can help them. They can look at this model when they
choose words and phrases and they can write works based on the poems they
already have, and they can write their own poems. The importance of poetry lies
in the author's thoughts and reflections. It doesn't matter how good AI poetry
is. The only thing that matters is for people to see and inspire them.",2022-11-04
Controllable Text Generation for Open-Domain Creativity and Fairness,2022-09-24 22:40:01+00:00,http://arxiv.org/abs/2209.12099v1,Nanyun Peng,"cs.CL, cs.AI",poetry,"Recent advances in large pre-trained language models have demonstrated strong
results in generating natural languages and significantly improved performances
for many natural language generation (NLG) applications such as machine
translation and text summarization. However, when the generation tasks are more
open-ended and the content is under-specified, existing techniques struggle to
generate long-term coherent and creative content. Moreover, the models exhibit
and even amplify social biases that are learned from the training corpora. This
happens because the generation models are trained to capture the surface
patterns (i.e. sequences of words), instead of capturing underlying semantics
and discourse structures, as well as background knowledge including social
norms. In this paper, I introduce our recent works on controllable text
generation to enhance the creativity and fairness of language generation
models. We explore hierarchical generation and constrained decoding, with
applications to creative language generation including story, poetry, and
figurative languages, and bias mitigation for generation models.",2022-09-24
Multi-Modal Experience Inspired AI Creation,2022-09-02 11:50:41+00:00,http://arxiv.org/abs/2209.02427v1,"Qian Cao, Xu Chen, Ruihua Song, Hao Jiang, Guang Yang, Zhao Cao",cs.AI,poetry,"AI creation, such as poem or lyrics generation, has attracted increasing
attention from both industry and academic communities, with many promising
models proposed in the past few years. Existing methods usually estimate the
outputs based on single and independent visual or textual information. However,
in reality, humans usually make creations according to their experiences, which
may involve different modalities and be sequentially correlated. To model such
human capabilities, in this paper, we define and solve a novel AI creation
problem based on human experiences. More specifically, we study how to generate
texts based on sequential multi-modal information. Compared with the previous
works, this task is much more difficult because the designed model has to well
understand and adapt the semantics among different modalities and effectively
convert them into the output in a sequential manner. To alleviate these
difficulties, we firstly design a multi-channel sequence-to-sequence
architecture equipped with a multi-modal attention network. For more effective
optimization, we then propose a curriculum negative sampling strategy tailored
for the sequential inputs. To benchmark this problem and demonstrate the
effectiveness of our model, we manually labeled a new multi-modal experience
dataset. With this dataset, we conduct extensive experiments by comparing our
model with a series of representative baselines, where we can demonstrate
significant improvements in our model based on both automatic and
human-centered metrics. The code and data are available at:
\url{https://github.com/Aman-4-Real/MMTG}.",2022-09-02
Romantic-Computing,2022-06-01 14:27:17+00:00,http://arxiv.org/abs/2206.11864v1,Elizabeth Horishny,"cs.CL, cs.AI",poetry,"In this paper we compare various text generation models' ability to write
poetry in the style of early English Romanticism. These models include:
Character-Level Recurrent Neural Networks with Long Short-Term Memory, Hugging
Face's GPT-2, OpenAI's GPT-3, and EleutherAI's GPT-NEO. Quality was measured
based syllable count and coherence with the automatic evaluation metric GRUEN.
Character-Level Recurrent Neural Networks performed far worse compared to
transformer models. And, as parameter-size increased, the quality of
transformer models' poems improved. These models are typically not compared in
a creative context, and we are happy to contribute.",2022-06-01
GPoeT-2: A GPT-2 Based Poem Generator,2022-05-18 10:25:12+00:00,http://arxiv.org/abs/2205.08847v1,"Kai-Ling Lo, Rami Ariss, Philipp Kurz","cs.CL, cs.AI",poetry,"This project aims to produce the next volume of machine-generated poetry, a
complex art form that can be structured and unstructured, and carries depth in
the meaning between the lines. GPoeT-2 is based on fine-tuning a state of the
art natural language model (i.e. GPT-2) to generate limericks, typically
humorous structured poems consisting of five lines with a AABBA rhyming scheme.
With a two-stage generation system utilizing both forward and reverse language
modeling, GPoeT-2 is capable of freely generating limericks in diverse topics
while following the rhyming structure without any seed phrase or a posteriori
constraints.Based on the automated generation process, we explore a wide
variety of evaluation metrics to quantify ""good poetry,"" including syntactical
correctness, lexical diversity, and subject continuity. Finally, we present a
collection of 94 categorized limericks that rank highly on the explored ""good
poetry"" metrics to provoke human creativity.",2022-05-18
Youling: an AI-Assisted Lyrics Creation System,2022-01-18 03:57:04+00:00,http://arxiv.org/abs/2201.06724v1,"Rongsheng Zhang, Xiaoxi Mao, Le Li, Lin Jiang, Lin Chen, Zhiwei Hu, Yadong Xi, Changjie Fan, Minlie Huang",cs.CL,poetry,"Recently, a variety of neural models have been proposed for lyrics
generation. However, most previous work completes the generation process in a
single pass with little human intervention. We believe that lyrics creation is
a creative process with human intelligence centered. AI should play a role as
an assistant in the lyrics creation process, where human interactions are
crucial for high-quality creation. This paper demonstrates \textit{Youling}, an
AI-assisted lyrics creation system, designed to collaborate with music
creators. In the lyrics generation process, \textit{Youling} supports
traditional one pass full-text generation mode as well as an interactive
generation mode, which allows users to select the satisfactory sentences from
generated candidates conditioned on preceding context. The system also provides
a revision module which enables users to revise undesired sentences or words of
lyrics repeatedly. Besides, \textit{Youling} allows users to use multifaceted
attributes to control the content and format of generated lyrics. The demo
video of the system is available at https://youtu.be/DFeNpHk0pm4.",2022-01-18
"Say What? Collaborative Pop Lyric Generation Using Multitask Transfer
  Learning",2021-11-15 08:13:26+00:00,http://arxiv.org/abs/2111.07592v1,"Naveen Ram, Tanay Gummadi, Rahul Bhethanabotla, Richard J. Savery, Gil Weinberg","cs.CL, cs.HC, cs.LG, I.2.7",poetry,"Lyric generation is a popular sub-field of natural language generation that
has seen growth in recent years. Pop lyrics are of unique interest due to the
genre's unique style and content, in addition to the high level of
collaboration that goes on behind the scenes in the professional pop
songwriting process. In this paper, we present a collaborative line-level lyric
generation system that utilizes transfer-learning via the T5 transformer model,
which, till date, has not been used to generate pop lyrics. By working and
communicating directly with professional songwriters, we develop a model that
is able to learn lyrical and stylistic tasks like rhyming, matching line beat
requirements, and ending lines with specific target words. Our approach
compares favorably to existing methods for multiple datasets and yields
positive results from our online studies and interviews with industry
songwriters.",2021-11-15
SP-GPT2: Semantics Improvement in Vietnamese Poetry Generation,2021-10-10 14:31:08+00:00,http://arxiv.org/abs/2110.15723v1,"Tuan Nguyen, Hanh Pham, Truong Bui, Tan Nguyen, Duc Luong, Phong Nguyen","cs.CL, cs.AI, cs.LG",poetry,"Automatic text generation has garnered growing attention in recent years as
an essential step towards computer creativity. Generative Pretraining
Transformer 2 (GPT2) is one of the state of the art approaches that have
excellent successes. In this paper, we took the first step to investigate the
power of GPT2 in traditional Vietnamese poetry generation. In the earlier time,
our experiment with base GPT2 was quite good at generating the poem in the
proper template. Though it can learn the patterns, including rhyme and tone
rules, from the training data, like almost all other text generation
approaches, the poems generated still has a topic drift and semantic
inconsistency. To improve the cohesion within the poems, we proposed a new
model SP-GPT2 (semantic poem GPT2) which was built on the top GPT2 model and an
additional loss to constrain context throughout the entire poem. For better
evaluation, we examined the methods by both automatic quantitative evaluation
and human evaluation. Both automatic and human evaluation demonstrated that our
approach can generate poems that have better cohesion without losing the
quality due to additional loss. At the same time, we are the pioneers of this
topic. We released the first computational scoring module for poems generated
in the template containing the style rule dictionary. Additionally, we are the
first to publish a Luc-Bat dataset, including 87609 Luc Bat poems, which is
equivalent to about 2.6 million sentences, combined with about 83579 poems in
other styles was also published for further exploration. The code is available
at https://github.com/fsoft-ailab/Poem-Generator",2021-10-10
"The Perils of Using Mechanical Turk to Evaluate Open-Ended Text
  Generation",2021-09-14 17:20:30+00:00,http://arxiv.org/abs/2109.06835v1,"Marzena Karpinska, Nader Akoury, Mohit Iyyer",cs.CL,poetry,"Recent text generation research has increasingly focused on open-ended
domains such as story and poetry generation. Because models built for such
tasks are difficult to evaluate automatically, most researchers in the space
justify their modeling choices by collecting crowdsourced human judgments of
text quality (e.g., Likert scores of coherence or grammaticality) from Amazon
Mechanical Turk (AMT). In this paper, we first conduct a survey of 45
open-ended text generation papers and find that the vast majority of them fail
to report crucial details about their AMT tasks, hindering reproducibility. We
then run a series of story evaluation experiments with both AMT workers and
English teachers and discover that even with strict qualification filters, AMT
workers (unlike teachers) fail to distinguish between model-generated text and
human-generated references. We show that AMT worker judgments improve when they
are shown model-generated output alongside human-generated references, which
enables the workers to better calibrate their ratings. Finally, interviews with
the English teachers provide deeper insights into the challenges of the
evaluation process, particularly when rating model-generated text.",2021-09-14
Lingxi: A Diversity-aware Chinese Modern Poetry Generation System,2021-08-27 03:33:28+00:00,http://arxiv.org/abs/2108.12108v1,"Xinran Zhang, Maosong Sun, Jiafeng Liu, Xiaobing Li",cs.CL,poetry,"Poetry generation has been a difficult task in natural language processing.
Unlike plain neural text generation tasks, poetry has a high requirement for
novelty, since an easily-understood sentence with too many high frequency words
might not be considered as poetic, while adequately ambiguous sentences with
low frequency words can possibly be novel and creative. Inspired by this, we
present Lingxi, a diversity-aware Chinese modern poetry generation system. We
propose nucleus sampling with randomized head (NS-RH) algorithm, which
randomizes the high frequency part (""head"") of the predicted distribution, in
order to emphasize on the ""comparatively low frequency"" words. The proposed
algorithm can significantly increase the novelty of generated poetry compared
with traditional sampling methods. The permutation of distribution is
controllable by tuning the filtering parameter that determines the ""head"" to
permutate, achieving diversity-aware sampling. We find that even when a large
portion of filtered vocabulary is randomized, it can actually generate fluent
poetry but with notably higher novelty. We also propose a
semantic-similarity-based rejection sampling algorithm, which creates longer
and more informative context on the basis of the short input poetry title while
maintaining high semantic similarity to the title, alleviating the off-topic
issue.",2021-08-27
Urdu & Hindi Poetry Generation using Neural Networks,2021-07-16 16:12:51+00:00,http://arxiv.org/abs/2107.14587v1,"Shakeeb A. M. Mukhtar, Pushkar S. Joglekar","cs.CL, cs.LG",poetry,"One of the major problems writers and poets face is the writer's block. It is
a condition in which an author loses the ability to produce new work or
experiences a creative slowdown. The problem is more difficult in the context
of poetry than prose, as in the latter case authors need not be very concise
while expressing their ideas, also the various aspects such as rhyme, poetic
meters are not relevant for prose. One of the most effective ways to overcome
this writing block for poets can be, to have a prompt system, which would help
their imagination and open their minds for new ideas. A prompt system can
possibly generate one liner, two liner or full ghazals. The purpose of this
work is to give an ode to the Urdu, Hindi poets, and helping them start their
next line of poetry, a couplet or a complete ghazal considering various factors
like rhymes, refrain, and meters. The result will help aspiring poets to get
new ideas and help them overcome writer's block by auto-generating pieces of
poetry using Deep Learning techniques. A concern with creative works like this,
especially in the literary context, is to ensure that the output is not
plagiarized. This work also addresses the concern and makes sure that the
resulting odes are not exact match with input data using parameters like
temperature and manual plagiarism check against input corpus. To the best of
our knowledge, although the automatic text generation problem has been studied
quite extensively in the literature, the specific problem of Urdu, Hindi poetry
generation has not been explored much. Apart from developing system to
auto-generate Urdu, Hindi poetry, another key contribution of our work is to
create a cleaned and preprocessed corpus of Urdu, Hindi poetry (derived from
authentic resources) and making it freely available for researchers in the
area.",2021-07-16
FUDGE: Controlled Text Generation With Future Discriminators,2021-04-12 05:59:53+00:00,http://arxiv.org/abs/2104.05218v1,"Kevin Yang, Dan Klein","cs.CL, cs.LG",poetry,"We propose Future Discriminators for Generation (FUDGE), a flexible and
modular method for controlled text generation. Given a pre-existing model G for
generating text from a distribution of interest, FUDGE enables conditioning on
a desired attribute a (for example, formality) while requiring access only to
G's output logits. FUDGE learns an attribute predictor operating on a partial
sequence, and uses this predictor's outputs to adjust G's original
probabilities. We show that FUDGE models terms corresponding to a Bayesian
decomposition of the conditional distribution of G given attribute a. Moreover,
FUDGE can easily compose predictors for multiple desired attributes. We
evaluate FUDGE on three tasks -- couplet completion in poetry, topic control in
language generation, and formality change in machine translation -- and observe
gains in all three tasks.",2021-04-12
WakaVT: A Sequential Variational Transformer for Waka Generation,2021-04-01 12:14:41+00:00,http://arxiv.org/abs/2104.00426v1,"Yuka Takeishi, Mingxuan Niu, Jing Luo, Zhong Jin, Xinyu Yang","cs.CL, cs.AI",poetry,"Poetry generation has long been a challenge for artificial intelligence. In
the scope of Japanese poetry generation, many researchers have paid attention
to Haiku generation, but few have focused on Waka generation. To further
explore the creative potential of natural language generation systems in
Japanese poetry creation, we propose a novel Waka generation model, WakaVT,
which automatically produces Waka poems given user-specified keywords. Firstly,
an additive mask-based approach is presented to satisfy the form constraint.
Secondly, the structures of Transformer and variational autoencoder are
integrated to enhance the quality of generated content. Specifically, to obtain
novelty and diversity, WakaVT employs a sequence of latent variables, which
effectively captures word-level variability in Waka data. To improve linguistic
quality in terms of fluency, coherence, and meaningfulness, we further propose
the fused multilevel self-attention mechanism, which properly models the
hierarchical linguistic structure of Waka. To the best of our knowledge, we are
the first to investigate Waka generation with models based on Transformer
and/or variational autoencoder. Both objective and subjective evaluation
results demonstrate that our model outperforms baselines significantly.",2021-04-01
AfriKI: Machine-in-the-Loop Afrikaans Poetry Generation,2021-03-30 09:17:56+00:00,http://arxiv.org/abs/2103.16190v1,"Imke van Heerden, Anil Bas","cs.CL, cs.LG",poetry,"This paper proposes a generative language model called AfriKI. Our approach
is based on an LSTM architecture trained on a small corpus of contemporary
fiction. With the aim of promoting human creativity, we use the model as an
authoring tool to explore machine-in-the-loop Afrikaans poetry generation. To
our knowledge, this is the first study to attempt creative text generation in
Afrikaans.",2021-03-30
"Controllable Generation from Pre-trained Language Models via Inverse
  Prompting",2021-03-19 08:36:52+00:00,http://arxiv.org/abs/2103.10685v1,"Xu Zou, Da Yin, Qingyang Zhong, Hongxia Yang, Zhilin Yang, Jie Tang","cs.CL, cs.AI, cs.LG",poetry,"Large-scale pre-trained language models have demonstrated strong capabilities
of generating realistic text. However, it remains challenging to control the
generation results. Previous approaches such as prompting are far from
sufficient, which limits the usage of language models. To tackle this
challenge, we propose an innovative method, inverse prompting, to better
control text generation. The core idea of inverse prompting is to use generated
text to inversely predict the prompt during beam search, which enhances the
relevance between the prompt and the generated text and provides better
controllability. Empirically, we pre-train a large-scale Chinese language model
to perform a systematic study using human evaluation on the tasks of
open-domain poem generation and open-domain long-form question answering. Our
results show that our proposed method substantially outperforms the baselines
and that our generation quality is close to human performance on some of the
tasks.
  Narrators can try our poem generation demo at
https://pretrain.aminer.cn/apps/poetry.html, while our QA demo can be found at
https://pretrain.aminer.cn/app/qa. For researchers, the code is provided in
https://github.com/THUDM/InversePrompting.",2021-03-19
"Exploring Transformers in Natural Language Generation: GPT, BERT, and
  XLNet",2021-02-16 09:18:16+00:00,http://arxiv.org/abs/2102.08036v1,"M. Onat Topal, Anil Bas, Imke van Heerden","cs.CL, cs.LG",poetry,"Recent years have seen a proliferation of attention mechanisms and the rise
of Transformers in Natural Language Generation (NLG). Previously,
state-of-the-art NLG architectures such as RNN and LSTM ran into vanishing
gradient problems; as sentences grew larger, distance between positions
remained linear, and sequential computation hindered parallelization since
sentences were processed word by word. Transformers usher in a new era. In this
paper, we explore three major Transformer-based models, namely GPT, BERT, and
XLNet, that carry significant implications for the field. NLG is a burgeoning
area that is now bolstered with rapid developments in attention mechanisms.
From poetry generation to summarization, text generation derives benefit as
Transformer-based language models achieve groundbreaking results.",2021-02-16
Generate and Revise: Reinforcement Learning in Neural Poetry,2021-02-08 10:35:33+00:00,http://arxiv.org/abs/2102.04114v1,"Andrea Zugarini, Luca Pasqualini, Stefano Melacci, Marco Maggini","cs.CL, cs.AI, cs.LG",poetry,"Writers, poets, singers usually do not create their compositions in just one
breath. Text is revisited, adjusted, modified, rephrased, even multiple times,
in order to better convey meanings, emotions and feelings that the author wants
to express. Amongst the noble written arts, Poetry is probably the one that
needs to be elaborated the most, since the composition has to formally respect
predefined meter and rhyming schemes. In this paper, we propose a framework to
generate poems that are repeatedly revisited and corrected, as humans do, in
order to improve their overall quality. We frame the problem of revising poems
in the context of Reinforcement Learning and, in particular, using Proximal
Policy Optimization. Our model generates poems from scratch and it learns to
progressively adjust the generated text in order to match a target criterion.
We evaluate this approach in the case of matching a rhyming scheme, without
having any information on which words are responsible of creating rhymes and on
how to coherently alter the poem words. The proposed framework is general and,
with an appropriate reward shaping, it can be applied to other text generation
problems.",2021-02-08
Weird AI Yankovic: Generating Parody Lyrics,2020-09-25 13:56:20+00:00,http://arxiv.org/abs/2009.12240v1,Mark Riedl,"cs.CL, cs.AI",poetry,"Lyrics parody swaps one set of words that accompany a melody with a new set
of words, preserving the number of syllables per line and the rhyme scheme.
Lyrics parody generation is a challenge for controllable text generation. We
show how a specialized sampling procedure, combined with backward text
generation with XLNet can produce parody lyrics that reliably meet the syllable
and rhyme scheme constraints.We introduce the Weird AI Yankovic system and
provide a case study evaluation. We conclude with societal implications of
neural lyric parody generation.",2020-09-25
"Artificial Intelligence versus Maya Angelou: Experimental evidence that
  people cannot differentiate AI-generated from human-written poetry",2020-05-20 11:52:28+00:00,http://arxiv.org/abs/2005.09980v2,"Nils Köbis, Luca Mossink","cs.AI, cs.CL, econ.GN, q-fin.EC",poetry,"The release of openly available, robust natural language generation
algorithms (NLG) has spurred much public attention and debate. One reason lies
in the algorithms' purported ability to generate human-like text across various
domains. Empirical evidence using incentivized tasks to assess whether people
(a) can distinguish and (b) prefer algorithm-generated versus human-written
text is lacking. We conducted two experiments assessing behavioral reactions to
the state-of-the-art Natural Language Generation algorithm GPT-2 (Ntotal =
830). Using the identical starting lines of human poems, GPT-2 produced samples
of poems. From these samples, either a random poem was chosen
(Human-out-of-the-loop) or the best one was selected (Human-in-the-loop) and in
turn matched with a human-written poem. In a new incentivized version of the
Turing Test, participants failed to reliably detect the
algorithmically-generated poems in the Human-in-the-loop treatment, yet
succeeded in the Human-out-of-the-loop treatment. Further, people reveal a
slight aversion to algorithm-generated poetry, independent on whether
participants were informed about the algorithmic origin of the poem
(Transparency) or not (Opacity). We discuss what these results convey about the
performance of NLG algorithms to produce human-like text and propose
methodologies to study such learning algorithms in human-agent experimental
settings.",2020-05-20
Rigid Formats Controlled Text Generation,2020-04-17 01:40:18+00:00,http://arxiv.org/abs/2004.08022v1,"Piji Li, Haisong Zhang, Xiaojiang Liu, Shuming Shi","cs.CL, cs.LG",poetry,"Neural text generation has made tremendous progress in various tasks. One
common characteristic of most of the tasks is that the texts are not restricted
to some rigid formats when generating. However, we may confront some special
text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi
(classical Chinese poetry of the Song dynasty), etc. The typical
characteristics of these texts are in three folds: (1) They must comply fully
with the rigid predefined formats. (2) They must obey some rhyming schemes. (3)
Although they are restricted to some formats, the sentence integrity must be
guaranteed. To the best of our knowledge, text generation based on the
predefined rigid formats has not been well investigated. Therefore, we propose
a simple and elegant framework named SongNet to tackle this problem. The
backbone of the framework is a Transformer-based auto-regressive language
model. Sets of symbols are tailor-designed to improve the modeling performance
especially on format, rhyme, and sentence integrity. We improve the attention
mechanism to impel the model to capture some future information on the format.
A pre-training and fine-tuning framework is designed to further improve the
generation quality. Extensive experiments conducted on two collected corpora
demonstrate that our proposed framework generates significantly better results
in terms of both automatic metrics and the human evaluation.",2020-04-17
"Generating Major Types of Chinese Classical Poetry in a Uniformed
  Framework",2020-03-13 14:16:25+00:00,http://arxiv.org/abs/2003.11528v1,"Jinyi Hu, Maosong Sun",cs.CL,poetry,"Poetry generation is an interesting research topic in the field of text
generation. As one of the most valuable literary and cultural heritages of
China, Chinese classical poetry is very familiar and loved by Chinese people
from generation to generation. It has many particular characteristics in its
language structure, ranging from form, sound to meaning, thus is regarded as an
ideal testing task for text generation. In this paper, we propose a GPT-2 based
uniformed framework for generating major types of Chinese classical poems. We
define a unified format for formulating all types of training samples by
integrating detailed form information, then present a simple form-stressed
weighting method in GPT-2 to strengthen the control to the form of the
generated poems, with special emphasis on those forms with longer body length.
Preliminary experimental results show this enhanced model can generate Chinese
classical poems of major types with high quality in both form and content,
validating the effectiveness of the proposed strategy. The model has been
incorporated into Jiuge, the most influential Chinese classical poetry
generation system developed by Tsinghua University (Guo et al., 2019).",2020-03-13
Introducing Aspects of Creativity in Automatic Poetry Generation,2020-02-06 20:44:12+00:00,http://arxiv.org/abs/2002.02511v1,"Brendan Bena, Jugal Kalita","cs.CL, cs.AI, cs.LG",poetry,"Poetry Generation involves teaching systems to automatically generate text
that resembles poetic work. A deep learning system can learn to generate poetry
on its own by training on a corpus of poems and modeling the particular style
of language. In this paper, we propose taking an approach that fine-tunes
GPT-2, a pre-trained language model, to our downstream task of poetry
generation. We extend prior work on poetry generation by introducing creative
elements. Specifically, we generate poems that express emotion and elicit the
same in readers, and poems that use the language of dreams---called dream
poetry. We are able to produce poems that correctly elicit the emotions of
sadness and joy 87.5 and 85 percent, respectively, of the time. We produce
dreamlike poetry by training on a corpus of texts that describe dreams. Poems
from this model are shown to capture elements of dream poetry with scores of no
less than 3.2 on the Likert scale. We perform crowdsourced human-evaluation for
all our poems. We also make use of the Coh-Metrix tool, outlining metrics we
use to gauge the quality of text generated.",2020-02-06
Let's FACE it. Finnish Poetry Generation with Aesthetics and Framing,2019-10-30 16:00:54+00:00,http://arxiv.org/abs/1910.13946v1,"Mika Hämäläinen, Khalid Alnajjar",cs.CL,poetry,"We present a creative poem generator for the morphologically rich Finnish
language. Our method falls into the master-apprentice paradigm, where a
computationally creative genetic algorithm teaches a BRNN model to generate
poetry. We model several parts of poetic aesthetics in the fitness function of
the genetic algorithm, such as sonic features, semantic coherence, imagery and
metaphor. Furthermore, we justify the creativity of our method based on the
FACE theory on computational creativity and take additional care in evaluating
our system by automatic metrics for concepts together with human evaluation for
aesthetics, framing and expressions.",2019-10-30
"Creative GANs for generating poems, lyrics, and metaphors",2019-09-20 14:40:18+00:00,http://arxiv.org/abs/1909.09534v1,"Asir Saeed, Suzana Ilić, Eva Zangerle","cs.CL, cs.LG",poetry,"Generative models for text have substantially contributed to tasks like
machine translation and language modeling, using maximum likelihood
optimization (MLE). However, for creative text generation, where multiple
outputs are possible and originality and uniqueness are encouraged, MLE falls
short. Methods optimized for MLE lead to outputs that can be generic,
repetitive and incoherent. In this work, we use a Generative Adversarial
Network framework to alleviate this problem. We evaluate our framework on
poetry, lyrics and metaphor datasets, each with widely different
characteristics, and report better performance of our objective function over
other generative models.",2019-09-20
"A Hierarchical Attention Based Seq2seq Model for Chinese Lyrics
  Generation",2019-06-15 06:58:42+00:00,http://arxiv.org/abs/1906.06481v1,"Haoshen Fan, Jie Wang, Bojin Zhuang, Shaojun Wang, Jing Xiao","cs.CL, cs.LG",poetry,"In this paper, we comprehensively study on context-aware generation of
Chinese song lyrics. Conventional text generative models generate a sequence or
sentence word by word, failing to consider the contextual relationship between
sentences. Taking account into the characteristics of lyrics, a hierarchical
attention based Seq2Seq (Sequence-to-Sequence) model is proposed for Chinese
lyrics generation. With encoding of word-level and sentence-level contextual
information, this model promotes the topic relevance and consistency of
generation. A large Chinese lyrics corpus is also leveraged for model training.
Eventually, results of automatic and human evaluations demonstrate that our
model is able to compose complete Chinese lyrics with one united topic
constraint.",2019-06-15
"Automated Speech Generation from UN General Assembly Statements: Mapping
  Risks in AI Generated Texts",2019-06-05 11:23:14+00:00,http://arxiv.org/abs/1906.01946v1,"Joseph Bullock, Miguel Luengo-Oroz","cs.CL, cs.AI",poetry,"Automated text generation has been applied broadly in many domains such as
marketing and robotics, and used to create chatbots, product reviews and write
poetry. The ability to synthesize text, however, presents many potential risks,
while access to the technology required to build generative models is becoming
increasingly easy. This work is aligned with the efforts of the United Nations
and other civil society organisations to highlight potential political and
societal risks arising through the malicious use of text generation software,
and their potential impact on human rights. As a case study, we present the
findings of an experiment to generate remarks in the style of political leaders
by fine-tuning a pretrained AWD- LSTM model on a dataset of speeches made at
the UN General Assembly. This work highlights the ease with which this can be
accomplished, as well as the threats of combining these techniques with other
technologies.",2019-06-05
Theme-aware generation model for chinese lyrics,2019-05-23 08:50:15+00:00,http://arxiv.org/abs/1906.02134v1,"Jie Wang, Xinyan Zhao","cs.CL, cs.LG",poetry,"With rapid development of neural networks, deep-learning has been extended to
various natural language generation fields, such as machine translation,
dialogue generation and even literature creation. In this paper, we propose a
theme-aware language generation model for Chinese music lyrics, which improves
the theme-connectivity and coherence of generated paragraphs greatly. A
multi-channel sequence-to-sequence (seq2seq) model encodes themes and previous
sentences as global and local contextual information. Moreover, attention
mechanism is incorporated for sequence decoding, enabling to fuse context into
predicted next texts. To prepare appropriate train corpus, LDA (Latent
Dirichlet Allocation) is applied for theme extraction. Generated lyrics is
grammatically correct and semantically coherent with selected themes, which
offers a valuable modelling method in other fields including multi-turn
chatbots, long paragraph generation and etc.",2019-05-23
Hierarchical Attention: What Really Counts in Various NLP Tasks,2018-08-10 23:28:33+00:00,http://arxiv.org/abs/1808.03728v1,"Zehao Dou, Zhihua Zhang",cs.CL,poetry,"Attention mechanisms in sequence to sequence models have shown great ability
and wonderful performance in various natural language processing (NLP) tasks,
such as sentence embedding, text generation, machine translation, machine
reading comprehension, etc. Unfortunately, existing attention mechanisms only
learn either high-level or low-level features. In this paper, we think that the
lack of hierarchical mechanisms is a bottleneck in improving the performance of
the attention mechanisms, and propose a novel Hierarchical Attention Mechanism
(Ham) based on the weighted sum of different layers of a multi-level attention.
Ham achieves a state-of-the-art BLEU score of 0.26 on Chinese poem generation
task and a nearly 6.5% averaged improvement compared with the existing machine
reading comprehension models such as BIDAF and Match-LSTM. Furthermore, our
experiments and theorems reveal that Ham has greater generalization and
representation ability than existing attention mechanisms.",2018-08-10
"Guess who? Multilingual approach for the automated generation of
  author-stylized poetry",2018-07-17 15:13:20+00:00,http://arxiv.org/abs/1807.07147v3,"Alexey Tikhonov, Ivan P. Yamshchikov","cs.CL, cs.AI, cs.LG",poetry,"This paper addresses the problem of stylized text generation in a
multilingual setup. A version of a language model based on a long short-term
memory (LSTM) artificial neural network with extended phonetic and semantic
embeddings is used for stylized poetry generation. The quality of the resulting
poems generated by the network is estimated through bilingual evaluation
understudy (BLEU), a survey and a new cross-entropy based metric that is
suggested for the problems of such type. The experiments show that the proposed
model consistently outperforms random sample and vanilla-LSTM baselines, humans
also tend to associate machine generated texts with the target author.",2018-07-17
Natural Language Statistical Features of LSTM-generated Texts,2018-04-10 13:17:36+00:00,http://arxiv.org/abs/1804.04087v2,"Marco Lippi, Marcelo A Montemurro, Mirko Degli Esposti, Giampaolo Cristadoro","cs.CL, cs.LG",poetry,"Long Short-Term Memory (LSTM) networks have recently shown remarkable
performance in several tasks dealing with natural language generation, such as
image captioning or poetry composition. Yet, only few works have analyzed text
generated by LSTMs in order to quantitatively evaluate to which extent such
artificial texts resemble those generated by humans. We compared the
statistical structure of LSTM-generated language to that of written natural
language, and to those produced by Markov models of various orders. In
particular, we characterized the statistical structure of language by assessing
word-frequency statistics, long-range correlations, and entropy measures. Our
main finding is that while both LSTM and Markov-generated texts can exhibit
features similar to real ones in their word-frequency statistics and entropy
measures, LSTM-texts are shown to reproduce long-range correlations at scales
comparable to those found in natural language. Moreover, for LSTM networks a
temperature-like parameter controlling the generation process shows an optimal
value---for which the produced texts are closest to real language---consistent
across all the different statistical features investigated.",2018-04-10
X575: writing rengas with web services,2016-06-25 20:04:42+00:00,http://arxiv.org/abs/1606.07955v1,"Daniel Winterstein, Joseph Corneli","cs.AI, cs.CL, I.2.1; J.5",poetry,"Our software system simulates the classical collaborative Japanese poetry
form, renga, made of linked haikus. We used NLP methods wrapped up as web
services. Our experiments were only a partial success, since results fail to
satisfy classical constraints. To gather ideas for future work, we examine
related research in semiotics, linguistics, and computing.",2016-06-25
