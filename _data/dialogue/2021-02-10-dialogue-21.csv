title,pubdate,id,authors,categories,search,abstract,displaydate
"Improving Model-Based Reinforcement Learning with Internal State
  Representations through Self-Supervision",2021-02-10 17:55:04+00:00,http://arxiv.org/abs/2102.05599v1,"Julien Scholz, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter",cs.LG,dialogue,"Using a model of the environment, reinforcement learning agents can plan
their future moves and achieve superhuman performance in board games like
Chess, Shogi, and Go, while remaining relatively sample-efficient. As
demonstrated by the MuZero Algorithm, the environment model can even be learned
dynamically, generalizing the agent to many more tasks while at the same time
achieving state-of-the-art performance. Notably, MuZero uses internal state
representations derived from real environment states for its predictions. In
this paper, we bind the model's predicted internal state representation to the
environment state via two additional terms: a reconstruction model loss and a
simpler consistency loss, both of which work independently and unsupervised,
acting as constraints to stabilize the learning process. Our experiments show
that this new integration of reconstruction model loss and simpler consistency
loss provide a significant performance increase in OpenAI Gym environments. Our
modifications also enable self-supervised pretraining for MuZero, so the
algorithm can learn about environment dynamics before a goal is made available.",2021-02-10
Multi-turn Dialogue Reading Comprehension with Pivot Turns and Knowledge,2021-02-10 15:00:12+00:00,http://arxiv.org/abs/2102.05474v1,"Zhuosheng Zhang, Junlong Li, Hai Zhao","cs.CL, cs.AI",dialogue,"Multi-turn dialogue reading comprehension aims to teach machines to read
dialogue contexts and solve tasks such as response selection and answering
questions. The major challenges involve noisy history contexts and especial
prerequisites of commonsense knowledge that is unseen in the given material.
Existing works mainly focus on context and response matching approaches. This
work thus makes the first attempt to tackle the above two challenges by
extracting substantially important turns as pivot utterances and utilizing
external knowledge to enhance the representation of context. We propose a
pivot-oriented deep selection model (PoDS) on top of the Transformer-based
language models for dialogue comprehension. In detail, our model first picks
out the pivot utterances from the conversation history according to the
semantic matching with the candidate response or question, if any. Besides,
knowledge items related to the dialogue context are extracted from a knowledge
graph as external knowledge. Then, the pivot utterances and the external
knowledge are combined with a well-designed mechanism for refining predictions.
Experimental results on four dialogue comprehension benchmark tasks show that
our proposed model achieves great improvements on baselines. A series of
empirical comparisons are conducted to show how our selection strategies and
the extra knowledge injection influence the results.",2021-02-10
"Automated and Distributed Statistical Analysis of Economic Agent-Based
  Models",2021-02-10 12:39:34+00:00,http://arxiv.org/abs/2102.05405v1,"Andrea Vandin, Daniele Giachini, Francesco Lamperti, Francesca Chiaromonte","econ.GN, cs.MA, cs.PF, q-fin.EC",dialogue,"We propose a novel approach to the statistical analysis of simulation models
and, especially, agent-based models (ABMs). Our main goal is to provide a fully
automated and model-independent tool-kit to inspect simulations and perform
counterfactual analysis. Our approach: (i) is easy-to-use by the modeller, (ii)
improves reproducibility of results, (iii) optimizes running time given the
modeller's machine, (iv) automatically chooses the number of required
simulations and simulation steps to reach user-specified statistical
confidence, and (v) automatically performs a variety of statistical tests. In
particular, our framework is designed to distinguish the transient dynamics of
the model from its steady-state behaviour (if any), estimate properties of the
model in both ""phases"", and provide indications on the ergodic (or non-ergodic)
nature of the simulated processes -- which, in turns allows one to gauge the
reliability of a steady-state analysis. Estimates are equipped with statistical
guarantees, allowing for robust comparisons across computational experiments.
To demonstrate the effectiveness of our approach, we apply it to two models
from the literature: a large scale macro-financial ABM and a small scale
prediction market model. Compared to prior analyses of these models, we obtain
new insights and we are able to identify and fix some erroneous conclusions.",2021-02-10
"Policy Augmentation: An Exploration Strategy for Faster Convergence of
  Deep Reinforcement Learning Algorithms",2021-02-10 03:51:45+00:00,http://arxiv.org/abs/2102.05249v1,Arash Mahyari,"cs.LG, cs.AI, cs.CV, cs.RO, cs.SY, eess.SY",dialogue,"Despite advancements in deep reinforcement learning algorithms, developing an
effective exploration strategy is still an open problem. Most existing
exploration strategies either are based on simple heuristics, or require the
model of the environment, or train additional deep neural networks to generate
imagination-augmented paths. In this paper, a revolutionary algorithm, called
Policy Augmentation, is introduced. Policy Augmentation is based on a newly
developed inductive matrix completion method. The proposed algorithm augments
the values of unexplored state-action pairs, helping the agent take actions
that will result in high-value returns while the agent is in the early
episodes. Training deep reinforcement learning algorithms with high-value
rollouts leads to the faster convergence of deep reinforcement learning
algorithms. Our experiments show the superior performance of Policy
Augmentation. The code can be found at:
https://github.com/arashmahyari/PolicyAugmentation.",2021-02-10
Decontextualization: Making Sentences Stand-Alone,2021-02-09 22:52:37+00:00,http://arxiv.org/abs/2102.05169v1,"Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, Michael Collins","cs.CL, cs.AI",dialogue,"Models for question answering, dialogue agents, and summarization often
interpret the meaning of a sentence in a rich context and use that meaning in a
new context. Taking excerpts of text can be problematic, as key pieces may not
be explicit in a local window. We isolate and define the problem of sentence
decontextualization: taking a sentence together with its context and rewriting
it to be interpretable out of context, while preserving its meaning. We
describe an annotation procedure, collect data on the Wikipedia corpus, and use
the data to train models to automatically decontextualize sentences. We present
preliminary studies that show the value of sentence decontextualization in a
user facing task, and as preprocessing for systems that perform document
understanding. We argue that decontextualization is an important subtask in
many downstream applications, and that the definitions and resources provided
can benefit tasks that operate on sentences that occur in a richer context.",2021-02-09
Scheduling the NASA Deep Space Network with Deep Reinforcement Learning,2021-02-09 22:48:05+00:00,http://arxiv.org/abs/2102.05167v1,"Edwin Goh, Hamsa Shwetha Venkataram, Mark Hoffmann, Mark Johnston, Brian Wilson",cs.LG,dialogue,"With three complexes spread evenly across the Earth, NASA's Deep Space
Network (DSN) is the primary means of communications as well as a significant
scientific instrument for dozens of active missions around the world. A rapidly
rising number of spacecraft and increasingly complex scientific instruments
with higher bandwidth requirements have resulted in demand that exceeds the
network's capacity across its 12 antennae. The existing DSN scheduling process
operates on a rolling weekly basis and is time-consuming; for a given week,
generation of the final baseline schedule of spacecraft tracking passes takes
roughly 5 months from the initial requirements submission deadline, with
several weeks of peer-to-peer negotiations in between. This paper proposes a
deep reinforcement learning (RL) approach to generate candidate DSN schedules
from mission requests and spacecraft ephemeris data with demonstrated
capability to address real-world operational constraints. A deep RL agent is
developed that takes mission requests for a given week as input, and interacts
with a DSN scheduling environment to allocate tracks such that its reward
signal is maximized. A comparison is made between an agent trained using
Proximal Policy Optimization and its random, untrained counterpart. The results
represent a proof-of-concept that, given a well-shaped reward signal, a deep RL
agent can learn the complex heuristics used by experts to schedule the DSN. A
trained agent can potentially be used to generate candidate schedules to
bootstrap the scheduling process and thus reduce the turnaround cycle for DSN
scheduling.",2021-02-09
AuGPT: Dialogue with Pre-trained Language Models and Data Augmentation,2021-02-09 20:53:34+00:00,http://arxiv.org/abs/2102.05126v1,"Jonáš Kulhánek, Vojtěch Hudeček, Tomáš Nekvinda, Ondřej Dušek","cs.CL, cs.AI, cs.LG",dialogue,"Attention-based pre-trained language models such as GPT-2 brought
considerable progress to end-to-end dialogue modelling. However, they also
present considerable risks for task-oriented dialogue, such as lack of
knowledge grounding or diversity. To address these issues, we introduce
modified training objectives for language model finetuning, and we employ
massive data augmentation via back-translation to increase the diversity of the
training data. We further examine the possibilities of combining data from
multiples sources to improve performance on the target dataset. We carefully
evaluate our contributions with both human and automatic methods. Our model
achieves state-of-the-art performance on the MultiWOZ data and shows
competitive performance in human evaluation.",2021-02-09
RL for Latent MDPs: Regret Guarantees and a Lower Bound,2021-02-09 16:49:58+00:00,http://arxiv.org/abs/2102.04939v1,"Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, Shie Mannor",cs.LG,dialogue,"In this work, we consider the regret minimization problem for reinforcement
learning in latent Markov Decision Processes (LMDP). In an LMDP, an MDP is
randomly drawn from a set of $M$ possible MDPs at the beginning of the
interaction, but the identity of the chosen MDP is not revealed to the agent.
We first show that a general instance of LMDPs requires at least
$\Omega((SA)^M)$ episodes to even approximate the optimal policy. Then, we
consider sufficient assumptions under which learning good policies requires
polynomial number of episodes. We show that the key link is a notion of
separation between the MDP system dynamics. With sufficient separation, we
provide an efficient algorithm with local guarantee, {\it i.e.,} providing a
sublinear regret guarantee when we are given a good initialization. Finally, if
we are given standard statistical sufficiency assumptions common in the
Predictive State Representation (PSR) literature (e.g., Boots et al.) and a
reachability assumption, we show that the need for initialization can be
removed.",2021-02-09
"Learning State Representations from Random Deep Action-conditional
  Predictions",2021-02-09 15:53:22+00:00,http://arxiv.org/abs/2102.04897v1,"Zeyu Zheng, Vivek Veeriah, Risto Vuorio, Richard Lewis, Satinder Singh","cs.LG, cs.AI",dialogue,"In this work, we study auxiliary prediction tasks defined by
temporal-difference networks (TD networks); these networks are a language for
expressing a rich space of general value function (GVF) prediction targets that
may be learned efficiently with TD. Through analysis in an illustrative domain
we show the benefits to learning state representations of exploiting the full
richness of TD networks, including both action-conditional predictions and
temporally deep predictions. Our main (and perhaps surprising) result is that
deep action-conditional TD networks with random structures that create random
prediction-questions about random features yield state representations that are
competitive with state-of-the-art hand-crafted value prediction and pixel
control auxiliary tasks in both Atari games and DeepMind Lab tasks. We also
show through stop-gradient experiments that learning the state representations
solely via these unsupervised random TD network prediction tasks yield agents
that outperform the end-to-end-trained actor-critic baseline.",2021-02-09
Persistent Rule-based Interactive Reinforcement Learning,2021-02-04 06:48:57+00:00,http://arxiv.org/abs/2102.02441v1,"Adam Bignold, Francisco Cruz, Richard Dazeley, Peter Vamplew, Cameron Foale","cs.AI, cs.MA",dialogue,"Interactive reinforcement learning has allowed speeding up the learning
process in autonomous agents by including a human trainer providing extra
information to the agent in real-time. Current interactive reinforcement
learning research has been limited to interactions that offer relevant advice
to the current state only. Additionally, the information provided by each
interaction is not retained and instead discarded by the agent after a
single-use. In this work, we propose a persistent rule-based interactive
reinforcement learning approach, i.e., a method for retaining and reusing
provided knowledge, allowing trainers to give general advice relevant to more
than just the current state. Our experimental results show persistent advice
substantially improves the performance of the agent while reducing the number
of interactions required for the trainer. Moreover, rule-based advice shows
similar performance impact as state-based advice, but with a substantially
reduced interaction count.",2021-02-04
"Converse, Focus and Guess -- Towards Multi-Document Driven Dialogue",2021-02-04 06:36:11+00:00,http://arxiv.org/abs/2102.02435v1,"Han Liu, Caixia Yuan, Xiaojie Wang, Yushu Yang, Huixing Jiang, Zhongyuan Wang",cs.CL,dialogue,"We propose a novel task, Multi-Document Driven Dialogue (MD3), in which an
agent can guess the target document that the user is interested in by leading a
dialogue. To benchmark progress, we introduce a new dataset of GuessMovie,
which contains 16,881 documents, each describing a movie, and associated 13,434
dialogues. Further, we propose the MD3 model. Keeping guessing the target
document in mind, it converses with the user conditioned on both document
engagement and user feedback. In order to incorporate large-scale external
documents into the dialogue, it pretrains a document representation which is
sensitive to attributes it talks about an object. Then it tracks dialogue state
by detecting evolvement of document belief and attribute belief, and finally
optimizes dialogue policy in principle of entropy decreasing and reward
increasing, which is expected to successfully guess the user's target in a
minimum number of turns. Experiments show that our method significantly
outperforms several strong baseline methods and is very close to human's
performance.",2021-02-04
Neural Recursive Belief States in Multi-Agent Reinforcement Learning,2021-02-03 20:10:23+00:00,http://arxiv.org/abs/2102.02274v1,"Pol Moreno, Edward Hughes, Kevin R. McKee, Bernardo Avila Pires, Théophane Weber","cs.LG, cs.AI, cs.MA",dialogue,"In multi-agent reinforcement learning, the problem of learning to act is
particularly difficult because the policies of co-players may be heavily
conditioned on information only observed by them. On the other hand, humans
readily form beliefs about the knowledge possessed by their peers and leverage
beliefs to inform decision-making. Such abilities underlie individual success
in a wide range of Markov games, from bluffing in Poker to conditional
cooperation in the Prisoner's Dilemma, to convention-building in Bridge.
Classical methods are usually not applicable to complex domains due to the
intractable nature of hierarchical beliefs (i.e. beliefs of other agents'
beliefs). We propose a scalable method to approximate these belief structures
using recursive deep generative models, and to use the belief models to obtain
representations useful to acting in complex tasks. Our agents trained with
belief models outperform model-free baselines with equivalent representational
capacity using common training paradigms. We also show that higher-order belief
models outperform agents with lower-order models.",2021-02-03
"DiSCoL: Toward Engaging Dialogue Systems through Conversational Line
  Guided Response Generation",2021-02-03 18:36:58+00:00,http://arxiv.org/abs/2102.02191v1,"Sarik Ghazarian, Zixi Liu, Tuhin Chakrabarty, Xuezhe Ma, Aram Galstyan, Nanyun Peng",cs.CL,dialogue,"Having engaging and informative conversations with users is the utmost goal
for open-domain conversational systems. Recent advances in transformer-based
language models and their applications to dialogue systems have succeeded to
generate fluent and human-like responses. However, they still lack control over
the generation process towards producing contentful responses and achieving
engaging conversations. To achieve this goal, we present \textbf{DiSCoL}
(\textbf{Di}alogue \textbf{S}ystems through \textbf{Co}versational
\textbf{L}ine guided response generation). DiSCoL is an open-domain dialogue
system that leverages conversational lines (briefly \textbf{convlines}) as
controllable and informative content-planning elements to guide the generation
model produce engaging and informative responses. Two primary modules in
DiSCoL's pipeline are conditional generators trained for 1) predicting relevant
and informative convlines for dialogue contexts and 2) generating high-quality
responses conditioned on the predicted convlines. Users can also change the
returned convlines to \textit{control} the direction of the conversations
towards topics that are more interesting for them. Through automatic and human
evaluations, we demonstrate the efficiency of the convlines in producing
engaging conversations.",2021-02-03
"Learning a Compact State Representation for Navigation Tasks by
  Autoencoding 2D-Lidar Scans",2021-02-03 16:10:26+00:00,http://arxiv.org/abs/2102.02127v1,"Christopher Gebauer, Maren Bennewitz","cs.RO, cs.LG",dialogue,"In this paper, we address the problem of generating a compact representation
of 2D-lidar scans for reinforcement learning in navigation tasks. By now only
little work focuses on the compactness of the provided state, which is a
necessary condition to successfully and efficiently train a navigation agent.
Our approach works in three stages. First, we propose a novel preprocessing of
the distance measurements and compute a local, egocentric, binary grid map
based on the current range measurements. We then autoencode the local map using
a variational autoencoder, where the latent space serves as state
representation. An important key for a compact and, at the same time,
meaningful representation is the degree of disentanglement, which describes the
correlation between each latent dimension. Therefore, we finally apply
state-of-the-art disentangling methods to improve the representation power.
Furthermore, we investige the possibilities of incorporating time-dependent
information into the latent space. In particular, we incorporate the relation
of consecutive scans, especially ego-motion, by applying a memory model. We
implemented our approach in python using tensorflow. Our datasets are simulated
with pybullet as well as recorded using a slamtec rplidar A3. The experiments
show the capability of our approach to highly compress lidar data, maintain a
meaningful distribution of the latent space, and even incorporate time-depended
information.",2021-02-03
Learning to Select External Knowledge with Multi-Scale Negative Sampling,2021-02-03 14:59:35+00:00,http://arxiv.org/abs/2102.02096v1,"Huang He, Hua Lu, Siqi Bao, Fan Wang, Hua Wu, Zhengyu Niu, Haifeng Wang",cs.CL,dialogue,"The Track-1 of DSTC9 aims to effectively answer user requests or questions
during task-oriented dialogues, which are out of the scope of APIs/DB. By
leveraging external knowledge resources, relevant information can be retrieved
and encoded into the response generation for these out-of-API-coverage queries.
In this work, we have explored several advanced techniques to enhance the
utilization of external knowledge and boost the quality of response generation,
including schema guided knowledge decision, negatives enhanced knowledge
selection, and knowledge grounded response generation. To evaluate the
performance of our proposed method, comprehensive experiments have been carried
out on the publicly available dataset. Our approach was ranked as the best in
human evaluation of DSTC9 Track-1.",2021-02-03
Advances and Challenges in Conversational Recommender Systems: A Survey,2021-01-23 08:53:15+00:00,http://arxiv.org/abs/2101.09459v4,"Chongming Gao, Wenqiang Lei, Xiangnan He, Maarten de Rijke, Tat-Seng Chua","cs.IR, cs.CL",dialogue,"Recommender systems exploit interaction history to estimate user preference,
having been heavily used in a wide range of industry applications. However,
static recommendation models are difficult to answer two important questions
well due to inherent shortcomings: (a) What exactly does a user like? (b) Why
does a user like an item? The shortcomings are due to the way that static
models learn user preference, i.e., without explicit instructions and active
feedback from users. The recent rise of conversational recommender systems
(CRSs) changes this situation fundamentally. In a CRS, users and the system can
dynamically communicate through natural language interactions, which provide
unprecedented opportunities to explicitly obtain the exact preference of users.
Considerable efforts, spread across disparate settings and applications, have
been put into developing CRSs. Existing models, technologies, and evaluation
methods for CRSs are far from mature. In this paper, we provide a systematic
review of the techniques used in current CRSs. We summarize the key challenges
of developing CRSs into five directions: (1) Question-based user preference
elicitation. (2) Multi-turn conversational recommendation strategies. (3)
Dialogue understanding and generation. (4) Exploitation-exploration trade-offs.
(5) Evaluation and user simulation. These research directions involve multiple
research fields like information retrieval (IR), natural language processing
(NLP), and human-computer interaction (HCI). Based on these research
directions, we discuss some future challenges and opportunities. We provide a
road map for researchers from multiple communities to get started in this area.
We hope this survey helps to identify and address challenges in CRSs and
inspire future research.",2021-01-23
"Beyond Domain APIs: Task-oriented Conversational Modeling with
  Unstructured Knowledge Access Track in DSTC9",2021-01-22 18:57:56+00:00,http://arxiv.org/abs/2101.09276v3,"Seokhwan Kim, Mihail Eric, Behnam Hedayatnia, Karthik Gopalakrishnan, Yang Liu, Chao-Wei Huang, Dilek Hakkani-Tur",cs.CL,dialogue,"Most prior work on task-oriented dialogue systems are restricted to a limited
coverage of domain APIs, while users oftentimes have domain related requests
that are not covered by the APIs. This challenge track aims to expand the
coverage of task-oriented dialogue systems by incorporating external
unstructured knowledge sources. We define three tasks: knowledge-seeking turn
detection, knowledge selection, and knowledge-grounded response generation. We
introduce the data sets and the neural baseline models for three tasks. The
challenge track received a total of 105 entries from 24 participating teams. In
the evaluation results, the ensemble methods with different large-scale
pretrained language models achieved high performances with improved knowledge
selection capability and better generalization into unseen data.",2021-01-22
"Rank the Episodes: A Simple Approach for Exploration in
  Procedurally-Generated Environments",2021-01-20 14:22:01+00:00,http://arxiv.org/abs/2101.08152v2,"Daochen Zha, Wenye Ma, Lei Yuan, Xia Hu, Ji Liu",cs.LG,dialogue,"Exploration under sparse reward is a long-standing challenge of model-free
reinforcement learning. The state-of-the-art methods address this challenge by
introducing intrinsic rewards to encourage exploration in novel states or
uncertain environment dynamics. Unfortunately, methods based on intrinsic
rewards often fall short in procedurally-generated environments, where a
different environment is generated in each episode so that the agent is not
likely to visit the same state more than once. Motivated by how humans
distinguish good exploration behaviors by looking into the entire episode, we
introduce RAPID, a simple yet effective episode-level exploration method for
procedurally-generated environments. RAPID regards each episode as a whole and
gives an episodic exploration score from both per-episode and long-term views.
Those highly scored episodes are treated as good exploration behaviors and are
stored in a small ranking buffer. The agent then imitates the episodes in the
buffer to reproduce the past good exploration behaviors. We demonstrate our
method on several procedurally-generated MiniGrid environments, a
first-person-view 3D Maze navigation task from MiniWorld, and several sparse
MuJoCo tasks. The results show that RAPID significantly outperforms the
state-of-the-art intrinsic reward strategies in terms of sample efficiency and
final performance. The code is available at https://github.com/daochenzha/rapid",2021-01-20
Transforming Multi-Conditioned Generation from Meaning Representation,2021-01-12 01:45:06+00:00,http://arxiv.org/abs/2101.04257v1,Joosung Lee,"cs.CL, cs.AI",dialogue,"In task-oriented conversation systems, natural language generation systems
that generate sentences with specific information related to conversation flow
are useful. Our study focuses on language generation by considering various
information representing the meaning of utterances as multiple conditions of
generation. NLG from meaning representations, the conditions for sentence
meaning, generally goes through two steps: sentence planning and surface
realization. However, we propose a simple one-stage framework to generate
utterances directly from MR (Meaning Representation). Our model is based on
GPT2 and generates utterances with flat conditions on slot and value pairs,
which does not need to determine the structure of the sentence. We evaluate
several systems in the E2E dataset with 6 automatic metrics. Our system is a
simple method, but it demonstrates comparable performance to previous systems
in automated metrics. In addition, using only 10\% of the data set without any
other techniques, our model achieves comparable performance, and shows the
possibility of performing zero-shot generation and expanding to other datasets.",2021-01-12
Continual Learning in Task-Oriented Dialogue Systems,2020-12-31 08:44:25+00:00,http://arxiv.org/abs/2012.15504v1,"Andrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Seungwhan Moon, Paul Crook, Bing Liu, Zhou Yu, Eunjoon Cho, Zhiguang Wang","cs.CL, cs.AI",dialogue,"Continual learning in task-oriented dialogue systems can allow us to add new
domains and functionalities through time without incurring the high cost of a
whole system retraining. In this paper, we propose a continual learning
benchmark for task-oriented dialogue systems with 37 domains to be learned
continuously in four settings, such as intent recognition, state tracking,
natural language generation, and end-to-end. Moreover, we implement and compare
multiple existing continual learning baselines, and we propose a simple yet
effective architectural method based on residual adapters. Our experiments
demonstrate that the proposed architectural method and a simple replay-based
strategy perform comparably well but they both achieve inferior performance to
the multi-task learning baseline, in where all the data are shown at once,
showing that continual learning in task-oriented dialogue systems is a
challenging task. Furthermore, we reveal several trade-offs between different
continual learning methods in term of parameter usage and memory size, which
are important in the design of a task-oriented dialogue system. The proposed
benchmark is released together with several baselines to promote more research
in this direction.",2020-12-31
"Interpretable NLG for Task-oriented Dialogue Systems with Heterogeneous
  Rendering Machines",2020-12-29 07:41:48+00:00,http://arxiv.org/abs/2012.14645v1,"Yangming Li, Kaisheng Yao",cs.CL,dialogue,"End-to-end neural networks have achieved promising performances in natural
language generation (NLG). However, they are treated as black boxes and lack
interpretability. To address this problem, we propose a novel framework,
heterogeneous rendering machines (HRM), that interprets how neural generators
render an input dialogue act (DA) into an utterance. HRM consists of a renderer
set and a mode switcher. The renderer set contains multiple decoders that vary
in both structure and functionality. For every generation step, the mode
switcher selects an appropriate decoder from the renderer set to generate an
item (a word or a phrase). To verify the effectiveness of our method, we have
conducted extensive experiments on 5 benchmark datasets. In terms of automatic
metrics (e.g., BLEU), our model is competitive with the current
state-of-the-art method. The qualitative analysis shows that our model can
interpret the rendering process of neural generators well. Human evaluation
also confirms the interpretability of our proposed approach.",2020-12-29
