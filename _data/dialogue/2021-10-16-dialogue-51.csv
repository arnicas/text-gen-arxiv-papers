title,pubdate,id,authors,categories,search,abstract,displaydate
"FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metricsfor
  Automatic Text Generation",2021-10-16 11:59:48+00:00,http://arxiv.org/abs/2110.08559v1,"Moussa Kamal Eddine, Guokan Shang, Antoine J. -P. Tixier, Michalis Vazirgiannis",cs.CL,dialogue,"Fast and reliable evaluation metrics are key to R&D progress. While
traditional natural language generation metrics are fast, they are not very
reliable. Conversely, new metrics based on large pretrained language models are
much more reliable, but require significant computational resources. In this
paper, we propose FrugalScore, an approach to learn a fixed, low cost version
of any expensive NLG metric, while retaining most of its original performance.
Experiments with BERTScore and MoverScore on summarization and translation show
that FrugalScore is on par with the original metrics (and sometimes better),
while having several orders of magnitude less parameters and running several
times faster. On average over all learned metrics, tasks, and variants,
FrugalScore retains 96.8% of the performance, runs 24 times faster, and has 35
times less parameters than the original metrics. We make our trained metrics
publicly available, to benefit the entire NLP community and in particular
researchers and practitioners with limited resources.",2021-10-16
"Hindsight: Posterior-guided training of retrievers for improved
  open-ended generation",2021-10-14 22:24:57+00:00,http://arxiv.org/abs/2110.07752v2,"Ashwin Paranjape, Omar Khattab, Christopher Potts, Matei Zaharia, Christopher D. Manning","cs.CL, cs.IR",dialogue,"Many text generation systems benefit from using a retriever to retrieve
passages from a textual knowledge corpus (e.g., Wikipedia) which are then
provided as additional context to the generator. For open-ended generation
tasks (like generating informative utterances in conversations) many varied
passages may be equally relevant and we find that existing methods that jointly
train the retriever and generator underperform: the retriever may not find
relevant passages even amongst the top-10 and hence the generator may not learn
a preference to ground its generated output in them. We propose using an
additional guide retriever that is allowed to use the target output and ""in
hindsight"" retrieve relevant passages during training. We model the guide
retriever after the posterior distribution Q of passages given the input and
the target output and train it jointly with the standard retriever and the
generator by maximizing the evidence lower bound (ELBo) in expectation over Q.
For informative conversations from the Wizard of Wikipedia dataset, with
posterior-guided training, the retriever finds passages with higher relevance
in the top-10 (23% relative improvement), the generator's responses are more
grounded in the retrieved passage (19% relative improvement) and the end-to-end
system produces better overall output (6.4% relative improvement).",2021-10-14
Federated Natural Language Generation for Personalized Dialogue System,2021-10-13 00:59:52+00:00,http://arxiv.org/abs/2110.06419v1,"Yujie Lu, Chao Huang, Huanli Zhan, Yong Zhuang","cs.CL, cs.AI",dialogue,"Neural conversational models have long suffered from the problem of
inconsistency and lacking coherent personality. To address the issue,
persona-based models capturing individual characteristics have been proposed,
but they still face the dilemma of model adaption and data privacy. To break
this dilemma, we propose a novel Federated Natural Language Generation (FedNLG)
framework, which learns personalized representations from various dataset on
distributed devices, and thus implements the personalized dialogue system
efficiently and safely. FedNLG first pre-trains parameters of standard neural
conversational model over a large dialogue corpus, and then fine-tune the model
parameters and persona embeddings on specific datasets, in a federated manner.
Thus, the model could simultaneously learn the persona embeddings in local
clients and learn shared model parameters by federated aggregation, which
achieves accuracyprivacy balance. By conducting extensive experiments, we
demonstrate the effectiveness of our model by pre-training model over Cornell
Movie-Dialogs Corpus and fine-tuning the model over two TV series dataset.",2021-10-13
"OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset
  with Visual Contexts",2021-09-27 02:10:29+00:00,http://arxiv.org/abs/2109.12761v2,"Shuhe Wang, Yuxian Meng, Xiaoya Li, Xiaofei Sun, Rongbin Ouyang, Jiwei Li",cs.CL,dialogue,"In order to better simulate the real human conversation process, models need
to generate dialogue utterances based on not only preceding textual contexts
but also visual contexts. However, with the development of multi-modal dialogue
learning, the dataset scale gradually becomes a bottleneck. In this report, we
release OpenViDial 2.0, a larger-scale open-domain multi-modal dialogue dataset
compared to the previous version OpenViDial 1.0. OpenViDial 2.0 contains a
total number of 5.6 million dialogue turns extracted from either movies or TV
series from different resources, and each dialogue turn is paired with its
corresponding visual context. We hope this large-scale dataset can help
facilitate future researches on open-domain multi-modal dialog generation,
e.g., multi-modal pretraining for dialogue generation.",2021-09-27
"An animated picture says at least a thousand words: Selecting Gif-based
  Replies in Multimodal Dialog",2021-09-24 21:48:27+00:00,http://arxiv.org/abs/2109.12212v1,"Xingyao Wang, David Jurgens","cs.CL, cs.CV, cs.CY",dialogue,"Online conversations include more than just text. Increasingly, image-based
responses such as memes and animated gifs serve as culturally recognized and
often humorous responses in conversation. However, while NLP has broadened to
multimodal models, conversational dialog systems have largely focused only on
generating text replies. Here, we introduce a new dataset of 1.56M text-gif
conversation turns and introduce a new multimodal conversational model Pepe the
King Prawn for selecting gif-based replies. We demonstrate that our model
produces relevant and high-quality gif responses and, in a large randomized
control trial of multiple models replying to real users, we show that our model
replies with gifs that are significantly better received by the community.",2021-09-24
Style Control for Schema-Guided Natural Language Generation,2021-09-24 21:47:58+00:00,http://arxiv.org/abs/2109.12211v1,"Alicia Y. Tsai, Shereen Oraby, Vittorio Perera, Jiun-Yu Kao, Yuheng Du, Anjali Narayan-Chen, Tagyoung Chung, Dilek Hakkani-Tur",cs.CL,dialogue,"Natural Language Generation (NLG) for task-oriented dialogue systems focuses
on communicating specific content accurately, fluently, and coherently. While
these attributes are crucial for a successful dialogue, it is also desirable to
simultaneously accomplish specific stylistic goals, such as response length,
point-of-view, descriptiveness, sentiment, formality, and empathy. In this
work, we focus on stylistic control and evaluation for schema-guided NLG, with
joint goals of achieving both semantic and stylistic control. We experiment in
detail with various controlled generation methods for large pretrained language
models: specifically, conditional training, guided fine-tuning, and guided
decoding. We discuss their advantages and limitations, and evaluate them with a
broad range of automatic and human evaluation metrics. Our results show that
while high style accuracy and semantic correctness are easier to achieve for
more lexically-defined styles with conditional training, stylistic control is
also achievable for more semantically complex styles using discriminator-based
guided decoding methods. The results also suggest that methods that are more
scalable (with less hyper-parameters tuning) and that disentangle content
generation and stylistic variations are more effective at achieving semantic
correctness and style accuracy.",2021-09-24
"Controllable Dialogue Generation with Disentangled Multi-grained Style
  Specification and Attribute Consistency Reward",2021-09-14 14:29:38+00:00,http://arxiv.org/abs/2109.06717v1,"Zhe Hu, Zhiwei Cao, Hou Pong Chan, Jiachen Liu, Xinyan Xiao, Jinsong Su, Hua Wu","cs.CL, cs.AI",dialogue,"Controllable text generation is an appealing but challenging task, which
allows users to specify particular attributes of the generated outputs. In this
paper, we propose a controllable dialogue generation model to steer response
generation under multi-attribute constraints. Specifically, we define and
categorize the commonly used control attributes into global and local ones,
which possess different granularities of effects on response generation. Then,
we significantly extend the conventional seq2seq framework by introducing a
novel two-stage decoder, which first uses a multi-grained style specification
layer to impose the stylistic constraints and determine word-level control
states of responses based on the attributes, and then employs a response
generation layer to generate final responses maintaining both semantic
relevancy to the contexts and fidelity to the attributes. Furthermore, we train
our model with an attribute consistency reward to promote response control with
explicit supervision signals. Extensive experiments and in-depth analyses on
two datasets indicate that our model can significantly outperform competitive
baselines in terms of response quality, content diversity and controllability.",2021-09-14
"End-to-End Conversational Search for Online Shopping with Utterance
  Transfer",2021-09-12 08:33:44+00:00,http://arxiv.org/abs/2109.05460v1,"Liqiang Xiao, Jun Ma2, Xin Luna Dong, Pascual Martinez-Gomez, Nasser Zalmout, Wei Chen, Tong Zhao, Hao He, Yaohui Jin","cs.CL, cs.AI",dialogue,"Successful conversational search systems can present natural, adaptive and
interactive shopping experience for online shopping customers. However,
building such systems from scratch faces real word challenges from both
imperfect product schema/knowledge and lack of training dialog data.In this
work we first propose ConvSearch, an end-to-end conversational search system
that deeply combines the dialog system with search. It leverages the text
profile to retrieve products, which is more robust against imperfect product
schema/knowledge compared with using product attributes alone. We then address
the lack of data challenges by proposing an utterance transfer approach that
generates dialogue utterances by using existing dialog from other domains, and
leveraging the search behavior data from e-commerce retailer. With utterance
transfer, we introduce a new conversational search dataset for online shopping.
Experiments show that our utterance transfer method can significantly improve
the availability of training dialogue data without crowd-sourcing, and the
conversational search system significantly outperformed the best tested
baseline.",2021-09-12
Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration,2021-09-12 04:17:53+00:00,http://arxiv.org/abs/2109.05426v1,"Chuanxin Tang, Chong Luo, Zhiyuan Zhao, Dacheng Yin, Yucheng Zhao, Wenjun Zeng","cs.SD, cs.AI, eess.AS",dialogue,"Given a piece of speech and its transcript text, text-based speech editing
aims to generate speech that can be seamlessly inserted into the given speech
by editing the transcript. Existing methods adopt a two-stage approach:
synthesize the input text using a generic text-to-speech (TTS) engine and then
transform the voice to the desired voice using voice conversion (VC). A major
problem of this framework is that VC is a challenging problem which usually
needs a moderate amount of parallel training data to work satisfactorily. In
this paper, we propose a one-stage context-aware framework to generate natural
and coherent target speech without any training data of the target speaker. In
particular, we manage to perform accurate zero-shot duration prediction for the
inserted text. The predicted duration is used to regulate both text embedding
and speech embedding. Then, based on the aligned cross-modality input, we
directly generate the mel-spectrogram of the edited speech with a
transformer-based decoder. Subjective listening tests show that despite the
lack of training data for the speaker, our method has achieved satisfactory
results. It outperforms a recent zero-shot TTS engine by a large margin.",2021-09-12
Refocusing on Relevance: Personalization in NLG,2021-09-10 23:50:02+00:00,http://arxiv.org/abs/2109.05140v1,"Shiran Dudy, Steven Bedrick, Bonnie Webber","cs.CL, cs.CY, cs.HC",dialogue,"Many NLG tasks such as summarization, dialogue response, or open domain
question answering focus primarily on a source text in order to generate a
target response. This standard approach falls short, however, when a user's
intent or context of work is not easily recoverable based solely on that source
text -- a scenario that we argue is more of the rule than the exception. In
this work, we argue that NLG systems in general should place a much higher
level of emphasis on making use of additional context, and suggest that
relevance (as used in Information Retrieval) be thought of as a crucial tool
for designing user-oriented text-generating tasks. We further discuss possible
harms and hazards around such personalization, and argue that value-sensitive
design represents a crucial path forward through these challenges.",2021-09-10
"Generating Self-Contained and Summary-Centric Question Answer Pairs via
  Differentiable Reward Imitation Learning",2021-09-10 06:34:55+00:00,http://arxiv.org/abs/2109.04689v1,"Li Zhou, Kevin Small, Yong Zhang, Sandeep Atluri","cs.CL, cs.AI, cs.LG",dialogue,"Motivated by suggested question generation in conversational news
recommendation systems, we propose a model for generating question-answer pairs
(QA pairs) with self-contained, summary-centric questions and
length-constrained, article-summarizing answers. We begin by collecting a new
dataset of news articles with questions as titles and pairing them with
summaries of varying length. This dataset is used to learn a QA pair generation
model producing summaries as answers that balance brevity with sufficiency
jointly with their corresponding questions. We then reinforce the QA pair
generation process with a differentiable reward function to mitigate exposure
bias, a common problem in natural language generation. Both automatic metrics
and human evaluation demonstrate these QA pairs successfully capture the
central gists of the articles and achieve high answer accuracy.",2021-09-10
"Hi, my name is Martha: Using names to measure and mitigate bias in
  generative dialogue models",2021-09-07 19:20:24+00:00,http://arxiv.org/abs/2109.03300v1,"Eric Michael Smith, Adina Williams",cs.CL,dialogue,"All AI models are susceptible to learning biases in data that they are
trained on. For generative dialogue models, being trained on real human
conversations containing unbalanced gender and race/ethnicity references can
lead to models that display learned biases, which we define here broadly as any
measurable differences in the distributions of words or semantic content of
conversations based on demographic groups. We measure the strength of such
biases by producing artificial conversations between two copies of a dialogue
model, conditioning one conversational partner to state a name commonly
associated with a certain gender and/or race/ethnicity. We find that larger
capacity models tend to exhibit more gender bias and greater stereotyping of
occupations by gender. We show that several methods of tuning these dialogue
models, specifically name scrambling, controlled generation, and unlikelihood
training, are effective in reducing bias in conversation, including on a
downstream conversational task. Name scrambling is also effective in lowering
differences in token usage across conversations where partners have names
associated with different genders or races/ethnicities.",2021-09-07
"Naturalness Evaluation of Natural Language Generation in Task-oriented
  Dialogues using BERT",2021-09-07 08:40:14+00:00,http://arxiv.org/abs/2109.02938v1,"Ye Liu, Wolfgang Maier, Wolfgang Minker, Stefan Ultes","cs.CL, cs.AI",dialogue,"This paper presents an automatic method to evaluate the naturalness of
natural language generation in dialogue systems. While this task was previously
rendered through expensive and time-consuming human labor, we present this
novel task of automatic naturalness evaluation of generated language. By
fine-tuning the BERT model, our proposed naturalness evaluation method shows
robust results and outperforms the baselines: support vector machines,
bi-directional LSTMs, and BLEURT. In addition, the training speed and
evaluation performance of naturalness model are improved by transfer learning
from quality and informativeness linguistic knowledge.",2021-09-07
"SideControl: Controlled Open-domain Dialogue Generation via Additive
  Side Networks",2021-09-05 01:15:26+00:00,http://arxiv.org/abs/2109.01958v1,"Wanyu Du, Yangfeng Ji",cs.CL,dialogue,"Transformer-based pre-trained language models boost the performance of
open-domain dialogue systems. Prior works leverage Transformer-based
pre-trained language models to generate texts with desired attributes in two
general approaches: (1) gradient-based methods: updating all latent
representations of pre-trained models with gradients from attribute models; (2)
weighted-decoding methods: re-ranking beam candidates from pre-trained models
with attribute functions. However, gradient-based methods lead to high
computation cost and can easily get overfitted on small training sets, while
weighted-decoding methods are inherently constrained by the low-variance
high-bias pre-trained model. In this work, we propose a novel approach to
control the generation of Transformer-based pre-trained language models: the
SideControl framework, which leverages a novel control attributes loss to
incorporate useful control signals, and is shown to perform well with very
limited training samples. We evaluate our proposed method on two benchmark
open-domain dialogue datasets, and results show that the SideControl framework
has better controllability, higher generation quality and better
sample-efficiency than existing gradient-based and weighted-decoding baselines.",2021-09-05
Task-Oriented Dialogue System as Natural Language Generation,2021-08-31 08:36:42+00:00,http://arxiv.org/abs/2108.13679v2,"Weizhi Wang, Zhirui Zhang, Junliang Guo, Yinpei Dai, Boxing Chen, Weihua Luo","cs.CL, cs.AI",dialogue,"In this paper, we propose to formulate the task-oriented dialogue system as
the purely natural language generation task, so as to fully leverage the
large-scale pre-trained models like GPT-2 and simplify complicated
delexicalization prepossessing. However, directly applying this method heavily
suffers from the dialogue entity inconsistency caused by the removal of
delexicalized tokens, as well as the catastrophic forgetting problem of the
pre-trained model during fine-tuning, leading to unsatisfactory performance. To
alleviate these problems, we design a novel GPT-Adapter-CopyNet network, which
incorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve
better performance on transfer learning and dialogue entity generation.
Experimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ
dataset demonstrate that our proposed approach significantly outperforms
baseline models with a remarkable performance on automatic and human
evaluations.",2021-08-31
Semantic-based Self-Critical Training For Question Generation,2021-08-26 20:33:35+00:00,http://arxiv.org/abs/2108.12026v1,"Loïc, Kwate Dassi","cs.CL, cs.AI",dialogue,"We present in this work a fully Transformer-based reinforcement learning
generator-evaluator architecture for neural question generation. Question
generation is a task that consists in generating questions given a context and
answer. To improve the quality of the generated question, we came up with a
semantic-based self-critical training layout in generator-evaluator
architecture, which goes beyond typical maximum likelihood training. Evaluation
metrics for language modeling only based on n-gram overlapping do not consider
semantic relations between reference and candidate strings. To improve the
evaluation step, we assess our model for both n-gram overlap using BLEU and
semantically using BERTScore and NUBIA, a novel state-of-the-art evaluation
metric for text generation. Question generation could be used in many
downstream applications, including in extending question answering datasets,
conversational systems, and educational assessment systems.",2021-08-26
"Just Say No: Analyzing the Stance of Neural Dialogue Generation in
  Offensive Contexts",2021-08-26 14:58:05+00:00,http://arxiv.org/abs/2108.11830v1,"Ashutosh Baheti, Maarten Sap, Alan Ritter, Mark Riedl",cs.CL,dialogue,"Dialogue models trained on human conversations inadvertently learn to
generate offensive responses. Moreover, models can insult anyone by agreeing
with an offensive context. To understand the dynamics of contextually offensive
language, we study the stance of dialogue model responses in offensive Reddit
conversations. Specifically, we crowd-annotate ToxiChat, a new dataset of 2,000
Reddit threads and model responses labeled with offensive language and stance.
Our analysis reveals that 42% of user responses agree with toxic comments; 3x
their agreement with safe comments (13%). Pre-trained transformer-based
classifiers fine-tuned on our dataset achieve 0.71 F1 for offensive labels and
0.53 Macro-F1 for stance labels. Finally, we analyze some existing controllable
text generation (CTG) methods to mitigate the contextual offensive behavior of
dialogue models. Compared to the baseline, our best CTG model obtains a 19%
reduction in agreement with offensive context and 29% fewer offensive
responses. This highlights the need for future work to characterize and analyze
more forms of inappropriate behavior in dialogue models to help make them
safer. Our code and corpus are available at
https://github.com/abaheti95/ToxiChat .",2021-08-26
Viola: A Topic Agnostic Generate-and-Rank Dialogue System,2021-08-25 06:20:34+00:00,http://arxiv.org/abs/2108.11063v1,"Hyundong Cho, Basel Shbita, Kartik Shenoy, Shuai Liu, Nikhil Patel, Hitesh Pindikanti, Jennifer Lee, Jonathan May",cs.CL,dialogue,"We present Viola, an open-domain dialogue system for spoken conversation that
uses a topic-agnostic dialogue manager based on a simple generate-and-rank
approach. Leveraging recent advances of generative dialogue systems powered by
large language models, Viola fetches a batch of response candidates from
various neural dialogue models trained with different datasets and
knowledge-grounding inputs. Additional responses originating from
template-based generators are also considered, depending on the user's input
and detected entities. The hand-crafted generators build on a dynamic knowledge
graph injected with rich content that is crawled from the web and automatically
processed on a daily basis. Viola's response ranker is a fine-tuned polyencoder
that chooses the best response given the dialogue history. While dedicated
annotations for the polyencoder alone can indirectly steer it away from
choosing problematic responses, we add rule-based safety nets to detect neural
degeneration and a dedicated classifier to filter out offensive content. We
analyze conversations that Viola took part in for the Alexa Prize Socialbot
Grand Challenge 4 and discuss the strengths and weaknesses of our approach.
Lastly, we suggest future work with a focus on curating conversation data
specifcially for socialbots that will contribute towards a more robust
data-driven socialbot.",2021-08-25
"Using BERT Encoding and Sentence-Level Language Model for Sentence
  Ordering",2021-08-24 23:03:36+00:00,http://arxiv.org/abs/2108.10986v1,"Melika Golestani, Seyedeh Zahra Razavi, Zeinab Borhanifard, Farnaz Tahmasebian, Hesham Faili",cs.CL,dialogue,"Discovering the logical sequence of events is one of the cornerstones in
Natural Language Understanding. One approach to learn the sequence of events is
to study the order of sentences in a coherent text. Sentence ordering can be
applied in various tasks such as retrieval-based Question Answering, document
summarization, storytelling, text generation, and dialogue systems.
Furthermore, we can learn to model text coherence by learning how to order a
set of shuffled sentences. Previous research has relied on RNN, LSTM, and
BiLSTM architecture for learning text language models. However, these networks
have performed poorly due to the lack of attention mechanisms. We propose an
algorithm for sentence ordering in a corpus of short stories. Our proposed
method uses a language model based on Universal Transformers (UT) that captures
sentences' dependencies by employing an attention mechanism. Our method
improves the previous state-of-the-art in terms of Perfect Match Ratio (PMR)
score in the ROCStories dataset, a corpus of nearly 100K short human-made
stories. The proposed model includes three components: Sentence Encoder,
Language Model, and Sentence Arrangement with Brute Force Search. The first
component generates sentence embeddings using SBERT-WK pre-trained model
fine-tuned on the ROCStories data. Then a Universal Transformer network
generates a sentence-level language model. For decoding, the network generates
a candidate sentence as the following sentence of the current sentence. We use
cosine similarity as a scoring function to assign scores to the candidate
embedding and the embeddings of other sentences in the shuffled set. Then a
Brute Force Search is employed to maximize the sum of similarities between
pairs of consecutive sentences.",2021-08-24
Taming the Beast: Learning to Control Neural Conversational Models,2021-08-24 07:58:16+00:00,http://arxiv.org/abs/2108.10561v1,Andrea Madotto,"cs.CL, cs.AI, cs.LG",dialogue,"This thesis investigates the controllability of deep learning-based,
end-to-end, generative dialogue systems in both task-oriented and chit-chat
scenarios. In particular, we study the different aspects of controlling
generative dialogue systems, including controlling styles and topics and
continuously adding and combining dialogue skills. In the three decades since
the first dialogue system was commercialized, the basic architecture of such
systems has remained substantially unchanged, consisting of four pipelined
basic components, namely, natural language understanding (NLU), dialogue state
tracking (DST), a dialogue manager (DM) and natural language generation (NLG).
The dialogue manager, which is the critical component of the modularized
system, controls the response content and style. This module is usually
programmed by rules and is designed to be highly controllable and easily
extendable. With the emergence of powerful ""deep learning"" architectures,
end-to-end generative dialogue systems have been proposed to optimize overall
system performance and simplify training. However, these systems cannot be
easily controlled and extended as the modularized dialogue manager can. This is
because a single neural system is used, which is usually a large pre-trained
language model (e.g., GPT-2), and thus it is hard to surgically change
desirable attributes (e.g., style, topics, etc.). More importantly,
uncontrollable dialogue systems can generate offensive and even toxic
responses. Therefore, in this thesis, we study controllable methods for
end-to-end generative dialogue systems in task-oriented and chit-chat
scenarios. Throughout the chapters, we describe 1) how to control the style and
topics of chit-chat models, 2) how to continuously control and extend
task-oriented dialogue systems, and 3) how to compose and control multi-skill
dialogue models.",2021-08-24
CGEMs: A Metric Model for Automatic Code Generation using GPT-3,2021-08-23 13:28:57+00:00,http://arxiv.org/abs/2108.10168v1,"Aishwarya Narasimhan, Krishna Prasad Agara Venkatesha Rao, Veena M B",cs.AI,dialogue,"Today, AI technology is showing its strengths in almost every industry and
walks of life. From text generation, text summarization, chatbots, NLP is being
used widely. One such paradigm is automatic code generation. An AI could be
generating anything; hence the output space is unconstrained. A self-driving
car is driven for 100 million miles to validate its safety, but tests cannot be
written to monitor and cover an unconstrained space. One of the solutions to
validate AI-generated content is to constrain the problem and convert it from
abstract to realistic, and this can be accomplished by either validating the
unconstrained algorithm using theoretical proofs or by using Monte-Carlo
simulation methods. In this case, we use the latter approach to test/validate a
statistically significant number of samples. This hypothesis of validating the
AI-generated code is the main motive of this work and to know if AI-generated
code is reliable, a metric model CGEMs is proposed. This is an extremely
challenging task as programs can have different logic with different naming
conventions, but the metrics must capture the structure and logic of the
program. This is similar to the importance grammar carries in AI-based text
generation, Q&A, translations, etc. The various metrics that are garnered in
this work to support the evaluation of generated code are as follows:
Compilation, NL description to logic conversion, number of edits needed, some
of the commonly used static-code metrics and NLP metrics. These metrics are
applied to 80 codes generated using OpenAI's GPT-3. Post which a Neural network
is designed for binary classification (acceptable/not acceptable quality of the
generated code). The inputs to this network are the values of the features
obtained from the metrics. The model achieves a classification accuracy of
76.92% and an F1 score of 55.56%. XAI is augmented for model interpretability.",2021-08-23
"A Neural Conversation Generation Model via Equivalent Shared Memory
  Investigation",2021-08-20 13:20:14+00:00,http://arxiv.org/abs/2108.09164v1,"Changzhen Ji, Yating Zhang, Xiaozhong Liu, Adam Jatowt, Changlong Sun, Conghui Zhu, Tiejun Zhao",cs.CL,dialogue,"Conversation generation as a challenging task in Natural Language Generation
(NLG) has been increasingly attracting attention over the last years. A number
of recent works adopted sequence-to-sequence structures along with external
knowledge, which successfully enhanced the quality of generated conversations.
Nevertheless, few works utilized the knowledge extracted from similar
conversations for utterance generation. Taking conversations in customer
service and court debate domains as examples, it is evident that essential
entities/phrases, as well as their associated logic and inter-relationships can
be extracted and borrowed from similar conversation instances. Such information
could provide useful signals for improving conversation generation. In this
paper, we propose a novel reading and memory framework called Deep Reading
Memory Network (DRMN) which is capable of remembering useful information of
similar conversations for improving utterance generation. We apply our model to
two large-scale conversation datasets of justice and e-commerce fields.
Experiments prove that the proposed model outperforms the state-of-the-art
approaches.",2021-08-20
Sentence Semantic Regression for Text Generation,2021-08-06 07:35:59+00:00,http://arxiv.org/abs/2108.02984v1,"Wei Wang, Piji Li, Hai-Tao Zheng",cs.CL,dialogue,"Recall the classical text generation works, the generation framework can be
briefly divided into two phases: \textbf{idea reasoning} and \textbf{surface
realization}. The target of idea reasoning is to figure out the main idea which
will be presented in the following talking/writing periods. Surface realization
aims to arrange the most appropriate sentence to depict and convey the
information distilled from the main idea. However, the current popular
token-by-token text generation methods ignore this crucial process and suffer
from many serious issues, such as idea/topic drift. To tackle the problems and
realize this two-phase paradigm, we propose a new framework named Sentence
Semantic Regression (\textbf{SSR}) based on sentence-level language modeling.
For idea reasoning, two architectures \textbf{SSR-AR} and \textbf{SSR-NonAR}
are designed to conduct sentence semantic regression autoregressively (like
GPT2/3) and bidirectionally (like BERT). In the phase of surface realization, a
mixed-granularity sentence decoder is designed to generate text with better
consistency by jointly incorporating the predicted sentence-level main idea as
well as the preceding contextual token-level information. We conduct
experiments on four tasks of story ending prediction, story ending generation,
dialogue generation, and sentence infilling. The results show that SSR can
obtain better performance in terms of automatic metrics and human evaluation.",2021-08-06
Internet-Augmented Dialogue Generation,2021-07-15 19:00:35+00:00,http://arxiv.org/abs/2107.07566v1,"Mojtaba Komeili, Kurt Shuster, Jason Weston","cs.AI, cs.CL",dialogue,"The largest store of continually updating knowledge on our planet can be
accessed via internet search. In this work we study giving access to this
information to conversational agents. Large language models, even though they
store an impressive amount of knowledge within their weights, are known to
hallucinate facts when generating dialogue (Shuster et al., 2021); moreover,
those facts are frozen in time at the point of model training. In contrast, we
propose an approach that learns to generate an internet search query based on
the context, and then conditions on the search results to finally generate a
response, a method that can employ up-to-the-minute relevant information. We
train and evaluate such models on a newly collected dataset of human-human
conversations whereby one of the speakers is given access to internet search
during knowledgedriven discussions in order to ground their responses. We find
that search-query based access of the internet in conversation provides
superior performance compared to existing approaches that either use no
augmentation or FAISS-based retrieval (Lewis et al., 2020).",2021-07-15
A Survey on Dialogue Summarization: Recent Advances and New Frontiers,2021-07-07 12:11:14+00:00,http://arxiv.org/abs/2107.03175v1,"Xiachong Feng, Xiaocheng Feng, Bing Qin",cs.CL,dialogue,"With the development of dialogue systems and natural language generation
techniques, the resurgence of dialogue summarization has attracted significant
research attentions, which aims to condense the original dialogue into a
shorter version covering salient information. However, there remains a lack of
comprehensive survey for this task. To this end, we take the first step and
present a thorough review of this research field. In detail, we provide an
overview of publicly available research datasets, summarize existing works
according to the domain of input dialogue as well as organize leaderboards
under unified metrics. Furthermore, we discuss some future directions and give
our thoughts. We hope that this first survey of dialogue summarization can
provide the community with a quick access and a general picture to this task
and motivate future researches.",2021-07-07
"Do Encoder Representations of Generative Dialogue Models Encode
  Sufficient Information about the Task ?",2021-06-20 04:52:37+00:00,http://arxiv.org/abs/2106.10622v1,"Prasanna Parthasarathi, Joelle Pineau, Sarath Chandar",cs.CL,dialogue,"Predicting the next utterance in dialogue is contingent on encoding of users'
input text to generate appropriate and relevant response in data-driven
approaches. Although the semantic and syntactic quality of the language
generated is evaluated, more often than not, the encoded representation of
input is not evaluated. As the representation of the encoder is essential for
predicting the appropriate response, evaluation of encoder representation is a
challenging yet important problem. In this work, we showcase evaluating the
text generated through human or automatic metrics is not sufficient to
appropriately evaluate soundness of the language understanding of dialogue
models and, to that end, propose a set of probe tasks to evaluate encoder
representation of different language encoders commonly used in dialogue models.
From experiments, we observe that some of the probe tasks are easier and some
are harder for even sophisticated model architectures to learn. And, through
experiments we observe that RNN based architectures have lower performance on
automatic metrics on text generation than transformer model but perform better
than the transformer model on the probe tasks indicating that RNNs might
preserve task information better than the Transformers.",2021-06-20
Local Explanation of Dialogue Response Generation,2021-06-11 17:58:36+00:00,http://arxiv.org/abs/2106.06528v1,"Yi-Lin Tuan, Connor Pryor, Wenhu Chen, Lise Getoor, William Yang Wang","cs.CL, stat.ML",dialogue,"In comparison to the interpretation of classification models, the explanation
of sequence generation models is also an important problem, however it has seen
little attention. In this work, we study model-agnostic explanations of a
representative text generation task -- dialogue response generation. Dialog
response generation is challenging with its open-ended sentences and multiple
acceptable responses. To gain insights into the reasoning process of a
generation model, we propose anew method, local explanation of response
generation (LERG) that regards the explanations as the mutual interaction of
segments in input and output sentences. LERG views the sequence prediction as
uncertainty estimation of a human response and then creates explanations by
perturbing the input and calculating the certainty change over the human
response. We show that LERG adheres to desired properties of explanations for
text generation including unbiased approximation, consistency and cause
identification. Empirically, our results show that our method consistently
improves other widely used methods on proposed automatic- and human- evaluation
metrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can
extract both explicit and implicit relations between input and output segments.",2021-06-11
"AUGNLG: Few-shot Natural Language Generation using Self-trained Data
  Augmentation",2021-06-10 08:45:28+00:00,http://arxiv.org/abs/2106.05589v1,"Xinnuo Xu, Guoyin Wang, Young-Bum Kim, Sungjin Lee",cs.CL,dialogue,"Natural Language Generation (NLG) is a key component in a task-oriented
dialogue system, which converts the structured meaning representation (MR) to
the natural language. For large-scale conversational systems, where it is
common to have over hundreds of intents and thousands of slots, neither
template-based approaches nor model-based approaches are scalable. Recently,
neural NLGs started leveraging transfer learning and showed promising results
in few-shot settings. This paper proposes AUGNLG, a novel data augmentation
approach that combines a self-trained neural retrieval model with a few-shot
learned NLU model, to automatically create MR-to-Text data from open-domain
texts. The proposed system mostly outperforms the state-of-the-art methods on
the FewShotWOZ data in both BLEU and Slot Error Rate. We further confirm
improved results on the FewShotSGD data and provide comprehensive analysis
results on key components of our system. Our code and data are available at
https://github.com/XinnuoXu/AugNLG.",2021-06-10
Defending against Backdoor Attacks in Natural Language Generation,2021-06-03 13:00:28+00:00,http://arxiv.org/abs/2106.01810v1,"Chun Fan, Xiaoya Li, Yuxian Meng, Xiaofei Sun, Xiang Ao, Fei Wu, Jiwei Li, Tianwei Zhang",cs.CL,dialogue,"The frustratingly fragile nature of neural network models make current
natural language generation (NLG) systems prone to backdoor attacks and
generate malicious sequences that could be sexist or offensive. Unfortunately,
little effort has been invested to how backdoor attacks can affect current NLG
models and how to defend against these attacks. In this work, we investigate
this problem on two important NLG tasks, machine translation and dialogue
generation. By giving a formal definition for backdoor attack and defense, and
developing corresponding benchmarks, we design methods to attack NLG models,
which achieve high attack success to ask NLG models to generate malicious
sequences. To defend against these attacks, we propose to detect the attack
trigger by examining the effect of deleting or replacing certain words on the
generation outputs, which we find successful for certain types of attacks. We
will discuss the limitation of this work, and hope this work can raise the
awareness of backdoor risks concealed in deep NLG systems. (Code and data are
available at https://github.com/ShannonAI/backdoor_nlg.)",2021-06-03
"Generate, Prune, Select: A Pipeline for Counterspeech Generation against
  Online Hate Speech",2021-06-03 06:54:03+00:00,http://arxiv.org/abs/2106.01625v1,"Wanzheng Zhu, Suma Bhat",cs.CL,dialogue,"Countermeasures to effectively fight the ever increasing hate speech online
without blocking freedom of speech is of great social interest. Natural
Language Generation (NLG), is uniquely capable of developing scalable
solutions. However, off-the-shelf NLG methods are primarily
sequence-to-sequence neural models and they are limited in that they generate
commonplace, repetitive and safe responses regardless of the hate speech (e.g.,
""Please refrain from using such language."") or irrelevant responses, making
them ineffective for de-escalating hateful conversations. In this paper, we
design a three-module pipeline approach to effectively improve the diversity
and relevance. Our proposed pipeline first generates various counterspeech
candidates by a generative model to promote diversity, then filters the
ungrammatical ones using a BERT model, and finally selects the most relevant
counterspeech response using a novel retrieval-based method. Extensive
Experiments on three representative datasets demonstrate the efficacy of our
approach in generating diverse and relevant counterspeech.",2021-06-03
"Detecting Bot-Generated Text by Characterizing Linguistic Accommodation
  in Human-Bot Interactions",2021-06-02 14:10:28+00:00,http://arxiv.org/abs/2106.01170v1,"Paras Bhatt, Anthony Rios",cs.CL,dialogue,"Language generation models' democratization benefits many domains, from
answering health-related questions to enhancing education by providing
AI-driven tutoring services. However, language generation models'
democratization also makes it easier to generate human-like text at-scale for
nefarious activities, from spreading misinformation to targeting specific
groups with hate speech. Thus, it is essential to understand how people
interact with bots and develop methods to detect bot-generated text. This paper
shows that bot-generated text detection methods are more robust across datasets
and models if we use information about how people respond to it rather than
using the bot's text directly. We also analyze linguistic alignment, providing
insight into differences between human-human and human-bot conversations.",2021-06-02
"NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based
  Simulation",2021-05-30 07:54:54+00:00,http://arxiv.org/abs/2105.14454v1,"Sungdong Kim, Minsuk Chang, Sang-Woo Lee",cs.CL,dialogue,"We propose NeuralWOZ, a novel dialogue collection framework that uses
model-based dialogue simulation. NeuralWOZ has two pipelined models, Collector
and Labeler. Collector generates dialogues from (1) user's goal instructions,
which are the user context and task constraints in natural language, and (2)
system's API call results, which is a list of possible query responses for user
requests from the given knowledge base. Labeler annotates the generated
dialogue by formulating the annotation as a multiple-choice problem, in which
the candidate labels are extracted from goal instructions and API call results.
We demonstrate the effectiveness of the proposed method in the zero-shot domain
transfer learning for dialogue state tracking. In the evaluation, the synthetic
dialogue corpus generated from NeuralWOZ achieves a new state-of-the-art with
improvements of 4.4% point joint goal accuracy on average across domains, and
improvements of 5.7% point of zero-shot coverage against the MultiWOZ 2.1
dataset.",2021-05-30
OTTers: One-turn Topic Transitions for Open-Domain Dialogue,2021-05-28 10:16:59+00:00,http://arxiv.org/abs/2105.13710v1,"Karin Sevegnani, David M. Howcroft, Ioannis Konstas, Verena Rieser",cs.CL,dialogue,"Mixed initiative in open-domain dialogue requires a system to pro-actively
introduce new topics. The one-turn topic transition task explores how a system
connects two topics in a cooperative and coherent manner. The goal of the task
is to generate a ""bridging"" utterance connecting the new topic to the topic of
the previous conversation turn. We are especially interested in commonsense
explanations of how a new topic relates to what has been mentioned before. We
first collect a new dataset of human one-turn topic transitions, which we call
OTTers. We then explore different strategies used by humans when asked to
complete such a task, and notice that the use of a bridging utterance to
connect the two topics is the approach used the most. We finally show how
existing state-of-the-art text generation models can be adapted to this task
and examine the performance of these baselines on different splits of the
OTTers data.",2021-05-28
Generative Adversarial Imitation Learning for Empathy-based AI,2021-05-27 17:37:37+00:00,http://arxiv.org/abs/2105.13328v1,"Pratyush Muthukumar, Karishma Muthukumar, Deepan Muthirayan, Pramod Khargonekar",cs.CL,dialogue,"Generative adversarial imitation learning (GAIL) is a model-free algorithm
that has been shown to provide strong results in imitating complex behaviors in
high-dimensional environments. In this paper, we utilize the GAIL model for
text generation to develop empathy-based context-aware conversational AI. Our
model uses an expert trajectory of empathetic prompt-response dialogues which
can accurately exhibit the correct empathetic emotion when generating a
response. The Generator of the GAIL model uses the GPT-2 sequential pre-trained
language model trained on 117 million parameters from 40 GB of internet data.
We propose a novel application of an approach used in transfer learning to fine
tune the GPT-2 model in order to generate concise, user-specific empathetic
responses validated against the Discriminator. Our novel GAIL model utilizes a
sentiment analysis history-based reinforcement learning approach to
empathetically respond to human interactions in a personalized manner. We find
that our model's response scores on various human-generated prompts collected
from the Facebook Empathetic Dialogues dataset outperform baseline
counterparts. Moreover, our model improves upon various history-based
conversational AI models developed recently, as our model's performance over a
sustained conversation of 3 or more interactions outperform similar
conversational AI models.",2021-05-27
"Empirical Error Modeling Improves Robustness of Noisy Neural Sequence
  Labeling",2021-05-25 12:15:45+00:00,http://arxiv.org/abs/2105.11872v1,"Marcin Namysl, Sven Behnke, Joachim Köhler",cs.CL,dialogue,"Despite recent advances, standard sequence labeling systems often fail when
processing noisy user-generated text or consuming the output of an Optical
Character Recognition (OCR) process. In this paper, we improve the noise-aware
training method by proposing an empirical error generation approach that
employs a sequence-to-sequence model trained to perform translation from
error-free to erroneous text. Using an OCR engine, we generated a large
parallel text corpus for training and produced several real-world noisy
sequence labeling benchmarks for evaluation. Moreover, to overcome the data
sparsity problem that exacerbates in the case of imperfect textual input, we
learned noisy language model-based embeddings. Our approach outperformed the
baseline noise generation and error correction techniques on the erroneous
sequence labeling data sets. To facilitate future research on robustness, we
make our code, embeddings, and data conversion scripts publicly available.",2021-05-25
"Towards a Universal NLG for Dialogue Systems and Simulators with Future
  Bridging",2021-05-21 10:37:10+00:00,http://arxiv.org/abs/2105.10267v2,"Philipp Ennen, Yen-Ting Lin, Ali Girayhan Ozbay, Ferdinando Insalata, Maolin Li, Ye Tian, Sepehr Jalali, Da-shan Shiu","cs.CL, cs.AI, cs.LG",dialogue,"In a dialogue system pipeline, a natural language generation (NLG) unit
converts the dialogue direction and content to a corresponding natural language
realization. A recent trend for dialogue systems is to first pre-train on large
datasets and then fine-tune in a supervised manner using datasets annotated
with application-specific features. Though novel behaviours can be learned from
custom annotation, the required effort severely bounds the quantity of the
training set, and the application-specific nature limits the reuse. In light of
the recent success of data-driven approaches, we propose the novel future
bridging NLG (FBNLG) concept for dialogue systems and simulators. The critical
step is for an FBNLG to accept a future user or system utterance to bridge the
present context towards. Future bridging enables self supervised training over
annotation-free datasets, decoupled the training of NLG from the rest of the
system. An FBNLG, pre-trained with massive datasets, is expected to apply in
classical or new dialogue scenarios with minimal adaptation effort. We evaluate
a prototype FBNLG to show that future bridging can be a viable approach to a
universal few-shot NLG for task-oriented and chit-chat dialogues.",2021-05-21
Retrieval-Augmented Transformer-XL for Close-Domain Dialog Generation,2021-05-19 16:34:33+00:00,http://arxiv.org/abs/2105.09235v1,"Giovanni Bonetta, Rossella Cancelliere, Ding Liu, Paul Vozila","cs.CL, cs.AI",dialogue,"Transformer-based models have demonstrated excellent capabilities of
capturing patterns and structures in natural language generation and achieved
state-of-the-art results in many tasks. In this paper we present a
transformer-based model for multi-turn dialog response generation. Our solution
is based on a hybrid approach which augments a transformer-based generative
model with a novel retrieval mechanism, which leverages the memorized
information in the training data via k-Nearest Neighbor search. Our system is
evaluated on two datasets made by customer/assistant dialogs: the Taskmaster-1,
released by Google and holding high quality, goal-oriented conversational data
and a proprietary dataset collected from a real customer service call center.
Both achieve better BLEU scores over strong baselines.",2021-05-19
"Retrieval-Free Knowledge-Grounded Dialogue Response Generation with
  Adapters",2021-05-13 12:33:23+00:00,http://arxiv.org/abs/2105.06232v1,"Yan Xu, Etsuko Ishii, Zihan Liu, Genta Indra Winata, Dan Su, Andrea Madotto, Pascale Fung","cs.CL, cs.AI",dialogue,"To diversify and enrich generated dialogue responses, knowledge-grounded
dialogue has been investigated in recent years. Despite the success of the
existing methods, they mainly follow the paradigm of retrieving the relevant
sentences over a large corpus and augment the dialogues with explicit extra
information, which is time- and resource-consuming. In this paper, we propose
KnowExpert, an end-to-end framework to bypass the retrieval process by
injecting prior knowledge into the pre-trained language models with lightweight
adapters. To the best of our knowledge, this is the first attempt to tackle
this task relying solely on a generation-based approach. Experimental results
show that KnowExpert performs comparably with the retrieval-based baselines,
demonstrating the potential of our proposed direction.",2021-05-13
Focused Attention Improves Document-Grounded Generation,2021-04-26 16:56:29+00:00,http://arxiv.org/abs/2104.12714v1,"Shrimai Prabhumoye, Kazuma Hashimoto, Yingbo Zhou, Alan W Black, Ruslan Salakhutdinov",cs.CL,dialogue,"Document grounded generation is the task of using the information provided in
a document to improve text generation. This work focuses on two different
document grounded generation tasks: Wikipedia Update Generation task and
Dialogue response generation. Our work introduces two novel adaptations of
large scale pre-trained encoder-decoder models focusing on building context
driven representation of the document and enabling specific attention to the
information in the document. Additionally, we provide a stronger BART baseline
for these tasks. Our proposed techniques outperform existing methods on both
automated (at least 48% increase in BLEU-4 points) and human evaluation for
closeness to reference and relevance to the document. Furthermore, we perform
comprehensive manual inspection of the generated output and categorize errors
to provide insights into future directions in modeling these tasks.",2021-04-26
"Estimating Subjective Crowd-Evaluations as an Additional Objective to
  Improve Natural Language Generation",2021-04-12 06:33:16+00:00,http://arxiv.org/abs/2104.05224v1,"Jakob Nyberg, Ramesh Manuvinakurike, Maike Paetzel-Prüsmann",cs.CL,dialogue,"Human ratings are one of the most prevalent methods to evaluate the
performance of natural language processing algorithms. Similarly, it is common
to measure the quality of sentences generated by a natural language generation
model using human raters. In this paper, we argue for exploring the use of
subjective evaluations within the process of training language generation
models in a multi-task learning setting. As a case study, we use a
crowd-authored dialogue corpus to fine-tune six different language generation
models. Two of these models incorporate multi-task learning and use subjective
ratings of lines as part of an explicit learning goal. A human evaluation of
the generated dialogue lines reveals that utterances generated by the
multi-tasking models were subjectively rated as the most typical, most moving
the conversation forward, and least offensive. Based on these promising first
results, we discuss future research directions for incorporating subjective
human evaluations into language model training and to hence keep the human user
in the loop during the development process.",2021-04-12
"Overprotective Training Environments Fall Short at Testing Time: Let
  Models Contribute to Their Own Training",2021-03-20 09:55:50+00:00,http://arxiv.org/abs/2103.11145v1,"Alberto Testoni, Raffaella Bernardi",cs.CL,dialogue,"Despite important progress, conversational systems often generate dialogues
that sound unnatural to humans. We conjecture that the reason lies in their
different training and testing conditions: agents are trained in a controlled
""lab"" setting but tested in the ""wild"". During training, they learn to generate
an utterance given the human dialogue history. On the other hand, during
testing, they must interact with each other, and hence deal with noisy data. We
propose to fill this gap by training the model with mixed batches containing
both samples of human and machine-generated dialogues. We assess the validity
of the proposed method on",2021-03-20
Causal-aware Safe Policy Improvement for Task-oriented dialogue,2021-03-10 22:34:28+00:00,http://arxiv.org/abs/2103.06370v1,"Govardana Sachithanandam Ramachandran, Kazuma Hashimoto, Caiming Xiong","cs.CL, cs.AI, cs.LG",dialogue,"The recent success of reinforcement learning's (RL) in solving complex tasks
is most often attributed to its capacity to explore and exploit an environment
where it has been trained. Sample efficiency is usually not an issue since
cheap simulators are available to sample data on-policy. On the other hand,
task oriented dialogues are usually learnt from offline data collected using
human demonstrations. Collecting diverse demonstrations and annotating them is
expensive. Unfortunately, use of RL methods trained on off-policy data are
prone to issues of bias and generalization, which are further exacerbated by
stochasticity in human response and non-markovian belief state of a dialogue
management system. To this end, we propose a batch RL framework for task
oriented dialogue policy learning: causal aware safe policy improvement
(CASPI). This method gives guarantees on dialogue policy's performance and also
learns to shape rewards according to intentions behind human responses, rather
than just mimicking demonstration data; this couple with batch-RL helps overall
with sample efficiency of the framework. We demonstrate the effectiveness of
this framework on a dialogue-context-to-text Generation and end-to-end dialogue
task of the Multiwoz2.0 dataset. The proposed method outperforms the current
state of the art on these metrics, in both case. In the end-to-end case, our
method trained only on 10\% of the data was able to out perform current state
in three out of four evaluation metrics.",2021-03-10
"Empathetic BERT2BERT Conversational Model: Learning Arabic Language
  Generation with Little Data",2021-03-07 13:23:51+00:00,http://arxiv.org/abs/2103.04353v1,"Tarek Naous, Wissam Antoun, Reem A. Mahmoud, Hazem Hajj",cs.CL,dialogue,"Enabling empathetic behavior in Arabic dialogue agents is an important aspect
of building human-like conversational models. While Arabic Natural Language
Processing has seen significant advances in Natural Language Understanding
(NLU) with language models such as AraBERT, Natural Language Generation (NLG)
remains a challenge. The shortcomings of NLG encoder-decoder models are
primarily due to the lack of Arabic datasets suitable to train NLG models such
as conversational agents. To overcome this issue, we propose a
transformer-based encoder-decoder initialized with AraBERT parameters. By
initializing the weights of the encoder and decoder with AraBERT pre-trained
weights, our model was able to leverage knowledge transfer and boost
performance in response generation. To enable empathy in our conversational
model, we train it using the ArabicEmpatheticDialogues dataset and achieve high
performance in empathetic response generation. Specifically, our model achieved
a low perplexity value of 17.0 and an increase in 5 BLEU points compared to the
previous state-of-the-art model. Also, our proposed model was rated highly by
85 human evaluators, validating its high capability in exhibiting empathy while
generating relevant and fluent responses in open-domain settings.",2021-03-07
Towards Conversational Humor Analysis and Design,2021-02-28 15:22:57+00:00,http://arxiv.org/abs/2103.00536v1,"Tanishq Chaudhary, Mayank Goel, Radhika Mamidi","cs.CL, cs.HC, cs.LG",dialogue,"Well-defined jokes can be divided neatly into a setup and a punchline. While
most works on humor today talk about a joke as a whole, the idea of generating
punchlines to a setup has applications in conversational humor, where funny
remarks usually occur with a non-funny context. Thus, this paper is based
around two core concepts: Classification and the Generation of a punchline from
a particular setup based on the Incongruity Theory. We first implement a
feature-based machine learning model to classify humor. For humor generation,
we use a neural model, and then merge the classical rule-based approaches with
the neural approach to create a hybrid model. The idea behind being: combining
insights gained from other tasks with the setup-punchline model and thus
applying it to existing text generation approaches. We then use and compare our
model with human written jokes with the help of human evaluators in a
double-blind study.",2021-02-28
"BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language
  Generation",2021-01-27 22:07:03+00:00,http://arxiv.org/abs/2101.11718v1,"Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, Rahul Gupta","cs.CL, cs.AI, cs.LG",dialogue,"Recent advances in deep learning techniques have enabled machines to generate
cohesive open-ended text when prompted with a sequence of words as context.
While these models now empower many downstream applications from conversation
bots to automatic storytelling, they have been shown to generate texts that
exhibit social biases. To systematically study and benchmark social biases in
open-ended language generation, we introduce the Bias in Open-Ended Language
Generation Dataset (BOLD), a large-scale dataset that consists of 23,679
English text generation prompts for bias benchmarking across five domains:
profession, gender, race, religion, and political ideology. We also propose new
automated metrics for toxicity, psycholinguistic norms, and text gender
polarity to measure social biases in open-ended text generation from multiple
angles. An examination of text generated from three popular language models
reveals that the majority of these models exhibit a larger social bias than
human-written Wikipedia text across all domains. With these results we
highlight the need to benchmark biases in open-ended language generation and
caution users of language generation models on downstream tasks to be cognizant
of these embedded prejudices.",2021-01-27
"An Empirical Study of Cross-Lingual Transferability in Generative
  Dialogue State Tracker",2021-01-27 12:45:55+00:00,http://arxiv.org/abs/2101.11360v1,"Yen-Ting Lin, Yun-Nung Chen",cs.CL,dialogue,"There has been a rapid development in data-driven task-oriented dialogue
systems with the benefit of large-scale datasets. However, the progress of
dialogue systems in low-resource languages lags far behind due to the lack of
high-quality data. To advance the cross-lingual technology in building dialog
systems, DSTC9 introduces the task of cross-lingual dialog state tracking,
where we test the DST module in a low-resource language given the rich-resource
training dataset.
  This paper studies the transferability of a cross-lingual generative dialogue
state tracking system using a multilingual pre-trained seq2seq model. We
experiment under different settings, including joint-training or pre-training
on cross-lingual and cross-ontology datasets. We also find out the low
cross-lingual transferability of our approaches and provides investigation and
discussion.",2021-01-27
BERT Transformer model for Detecting Arabic GPT2 Auto-Generated Tweets,2021-01-22 21:50:38+00:00,http://arxiv.org/abs/2101.09345v1,"Fouzi Harrag, Maria Debbah, Kareem Darwish, Ahmed Abdelali",cs.CL,dialogue,"During the last two decades, we have progressively turned to the Internet and
social media to find news, entertain conversations and share opinion. Recently,
OpenAI has developed a ma-chine learning system called GPT-2 for Generative
Pre-trained Transformer-2, which can pro-duce deepfake texts. It can generate
blocks of text based on brief writing prompts that look like they were written
by humans, facilitating the spread false or auto-generated text. In line with
this progress, and in order to counteract potential dangers, several methods
have been pro-posed for detecting text written by these language models. In
this paper, we propose a transfer learning based model that will be able to
detect if an Arabic sentence is written by humans or automatically generated by
bots. Our dataset is based on tweets from a previous work, which we have
crawled and extended using the Twitter API. We used GPT2-Small-Arabic to
generate fake Arabic Sentences. For evaluation, we compared different recurrent
neural network (RNN) word embeddings based baseline models, namely: LSTM,
BI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new
transfer-learning model has obtained an accuracy up to 98%. To the best of our
knowledge, this work is the first study where ARABERT and GPT2 were combined to
detect and classify the Arabic auto-generated texts.",2021-01-22
Transforming Multi-Conditioned Generation from Meaning Representation,2021-01-12 01:45:06+00:00,http://arxiv.org/abs/2101.04257v1,Joosung Lee,"cs.CL, cs.AI",dialogue,"In task-oriented conversation systems, natural language generation systems
that generate sentences with specific information related to conversation flow
are useful. Our study focuses on language generation by considering various
information representing the meaning of utterances as multiple conditions of
generation. NLG from meaning representations, the conditions for sentence
meaning, generally goes through two steps: sentence planning and surface
realization. However, we propose a simple one-stage framework to generate
utterances directly from MR (Meaning Representation). Our model is based on
GPT2 and generates utterances with flat conditions on slot and value pairs,
which does not need to determine the structure of the sentence. We evaluate
several systems in the E2E dataset with 6 automatic metrics. Our system is a
simple method, but it demonstrates comparable performance to previous systems
in automated metrics. In addition, using only 10\% of the data set without any
other techniques, our model achieves comparable performance, and shows the
possibility of performing zero-shot generation and expanding to other datasets.",2021-01-12
Continual Learning in Task-Oriented Dialogue Systems,2020-12-31 08:44:25+00:00,http://arxiv.org/abs/2012.15504v1,"Andrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Seungwhan Moon, Paul Crook, Bing Liu, Zhou Yu, Eunjoon Cho, Zhiguang Wang","cs.CL, cs.AI",dialogue,"Continual learning in task-oriented dialogue systems can allow us to add new
domains and functionalities through time without incurring the high cost of a
whole system retraining. In this paper, we propose a continual learning
benchmark for task-oriented dialogue systems with 37 domains to be learned
continuously in four settings, such as intent recognition, state tracking,
natural language generation, and end-to-end. Moreover, we implement and compare
multiple existing continual learning baselines, and we propose a simple yet
effective architectural method based on residual adapters. Our experiments
demonstrate that the proposed architectural method and a simple replay-based
strategy perform comparably well but they both achieve inferior performance to
the multi-task learning baseline, in where all the data are shown at once,
showing that continual learning in task-oriented dialogue systems is a
challenging task. Furthermore, we reveal several trade-offs between different
continual learning methods in term of parameter usage and memory size, which
are important in the design of a task-oriented dialogue system. The proposed
benchmark is released together with several baselines to promote more research
in this direction.",2020-12-31
"OpenViDial: A Large-Scale, Open-Domain Dialogue Dataset with Visual
  Contexts",2020-12-30 03:02:50+00:00,http://arxiv.org/abs/2012.15015v1,"Yuxian Meng, Shuhe Wang, Qinghong Han, Xiaofei Sun, Fei Wu, Rui Yan, Jiwei Li",cs.CL,dialogue,"When humans converse, what a speaker will say next significantly depends on
what he sees. Unfortunately, existing dialogue models generate dialogue
utterances only based on preceding textual contexts, and visual contexts are
rarely considered. This is due to a lack of a large-scale multi-module dialogue
dataset with utterances paired with visual contexts. In this paper, we release
{\bf OpenViDial}, a large-scale multi-module dialogue dataset. The dialogue
turns and visual contexts are extracted from movies and TV series, where each
dialogue turn is paired with the corresponding visual context in which it takes
place. OpenViDial contains a total number of 1.1 million dialogue turns, and
thus 1.1 million visual contexts stored in images. Based on this dataset, we
propose a family of encoder-decoder models leveraging both textual and visual
contexts, from coarse-grained image features extracted from CNNs to
fine-grained object features extracted from Faster R-CNNs. We observe that
visual information significantly improves dialogue generation qualities,
verifying the necessity of integrating multi-modal features for dialogue
learning. Our work marks an important step towards large-scale multi-modal
dialogue learning.",2020-12-30
"Interpretable NLG for Task-oriented Dialogue Systems with Heterogeneous
  Rendering Machines",2020-12-29 07:41:48+00:00,http://arxiv.org/abs/2012.14645v2,"Yangming Li, Kaisheng Yao",cs.CL,dialogue,"End-to-end neural networks have achieved promising performances in natural
language generation (NLG). However, they are treated as black boxes and lack
interpretability. To address this problem, we propose a novel framework,
heterogeneous rendering machines (HRM), that interprets how neural generators
render an input dialogue act (DA) into an utterance. HRM consists of a renderer
set and a mode switcher. The renderer set contains multiple decoders that vary
in both structure and functionality. For every generation step, the mode
switcher selects an appropriate decoder from the renderer set to generate an
item (a word or a phrase). To verify the effectiveness of our method, we have
conducted extensive experiments on 5 benchmark datasets. In terms of automatic
metrics (e.g., BLEU), our model is competitive with the current
state-of-the-art method. The qualitative analysis shows that our model can
interpret the rendering process of neural generators well. Human evaluation
also confirms the interpretability of our proposed approach.",2020-12-29
