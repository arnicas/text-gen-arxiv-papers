title,pubdate,id,authors,categories,search,abstract,displaydate
TextBox 2.0: A Text Generation Library with Pre-trained Language Models,2022-12-26 03:50:36+00:00,http://arxiv.org/abs/2212.13005v1,"Tianyi Tang, Junyi Li, Zhipeng Chen, Yiwen Hu, Zhuohao Yu, Wenxun Dai, Zican Dong, Xiaoxue Cheng, Yuhao Wang, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen",cs.CL,dialogue,"To facilitate research on text generation, this paper presents a
comprehensive and unified library, TextBox 2.0, focusing on the use of
pre-trained language models (PLMs). To be comprehensive, our library covers
$13$ common text generation tasks and their corresponding $83$ datasets and
further incorporates $45$ PLMs covering general, translation, Chinese,
dialogue, controllable, distilled, prompting, and lightweight PLMs. We also
implement $4$ efficient training strategies and provide $4$ generation
objectives for pre-training new PLMs from scratch. To be unified, we design the
interfaces to support the entire research pipeline (from data loading to
training and evaluation), ensuring that each step can be fulfilled in a unified
way. Despite the rich functionality, it is easy to use our library, either
through the friendly Python API or command line. To validate the effectiveness
of our library, we conduct extensive experiments and exemplify four types of
research scenarios. The project is released at the link:
https://github.com/RUCAIBox/TextBox.",2022-12-26
"On Realization of Intelligent Decision-Making in the Real World: A
  Foundation Decision Model Perspective",2022-12-24 06:16:45+00:00,http://arxiv.org/abs/2212.12669v1,"Ying Wen, Ziyu Wan, Ming Zhou, Shufang Hou, Zhe Cao, Chenyang Le, Jingxiao Chen, Zheng Tian, Weinan Zhang, Jun Wang","cs.AI, cs.LG",dialogue,"Our situated environment is full of uncertainty and highly dynamic, thus
hindering the widespread adoption of machine-led Intelligent Decision-Making
(IDM) in real world scenarios. This means IDM should have the capability of
continuously learning new skills and efficiently generalizing across wider
applications. IDM benefits from any new approaches and theoretical
breakthroughs that exhibit Artificial General Intelligence (AGI) breaking the
barriers between tasks and applications. Recent research has well-examined
neural architecture, Transformer, as a backbone foundation model and its
generalization to various tasks, including computer vision, natural language
processing, and reinforcement learning. We therefore argue that a foundation
decision model (FDM) can be established by formulating various decision-making
tasks as a sequence decoding task using the Transformer architecture; this
would be a promising solution to advance the applications of IDM in more
complex real world tasks. In this paper, we elaborate on how a foundation
decision model improves the efficiency and generalization of IDM. We also
discuss potential applications of a FDM in multi-agent game AI, production
scheduling, and robotics tasks. Finally, through a case study, we demonstrate
our realization of the FDM, DigitalBrain (DB1) with 1.2 billion parameters,
which achieves human-level performance over 453 tasks, including text
generation, images caption, video games playing, robotic control, and traveling
salesman problems. As a foundation decision model, DB1 would be a baby step
towards more autonomous and efficient real world IDM applications.",2022-12-24
Ontologically Faithful Generation of Non-Player Character Dialogues,2022-12-20 19:48:10+00:00,http://arxiv.org/abs/2212.10618v1,"Nathaniel Weir, Ryan Thomas, Randolph D'Amore, Kellie Hill, Benjamin Van Durme, Harsh Jhamtani",cs.CL,dialogue,"We introduce a language generation task grounded in a popular video game
environment. KNUDGE (KNowledge Constrained User-NPC Dialogue GEneration)
involves generating dialogue trees conditioned on an ontology captured in
natural language passages providing quest and entity specifications. KNUDGE is
constructed from side quest dialogues drawn directly from game data of Obsidian
Entertainment's The Outer Worlds, leading to real-world complexities in
generation: (1) dialogues are branching trees as opposed to linear chains of
utterances; (2) utterances must remain faithful to the game lore--character
personas, backstories, and entity relationships; and (3) a dialogue must
accurately reveal new quest-related details to the human player. We report
results for supervised and in-context learning techniques, finding there is
significant room for future work on creating realistic game-quality dialogues.",2022-12-20
Enhancing Task Bot Engagement with Synthesized Open-Domain Dialog,2022-12-20 05:51:47+00:00,http://arxiv.org/abs/2212.10008v1,"Miaoran Li, Baolin Peng, Michel Galley, Jianfeng Gao, Zhu Zhang","cs.CL, cs.AI",dialogue,"Many efforts have been made to construct dialog systems for different types
of conversations, such as task-oriented dialog (TOD) and open-domain dialog
(ODD). To better mimic human-level conversations that usually fuse various
dialog modes, it is essential to build a system that can effectively handle
both TOD and ODD and access different knowledge sources. To address the lack of
available data for the fused task, we propose a framework for automatically
generating dialogues that combine knowledge-grounded ODDs and TODs in various
settings. Additionally, we introduce a unified model PivotBot that is capable
of appropriately adopting TOD and ODD modes and accessing different knowledge
sources in order to effectively tackle the fused task. Evaluation results
demonstrate the superior ability of the proposed model to switch seamlessly
between TOD and ODD tasks.",2022-12-20
"Future Sight: Dynamic Story Generation with Large Pretrained Language
  Models",2022-12-20 01:53:26+00:00,http://arxiv.org/abs/2212.09947v1,"Brian D. Zimmerman, Gaurav Sahu, Olga Vechtomova","cs.CL, cs.AI, cs.LG",dialogue,"Recent advances in deep learning research, such as transformers, have
bolstered the ability for automated agents to generate creative texts similar
to those that a human would write. By default, transformer decoders can only
generate new text with respect to previously generated text. The output
distribution of candidate tokens at any position is conditioned on previously
selected tokens using a self-attention mechanism to emulate the property of
autoregression. This is inherently limiting for tasks such as controllable
story generation where it may be necessary to condition on future plot events
when writing a story. In this work, we propose Future Sight, a method for
finetuning a pretrained generative transformer on the task of future
conditioning. Transformer decoders are typically pretrained on the task of
completing a context, one token at a time, by means of self-attention. Future
Sight additionally enables a decoder to attend to an encoded future plot event.
This motivates the decoder to expand on the context in a way that logically
concludes with the provided future. During inference, the future plot event can
be written by a human author to steer the narrative being generated in a
certain direction. We evaluate the efficacy of our approach on a story
generation task with human evaluators.",2022-12-20
SEScore2: Retrieval Augmented Pretraining for Text Generation Evaluation,2022-12-19 09:02:16+00:00,http://arxiv.org/abs/2212.09305v1,"Wenda Xu, Xian Qian, Mingxuan Wang, Lei Li, William Yang Wang",cs.CL,dialogue,"Is it possible to leverage large scale raw and raw parallel corpora to build
a general learned metric? Existing learned metrics have gaps to human
judgements, are model-dependent or are limited to the domains or tasks where
human ratings are available. In this paper, we propose SEScore2, a model-based
metric pretrained over million-scale synthetic dataset constructed by our novel
retrieval augmented data synthesis pipeline. SEScore2 achieves high correlation
to human judgements without any human rating supervisions. Importantly, our
unsupervised SEScore2 can outperform supervised metrics, which are trained on
the News human ratings, at the TED domain. We evaluate SEScore2 over four text
generation tasks across three languages. SEScore2 outperforms all prior
unsupervised evaluation metrics in machine translation, speech translation,
data-to-text and dialogue generation, with average Kendall improvements 0.158.
SEScore2 even outperforms SOTA supervised BLEURT at data-to-text, dialogue
generation and overall correlation.",2022-12-19
ChatGPT: The End of Online Exam Integrity?,2022-12-19 08:15:16+00:00,http://arxiv.org/abs/2212.09292v1,Teo Susnjak,"cs.AI, cs.CL",dialogue,"This study evaluated the ability of ChatGPT, a recently developed artificial
intelligence (AI) agent, to perform high-level cognitive tasks and produce text
that is indistinguishable from human-generated text. This capacity raises
concerns about the potential use of ChatGPT as a tool for academic misconduct
in online exams. The study found that ChatGPT is capable of exhibiting critical
thinking skills and generating highly realistic text with minimal input, making
it a potential threat to the integrity of online exams, particularly in
tertiary education settings where such exams are becoming more prevalent.
Returning to invigilated and oral exams could form part of the solution, while
using advanced proctoring techniques and AI-text output detectors may be
effective in addressing this issue, they are not likely to be foolproof
solutions. Further research is needed to fully understand the implications of
large language models like ChatGPT and to devise strategies for combating the
risk of cheating using these tools. It is crucial for educators and
institutions to be aware of the possibility of ChatGPT being used for cheating
and to investigate measures to address it in order to maintain the fairness and
validity of online exams for all students.",2022-12-19
"Rainproof: An Umbrella To Shield Text Generators From
  Out-Of-Distribution Data",2022-12-18 21:22:28+00:00,http://arxiv.org/abs/2212.09171v1,"Maxime Darrin, Pablo Piantanida, Pierre Colombo",cs.CL,dialogue,"As more and more conversational and translation systems are deployed in
production, it is essential to implement and to develop effective control
mechanisms guaranteeing their proper functioning and security. An essential
component to ensure safe system behavior is out-of-distribution (OOD)
detection, which aims at detecting whether an input sample is statistically far
from the training distribution. Although OOD detection is a widely covered
topic in classification tasks, it has received much less attention in text
generation. This paper addresses the problem of OOD detection for machine
translation and dialog generation from an operational perspective. Our
contributions include: (i) RAINPROOF a Relative informAItioN Projection ODD
detection framework; and (ii) a more operational evaluation setting for OOD
detection. Surprisingly, we find that OOD detection is not necessarily aligned
with task-specific measures. The OOD detector may filter out samples that are
well processed by the model and keep samples that are not, leading to weaker
performance. Our results show that RAINPROOF breaks this curse and achieve good
results in OOD detection while increasing performance.",2022-12-18
Plansformer: Generating Symbolic Plans using Transformers,2022-12-16 19:06:49+00:00,http://arxiv.org/abs/2212.08681v1,"Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Lior Horesh, Biplav Srivastava, Francesco Fabiano, Andrea Loreggia",cs.AI,dialogue,"Large Language Models (LLMs) have been the subject of active research,
significantly advancing the field of Natural Language Processing (NLP). From
BERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural
language tasks such as question answering, summarization, and text generation.
Many ongoing efforts focus on understanding LLMs' capabilities, including their
knowledge of the world, syntax, and semantics. However, extending the textual
prowess of LLMs to symbolic reasoning has been slow and predominantly focused
on tackling problems related to the mathematical field. In this paper, we
explore the use of LLMs for automated planning - a branch of AI concerned with
the realization of action sequences (plans) to achieve a goal, typically
executed by intelligent agents, autonomous robots, and unmanned vehicles. We
introduce Plansformer; an LLM fine-tuned on planning problems and capable of
generating plans with favorable behavior in terms of correctness and length
with reduced knowledge-engineering efforts. We also demonstrate the
adaptability of Plansformer in solving different planning domains with varying
complexities, owing to the transfer learning abilities of LLMs. For one
configuration of Plansformer, we achieve ~97% valid plans, out of which ~95%
are optimal for Towers of Hanoi - a puzzle-solving domain.",2022-12-16
"ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data
  Format",2022-11-30 16:37:42+00:00,http://arxiv.org/abs/2211.17148v1,"Qi Zhu, Christian Geishauser, Hsien-chin Lin, Carel van Niekerk, Baolin Peng, Zheng Zhang, Michael Heck, Nurul Lubis, Dazhen Wan, Xiaochen Zhu, Jianfeng Gao, Milica Gašić, Minlie Huang","cs.CL, cs.AI",dialogue,"Diverse data formats and ontologies of task-oriented dialogue (TOD) datasets
hinder us from developing general dialogue models that perform well on many
datasets and studying knowledge transfer between datasets. To address this
issue, we present ConvLab-3, a flexible dialogue system toolkit based on a
unified TOD data format. In ConvLab-3, different datasets are transformed into
one unified format and loaded by models in the same way. As a result, the cost
of adapting a new model or dataset is significantly reduced. Compared to the
previous releases of ConvLab (Lee et al., 2019b; Zhu et al., 2020b), ConvLab-3
allows developing dialogue systems with much more datasets and enhances the
utility of the reinforcement learning (RL) toolkit for dialogue policies. To
showcase the use of ConvLab-3 and inspire future work, we present a
comprehensive study with various settings. We show the benefit of pre-training
on other datasets for few-shot fine-tuning and RL, and encourage evaluating
policy with diverse user simulators.",2022-11-30
"MUSIED: A Benchmark for Event Detection from Multi-Source Heterogeneous
  Informal Texts",2022-11-25 05:05:29+00:00,http://arxiv.org/abs/2211.13896v1,"Xiangyu Xi, Jianwei Lv, Shuaipeng Liu, Wei Ye, Fan Yang, Guanglu Wan",cs.CL,dialogue,"Event detection (ED) identifies and classifies event triggers from
unstructured texts, serving as a fundamental task for information extraction.
Despite the remarkable progress achieved in the past several years, most
research efforts focus on detecting events from formal texts (e.g., news
articles, Wikipedia documents, financial announcements). Moreover, the texts in
each dataset are either from a single source or multiple yet relatively
homogeneous sources. With massive amounts of user-generated text accumulating
on the Web and inside enterprises, identifying meaningful events in these
informal texts, usually from multiple heterogeneous sources, has become a
problem of significant practical value. As a pioneering exploration that
expands event detection to the scenarios involving informal and heterogeneous
texts, we propose a new large-scale Chinese event detection dataset based on
user reviews, text conversations, and phone conversations in a leading
e-commerce platform for food service. We carefully investigate the proposed
dataset's textual informality and multi-source heterogeneity characteristics by
inspecting data samples quantitatively and qualitatively. Extensive experiments
with state-of-the-art event detection methods verify the unique challenges
posed by these characteristics, indicating that multi-source informal event
detection remains an open problem and requires further efforts. Our benchmark
and code are released at \url{https://github.com/myeclipse/MUSIED}.",2022-11-25
"Human-Machine Collaboration Approaches to Build a Dialogue Dataset for
  Hate Speech Countering",2022-11-07 10:37:13+00:00,http://arxiv.org/abs/2211.03433v1,"Helena Bonaldi, Sara Dellantonio, Serra Sinem Tekiroglu, Marco Guerini","cs.CL, cs.CY",dialogue,"Fighting online hate speech is a challenge that is usually addressed using
Natural Language Processing via automatic detection and removal of hate
content. Besides this approach, counter narratives have emerged as an effective
tool employed by NGOs to respond to online hate on social media platforms. For
this reason, Natural Language Generation is currently being studied as a way to
automatize counter narrative writing. However, the existing resources necessary
to train NLG models are limited to 2-turn interactions (a hate speech and a
counter narrative as response), while in real life, interactions can consist of
multiple turns. In this paper, we present a hybrid approach for dialogical data
collection, which combines the intervention of human expert annotators over
machine generated dialogues obtained using 19 different configurations. The
result of this work is DIALOCONAN, the first dataset comprising over 3000
fictitious multi-turn dialogues between a hater and an NGO operator, covering 6
targets of hate.",2022-11-07
"Is MultiWOZ a Solved Task? An Interactive TOD Evaluation Framework with
  User Simulator",2022-10-26 07:41:32+00:00,http://arxiv.org/abs/2210.14529v1,"Qinyuan Cheng, Linyang Li, Guofeng Quan, Feng Gao, Xiaofeng Mou, Xipeng Qiu",cs.CL,dialogue,"Task-Oriented Dialogue (TOD) systems are drawing more and more attention in
recent studies. Current methods focus on constructing pre-trained models or
fine-tuning strategies while the evaluation of TOD is limited by a policy
mismatch problem. That is, during evaluation, the user utterances are from the
annotated dataset while these utterances should interact with previous
responses which can have many alternatives besides annotated texts. Therefore,
in this work, we propose an interactive evaluation framework for TOD. We first
build a goal-oriented user simulator based on pre-trained models and then use
the user simulator to interact with the dialogue system to generate dialogues.
Besides, we introduce a sentence-level and a session-level score to measure the
sentence fluency and session coherence in the interactive evaluation.
Experimental results show that RL-based TOD systems trained by our proposed
user simulator can achieve nearly 98% inform and success rates in the
interactive evaluation of MultiWOZ dataset and the proposed scores measure the
response quality besides the inform and success rates. We are hoping that our
work will encourage simulator-based interactive evaluations in the TOD task.",2022-10-26
"Are Current Decoding Strategies Capable of Facing the Challenges of
  Visual Dialogue?",2022-10-24 07:34:39+00:00,http://arxiv.org/abs/2210.12997v1,"Amit Kumar Chaudhary, Alex J. Lucassen, Ioanna Tsani, Alberto Testoni","cs.CL, cs.CV",dialogue,"Decoding strategies play a crucial role in natural language generation
systems. They are usually designed and evaluated in open-ended text-only tasks,
and it is not clear how different strategies handle the numerous challenges
that goal-oriented multimodal systems face (such as grounding and
informativeness). To answer this question, we compare a wide variety of
different decoding strategies and hyper-parameter configurations in a Visual
Dialogue referential game. Although none of them successfully balance lexical
richness, accuracy in the task, and visual grounding, our in-depth analysis
allows us to highlight the strengths and weaknesses of each decoding strategy.
We believe our findings and suggestions may serve as a starting point for
designing more effective decoding algorithms that handle the challenges of
Visual Dialogue tasks.",2022-10-24
Language Detoxification with Attribute-Discriminative Latent Space,2022-10-19 06:54:42+00:00,http://arxiv.org/abs/2210.10329v1,"Jin Myung Kwak, Minseon Kim, Sung Ju Hwang","cs.CL, cs.AI",dialogue,"Transformer-based Language Models (LMs) achieve remarkable performances on a
variety of NLU tasks, but are also prone to generating toxic texts such as
insults, threats, and profanities which limit their adaptations to the
real-world applications. To overcome this issue, a few text generation
approaches aim to detoxify toxic texts with additional LMs or perturbations.
However, previous methods require excessive memory, computations, and time
which are serious bottlenecks in their real-world application. To address such
limitations, we propose an effective yet efficient method for language
detoxification using an attribute-discriminative latent space. Specifically, we
project the latent space of an original Transformer LM to a discriminative
latent space on which the texts are well-separated by their attributes, with
the help of a projection block and a discriminator. This allows the LM to
control the text generation to be non-toxic with minimal memory and computation
overhead. We validate our model, Attribute-Discriminative Language Model (ADLM)
on detoxified language and dialogue generation tasks, on which our method
significantly outperforms baselines both in performance and efficiency.",2022-10-19
"Team Flow at DRC2022: Pipeline System for Travel Destination
  Recommendation Task in Spoken Dialogue",2022-10-18 01:11:16+00:00,http://arxiv.org/abs/2210.09518v1,"Ryu Hirai, Atsumoto Ohashi, Ao Guo, Hideki Shiroma, Xulin Zhou, Yukihiko Tone, Shinya Iizuka, Ryuichiro Higashinaka","cs.CL, cs.AI, cs.RO",dialogue,"To improve the interactive capabilities of a dialogue system, e.g., to adapt
to different customers, the Dialogue Robot Competition (DRC2022) was held. As
one of the teams, we built a dialogue system with a pipeline structure
containing four modules. The natural language understanding (NLU) and natural
language generation (NLG) modules were GPT-2 based models, and the dialogue
state tracking (DST) and policy modules were designed on the basis of
hand-crafted rules. After the preliminary round of the competition, we found
that the low variation in training examples for the NLU and failed
recommendation due to the policy used were probably the main reasons for the
limited performance of the system.",2022-10-18
"LEATHER: A Framework for Learning to Generate Human-like Text in
  Dialogue",2022-10-14 13:05:11+00:00,http://arxiv.org/abs/2210.07777v1,"Anthony Sicilia, Malihe Alikhani","cs.CL, cs.LG",dialogue,"Algorithms for text-generation in dialogue can be misguided. For example, in
task-oriented settings, reinforcement learning that optimizes only task-success
can lead to abysmal lexical diversity. We hypothesize this is due to poor
theoretical understanding of the objectives in text-generation and their
relation to the learning process (i.e., model training). To this end, we
propose a new theoretical framework for learning to generate text in dialogue.
Compared to existing theories of learning, our framework allows for analysis of
the multi-faceted goals inherent to text-generation. We use our framework to
develop theoretical guarantees for learners that adapt to unseen data. As an
example, we apply our theory to study data-shift within a cooperative learning
algorithm proposed for the GuessWhat?! visual dialogue game. From this insight,
we propose a new algorithm, and empirically, we demonstrate our proposal
improves both task-success and human-likeness of the generated text. Finally,
we show statistics from our theory are empirically predictive of multiple
qualities of the generated dialogue, suggesting our theory is useful for
model-selection when human evaluations are not available.",2022-10-14
Towards a Unified Multi-Dimensional Evaluator for Text Generation,2022-10-13 17:17:03+00:00,http://arxiv.org/abs/2210.07197v1,"Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, Jiawei Han",cs.CL,dialogue,"Multi-dimensional evaluation is the dominant paradigm for human evaluation in
Natural Language Generation (NLG), i.e., evaluating the generated text from
multiple explainable dimensions, such as coherence and fluency. However,
automatic evaluation in NLG is still dominated by similarity-based metrics, and
we lack a reliable framework for a more comprehensive evaluation of advanced
models. In this paper, we propose a unified multi-dimensional evaluator UniEval
for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task,
and by guiding the model with different questions, we can use one evaluator to
evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean
QA format, we are able to introduce an intermediate learning phase that enables
UniEval to incorporate external knowledge from multiple related tasks and gain
further improvement. Experiments on three typical NLG tasks show that UniEval
correlates substantially better with human judgments than existing metrics.
Specifically, compared to the top-performing unified evaluators, UniEval
achieves a 23% higher correlation on text summarization, and over 43% on
dialogue response generation. Also, UniEval demonstrates a strong zero-shot
learning ability for unseen evaluation dimensions and tasks. Source code, data
and all pre-trained evaluators are available on our GitHub repository
(https://github.com/maszhongming/UniEval).",2022-10-13
Controllable Dialogue Simulation with In-Context Learning,2022-10-09 06:32:58+00:00,http://arxiv.org/abs/2210.04185v1,"Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, Xifeng Yan","cs.CL, cs.AI",dialogue,"Building dialogue systems requires a large corpus of annotated dialogues.
Such datasets are usually created via crowdsourcing, which is expensive and
time-consuming. In this paper, we propose a novel method for dialogue
simulation based on language model in-context learning, dubbed as
\textsc{Dialogic}. Seeded with a few annotated dialogues, \textsc{Dialogic}
automatically selects in-context examples for demonstration and prompts GPT-3
to generate new dialogues and their annotations in a controllable way.
Leveraging the strong in-context learning ability of GPT-3, our method can be
used to rapidly expand a small set of dialogue data without requiring
\textit{human involvement} or \textit{parameter update}, and is thus much more
cost-efficient and time-saving than crowdsourcing. Experimental results on the
MultiWOZ dataset demonstrate that training a model on the simulated dialogues
leads to even better performance than using the same amount of human-generated
dialogues in the low-resource settings, with as few as 85 dialogues as the seed
data. Human evaluation results also show that our simulated dialogues has high
language fluency and annotation accuracy. The code and data are available at
\href{https://github.com/Leezekun/dialogic}{https://github.com/Leezekun/dialogic}.",2022-10-09
"Unsupervised Neural Stylistic Text Generation using Transfer learning
  and Adapters",2022-10-07 00:09:22+00:00,http://arxiv.org/abs/2210.03264v1,"Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah, Dan Roth",cs.CL,dialogue,"Research has shown that personality is a key driver to improve engagement and
user experience in conversational systems. Conversational agents should also
maintain a consistent persona to have an engaging conversation with a user.
However, text generation datasets are often crowd sourced and thereby have an
averaging effect where the style of the generation model is an average style of
all the crowd workers that have contributed to the dataset. While one can
collect persona-specific datasets for each task, it would be an expensive and
time consuming annotation effort. In this work, we propose a novel transfer
learning framework which updates only $0.3\%$ of model parameters to learn
style specific attributes for response generation. For the purpose of this
study, we tackle the problem of stylistic story ending generation using the ROC
stories Corpus. We learn style specific attributes from the
PERSONALITY-CAPTIONS dataset. Through extensive experiments and evaluation
metrics we show that our novel training procedure can improve the style
generation by 200 over Encoder-Decoder baselines while maintaining on-par
content relevance metrics with",2022-10-07
"Learning functional sections in medical conversations: iterative
  pseudo-labeling and human-in-the-loop approach",2022-10-06 03:33:00+00:00,http://arxiv.org/abs/2210.02658v2,"Mengqian Wang, Ilya Valmianski, Xavier Amatriain, Anitha Kannan",cs.CL,dialogue,"Medical conversations between patients and medical professionals have
implicit functional sections, such as ""history taking"", ""summarization"",
""education"", and ""care plan."" In this work, we are interested in learning to
automatically extract these sections. A direct approach would require
collecting large amounts of expert annotations for this task, which is
inherently costly due to the contextual inter-and-intra variability between
these sections. This paper presents an approach that tackles the problem of
learning to classify medical dialogue into functional sections without
requiring a large number of annotations. Our approach combines pseudo-labeling
and human-in-the-loop. First, we bootstrap using weak supervision with
pseudo-labeling to generate dialogue turn-level pseudo-labels and train a
transformer-based model, which is then applied to individual sentences to
create noisy sentence-level labels. Second, we iteratively refine
sentence-level labels using a cluster-based human-in-the-loop approach. Each
iteration requires only a few dozen annotator decisions. We evaluate the
results on an expert-annotated dataset of 100 dialogues and find that while our
models start with 69.5% accuracy, we can iteratively improve it to 82.5%. The
code used to perform all experiments described in this paper can be found here:
https://github.com/curai/curai-research/tree/main/functional-sections.",2022-10-06
"Dancing with the Unexpected and Beyond: The Use of AI Assistance in
  Design Fiction Creation",2022-10-03 11:26:39+00:00,http://arxiv.org/abs/2210.00829v1,"Yiying Wu, Yunye Yu, Pengcheng An",cs.HC,dialogue,"The creation process of design fiction is going participatory and inclusive
with non experts. Recognizing the potential of artificial intelligence in
creativity support, we explore the use of AI assistance in creating design
fiction. This investigation is based on a workshop on future work in 2040 with
Chinese youth. We look into fiction quality, participants experiences with the
AI agent, and their ways of incorporating those texts into writing. Our
findings show that human writers while responding to messy and unexpected
AI-generated texts, can elevate the richness and creativity in writing and
initiate joyful and inspirational interactions. Furthermore, for the design of
AI assistance in creativity support, we suggest two implications of enhancing
interactional quality between human and AI and prompt programming. Our study
indicates the potential of applying design fiction outside the design context
using a more inclusive approach for future speculation with critical reflection
on technology.",2022-10-03
"Co-Writing Screenplays and Theatre Scripts with Language Models: An
  Evaluation by Industry Professionals",2022-09-29 17:26:22+00:00,http://arxiv.org/abs/2209.14958v1,"Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, Richard Evans","cs.HC, cs.CL",dialogue,"Language models are increasingly attracting interest from writers. However,
such models lack long-range semantic coherence, limiting their usefulness for
longform creative writing. We address this limitation by applying language
models hierarchically, in a system we call Dramatron. By building structural
context via prompt chaining, Dramatron can generate coherent scripts and
screenplays complete with title, characters, story beats, location
descriptions, and dialogue. We illustrate Dramatron's usefulness as an
interactive co-creative system with a user study of 15 theatre and film
industry professionals. Participants co-wrote theatre scripts and screenplays
with Dramatron and engaged in open-ended interviews. We report critical
reflections both from our interviewees and from independent reviewers who
watched stagings of the works to illustrate how both Dramatron and hierarchical
text generation could be useful for human-machine co-creativity. Finally, we
discuss the suitability of Dramatron for co-creativity, ethical considerations
-- including plagiarism and bias -- and participatory models for the design and
deployment of such tools.",2022-09-29
"A Benchmark for Understanding and Generating Dialogue between Characters
  in Stories",2022-09-18 10:19:04+00:00,http://arxiv.org/abs/2209.08524v1,"Jianzhu Yao, Ziqi Liu, Jian Guan, Minlie Huang","cs.CL, cs.AI",dialogue,"Many classical fairy tales, fiction, and screenplays leverage dialogue to
advance story plots and establish characters. We present the first study to
explore whether machines can understand and generate dialogue in stories, which
requires capturing traits of different characters and the relationships between
them. To this end, we propose two new tasks including Masked Dialogue
Generation and Dialogue Speaker Recognition, i.e., generating missing dialogue
turns and predicting speakers for specified dialogue turns, respectively. We
build a new dataset DialStory, which consists of 105k Chinese stories with a
large amount of dialogue weaved into the plots to support the evaluation. We
show the difficulty of the proposed tasks by testing existing models with
automatic and manual evaluation on DialStory. Furthermore, we propose to learn
explicit character representations to improve performance on these tasks.
Extensive experiments and case studies show that our approach can generate more
coherent and informative dialogue, and achieve higher speaker recognition
accuracy than strong baselines.",2022-09-18
"Adaptive Natural Language Generation for Task-oriented Dialogue via
  Reinforcement Learning",2022-09-16 12:08:57+00:00,http://arxiv.org/abs/2209.07873v1,"Atsumoto Ohashi, Ryuichiro Higashinaka","cs.CL, cs.AI",dialogue,"When a natural language generation (NLG) component is implemented in a
real-world task-oriented dialogue system, it is necessary to generate not only
natural utterances as learned on training data but also utterances adapted to
the dialogue environment (e.g., noise from environmental sounds) and the user
(e.g., users with low levels of understanding ability). Inspired by recent
advances in reinforcement learning (RL) for language generation tasks, we
propose ANTOR, a method for Adaptive Natural language generation for
Task-Oriented dialogue via Reinforcement learning. In ANTOR, a natural language
understanding (NLU) module, which corresponds to the user's understanding of
system utterances, is incorporated into the objective function of RL. If the
NLG's intentions are correctly conveyed to the NLU, which understands a
system's utterances, the NLG is given a positive reward. We conducted
experiments on the MultiWOZ dataset, and we confirmed that ANTOR could generate
adaptive utterances against speech recognition errors and the different
vocabulary levels of users.",2022-09-16
"OPAL: Ontology-Aware Pretrained Language Model for End-to-End
  Task-Oriented Dialogue",2022-09-10 04:38:27+00:00,http://arxiv.org/abs/2209.04595v1,"Zhi Chen, Yuncong Liu, Lu Chen, Su Zhu, Mengyue Wu, Kai Yu",cs.CL,dialogue,"This paper presents an ontology-aware pretrained language model (OPAL) for
end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models,
task-oriented dialogue models fulfill at least two task-specific modules:
dialogue state tracker (DST) and response generator (RG). The dialogue state
consists of the domain-slot-value triples, which are regarded as the user's
constraints to search the domain-related databases. The large-scale
task-oriented dialogue data with the annotated structured dialogue state
usually are inaccessible. It prevents the development of the pretrained
language model for the task-oriented dialogue. We propose a simple yet
effective pretraining method to alleviate this problem, which consists of two
pretraining phases. The first phase is to pretrain on large-scale contextual
text data, where the structured information of the text is extracted by the
information extracting tool. To bridge the gap between the pretraining method
and downstream tasks, we design two pretraining tasks: ontology-like triple
recovery and next-text generation, which simulates the DST and RG,
respectively. The second phase is to fine-tune the pretrained model on the TOD
data. The experimental results show that our proposed method achieves an
exciting boost and get competitive performance even without any TOD data on
CamRest676 and MultiWOZ benchmarks.",2022-09-10
Unified Knowledge Prompt Pre-training for Customer Service Dialogues,2022-08-31 06:23:53+00:00,http://arxiv.org/abs/2208.14652v1,"Keqing He, Jingang Wang, Chaobo Sun, Wei Wu",cs.CL,dialogue,"Dialogue bots have been widely applied in customer service scenarios to
provide timely and user-friendly experience. These bots must classify the
appropriate domain of a dialogue, understand the intent of users, and generate
proper responses. Existing dialogue pre-training models are designed only for
several dialogue tasks and ignore weakly-supervised expert knowledge in
customer service dialogues. In this paper, we propose a novel unified knowledge
prompt pre-training framework, UFA (\textbf{U}nified Model \textbf{F}or
\textbf{A}ll Tasks), for customer service dialogues. We formulate all the tasks
of customer service dialogues as a unified text-to-text generation task and
introduce a knowledge-driven prompt strategy to jointly learn from a mixture of
distinct dialogue tasks. We pre-train UFA on a large-scale Chinese customer
service corpus collected from practical scenarios and get significant
improvements on both natural language understanding (NLU) and natural language
generation (NLG) benchmarks.",2022-08-31
"GenTUS: Simulating User Behaviour and Language in Task-oriented
  Dialogues with Generative Transformers",2022-08-23 09:01:17+00:00,http://arxiv.org/abs/2208.10817v1,"Hsien-Chin Lin, Christian Geishauser, Shutong Feng, Nurul Lubis, Carel van Niekerk, Michael Heck, Milica Gašić",cs.CL,dialogue,"User simulators (USs) are commonly used to train task-oriented dialogue
systems (DSs) via reinforcement learning. The interactions often take place on
semantic level for efficiency, but there is still a gap from semantic actions
to natural language, which causes a mismatch between training and deployment
environment. Incorporating a natural language generation (NLG) module with USs
during training can partly deal with this problem. However, since the policy
and NLG of USs are optimised separately, these simulated user utterances may
not be natural enough in a given context. In this work, we propose a generative
transformer-based user simulator (GenTUS). GenTUS consists of an
encoder-decoder structure, which means it can optimise both the user policy and
natural language generation jointly. GenTUS generates both semantic actions and
natural language utterances, preserving interpretability and enhancing language
variation. In addition, by representing the inputs and outputs as word
sequences and by using a large pre-trained language model we can achieve
generalisability in feature representation. We evaluate GenTUS with automatic
metrics and human evaluation. Our results show that GenTUS generates more
natural language and is able to transfer to an unseen ontology in a zero-shot
fashion. In addition, its behaviour can be further shaped with reinforcement
learning opening the door to training specialised user simulators.",2022-08-23
"Efficient Task-Oriented Dialogue Systems with Response Selection as an
  Auxiliary Task",2022-08-15 09:59:44+00:00,http://arxiv.org/abs/2208.07097v1,"Radostin Cholakov, Todor Kolev","cs.CL, cs.AI",dialogue,"The adoption of pre-trained language models in task-oriented dialogue systems
has resulted in significant enhancements of their text generation abilities.
However, these architectures are slow to use because of the large number of
trainable parameters and can sometimes fail to generate diverse responses. To
address these limitations, we propose two models with auxiliary tasks for
response selection - (1) distinguishing distractors from ground truth responses
and (2) distinguishing synthetic responses from ground truth labels. They
achieve state-of-the-art results on the MultiWOZ 2.1 dataset with combined
scores of 107.5 and 108.3 and outperform a baseline with three times more
parameters. We publish reproducible code and checkpoints and discuss the
effects of applying auxiliary tasks to T5-based architectures.",2022-08-15
"Dynamically Retrieving Knowledge via Query Generation for informative
  dialogue response",2022-07-30 03:05:43+00:00,http://arxiv.org/abs/2208.00128v1,"Zhongtian Hu, Yangqi Chen, Yushuang Liu, Lifang Wang",cs.CL,dialogue,"Knowledge-driven dialogue generation has recently made remarkable
breakthroughs. Compared with general dialogue systems, superior
knowledge-driven dialogue systems can generate more informative and
knowledgeable responses with pre-provided knowledge. However, in practical
applications, the dialogue system cannot be provided with corresponding
knowledge in advance. In order to solve the problem, we design a
knowledge-driven dialogue system named DRKQG (\emph{Dynamically Retrieving
Knowledge via Query Generation for informative dialogue response}).
Specifically, the system can be divided into two modules: query generation
module and dialogue generation module. First, a time-aware mechanism is
utilized to capture context information and a query can be generated for
retrieving knowledge. Then, we integrate copy Mechanism and Transformers, which
allows the response generation module produces responses derived from the
context and retrieved knowledge. Experimental results at LIC2022, Language and
Intelligence Technology Competition, show that our module outperforms the
baseline model by a large margin on automatic evaluation metrics, while human
evaluation by Baidu Linguistics team shows that our system achieves impressive
results in Factually Correct and Knowledgeable.",2022-07-30
Sequence to sequence pretraining for a less-resourced Slovenian language,2022-07-28 10:08:50+00:00,http://arxiv.org/abs/2207.13988v1,"Matej Ulčar, Marko Robnik-Šikonja",cs.CL,dialogue,"Large pretrained language models have recently conquered the area of natural
language processing. As an alternative to predominant masked language modelling
introduced in BERT, the T5 model has introduced a more general training
objective, namely sequence to sequence transformation, which includes masked
language model but more naturally fits text generation tasks such as machine
translation, summarization, open-domain question answering, text
simplification, dialogue systems, etc. The monolingual variants of T5 models
have been limited to well-resourced languages, while the massively multilingual
T5 model supports 101 languages. In contrast, we trained two different sized
T5-type sequence to sequence models for morphologically rich Slovene language
with much less resources and analyzed their behavior. Concerning classification
tasks, the SloT5 models mostly lag behind the monolingual Slovene SloBERTa
model but are to be considered for the generative tasks.",2022-07-28
A Multi-Party Dialogue Ressource in French,2022-07-25 13:02:54+00:00,http://arxiv.org/abs/2207.12162v1,"Maria Boritchev, Maxime Amblard",cs.AI,dialogue,"We present Dialogues in Games (DinG), a corpus of manual transcriptions of
real-life, oral, spontaneous multi-party dialogues between French-speaking
players of the board game Catan. Our objective is to make available a quality
resource for French, composed of long dialogues, to facilitate their study in
the style of (Asher et al., 2016). In a general dialogue setting, participants
share personal information, which makes it impossible to disseminate the
resource freely and openly. In DinG, the attention of the participants is
focused on the game, which prevents them from talking about themselves. In
addition, we are conducting a study on the nature of the questions in dialogue,
through annotation (Cruz Blandon et al., 2019), in order to develop more
natural automatic dialogue systems.",2022-07-25
Towards a Sentiment-Aware Conversational Agent,2022-07-24 16:59:44+00:00,http://arxiv.org/abs/2207.11774v1,"Isabel Dias, Ricardo Rei, Patrícia Pereira, Luisa Coheur",cs.CL,dialogue,"In this paper, we propose an end-to-end sentiment-aware conversational agent
based on two models: a reply sentiment prediction model, which leverages the
context of the dialogue to predict an appropriate sentiment for the agent to
express in its reply; and a text generation model, which is conditioned on the
predicted sentiment and the context of the dialogue, to produce a reply that is
both context and sentiment appropriate. Additionally, we propose to use a
sentiment classification model to evaluate the sentiment expressed by the agent
during the development of the model. This allows us to evaluate the agent in an
automatic way. Both automatic and human evaluation results show that explicitly
guiding the text generation model with a pre-defined set of sentences leads to
clear improvements, both regarding the expressed sentiment and the quality of
the generated text.",2022-07-24
"TalkToModel: Understanding Machine Learning Models With Open Ended
  Dialogues",2022-07-08 23:42:56+00:00,http://arxiv.org/abs/2207.04154v1,"Dylan Slack, Satyapriya Krishna, Himabindu Lakkaraju, Sameer Singh","cs.LG, cs.AI, cs.CL",dialogue,"Machine Learning (ML) models are increasingly used to make critical decisions
in real-world applications, yet they have also become more complex, making them
harder to understand. To this end, several techniques to explain model
predictions have been proposed. However, practitioners struggle to leverage
explanations because they often do not know which to use, how to interpret the
results, and may have insufficient data science experience to obtain
explanations. In addition, most current works focus on generating one-shot
explanations and do not allow users to follow up and ask fine-grained questions
about the explanations, which can be frustrating. In this work, we address
these challenges by introducing TalkToModel: an open-ended dialogue system for
understanding machine learning models. Specifically, TalkToModel comprises
three key components: 1) a natural language interface for engaging in
dialogues, making understanding ML models highly accessible, 2) a dialogue
engine that adapts to any tabular model and dataset, interprets natural
language, maps it to appropriate operations (e.g., feature importance
explanations, counterfactual explanations, showing model errors), and generates
text responses, and 3) an execution component that run the operations and
ensures explanations are accurate. We carried out quantitative and human
subject evaluations of TalkToModel. We found the system understands user
questions on novel datasets and models with high accuracy, demonstrating the
system's capacity to generalize to new situations. In human evaluations, 73% of
healthcare workers (e.g., doctors and nurses) agreed they would use TalkToModel
over baseline point-and-click systems, and 84.6% of ML graduate students agreed
TalkToModel was easier to use.",2022-07-08
Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk,2022-07-02 04:30:07+00:00,http://arxiv.org/abs/2207.00735v1,"Benyou Wang, Xiangbo Wu, Xiaokang Liu, Jianquan Li, Prayag Tiwari, Qianqian Xie",cs.CL,dialogue,"Language is the principal tool for human communication, in which humor is one
of the most attractive parts. Producing natural language like humans using
computers, a.k.a, Natural Language Generation (NLG), has been widely used for
dialogue systems, chatbots, machine translation, as well as computer-aid
creation e.g., idea generations, scriptwriting. However, the humor aspect of
natural language is relatively under-investigated, especially in the age of
pre-trained language models. In this work, we aim to preliminarily test whether
NLG can generate humor as humans do. We build a new dataset consisting of
numerous digitized Chinese Comical Crosstalk scripts (called C$^3$ in short),
which is for a popular Chinese performing art called `Xiangsheng' since 1800s.
(For convenience for non-Chinese speakers, we called `crosstalk' for
`Xiangsheng' in this paper.) We benchmark various generation approaches
including training-from-scratch Seq2seq, fine-tuned middle-scale PLMs, and
large-scale PLMs (with and without fine-tuning). Moreover, we also conduct a
human assessment, showing that 1) large-scale pretraining largely improves
crosstalk generation quality; and 2) even the scripts generated from the best
PLM is far from what we expect, with only 65% quality of human-created
crosstalk. We conclude, humor generation could be largely improved using
large-scaled PLMs, but it is still in its infancy.
  The data and benchmarking code is publicly available in
\url{https://github.com/anonNo2/crosstalk-generation}.",2022-07-02
"Comparing informativeness of an NLG chatbot vs graphical app in
  diet-information domain",2022-06-23 07:15:58+00:00,http://arxiv.org/abs/2206.13435v1,"Simone Balloccu, Ehud Reiter","cs.CL, cs.AI",dialogue,"Visual representation of data like charts and tables can be challenging to
understand for readers. Previous work showed that combining visualisations with
text can improve the communication of insights in static contexts, but little
is known about interactive ones. In this work we present an NLG chatbot that
processes natural language queries and provides insights through a combination
of charts and text. We apply it to nutrition, a domain communication quality is
critical. Through crowd-sourced evaluation we compare the informativeness of
our chatbot against traditional, static diet-apps. We find that the
conversational context significantly improved users' understanding of dietary
data in various tasks, and that users considered the chatbot as more useful and
quick to use than traditional apps.",2022-06-23
"Automatic Summarization of Russian Texts: Comparison of Extractive and
  Abstractive Methods",2022-06-18 17:28:04+00:00,http://arxiv.org/abs/2206.09253v1,"Valeriya Goloviznina, Evgeny Kotelnikov",cs.CL,dialogue,"The development of large and super-large language models, such as GPT-3, T5,
Switch Transformer, ERNIE, etc., has significantly improved the performance of
text generation. One of the important research directions in this area is the
generation of texts with arguments. The solution of this problem can be used in
business meetings, political debates, dialogue systems, for preparation of
student essays. One of the main domains for these applications is the economic
sphere. The key problem of the argument text generation for the Russian
language is the lack of annotated argumentation corpora. In this paper, we use
translated versions of the Argumentative Microtext, Persuasive Essays and UKP
Sentential corpora to fine-tune RuBERT model. Further, this model is used to
annotate the corpus of economic news by argumentation. Then the annotated
corpus is employed to fine-tune the ruGPT-3 model, which generates argument
texts. The results show that this approach improves the accuracy of the
argument generation by more than 20 percentage points (63.2% vs. 42.5%)
compared to the original ruGPT-3 model.",2022-06-18
Argumentative Text Generation in Economic Domain,2022-06-18 17:22:06+00:00,http://arxiv.org/abs/2206.09251v1,"Irina Fishcheva, Dmitriy Osadchiy, Klavdiya Bochenina, Evgeny Kotelnikov",cs.CL,dialogue,"The development of large and super-large language models, such as GPT-3, T5,
Switch Transformer, ERNIE, etc., has significantly improved the performance of
text generation. One of the important research directions in this area is the
generation of texts with arguments. The solution of this problem can be used in
business meetings, political debates, dialogue systems, for preparation of
student essays. One of the main domains for these applications is the economic
sphere. The key problem of the argument text generation for the Russian
language is the lack of annotated argumentation corpora. In this paper, we use
translated versions of the Argumentative Microtext, Persuasive Essays and UKP
Sentential corpora to fine-tune RuBERT model. Further, this model is used to
annotate the corpus of economic news by argumentation. Then the annotated
corpus is employed to fine-tune the ruGPT-3 model, which generates argument
texts. The results show that this approach improves the accuracy of the
argument generation by more than 20 percentage points (63.2\% vs. 42.5\%)
compared to the original ruGPT-3 model.",2022-06-18
"Offline RL for Natural Language Generation with Implicit Language Q
  Learning",2022-06-05 18:38:42+00:00,http://arxiv.org/abs/2206.11871v1,"Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, Sergey Levine","cs.CL, cs.LG",dialogue,"Large language models distill broad knowledge from text corpora. However,
they can be inconsistent when it comes to completing user specified tasks. This
issue can be addressed by finetuning such models via supervised learning on
curated datasets, or via reinforcement learning. In this work, we propose a
novel offline RL motivated method, implicit language Q-learning (ILQL),
designed for use on language models, that combines both the flexible utility
optimization framework of traditional RL algorithms with supervised learning's
ability to leverage existing data and its simplicity and stability. Our method,
based on dynamic programming, employs a blend of value conservatism alongside
an implicit dataset support constraint in learning value functions, which are
then used to guide language model generations towards maximizing utility. In
addition to empirically validating ILQL, we present a detailed empirical
analysis of situations where offline RL can be useful in natural language
generation settings, demonstrating how it can be a more effective utility
optimizer than prior approaches for end-to-end dialogue, and how it can
effectively optimize high variance reward functions based on subjective
judgement, such as whether to label a comment as an example of toxic speech or
not.",2022-06-05
"Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for
  Text-to-Speech",2022-06-05 10:50:34+00:00,http://arxiv.org/abs/2206.02147v1,"Ziyue Jiang, Su Zhe, Zhou Zhao, Qian Yang, Yi Ren, Jinglin Liu, Zhenhui Ye","eess.AS, cs.CL, cs.SD",dialogue,"Polyphone disambiguation aims to capture accurate pronunciation knowledge
from natural text sequences for reliable Text-to-speech (TTS) systems. However,
previous approaches require substantial annotated training data and additional
efforts from language experts, making it difficult to extend high-quality
neural TTS systems to out-of-domain daily conversations and countless languages
worldwide. This paper tackles the polyphone disambiguation problem from a
concise and novel perspective: we propose Dict-TTS, a semantic-aware generative
text-to-speech model with an online website dictionary (the existing prior
information in the natural language). Specifically, we design a
semantics-to-pronunciation attention (S2PA) module to match the semantic
patterns between the input text sequence and the prior semantics in the
dictionary and obtain the corresponding pronunciations; The S2PA module can be
easily trained with the end-to-end TTS model without any annotated phoneme
labels. Experimental results in three languages show that our model outperforms
several strong baseline models in terms of pronunciation accuracy and improves
the prosody modeling of TTS systems. Further extensive analyses with different
linguistic encoders demonstrate that each design in Dict-TTS is effective.
Audio samples are available at \url{https://dicttts.github.io/DictTTS-Demo/}.",2022-06-05
"Findings of the The RuATD Shared Task 2022 on Artificial Text Detection
  in Russian",2022-06-03 14:12:33+00:00,http://arxiv.org/abs/2206.01583v1,"Tatiana Shamardina, Vladislav Mikhailov, Daniil Chernianskii, Alena Fenogenova, Marat Saidov, Anastasiya Valeeva, Tatiana Shavrina, Ivan Smurov, Elena Tutubalina, Ekaterina Artemova",cs.CL,dialogue,"We present the shared task on artificial text detection in Russian, which is
organized as a part of the Dialogue Evaluation initiative, held in 2022. The
shared task dataset includes texts from 14 text generators, i.e., one human
writer and 13 text generative models fine-tuned for one or more of the
following generation tasks: machine translation, paraphrase generation, text
summarization, text simplification. We also consider back-translation and
zero-shot generation approaches. The human-written texts are collected from
publicly available resources across multiple domains. The shared task consists
of two sub-tasks: (i) to determine if a given text is automatically generated
or written by a human; (ii) to identify the author of a given text. The first
task is framed as a binary classification problem. The second task is a
multi-class classification problem. We provide count-based and BERT-based
baselines, along with the human evaluation on the first sub-task. A total of 30
and 8 systems have been submitted to the binary and multi-class sub-tasks,
correspondingly. Most teams outperform the baselines by a wide margin. We
publicly release our codebase, human evaluation results, and other materials in
our GitHub repository (https://github.com/dialogue-evaluation/RuATD).",2022-06-03
Clinical Dialogue Transcription Error Correction using Seq2Seq Models,2022-05-26 18:27:17+00:00,http://arxiv.org/abs/2205.13572v1,"Gayani Nanayakkara, Nirmalie Wiratunga, David Corsar, Kyle Martin, Anjana Wijekoon","cs.CL, cs.AI",dialogue,"Good communication is critical to good healthcare. Clinical dialogue is a
conversation between health practitioners and their patients, with the explicit
goal of obtaining and sharing medical information. This information contributes
to medical decision-making regarding the patient and plays a crucial role in
their healthcare journey. The reliance on note taking and manual scribing
processes are extremely inefficient and leads to manual transcription errors
when digitizing notes. Automatic Speech Recognition (ASR) plays a significant
role in speech-to-text applications, and can be directly used as a text
generator in conversational applications. However, recording clinical dialogue
presents a number of general and domain-specific challenges. In this paper, we
present a seq2seq learning approach for ASR transcription error correction of
clinical dialogues. We introduce a new Gastrointestinal Clinical Dialogue (GCD)
Dataset which was gathered by healthcare professionals from a NHS Inflammatory
Bowel Disease clinic and use this in a comparative study with four commercial
ASR systems. Using self-supervision strategies, we fine-tune a seq2seq model on
a mask-filling task using a domain-specific PubMed dataset which we have shared
publicly for future research. The BART model fine-tuned for mask-filling was
able to correct transcription errors and achieve lower word error rates for
three out of four commercial ASR outputs.",2022-05-26
"The Dialog Must Go On: Improving Visual Dialog via Generative
  Self-Training",2022-05-25 05:40:00+00:00,http://arxiv.org/abs/2205.12502v1,"Gi-Cheon Kang, Sungdong Kim, Jin-Hwa Kim, Donghyun Kwak, Byoung-Tak Zhang","cs.CV, cs.CL, cs.LG",dialogue,"Visual dialog (VisDial) is a task of answering a sequence of questions
grounded in an image, using the dialog history as context. Prior work has
trained the dialog agents solely on VisDial data via supervised learning or
leveraged pre-training on related vision-and-language datasets. This paper
presents a semi-supervised learning approach for visually-grounded dialog,
called Generative Self-Training (GST), to leverage unlabeled images on the Web.
Specifically, GST first retrieves in-domain images through out-of-distribution
detection and generates synthetic dialogs regarding the images via multimodal
conditional text generation. GST then trains a dialog agent on the synthetic
and the original VisDial data. As a result, GST scales the amount of training
data up to an order of magnitude that of VisDial (1.2M to 12.9M QA data). For
robust training of the generated dialogs, we also propose perplexity-based data
selection and multimodal consistency regularization. Evaluation on VisDial v1.0
and v0.9 datasets shows that GST achieves new state-of-the-art results on both
datasets. We further observe strong performance gains in the low-data regime
(up to 9.35 absolute points on NDCG).",2022-05-25
"CORAL: Contextual Response Retrievability Loss Function for Training
  Dialog Generation Models",2022-05-21 10:36:22+00:00,http://arxiv.org/abs/2205.10558v1,"Bishal Santra, Ravi Ghadia, Arpit Dwivedi, Manish Gupta, Pawan Goyal",cs.CL,dialogue,"Natural Language Generation (NLG) represents a large collection of tasks in
the field of NLP. While many of these tasks have been tackled well by the
cross-entropy (CE) loss, the task of dialog generation poses a few unique
challenges for this loss function. First, CE loss assumes that for any given
input, the only possible output is the one available as the ground truth in the
training dataset. In general, this is not true for any task, as there can be
multiple semantically equivalent sentences, each with a different surface form.
This problem gets exaggerated further for the dialog generation task, as there
can be multiple valid responses (for a given context) that not only have
different surface forms but are also not semantically equivalent. Second, CE
loss does not take the context into consideration while processing the response
and, hence, it treats all ground truths with equal importance irrespective of
the context. But, we may want our final agent to avoid certain classes of
responses (e.g. bland, non-informative or biased responses) and give relatively
higher weightage for more context-specific responses. To circumvent these
shortcomings of the CE loss, in this paper, we propose a novel loss function,
CORAL, that directly optimizes recently proposed estimates of human preference
for generated responses. Using CORAL, we can train dialog generation models
without assuming non-existence of response other than the ground-truth. Also,
the CORAL loss is computed based on both the context and the response.
Extensive comparisons on two benchmark datasets show that the proposed methods
outperform strong state-of-the-art baseline models of different sizes.",2022-05-21
Self-augmented Data Selection for Few-shot Dialogue Generation,2022-05-19 16:25:50+00:00,http://arxiv.org/abs/2205.09661v1,"Wanyu Du, Hanjie Chen, Yangfeng Ji",cs.CL,dialogue,"The natural language generation (NLG) module in task-oriented dialogue
systems translates structured meaning representations (MRs) into text
responses, which has a great impact on users' experience as the human-machine
interaction interface. However, in practice, developers often only have a few
well-annotated data and confront a high data collection cost to build the NLG
module. In this work, we adopt the self-training framework to deal with the
few-shot MR-to-Text generation problem. We leverage the pre-trained language
model to self-augment many pseudo-labeled data. To prevent the gradual drift
from target data distribution to noisy augmented data distribution, we propose
a novel data selection strategy to select the data that our generation model is
most uncertain about. Compared with existing data selection methods, our method
is: (1) parameter-efficient, which does not require training any additional
neural models, (2) computation-efficient, which only needs to apply several
stochastic forward passes of the model to estimate the uncertainty. We conduct
empirical experiments on two benchmark datasets: FewShotWOZ and FewShotSGD, and
show that our proposed framework consistently outperforms other baselines in
terms of BLEU and ERR.",2022-05-19
Diversifying Neural Dialogue Generation via Negative Distillation,2022-05-05 17:14:56+00:00,http://arxiv.org/abs/2205.02795v1,"Yiwei Li, Shaoxiong Feng, Bin Sun, Kan Li","cs.CL, cs.AI",dialogue,"Generative dialogue models suffer badly from the generic response problem,
limiting their applications to a few toy scenarios. Recently, an interesting
approach, namely negative training, has been proposed to alleviate this problem
by reminding the model not to generate high-frequency responses during
training. However, its performance is hindered by two issues, ignoring
low-frequency but generic responses and bringing low-frequency but meaningless
responses. In this paper, we propose a novel negative training paradigm, called
negative distillation, to keep the model away from the undesirable generic
responses while avoiding the above problems. First, we introduce a negative
teacher model that can produce query-wise generic responses, and then the
student model is required to maximize the distance with multi-level negative
knowledge. Empirical results show that our method outperforms previous negative
training methods significantly.",2022-05-05
AI Personification: Estimating the Personality of Language Models,2022-04-25 23:53:53+00:00,http://arxiv.org/abs/2204.12000v1,"Saketh Reddy Karra, Son Nguyen, Theja Tulabandhula","cs.CL, cs.AI",dialogue,"Technology for open-ended language generation, a key application of
artificial intelligence, has advanced to a great extent in recent years.
Large-scale language models, which are trained on large corpora of text, are
being used in a wide range of applications everywhere, from virtual assistants
to conversational bots. While these language models output fluent text,
existing research shows that these models can and do capture human biases. Many
of these biases, especially those that could potentially cause harm, are being
well investigated. On the other hand, studies that infer and change personality
traits inherited by these models have been scarce or non-existent. In this
work, we explore the personality traits of several large-scale language models
designed for open-ended text generation and the datasets used for training
them. Our work builds on the popular Big Five factors and develops robust
methods that quantify the personality traits of these models and their
underlying datasets. In particular, we trigger the models with a questionnaire
designed for personality assessment and subsequently classify the text
responses into quantifiable traits using a Zero-shot classifier. Our
classification sheds light on an important anthropomorphic element found in
such AI models and can help stakeholders decide how they should be applied and
how society could perceive them. We augment our analysis by studying approaches
that can alter these personalities.",2022-04-25
SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues,2022-04-22 09:31:13+00:00,http://arxiv.org/abs/2204.10591v1,"Ssu Chiu, Maolin Li, Yen-Ting Lin, Yun-Nung Chen","cs.CL, cs.AI",dialogue,"Dialogue systems are usually categorized into two types, open-domain and
task-oriented. The first one focuses on chatting with users and making them
engage in the conversations, where selecting a proper topic to fit the dialogue
context is essential for a successful dialogue. The other one focuses on a
specific task instead of casual talks, e.g., finding a movie on Friday night,
or playing a song. These two directions have been studied separately due to
their different purposes. However, how smoothly transitioning from social
chatting to task-oriented dialogues is important for triggering business
opportunities, and there is no public data focusing on such scenarios. Hence,
this paper focuses on investigating the conversations starting from open-domain
social chatting and then gradually transitioning to task-oriented purposes, and
releases a large-scale dataset with detailed annotations for encouraging this
research direction. To achieve this goal, this paper proposes a framework to
automatically generate many dialogues without human involvement, in which any
powerful open-domain dialogue generation model can be easily leveraged. The
human evaluation shows that our generated dialogue data has a natural flow at a
reasonable quality, showing that our released data has a great potential of
guiding future research directions and commercial activities. Furthermore, the
released models allow researchers to automatically generate unlimited dialogues
in the target scenarios, which can greatly benefit semi-supervised and
unsupervised approaches.",2022-04-22
Event Transition Planning for Open-ended Text Generation,2022-04-20 13:37:51+00:00,http://arxiv.org/abs/2204.09453v1,"Qintong Li, Piji Li, Wei Bi, Zhaochun Ren, Yuxuan Lai, Lingpeng Kong",cs.CL,dialogue,"Open-ended text generation tasks, such as dialogue generation and story
completion, require models to generate a coherent continuation given limited
preceding context. The open-ended nature of these tasks brings new challenges
to the neural auto-regressive text generators nowadays. Despite these neural
models are good at producing human-like text, it is difficult for them to
arrange causalities and relations between given facts and possible ensuing
events. To bridge this gap, we propose a novel two-stage method which
explicitly arranges the ensuing events in open-ended text generation. Our
approach can be understood as a specially-trained coarse-to-fine algorithm,
where an event transition planner provides a ""coarse"" plot skeleton and a text
generator in the second stage refines the skeleton. Experiments on two
open-ended text generation tasks demonstrate that our proposed method
effectively improves the quality of the generated text, especially in coherence
and diversity. The code is available at:
\url{https://github.com/qtli/EventPlanforTextGen}.",2022-04-20
"A Survey on Non-Autoregressive Generation for Neural Machine Translation
  and Beyond",2022-04-20 07:25:22+00:00,http://arxiv.org/abs/2204.09269v1,"Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, Tie-yan Liu","cs.CL, cs.LG",dialogue,"Non-autoregressive (NAR) generation, which is first proposed in neural
machine translation (NMT) to speed up inference, has attracted much attention
in both machine learning and natural language processing communities. While NAR
generation can significantly accelerate inference speed for machine
translation, the speedup comes at the cost of sacrificed translation accuracy
compared to its counterpart, auto-regressive (AR) generation. In recent years,
many new models and algorithms have been designed/proposed to bridge the
accuracy gap between NAR generation and AR generation. In this paper, we
conduct a systematic survey with comparisons and discussions of various
non-autoregressive translation (NAT) models from different aspects.
Specifically, we categorize the efforts of NAT into several groups, including
data manipulation, modeling methods, training criterion, decoding algorithms,
and the benefit from pre-trained models. Furthermore, we briefly review other
applications of NAR models beyond machine translation, such as dialogue
generation, text summarization, grammar error correction, semantic parsing,
speech synthesis, and automatic speech recognition. In addition, we also
discuss potential directions for future exploration, including releasing the
dependency of KD, dynamic length prediction, pre-training for NAR, and wider
applications, etc. We hope this survey can help researchers capture the latest
progress in NAR generation, inspire the design of advanced NAR models and
algorithms, and enable industry practitioners to choose appropriate solutions
for their applications. The web page of this survey is at
\url{https://github.com/LitterBrother-Xiao/Overview-of-Non-autoregressive-Applications}.",2022-04-20
"Evaluating Mixed-initiative Conversational Search Systems via User
  Simulation",2022-04-17 16:27:33+00:00,http://arxiv.org/abs/2204.08046v1,"Ivan Sekulić, Mohammad Aliannejadi, Fabio Crestani","cs.CL, cs.IR",dialogue,"Clarifying the underlying user information need by asking clarifying
questions is an important feature of modern conversational search system.
However, evaluation of such systems through answering prompted clarifying
questions requires significant human effort, which can be time-consuming and
expensive. In this paper, we propose a conversational User Simulator, called
USi, for automatic evaluation of such conversational search systems. Given a
description of an information need, USi is capable of automatically answering
clarifying questions about the topic throughout the search session. Through a
set of experiments, including automated natural language generation metrics and
crowdsourcing studies, we show that responses generated by USi are both inline
with the underlying information need and comparable to human-generated answers.
Moreover, we make the first steps towards multi-turn interactions, where
conversational search systems asks multiple questions to the (simulated) user
with a goal of clarifying the user need. To this end, we expand on currently
available datasets for studying clarifying questions, i.e., Qulac and ClariQ,
by performing a crowdsourcing-based multi-turn data acquisition. We show that
our generative, GPT2-based model, is capable of providing accurate and natural
answers to unseen clarifying questions in the single-turn setting and discuss
capabilities of our model in the multi-turn setting. We provide the code, data,
and the pre-trained model to be used for further research on the topic.",2022-04-17
UniDU: Towards A Unified Generative Dialogue Understanding Framework,2022-04-10 09:32:34+00:00,http://arxiv.org/abs/2204.04637v1,"Zhi Chen, Lu Chen, Bei Chen, Libo Qin, Yuncong Liu, Su Zhu, Jian-Guang Lou, Kai Yu",cs.CL,dialogue,"With the development of pre-trained language models, remarkable success has
been witnessed in dialogue understanding (DU) direction. However, the current
DU approaches just employ an individual model for each DU task, independently,
without considering the shared knowledge across different DU tasks. In this
paper, we investigate a unified generative dialogue understanding framework,
namely UniDU, to achieve information exchange among DU tasks. Specifically, we
reformulate the DU tasks into unified generative paradigm. In addition, to
consider different training data for each task, we further introduce
model-agnostic training strategy to optimize unified model in a balanced
manner. We conduct the experiments on ten dialogue understanding datasets,
which span five fundamental tasks: dialogue summary, dialogue completion, slot
filling, intent detection and dialogue state tracking. The proposed UniDU
framework outperforms task-specific well-designed methods on all 5 tasks. We
further conduct comprehensive analysis experiments to study the effect factors.
The experimental results also show that the proposed method obtains promising
performance on unseen dialogue domain.",2022-04-10
"BioBART: Pretraining and Evaluation of A Biomedical Generative Language
  Model",2022-04-08 08:07:42+00:00,http://arxiv.org/abs/2204.03905v1,"Hongyi Yuan, Zheng Yuan, Ruyi Gan, Jiaxing Zhang, Yutao Xie, Sheng Yu",cs.CL,dialogue,"Pretrained language models have served as important backbones for natural
language processing. Recently, in-domain pretraining has been shown to benefit
various domain-specific downstream tasks. In the biomedical domain, natural
language generation (NLG) tasks are of critical importance, while understudied.
Approaching natural language understanding (NLU) tasks as NLG achieves
satisfying performance in the general domain through constrained language
generation or language prompting. We emphasize the lack of in-domain generative
language models and the unsystematic generative downstream benchmarks in the
biomedical domain, hindering the development of the research community. In this
work, we introduce the generative language model BioBART that adapts BART to
the biomedical domain. We collate various biomedical language generation tasks
including dialogue, summarization, entity linking, and named entity
recognition. BioBART pretrained on PubMed abstracts has enhanced performance
compared to BART and set strong baselines on several tasks. Furthermore, we
conduct ablation studies on the pretraining tasks for BioBART and find that
sentence permutation has negative effects on downstream tasks.",2022-04-08
A Roadmap for Big Model,2022-03-26 15:38:00+00:00,http://arxiv.org/abs/2203.14101v1,"Sha Yuan, Hanyu Zhao, Shuai Zhao, Jiahong Leng, Yangxiao Liang, Xiaozhi Wang, Jifan Yu, Xin Lv, Zhou Shao, Jiaao He, Yankai Lin, Xu Han, Zhenghao Liu, Ning Ding, Yongming Rao, Yizhao Gao, Liang Zhang, Ming Ding, Cong Fang, Yisen Wang, Mingsheng Long, Jing Zhang, Yinpeng Dong, Tianyu Pang, Peng Cui, Lingxiao Huang, Zheng Liang, Huawei Shen, Hui Zhang, Quanshi Zhang, Qingxiu Dong, Zhixing Tan, Mingxuan Wang, Shuo Wang, Long Zhou, Haoran Li, Junwei Bao, Yingwei Pan, Weinan Zhang, Zhou Yu, Rui Yan, Chence Shi, Minghao Xu, Zuobai Zhang, Guoqiang Wang, Xiang Pan, Mengjie Li, Xiaoyu Chu, Zijun Yao, Fangwei Zhu, Shulin Cao, Weicheng Xue, Zixuan Ma, Zhengyan Zhang, Shengding Hu, Yujia Qin, Chaojun Xiao, Zheni Zeng, Ganqu Cui, Weize Chen, Weilin Zhao, Yuan Yao, Peng Li, Wenzhao Zheng, Wenliang Zhao, Ziyi Wang, Borui Zhang, Nanyi Fei, Anwen Hu, Zenan Ling, Haoyang Li, Boxi Cao, Xianpei Han, Weidong Zhan, Baobao Chang, Hao Sun, Jiawen Deng, Juanzi Li, Lei Hou, Xigang Cao, Jidong Zhai, Zhiyuan Liu, Maosong Sun, Jiwen Lu, Zhiwu Lu, Qin Jin, Ruihua Song, Ji-Rong Wen, Zhouchen Lin, Liwei Wang, Hang Su, Jun Zhu, Zhifang Sui, Jiajun Zhang, Yang Liu, Xiaodong He, Minlie Huang, Jian Tang, Jie Tang","cs.LG, cs.AI, cs.CL",dialogue,"With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view.",2022-03-26
"GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate
  Degradation of Artificial Neural Language Models",2022-03-25 00:25:42+00:00,http://arxiv.org/abs/2203.13397v1,"Changye Li, David Knopman, Weizhe Xu, Trevor Cohen, Serguei Pakhomov",cs.CL,dialogue,"Deep learning (DL) techniques involving fine-tuning large numbers of model
parameters have delivered impressive performance on the task of discriminating
between language produced by cognitively healthy individuals, and those with
Alzheimer's disease (AD). However, questions remain about their ability to
generalize beyond the small reference sets that are publicly available for
research. As an alternative to fitting model parameters directly, we propose a
novel method by which a Transformer DL model (GPT-2) pre-trained on general
English text is paired with an artificially degraded version of itself (GPT-D),
to compute the ratio between these two models' \textit{perplexities} on
language from cognitively healthy and impaired individuals. This technique
approaches state-of-the-art performance on text data from a widely used ""Cookie
Theft"" picture description task, and unlike established alternatives also
generalizes well to spontaneous conversations. Furthermore, GPT-D generates
text with characteristics known to be associated with AD, demonstrating the
induction of dementia-related linguistic anomalies. Our study is a step toward
better understanding of the relationships between the inner workings of
generative neural language models, the language that they produce, and the
deleterious effects of dementia on human speech and language characteristics.",2022-03-25
Immersive Text Game and Personality Classification,2022-03-20 18:37:03+00:00,http://arxiv.org/abs/2203.10621v1,"Wanshui Li, Yifan Bai, Jiaxuan Lu, Kexin Yi","cs.CL, cs.AI, cs.LG",dialogue,"We designed and built a game called \textit{Immersive Text Game}, which
allows the player to choose a story and a character, and interact with other
characters in the story in an immersive manner of dialogues. The game is based
on several latest models, including text generation language model, information
extraction model, commonsense reasoning model, and psychology evaluation model.
In the past, similar text games usually let players choose from limited actions
instead of answering on their own, and not every time what characters said are
determined by the player. Through the combination of these models and elaborate
game mechanics and modes, the player will find some novel experiences as driven
through the storyline.",2022-03-20
"Time Dependency, Data Flow, and Competitive Advantage",2022-03-17 07:09:30+00:00,http://arxiv.org/abs/2203.09128v1,"Ehsan Valavi, Joel Hestness, Marco Iansiti, Newsha Ardalani, Feng Zhu, Karim R. Lakhani","cs.LG, cs.CL, econ.GN, q-fin.EC",dialogue,"Data is fundamental to machine learning-based products and services and is
considered strategic due to its externalities for businesses, governments,
non-profits, and more generally for society. It is renowned that the value of
organizations (businesses, government agencies and programs, and even
industries) scales with the volume of available data. What is often less
appreciated is that the data value in making useful organizational predictions
will range widely and is prominently a function of data characteristics and
underlying algorithms.
  In this research, our goal is to study how the value of data changes over
time and how this change varies across contexts and business areas (e.g. next
word prediction in the context of history, sports, politics). We focus on data
from Reddit.com and compare the value's time-dependency across various Reddit
topics (Subreddits). We make this comparison by measuring the rate at which
user-generated text data loses its relevance to the algorithmic prediction of
conversations. We show that different subreddits have different rates of
relevance decline over time.
  Relating the text topics to various business areas of interest, we argue that
competing in a business area in which data value decays rapidly alters
strategies to acquire competitive advantage. When data value decays rapidly,
access to a continuous flow of data will be more valuable than access to a
fixed stock of data. In this kind of setting, improving user engagement and
increasing user-base help creating and maintaining a competitive advantage.",2022-03-17
"TegTok: Augmenting Text Generation via Task-specific and Open-world
  Knowledge",2022-03-16 10:37:59+00:00,http://arxiv.org/abs/2203.08517v1,"Chao-Hong Tan, Jia-Chen Gu, Chongyang Tao, Zhen-Hua Ling, Can Xu, Huang Hu, Xiubo Geng, Daxin Jiang","cs.CL, cs.AI",dialogue,"Generating natural and informative texts has been a long-standing problem in
NLP. Much effort has been dedicated into incorporating pre-trained language
models (PLMs) with various open-world knowledge, such as knowledge graphs or
wiki pages. However, their ability to access and manipulate the task-specific
knowledge is still limited on downstream tasks, as this type of knowledge is
usually not well covered in PLMs and is hard to acquire. To address the
problem, we propose augmenting TExt Generation via Task-specific and Open-world
Knowledge (TegTok) in a unified framework. Our model selects knowledge entries
from two types of knowledge sources through dense retrieval and then injects
them into the input encoding and output decoding stages respectively on the
basis of PLMs. With the help of these two types of knowledge, our model can
learn what and how to generate. Experiments on two text generation tasks of
dialogue generation and question generation, and on two datasets show that our
method achieves better performance than various baseline models.",2022-03-16
"Faithfulness in Natural Language Generation: A Systematic Survey of
  Analysis, Evaluation and Optimization Methods",2022-03-10 08:28:32+00:00,http://arxiv.org/abs/2203.05227v1,"Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, Hua Wu",cs.CL,dialogue,"Natural Language Generation (NLG) has made great progress in recent years due
to the development of deep learning techniques such as pre-trained language
models. This advancement has resulted in more fluent, coherent and even
properties controllable (e.g. stylistic, sentiment, length etc.) generation,
naturally leading to development in downstream tasks such as abstractive
summarization, dialogue generation, machine translation, and data-to-text
generation. However, the faithfulness problem that the generated text usually
contains unfaithful or non-factual information has become the biggest
challenge, which makes the performance of text generation unsatisfactory for
practical applications in many real-world scenarios. Many studies on analysis,
evaluation, and optimization methods for faithfulness problems have been
proposed for various tasks, but have not been organized, compared and discussed
in a combined manner. In this survey, we provide a systematic overview of the
research progress on the faithfulness problem of NLG, including problem
analysis, evaluation metrics and optimization methods. We organize the
evaluation and optimization methods for different tasks into a unified taxonomy
to facilitate comparison and learning across tasks. Several research trends are
discussed further.",2022-03-10
"Towards Generalized Models for Task-oriented Dialogue Modeling on Spoken
  Conversations",2022-03-08 12:26:57+00:00,http://arxiv.org/abs/2203.04045v1,"Ruijie Yan, Shuang Peng, Haitao Mi, Liang Jiang, Shihui Yang, Yuchi Zhang, Jiajun Li, Liangrui Peng, Yongliang Wang, Zujie Wen",cs.CL,dialogue,"Building robust and general dialogue models for spoken conversations is
challenging due to the gap in distributions of spoken and written data. This
paper presents our approach to build generalized models for the
Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations
Challenge of DSTC-10. In order to mitigate the discrepancies between spoken and
written text, we mainly employ extensive data augmentation strategies on
written data, including artificial error injection and round-trip text-speech
transformation. To train robust models for spoken conversations, we improve
pre-trained language models, and apply ensemble algorithms for each sub-task.
Typically, for the detection task, we fine-tune \roberta and ELECTRA, and run
an error-fixing ensemble algorithm. For the selection task, we adopt a
two-stage framework that consists of entity tracking and knowledge ranking, and
propose a multi-task learning method to learn multi-level semantic information
by domain classification and entity selection. For the generation task, we
adopt a cross-validation data process to improve pre-trained generative
language models, followed by a consensus decoding algorithm, which can add
arbitrary features like relative \rouge metric, and tune associated feature
weights toward \bleu directly. Our approach ranks third on the objective
evaluation and second on the final official human evaluation.",2022-03-08
Deep Latent-Variable Models for Text Generation,2022-03-03 23:06:39+00:00,http://arxiv.org/abs/2203.02055v1,Xiaoyu Shen,cs.CL,dialogue,"Text generation aims to produce human-like natural language output for
down-stream tasks. It covers a wide range of applications like machine
translation, document summarization, dialogue generation and so on. Recently
deep neural network-based end-to-end architectures have been widely adopted.
The end-to-end approach conflates all sub-modules, which used to be designed by
complex handcrafted rules, into a holistic encode-decode architecture. Given
enough training data, it is able to achieve state-of-the-art performance yet
avoiding the need of language/domain-dependent knowledge. Nonetheless, deep
learning models are known to be extremely data-hungry, and text generated from
them usually suffer from low diversity, interpretability and controllability.
As a result, it is difficult to trust the output from them in real-life
applications. Deep latent-variable models, by specifying the probabilistic
distribution over an intermediate latent process, provide a potential way of
addressing these problems while maintaining the expressive power of deep neural
networks. This dissertation presents how deep latent-variable models can
improve over the standard encoder-decoder model for text generation.",2022-03-03
Capturing Failures of Large Language Models via Human Cognitive Biases,2022-02-24 18:58:52+00:00,http://arxiv.org/abs/2202.12299v1,"Erik Jones, Jacob Steinhardt","cs.CL, cs.AI, cs.LG",dialogue,"Large language models generate complex, open-ended outputs: instead of
outputting a single class, they can write summaries, generate dialogue, and
produce working code. In order to study the reliability of these open-ended
systems, we must understand not just when they fail, but also how they fail. To
approach this, we draw inspiration from human cognitive biases -- systematic
patterns of deviation from rational judgement. Specifically, we use cognitive
biases to (i) identify inputs that models are likely to err on, and (ii)
develop tests to qualitatively characterize their errors on these inputs. Using
code generation as a case study, we find that OpenAI's Codex errs predictably
based on how the input prompt is framed, adjusts outputs towards anchors, and
is biased towards outputs that mimic frequent training examples. We then use
our framework to uncover high-impact errors such as incorrectly deleting files.
Our experiments suggest that cognitive science can be a useful jumping-off
point to better understand how contemporary machine learning systems behave.",2022-02-24
"Integrating AI Planning with Natural Language Processing: A Combination
  of Explicit and Tacit Knowledge",2022-02-15 02:19:09+00:00,http://arxiv.org/abs/2202.07138v1,"Kebing Jin, Hankz Hankui Zhuo","cs.AI, cs.CL",dialogue,"Automated planning focuses on strategies, building domain models and
synthesizing plans to transit initial states to goals. Natural language
processing concerns with the interactions between agents and human language,
especially processing and analyzing large amounts of natural language data.
These two fields have abilities to generate explicit knowledge, e.g.,
preconditions and effects of action models, and learn from tacit knowledge,
e.g., neural models, respectively. Integrating AI planning and natural language
processing effectively improves the communication between human and intelligent
agents. This paper outlines the commons and relations between AI planning and
natural language processing, argues that each of them can effectively impact on
the other one by four areas: (1) planning-based text understanding, (2)
planning-based text generation, (3) text-based human-robot interaction, and (4)
text-based explainable planning. We also explore some potential future issues
between AI planning and natural language processing.",2022-02-15
Survey of Hallucination in Natural Language Generation,2022-02-08 03:55:01+00:00,http://arxiv.org/abs/2202.03629v1,"Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, Pascale Fung","cs.CL, A.1",dialogue,"Natural Language Generation (NLG) has improved exponentially in recent years
thanks to the development of deep learning technologies such as
Transformer-based language models. This advancement has led to more fluent and
coherent natural language generation, naturally leading to development in
downstream tasks such as abstractive summarization, dialogue generation and
data-to-text generation. However, it is also investigated that such generation
includes hallucinated texts, which makes the performances of text generation
fail to meet users' expectations in many real-world scenarios. In order to
address this issue, studies in evaluation and mitigation methods of
hallucinations have been presented in various tasks, but have not been reviewed
in a combined manner. In this survey, we provide a broad overview of the
research progress and challenges in the hallucination problem of NLG. The
survey is organized into two big divisions: (i) a general overview of metrics,
mitigation methods, and future directions; (ii) task-specific research progress
for hallucinations in a large set of downstream tasks: abstractive
summarization, dialogue generation, generative question answering, data-to-text
generation, and machine translation. This survey could facilitate collaborative
efforts among researchers in these tasks.",2022-02-08
Red Teaming Language Models with Language Models,2022-02-07 15:22:17+00:00,http://arxiv.org/abs/2202.03286v1,"Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, Geoffrey Irving","cs.CL, cs.AI, cs.CR, cs.LG",dialogue,"Language Models (LMs) often cannot be deployed because of their potential to
harm users in hard-to-predict ways. Prior work identifies harmful behaviors
before deployment by using human annotators to hand-write test cases. However,
human annotation is expensive, limiting the number and diversity of test cases.
In this work, we automatically find cases where a target LM behaves in a
harmful way, by generating test cases (""red teaming"") using another LM. We
evaluate the target LM's replies to generated test questions using a classifier
trained to detect offensive content, uncovering tens of thousands of offensive
replies in a 280B parameter LM chatbot. We explore several methods, from
zero-shot generation to reinforcement learning, for generating test cases with
varying levels of diversity and difficulty. Furthermore, we use prompt
engineering to control LM-generated test cases to uncover a variety of other
harms, automatically finding groups of people that the chatbot discusses in
offensive ways, personal and hospital phone numbers generated as the chatbot's
own contact info, leakage of private training data in generated text, and harms
that occur over the course of a conversation. Overall, LM-based red teaming is
one promising tool (among many needed) for finding and fixing diverse,
undesirable LM behaviors before impacting users.",2022-02-07
A Survey on Retrieval-Augmented Text Generation,2022-02-02 16:18:41+00:00,http://arxiv.org/abs/2202.01110v1,"Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu",cs.CL,dialogue,"Recently, retrieval-augmented text generation attracted increasing attention
of the computational linguistics community. Compared with conventional
generation models, retrieval-augmented text generation has remarkable
advantages and particularly has achieved state-of-the-art performance in many
NLP tasks. This paper aims to conduct a survey about retrieval-augmented text
generation. It firstly highlights the generic paradigm of retrieval-augmented
generation, and then it reviews notable approaches according to different tasks
including dialogue response generation, machine translation, and other
generation tasks. Finally, it points out some important directions on top of
recent methods to facilitate future research.",2022-02-02
"Language Generation for Broad-Coverage, Explainable Cognitive Systems",2022-01-25 16:09:19+00:00,http://arxiv.org/abs/2201.10422v1,"Marjorie McShane, Ivan Leon","cs.CL, cs.AI",dialogue,"This paper describes recent progress on natural language generation (NLG) for
language-endowed intelligent agents (LEIAs) developed within the OntoAgent
cognitive architecture. The approach draws heavily from past work on natural
language understanding in this paradigm: it uses the same knowledge bases,
theory of computational linguistics, agent architecture, and methodology of
developing broad-coverage capabilities over time while still supporting
near-term applications.",2022-01-25
Measuring Attribution in Natural Language Generation Models,2021-12-23 22:33:20+00:00,http://arxiv.org/abs/2112.12870v1,"Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, David Reitter",cs.CL,dialogue,"With recent improvements in natural language generation (NLG) models for
various applications, it has become imperative to have the means to identify
and evaluate whether NLG output is only sharing verifiable information about
the external world. In this work, we present a new evaluation framework
entitled Attributable to Identified Sources (AIS) for assessing the output of
natural language generation models, when such output pertains to the external
world. We first define AIS and introduce a two-stage annotation pipeline for
allowing annotators to appropriately evaluate model output according to AIS
guidelines. We empirically validate this approach on three generation datasets
(two in the conversational QA domain and one in summarization) via human
evaluation studies that suggest that AIS could serve as a common framework for
measuring whether model-generated statements are supported by underlying
sources. We release guidelines for the human evaluation studies.",2021-12-23
Taming Repetition in Dialogue Generation,2021-12-16 06:25:46+00:00,http://arxiv.org/abs/2112.08657v1,"Yadong Xi, Jiashu Pu, Xiaoxi Mao",cs.CL,dialogue,"The wave of pre-training language models has been continuously improving the
quality of the machine-generated conversations, however, some of the generated
responses still suffer from excessive repetition, sometimes repeating words
from utterance, sometimes repeating words within self-generated responses, or
both. Inappropriate repetition of words can significantly degrade the quality
of the generated texts. Penalized sampling is one popular solution, reducing
the sampling probability of existing words during inference, however, it is
highly vulnerable to the inappropriate setting of the static weight. Setting it
too high can yield strange and unrealistic sentences while setting it too low
makes the task of suppressing repetition trivial. To remedy the shortcomings of
the above methods, we design a context-aware classifier to explicitly decide
when to allow repetition and when to employ penalized sampling. Such a
classifier can be easily integrated with existing decoding methods, reducing
repetitions where appropriate while preserving the diversity of the text.
Experimental results demonstrate that our method can generate higher quality
and more authentic dialogues.",2021-12-16
DG2: Data Augmentation Through Document Grounded Dialogue Generation,2021-12-15 18:50:14+00:00,http://arxiv.org/abs/2112.08342v1,"Qingyang Wu, Song Feng, Derek Chen, Sachindra Joshi, Luis A. Lastras, Zhou Yu",cs.CL,dialogue,"Collecting data for training dialog systems can be extremely expensive due to
the involvement of human participants and need for extensive annotation.
Especially in document-grounded dialog systems, human experts need to carefully
read the unstructured documents to answer the users' questions. As a result,
existing document-grounded dialog datasets are relatively small-scale and
obstruct the effective training of dialogue systems. In this paper, we propose
an automatic data augmentation technique grounded on documents through a
generative dialogue model. The dialogue model consists of a user bot and agent
bot that can synthesize diverse dialogues given an input document, which are
then used to train a downstream model. When supplementing the original dataset,
our method achieves significant improvement over traditional data augmentation
methods. We also achieve great performance in the low-resource setting.",2021-12-15
Dynamic Human Evaluation for Relative Model Comparisons,2021-12-15 11:32:13+00:00,http://arxiv.org/abs/2112.08048v1,"Thórhildur Thorleiksdóttir, Cedric Renggli, Nora Hollenstein, Ce Zhang",cs.CL,dialogue,"Collecting human judgements is currently the most reliable evaluation method
for natural language generation systems. Automatic metrics have reported flaws
when applied to measure quality aspects of generated text and have been shown
to correlate poorly with human judgements. However, human evaluation is time
and cost-intensive, and we lack consensus on designing and conducting human
evaluation experiments. Thus there is a need for streamlined approaches for
efficient collection of human judgements when evaluating natural language
generation systems. Therefore, we present a dynamic approach to measure the
required number of human annotations when evaluating generated outputs in
relative comparison settings. We propose an agent-based framework of human
evaluation to assess multiple labelling strategies and methods to decide the
better model in a simulation and a crowdsourcing case study. The main results
indicate that a decision about the superior model can be made with high
probability across different labelling strategies, where assigning a single
random worker per task requires the least overall labelling effort and thus the
least cost.",2021-12-15
Controlled Cue Generation for Play Scripts,2021-12-13 19:00:17+00:00,http://arxiv.org/abs/2112.06953v1,"Alara Dirik, Hilal Donmez, Pinar Yanardag","cs.CL, cs.AI, cs.LG",dialogue,"In this paper, we use a large-scale play scripts dataset to propose the novel
task of theatrical cue generation from dialogues. Using over one million lines
of dialogue and cues, we approach the problem of cue generation as a controlled
text generation task, and show how cues can be used to enhance the impact of
dialogue using a language model conditioned on a dialogue/cue discriminator. In
addition, we explore the use of topic keywords and emotions for controlled text
generation. Extensive quantitative and qualitative experiments show that
language models can be successfully used to generate plausible and
attribute-controlled texts in highly specialised domains such as play scripts.
Supporting materials can be found at: https://catlab-team.github.io/cuegen.",2021-12-13
"Representation Learning for Conversational Data using Discourse Mutual
  Information Maximization",2021-12-04 13:17:07+00:00,http://arxiv.org/abs/2112.05787v1,"Bishal Santra, Sumegh Roychowdhury, Aishik Mandal, Vasu Gurram, Atharva Naik, Manish Gupta, Pawan Goyal",cs.CL,dialogue,"Although many pretrained models exist for text or images, there have been
relatively fewer attempts to train representations specifically for dialog
understanding. Prior works usually relied on finetuned representations based on
generic text representation models like BERT or GPT-2. But, existing
pretraining objectives do not take the structural information of text into
consideration. Although generative dialog models can learn structural features
too, we argue that the structure-unaware word-by-word generation is not
suitable for effective conversation modeling. We empirically demonstrate that
such representations do not perform consistently across various dialog
understanding tasks. Hence, we propose a structure-aware Mutual Information
based loss-function DMI (Discourse Mutual Information) for training
dialog-representation models, that additionally captures the inherent
uncertainty in response prediction. Extensive evaluation on nine diverse dialog
modeling tasks shows that our proposed DMI-based models outperform strong
baselines by significant margins, even with small-scale pretraining. Our models
show the most promising performance on the dialog evaluation task
DailyDialog++, in both random and adversarial negative scenarios.",2021-12-04
Realistic simulation of users for IT systems in cyber ranges,2021-11-23 10:53:29+00:00,http://arxiv.org/abs/2111.11785v1,"Alexandre Dey, Benjamin Costé, Éric Totel, Adrien Bécue","cs.AI, cs.CR",dialogue,"Generating user activity is a key capability for both evaluating security
monitoring tools as well as improving the credibility of attacker analysis
platforms (e.g., honeynets). In this paper, to generate this activity, we
instrument each machine by means of an external agent. This agent combines both
deterministic and deep learning based methods to adapt to different environment
(e.g., multiple OS, software versions, etc.), while maintaining high
performances. We also propose conditional text generation models to facilitate
the creation of conversations and documents to accelerate the definition of
coherent, system-wide, life scenarios.",2021-11-23
"MEDCOD: A Medically-Accurate, Emotive, Diverse, and Controllable Dialog
  System",2021-11-17 20:31:16+00:00,http://arxiv.org/abs/2111.09381v1,"Rhys Compton, Ilya Valmianski, Li Deng, Costa Huang, Namit Katariya, Xavier Amatriain, Anitha Kannan","cs.CL, cs.AI, cs.LG",dialogue,"We present MEDCOD, a Medically-Accurate, Emotive, Diverse, and Controllable
Dialog system with a unique approach to the natural language generator module.
MEDCOD has been developed and evaluated specifically for the history taking
task. It integrates the advantage of a traditional modular approach to
incorporate (medical) domain knowledge with modern deep learning techniques to
generate flexible, human-like natural language expressions. Two key aspects of
MEDCOD's natural language output are described in detail. First, the generated
sentences are emotive and empathetic, similar to how a doctor would communicate
to the patient. Second, the generated sentence structures and phrasings are
varied and diverse while maintaining medical consistency with the desired
medical concept (provided by the dialogue manager module of MEDCOD).
Experimental results demonstrate the effectiveness of our approach in creating
a human-like medical dialogue system. Relevant code is available at
https://github.com/curai/curai-research/tree/main/MEDCOD",2021-11-17
A Review of Dialogue Systems: From Trained Monkeys to Stochastic Parrots,2021-11-02 08:07:55+00:00,http://arxiv.org/abs/2111.01414v1,"Atharv Singh Patlan, Shiven Tripathi, Shubham Korde","cs.CL, cs.AI",dialogue,"In spoken dialogue systems, we aim to deploy artificial intelligence to build
automated dialogue agents that can converse with humans. Dialogue systems are
increasingly being designed to move beyond just imitating conversation and also
improve from such interactions over time. In this survey, we present a broad
overview of methods developed to build dialogue systems over the years.
Different use cases for dialogue systems ranging from task-based systems to
open domain chatbots motivate and necessitate specific systems. Starting from
simple rule-based systems, research has progressed towards increasingly complex
architectures trained on a massive corpus of datasets, like deep learning
systems. Motivated with the intuition of resembling human dialogues, progress
has been made towards incorporating emotions into the natural language
generator, using reinforcement learning. While we see a trend of highly
marginal improvement on some metrics, we find that limited justification exists
for the metrics, and evaluation practices are not uniform. To conclude, we flag
these concerns and highlight possible research directions.",2021-11-02
"Dynamic population-based meta-learning for multi-agent communication
  with natural language",2021-10-27 07:50:02+00:00,http://arxiv.org/abs/2110.14241v1,"Abhinav Gupta, Marc Lanctot, Angeliki Lazaridou","cs.LG, cs.AI, cs.CL, cs.MA",dialogue,"In this work, our goal is to train agents that can coordinate with seen,
unseen as well as human partners in a multi-agent communication environment
involving natural language. Previous work using a single set of agents has
shown great progress in generalizing to known partners, however it struggles
when coordinating with unfamiliar agents. To mitigate that, recent work
explored the use of population-based approaches, where multiple agents interact
with each other with the goal of learning more generic protocols. These
methods, while able to result in good coordination between unseen partners,
still only achieve so in cases of simple languages, thus failing to adapt to
human partners using natural language. We attribute this to the use of static
populations and instead propose a dynamic population-based meta-learning
approach that builds such a population in an iterative manner. We perform a
holistic evaluation of our method on two different referential games, and show
that our agents outperform all prior work when communicating with seen partners
and humans. Furthermore, we analyze the natural language generation skills of
our agents, where we find that our agents also outperform strong baselines.
Finally, we test the robustness of our agents when communicating with
out-of-population agents and carefully test the importance of each component of
our method through ablation studies.",2021-10-27
"FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metricsfor
  Automatic Text Generation",2021-10-16 11:59:48+00:00,http://arxiv.org/abs/2110.08559v1,"Moussa Kamal Eddine, Guokan Shang, Antoine J. -P. Tixier, Michalis Vazirgiannis",cs.CL,dialogue,"Fast and reliable evaluation metrics are key to R&D progress. While
traditional natural language generation metrics are fast, they are not very
reliable. Conversely, new metrics based on large pretrained language models are
much more reliable, but require significant computational resources. In this
paper, we propose FrugalScore, an approach to learn a fixed, low cost version
of any expensive NLG metric, while retaining most of its original performance.
Experiments with BERTScore and MoverScore on summarization and translation show
that FrugalScore is on par with the original metrics (and sometimes better),
while having several orders of magnitude less parameters and running several
times faster. On average over all learned metrics, tasks, and variants,
FrugalScore retains 96.8% of the performance, runs 24 times faster, and has 35
times less parameters than the original metrics. We make our trained metrics
publicly available, to benefit the entire NLP community and in particular
researchers and practitioners with limited resources.",2021-10-16
"Hindsight: Posterior-guided training of retrievers for improved
  open-ended generation",2021-10-14 22:24:57+00:00,http://arxiv.org/abs/2110.07752v2,"Ashwin Paranjape, Omar Khattab, Christopher Potts, Matei Zaharia, Christopher D. Manning","cs.CL, cs.IR",dialogue,"Many text generation systems benefit from using a retriever to retrieve
passages from a textual knowledge corpus (e.g., Wikipedia) which are then
provided as additional context to the generator. For open-ended generation
tasks (like generating informative utterances in conversations) many varied
passages may be equally relevant and we find that existing methods that jointly
train the retriever and generator underperform: the retriever may not find
relevant passages even amongst the top-10 and hence the generator may not learn
a preference to ground its generated output in them. We propose using an
additional guide retriever that is allowed to use the target output and ""in
hindsight"" retrieve relevant passages during training. We model the guide
retriever after the posterior distribution Q of passages given the input and
the target output and train it jointly with the standard retriever and the
generator by maximizing the evidence lower bound (ELBo) in expectation over Q.
For informative conversations from the Wizard of Wikipedia dataset, with
posterior-guided training, the retriever finds passages with higher relevance
in the top-10 (23% relative improvement), the generator's responses are more
grounded in the retrieved passage (19% relative improvement) and the end-to-end
system produces better overall output (6.4% relative improvement).",2021-10-14
Federated Natural Language Generation for Personalized Dialogue System,2021-10-13 00:59:52+00:00,http://arxiv.org/abs/2110.06419v1,"Yujie Lu, Chao Huang, Huanli Zhan, Yong Zhuang","cs.CL, cs.AI",dialogue,"Neural conversational models have long suffered from the problem of
inconsistency and lacking coherent personality. To address the issue,
persona-based models capturing individual characteristics have been proposed,
but they still face the dilemma of model adaption and data privacy. To break
this dilemma, we propose a novel Federated Natural Language Generation (FedNLG)
framework, which learns personalized representations from various dataset on
distributed devices, and thus implements the personalized dialogue system
efficiently and safely. FedNLG first pre-trains parameters of standard neural
conversational model over a large dialogue corpus, and then fine-tune the model
parameters and persona embeddings on specific datasets, in a federated manner.
Thus, the model could simultaneously learn the persona embeddings in local
clients and learn shared model parameters by federated aggregation, which
achieves accuracyprivacy balance. By conducting extensive experiments, we
demonstrate the effectiveness of our model by pre-training model over Cornell
Movie-Dialogs Corpus and fine-tuning the model over two TV series dataset.",2021-10-13
"OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset
  with Visual Contexts",2021-09-27 02:10:29+00:00,http://arxiv.org/abs/2109.12761v2,"Shuhe Wang, Yuxian Meng, Xiaoya Li, Xiaofei Sun, Rongbin Ouyang, Jiwei Li",cs.CL,dialogue,"In order to better simulate the real human conversation process, models need
to generate dialogue utterances based on not only preceding textual contexts
but also visual contexts. However, with the development of multi-modal dialogue
learning, the dataset scale gradually becomes a bottleneck. In this report, we
release OpenViDial 2.0, a larger-scale open-domain multi-modal dialogue dataset
compared to the previous version OpenViDial 1.0. OpenViDial 2.0 contains a
total number of 5.6 million dialogue turns extracted from either movies or TV
series from different resources, and each dialogue turn is paired with its
corresponding visual context. We hope this large-scale dataset can help
facilitate future researches on open-domain multi-modal dialog generation,
e.g., multi-modal pretraining for dialogue generation.",2021-09-27
"An animated picture says at least a thousand words: Selecting Gif-based
  Replies in Multimodal Dialog",2021-09-24 21:48:27+00:00,http://arxiv.org/abs/2109.12212v1,"Xingyao Wang, David Jurgens","cs.CL, cs.CV, cs.CY",dialogue,"Online conversations include more than just text. Increasingly, image-based
responses such as memes and animated gifs serve as culturally recognized and
often humorous responses in conversation. However, while NLP has broadened to
multimodal models, conversational dialog systems have largely focused only on
generating text replies. Here, we introduce a new dataset of 1.56M text-gif
conversation turns and introduce a new multimodal conversational model Pepe the
King Prawn for selecting gif-based replies. We demonstrate that our model
produces relevant and high-quality gif responses and, in a large randomized
control trial of multiple models replying to real users, we show that our model
replies with gifs that are significantly better received by the community.",2021-09-24
Style Control for Schema-Guided Natural Language Generation,2021-09-24 21:47:58+00:00,http://arxiv.org/abs/2109.12211v1,"Alicia Y. Tsai, Shereen Oraby, Vittorio Perera, Jiun-Yu Kao, Yuheng Du, Anjali Narayan-Chen, Tagyoung Chung, Dilek Hakkani-Tur",cs.CL,dialogue,"Natural Language Generation (NLG) for task-oriented dialogue systems focuses
on communicating specific content accurately, fluently, and coherently. While
these attributes are crucial for a successful dialogue, it is also desirable to
simultaneously accomplish specific stylistic goals, such as response length,
point-of-view, descriptiveness, sentiment, formality, and empathy. In this
work, we focus on stylistic control and evaluation for schema-guided NLG, with
joint goals of achieving both semantic and stylistic control. We experiment in
detail with various controlled generation methods for large pretrained language
models: specifically, conditional training, guided fine-tuning, and guided
decoding. We discuss their advantages and limitations, and evaluate them with a
broad range of automatic and human evaluation metrics. Our results show that
while high style accuracy and semantic correctness are easier to achieve for
more lexically-defined styles with conditional training, stylistic control is
also achievable for more semantically complex styles using discriminator-based
guided decoding methods. The results also suggest that methods that are more
scalable (with less hyper-parameters tuning) and that disentangle content
generation and stylistic variations are more effective at achieving semantic
correctness and style accuracy.",2021-09-24
"Controllable Dialogue Generation with Disentangled Multi-grained Style
  Specification and Attribute Consistency Reward",2021-09-14 14:29:38+00:00,http://arxiv.org/abs/2109.06717v1,"Zhe Hu, Zhiwei Cao, Hou Pong Chan, Jiachen Liu, Xinyan Xiao, Jinsong Su, Hua Wu","cs.CL, cs.AI",dialogue,"Controllable text generation is an appealing but challenging task, which
allows users to specify particular attributes of the generated outputs. In this
paper, we propose a controllable dialogue generation model to steer response
generation under multi-attribute constraints. Specifically, we define and
categorize the commonly used control attributes into global and local ones,
which possess different granularities of effects on response generation. Then,
we significantly extend the conventional seq2seq framework by introducing a
novel two-stage decoder, which first uses a multi-grained style specification
layer to impose the stylistic constraints and determine word-level control
states of responses based on the attributes, and then employs a response
generation layer to generate final responses maintaining both semantic
relevancy to the contexts and fidelity to the attributes. Furthermore, we train
our model with an attribute consistency reward to promote response control with
explicit supervision signals. Extensive experiments and in-depth analyses on
two datasets indicate that our model can significantly outperform competitive
baselines in terms of response quality, content diversity and controllability.",2021-09-14
"End-to-End Conversational Search for Online Shopping with Utterance
  Transfer",2021-09-12 08:33:44+00:00,http://arxiv.org/abs/2109.05460v1,"Liqiang Xiao, Jun Ma2, Xin Luna Dong, Pascual Martinez-Gomez, Nasser Zalmout, Wei Chen, Tong Zhao, Hao He, Yaohui Jin","cs.CL, cs.AI",dialogue,"Successful conversational search systems can present natural, adaptive and
interactive shopping experience for online shopping customers. However,
building such systems from scratch faces real word challenges from both
imperfect product schema/knowledge and lack of training dialog data.In this
work we first propose ConvSearch, an end-to-end conversational search system
that deeply combines the dialog system with search. It leverages the text
profile to retrieve products, which is more robust against imperfect product
schema/knowledge compared with using product attributes alone. We then address
the lack of data challenges by proposing an utterance transfer approach that
generates dialogue utterances by using existing dialog from other domains, and
leveraging the search behavior data from e-commerce retailer. With utterance
transfer, we introduce a new conversational search dataset for online shopping.
Experiments show that our utterance transfer method can significantly improve
the availability of training dialogue data without crowd-sourcing, and the
conversational search system significantly outperformed the best tested
baseline.",2021-09-12
Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration,2021-09-12 04:17:53+00:00,http://arxiv.org/abs/2109.05426v1,"Chuanxin Tang, Chong Luo, Zhiyuan Zhao, Dacheng Yin, Yucheng Zhao, Wenjun Zeng","cs.SD, cs.AI, eess.AS",dialogue,"Given a piece of speech and its transcript text, text-based speech editing
aims to generate speech that can be seamlessly inserted into the given speech
by editing the transcript. Existing methods adopt a two-stage approach:
synthesize the input text using a generic text-to-speech (TTS) engine and then
transform the voice to the desired voice using voice conversion (VC). A major
problem of this framework is that VC is a challenging problem which usually
needs a moderate amount of parallel training data to work satisfactorily. In
this paper, we propose a one-stage context-aware framework to generate natural
and coherent target speech without any training data of the target speaker. In
particular, we manage to perform accurate zero-shot duration prediction for the
inserted text. The predicted duration is used to regulate both text embedding
and speech embedding. Then, based on the aligned cross-modality input, we
directly generate the mel-spectrogram of the edited speech with a
transformer-based decoder. Subjective listening tests show that despite the
lack of training data for the speaker, our method has achieved satisfactory
results. It outperforms a recent zero-shot TTS engine by a large margin.",2021-09-12
Refocusing on Relevance: Personalization in NLG,2021-09-10 23:50:02+00:00,http://arxiv.org/abs/2109.05140v1,"Shiran Dudy, Steven Bedrick, Bonnie Webber","cs.CL, cs.CY, cs.HC",dialogue,"Many NLG tasks such as summarization, dialogue response, or open domain
question answering focus primarily on a source text in order to generate a
target response. This standard approach falls short, however, when a user's
intent or context of work is not easily recoverable based solely on that source
text -- a scenario that we argue is more of the rule than the exception. In
this work, we argue that NLG systems in general should place a much higher
level of emphasis on making use of additional context, and suggest that
relevance (as used in Information Retrieval) be thought of as a crucial tool
for designing user-oriented text-generating tasks. We further discuss possible
harms and hazards around such personalization, and argue that value-sensitive
design represents a crucial path forward through these challenges.",2021-09-10
"Generating Self-Contained and Summary-Centric Question Answer Pairs via
  Differentiable Reward Imitation Learning",2021-09-10 06:34:55+00:00,http://arxiv.org/abs/2109.04689v1,"Li Zhou, Kevin Small, Yong Zhang, Sandeep Atluri","cs.CL, cs.AI, cs.LG",dialogue,"Motivated by suggested question generation in conversational news
recommendation systems, we propose a model for generating question-answer pairs
(QA pairs) with self-contained, summary-centric questions and
length-constrained, article-summarizing answers. We begin by collecting a new
dataset of news articles with questions as titles and pairing them with
summaries of varying length. This dataset is used to learn a QA pair generation
model producing summaries as answers that balance brevity with sufficiency
jointly with their corresponding questions. We then reinforce the QA pair
generation process with a differentiable reward function to mitigate exposure
bias, a common problem in natural language generation. Both automatic metrics
and human evaluation demonstrate these QA pairs successfully capture the
central gists of the articles and achieve high answer accuracy.",2021-09-10
"Hi, my name is Martha: Using names to measure and mitigate bias in
  generative dialogue models",2021-09-07 19:20:24+00:00,http://arxiv.org/abs/2109.03300v1,"Eric Michael Smith, Adina Williams",cs.CL,dialogue,"All AI models are susceptible to learning biases in data that they are
trained on. For generative dialogue models, being trained on real human
conversations containing unbalanced gender and race/ethnicity references can
lead to models that display learned biases, which we define here broadly as any
measurable differences in the distributions of words or semantic content of
conversations based on demographic groups. We measure the strength of such
biases by producing artificial conversations between two copies of a dialogue
model, conditioning one conversational partner to state a name commonly
associated with a certain gender and/or race/ethnicity. We find that larger
capacity models tend to exhibit more gender bias and greater stereotyping of
occupations by gender. We show that several methods of tuning these dialogue
models, specifically name scrambling, controlled generation, and unlikelihood
training, are effective in reducing bias in conversation, including on a
downstream conversational task. Name scrambling is also effective in lowering
differences in token usage across conversations where partners have names
associated with different genders or races/ethnicities.",2021-09-07
"Naturalness Evaluation of Natural Language Generation in Task-oriented
  Dialogues using BERT",2021-09-07 08:40:14+00:00,http://arxiv.org/abs/2109.02938v1,"Ye Liu, Wolfgang Maier, Wolfgang Minker, Stefan Ultes","cs.CL, cs.AI",dialogue,"This paper presents an automatic method to evaluate the naturalness of
natural language generation in dialogue systems. While this task was previously
rendered through expensive and time-consuming human labor, we present this
novel task of automatic naturalness evaluation of generated language. By
fine-tuning the BERT model, our proposed naturalness evaluation method shows
robust results and outperforms the baselines: support vector machines,
bi-directional LSTMs, and BLEURT. In addition, the training speed and
evaluation performance of naturalness model are improved by transfer learning
from quality and informativeness linguistic knowledge.",2021-09-07
"SideControl: Controlled Open-domain Dialogue Generation via Additive
  Side Networks",2021-09-05 01:15:26+00:00,http://arxiv.org/abs/2109.01958v1,"Wanyu Du, Yangfeng Ji",cs.CL,dialogue,"Transformer-based pre-trained language models boost the performance of
open-domain dialogue systems. Prior works leverage Transformer-based
pre-trained language models to generate texts with desired attributes in two
general approaches: (1) gradient-based methods: updating all latent
representations of pre-trained models with gradients from attribute models; (2)
weighted-decoding methods: re-ranking beam candidates from pre-trained models
with attribute functions. However, gradient-based methods lead to high
computation cost and can easily get overfitted on small training sets, while
weighted-decoding methods are inherently constrained by the low-variance
high-bias pre-trained model. In this work, we propose a novel approach to
control the generation of Transformer-based pre-trained language models: the
SideControl framework, which leverages a novel control attributes loss to
incorporate useful control signals, and is shown to perform well with very
limited training samples. We evaluate our proposed method on two benchmark
open-domain dialogue datasets, and results show that the SideControl framework
has better controllability, higher generation quality and better
sample-efficiency than existing gradient-based and weighted-decoding baselines.",2021-09-05
Task-Oriented Dialogue System as Natural Language Generation,2021-08-31 08:36:42+00:00,http://arxiv.org/abs/2108.13679v2,"Weizhi Wang, Zhirui Zhang, Junliang Guo, Yinpei Dai, Boxing Chen, Weihua Luo","cs.CL, cs.AI",dialogue,"In this paper, we propose to formulate the task-oriented dialogue system as
the purely natural language generation task, so as to fully leverage the
large-scale pre-trained models like GPT-2 and simplify complicated
delexicalization prepossessing. However, directly applying this method heavily
suffers from the dialogue entity inconsistency caused by the removal of
delexicalized tokens, as well as the catastrophic forgetting problem of the
pre-trained model during fine-tuning, leading to unsatisfactory performance. To
alleviate these problems, we design a novel GPT-Adapter-CopyNet network, which
incorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve
better performance on transfer learning and dialogue entity generation.
Experimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ
dataset demonstrate that our proposed approach significantly outperforms
baseline models with a remarkable performance on automatic and human
evaluations.",2021-08-31
Semantic-based Self-Critical Training For Question Generation,2021-08-26 20:33:35+00:00,http://arxiv.org/abs/2108.12026v1,"Loïc, Kwate Dassi","cs.CL, cs.AI",dialogue,"We present in this work a fully Transformer-based reinforcement learning
generator-evaluator architecture for neural question generation. Question
generation is a task that consists in generating questions given a context and
answer. To improve the quality of the generated question, we came up with a
semantic-based self-critical training layout in generator-evaluator
architecture, which goes beyond typical maximum likelihood training. Evaluation
metrics for language modeling only based on n-gram overlapping do not consider
semantic relations between reference and candidate strings. To improve the
evaluation step, we assess our model for both n-gram overlap using BLEU and
semantically using BERTScore and NUBIA, a novel state-of-the-art evaluation
metric for text generation. Question generation could be used in many
downstream applications, including in extending question answering datasets,
conversational systems, and educational assessment systems.",2021-08-26
"Just Say No: Analyzing the Stance of Neural Dialogue Generation in
  Offensive Contexts",2021-08-26 14:58:05+00:00,http://arxiv.org/abs/2108.11830v1,"Ashutosh Baheti, Maarten Sap, Alan Ritter, Mark Riedl",cs.CL,dialogue,"Dialogue models trained on human conversations inadvertently learn to
generate offensive responses. Moreover, models can insult anyone by agreeing
with an offensive context. To understand the dynamics of contextually offensive
language, we study the stance of dialogue model responses in offensive Reddit
conversations. Specifically, we crowd-annotate ToxiChat, a new dataset of 2,000
Reddit threads and model responses labeled with offensive language and stance.
Our analysis reveals that 42% of user responses agree with toxic comments; 3x
their agreement with safe comments (13%). Pre-trained transformer-based
classifiers fine-tuned on our dataset achieve 0.71 F1 for offensive labels and
0.53 Macro-F1 for stance labels. Finally, we analyze some existing controllable
text generation (CTG) methods to mitigate the contextual offensive behavior of
dialogue models. Compared to the baseline, our best CTG model obtains a 19%
reduction in agreement with offensive context and 29% fewer offensive
responses. This highlights the need for future work to characterize and analyze
more forms of inappropriate behavior in dialogue models to help make them
safer. Our code and corpus are available at
https://github.com/abaheti95/ToxiChat .",2021-08-26
Viola: A Topic Agnostic Generate-and-Rank Dialogue System,2021-08-25 06:20:34+00:00,http://arxiv.org/abs/2108.11063v1,"Hyundong Cho, Basel Shbita, Kartik Shenoy, Shuai Liu, Nikhil Patel, Hitesh Pindikanti, Jennifer Lee, Jonathan May",cs.CL,dialogue,"We present Viola, an open-domain dialogue system for spoken conversation that
uses a topic-agnostic dialogue manager based on a simple generate-and-rank
approach. Leveraging recent advances of generative dialogue systems powered by
large language models, Viola fetches a batch of response candidates from
various neural dialogue models trained with different datasets and
knowledge-grounding inputs. Additional responses originating from
template-based generators are also considered, depending on the user's input
and detected entities. The hand-crafted generators build on a dynamic knowledge
graph injected with rich content that is crawled from the web and automatically
processed on a daily basis. Viola's response ranker is a fine-tuned polyencoder
that chooses the best response given the dialogue history. While dedicated
annotations for the polyencoder alone can indirectly steer it away from
choosing problematic responses, we add rule-based safety nets to detect neural
degeneration and a dedicated classifier to filter out offensive content. We
analyze conversations that Viola took part in for the Alexa Prize Socialbot
Grand Challenge 4 and discuss the strengths and weaknesses of our approach.
Lastly, we suggest future work with a focus on curating conversation data
specifcially for socialbots that will contribute towards a more robust
data-driven socialbot.",2021-08-25
"Using BERT Encoding and Sentence-Level Language Model for Sentence
  Ordering",2021-08-24 23:03:36+00:00,http://arxiv.org/abs/2108.10986v1,"Melika Golestani, Seyedeh Zahra Razavi, Zeinab Borhanifard, Farnaz Tahmasebian, Hesham Faili",cs.CL,dialogue,"Discovering the logical sequence of events is one of the cornerstones in
Natural Language Understanding. One approach to learn the sequence of events is
to study the order of sentences in a coherent text. Sentence ordering can be
applied in various tasks such as retrieval-based Question Answering, document
summarization, storytelling, text generation, and dialogue systems.
Furthermore, we can learn to model text coherence by learning how to order a
set of shuffled sentences. Previous research has relied on RNN, LSTM, and
BiLSTM architecture for learning text language models. However, these networks
have performed poorly due to the lack of attention mechanisms. We propose an
algorithm for sentence ordering in a corpus of short stories. Our proposed
method uses a language model based on Universal Transformers (UT) that captures
sentences' dependencies by employing an attention mechanism. Our method
improves the previous state-of-the-art in terms of Perfect Match Ratio (PMR)
score in the ROCStories dataset, a corpus of nearly 100K short human-made
stories. The proposed model includes three components: Sentence Encoder,
Language Model, and Sentence Arrangement with Brute Force Search. The first
component generates sentence embeddings using SBERT-WK pre-trained model
fine-tuned on the ROCStories data. Then a Universal Transformer network
generates a sentence-level language model. For decoding, the network generates
a candidate sentence as the following sentence of the current sentence. We use
cosine similarity as a scoring function to assign scores to the candidate
embedding and the embeddings of other sentences in the shuffled set. Then a
Brute Force Search is employed to maximize the sum of similarities between
pairs of consecutive sentences.",2021-08-24
Taming the Beast: Learning to Control Neural Conversational Models,2021-08-24 07:58:16+00:00,http://arxiv.org/abs/2108.10561v1,Andrea Madotto,"cs.CL, cs.AI, cs.LG",dialogue,"This thesis investigates the controllability of deep learning-based,
end-to-end, generative dialogue systems in both task-oriented and chit-chat
scenarios. In particular, we study the different aspects of controlling
generative dialogue systems, including controlling styles and topics and
continuously adding and combining dialogue skills. In the three decades since
the first dialogue system was commercialized, the basic architecture of such
systems has remained substantially unchanged, consisting of four pipelined
basic components, namely, natural language understanding (NLU), dialogue state
tracking (DST), a dialogue manager (DM) and natural language generation (NLG).
The dialogue manager, which is the critical component of the modularized
system, controls the response content and style. This module is usually
programmed by rules and is designed to be highly controllable and easily
extendable. With the emergence of powerful ""deep learning"" architectures,
end-to-end generative dialogue systems have been proposed to optimize overall
system performance and simplify training. However, these systems cannot be
easily controlled and extended as the modularized dialogue manager can. This is
because a single neural system is used, which is usually a large pre-trained
language model (e.g., GPT-2), and thus it is hard to surgically change
desirable attributes (e.g., style, topics, etc.). More importantly,
uncontrollable dialogue systems can generate offensive and even toxic
responses. Therefore, in this thesis, we study controllable methods for
end-to-end generative dialogue systems in task-oriented and chit-chat
scenarios. Throughout the chapters, we describe 1) how to control the style and
topics of chit-chat models, 2) how to continuously control and extend
task-oriented dialogue systems, and 3) how to compose and control multi-skill
dialogue models.",2021-08-24
CGEMs: A Metric Model for Automatic Code Generation using GPT-3,2021-08-23 13:28:57+00:00,http://arxiv.org/abs/2108.10168v1,"Aishwarya Narasimhan, Krishna Prasad Agara Venkatesha Rao, Veena M B",cs.AI,dialogue,"Today, AI technology is showing its strengths in almost every industry and
walks of life. From text generation, text summarization, chatbots, NLP is being
used widely. One such paradigm is automatic code generation. An AI could be
generating anything; hence the output space is unconstrained. A self-driving
car is driven for 100 million miles to validate its safety, but tests cannot be
written to monitor and cover an unconstrained space. One of the solutions to
validate AI-generated content is to constrain the problem and convert it from
abstract to realistic, and this can be accomplished by either validating the
unconstrained algorithm using theoretical proofs or by using Monte-Carlo
simulation methods. In this case, we use the latter approach to test/validate a
statistically significant number of samples. This hypothesis of validating the
AI-generated code is the main motive of this work and to know if AI-generated
code is reliable, a metric model CGEMs is proposed. This is an extremely
challenging task as programs can have different logic with different naming
conventions, but the metrics must capture the structure and logic of the
program. This is similar to the importance grammar carries in AI-based text
generation, Q&A, translations, etc. The various metrics that are garnered in
this work to support the evaluation of generated code are as follows:
Compilation, NL description to logic conversion, number of edits needed, some
of the commonly used static-code metrics and NLP metrics. These metrics are
applied to 80 codes generated using OpenAI's GPT-3. Post which a Neural network
is designed for binary classification (acceptable/not acceptable quality of the
generated code). The inputs to this network are the values of the features
obtained from the metrics. The model achieves a classification accuracy of
76.92% and an F1 score of 55.56%. XAI is augmented for model interpretability.",2021-08-23
"A Neural Conversation Generation Model via Equivalent Shared Memory
  Investigation",2021-08-20 13:20:14+00:00,http://arxiv.org/abs/2108.09164v1,"Changzhen Ji, Yating Zhang, Xiaozhong Liu, Adam Jatowt, Changlong Sun, Conghui Zhu, Tiejun Zhao",cs.CL,dialogue,"Conversation generation as a challenging task in Natural Language Generation
(NLG) has been increasingly attracting attention over the last years. A number
of recent works adopted sequence-to-sequence structures along with external
knowledge, which successfully enhanced the quality of generated conversations.
Nevertheless, few works utilized the knowledge extracted from similar
conversations for utterance generation. Taking conversations in customer
service and court debate domains as examples, it is evident that essential
entities/phrases, as well as their associated logic and inter-relationships can
be extracted and borrowed from similar conversation instances. Such information
could provide useful signals for improving conversation generation. In this
paper, we propose a novel reading and memory framework called Deep Reading
Memory Network (DRMN) which is capable of remembering useful information of
similar conversations for improving utterance generation. We apply our model to
two large-scale conversation datasets of justice and e-commerce fields.
Experiments prove that the proposed model outperforms the state-of-the-art
approaches.",2021-08-20
Sentence Semantic Regression for Text Generation,2021-08-06 07:35:59+00:00,http://arxiv.org/abs/2108.02984v1,"Wei Wang, Piji Li, Hai-Tao Zheng",cs.CL,dialogue,"Recall the classical text generation works, the generation framework can be
briefly divided into two phases: \textbf{idea reasoning} and \textbf{surface
realization}. The target of idea reasoning is to figure out the main idea which
will be presented in the following talking/writing periods. Surface realization
aims to arrange the most appropriate sentence to depict and convey the
information distilled from the main idea. However, the current popular
token-by-token text generation methods ignore this crucial process and suffer
from many serious issues, such as idea/topic drift. To tackle the problems and
realize this two-phase paradigm, we propose a new framework named Sentence
Semantic Regression (\textbf{SSR}) based on sentence-level language modeling.
For idea reasoning, two architectures \textbf{SSR-AR} and \textbf{SSR-NonAR}
are designed to conduct sentence semantic regression autoregressively (like
GPT2/3) and bidirectionally (like BERT). In the phase of surface realization, a
mixed-granularity sentence decoder is designed to generate text with better
consistency by jointly incorporating the predicted sentence-level main idea as
well as the preceding contextual token-level information. We conduct
experiments on four tasks of story ending prediction, story ending generation,
dialogue generation, and sentence infilling. The results show that SSR can
obtain better performance in terms of automatic metrics and human evaluation.",2021-08-06
Internet-Augmented Dialogue Generation,2021-07-15 19:00:35+00:00,http://arxiv.org/abs/2107.07566v1,"Mojtaba Komeili, Kurt Shuster, Jason Weston","cs.AI, cs.CL",dialogue,"The largest store of continually updating knowledge on our planet can be
accessed via internet search. In this work we study giving access to this
information to conversational agents. Large language models, even though they
store an impressive amount of knowledge within their weights, are known to
hallucinate facts when generating dialogue (Shuster et al., 2021); moreover,
those facts are frozen in time at the point of model training. In contrast, we
propose an approach that learns to generate an internet search query based on
the context, and then conditions on the search results to finally generate a
response, a method that can employ up-to-the-minute relevant information. We
train and evaluate such models on a newly collected dataset of human-human
conversations whereby one of the speakers is given access to internet search
during knowledgedriven discussions in order to ground their responses. We find
that search-query based access of the internet in conversation provides
superior performance compared to existing approaches that either use no
augmentation or FAISS-based retrieval (Lewis et al., 2020).",2021-07-15
A Survey on Dialogue Summarization: Recent Advances and New Frontiers,2021-07-07 12:11:14+00:00,http://arxiv.org/abs/2107.03175v1,"Xiachong Feng, Xiaocheng Feng, Bing Qin",cs.CL,dialogue,"With the development of dialogue systems and natural language generation
techniques, the resurgence of dialogue summarization has attracted significant
research attentions, which aims to condense the original dialogue into a
shorter version covering salient information. However, there remains a lack of
comprehensive survey for this task. To this end, we take the first step and
present a thorough review of this research field. In detail, we provide an
overview of publicly available research datasets, summarize existing works
according to the domain of input dialogue as well as organize leaderboards
under unified metrics. Furthermore, we discuss some future directions and give
our thoughts. We hope that this first survey of dialogue summarization can
provide the community with a quick access and a general picture to this task
and motivate future researches.",2021-07-07
"Do Encoder Representations of Generative Dialogue Models Encode
  Sufficient Information about the Task ?",2021-06-20 04:52:37+00:00,http://arxiv.org/abs/2106.10622v1,"Prasanna Parthasarathi, Joelle Pineau, Sarath Chandar",cs.CL,dialogue,"Predicting the next utterance in dialogue is contingent on encoding of users'
input text to generate appropriate and relevant response in data-driven
approaches. Although the semantic and syntactic quality of the language
generated is evaluated, more often than not, the encoded representation of
input is not evaluated. As the representation of the encoder is essential for
predicting the appropriate response, evaluation of encoder representation is a
challenging yet important problem. In this work, we showcase evaluating the
text generated through human or automatic metrics is not sufficient to
appropriately evaluate soundness of the language understanding of dialogue
models and, to that end, propose a set of probe tasks to evaluate encoder
representation of different language encoders commonly used in dialogue models.
From experiments, we observe that some of the probe tasks are easier and some
are harder for even sophisticated model architectures to learn. And, through
experiments we observe that RNN based architectures have lower performance on
automatic metrics on text generation than transformer model but perform better
than the transformer model on the probe tasks indicating that RNNs might
preserve task information better than the Transformers.",2021-06-20
Local Explanation of Dialogue Response Generation,2021-06-11 17:58:36+00:00,http://arxiv.org/abs/2106.06528v1,"Yi-Lin Tuan, Connor Pryor, Wenhu Chen, Lise Getoor, William Yang Wang","cs.CL, stat.ML",dialogue,"In comparison to the interpretation of classification models, the explanation
of sequence generation models is also an important problem, however it has seen
little attention. In this work, we study model-agnostic explanations of a
representative text generation task -- dialogue response generation. Dialog
response generation is challenging with its open-ended sentences and multiple
acceptable responses. To gain insights into the reasoning process of a
generation model, we propose anew method, local explanation of response
generation (LERG) that regards the explanations as the mutual interaction of
segments in input and output sentences. LERG views the sequence prediction as
uncertainty estimation of a human response and then creates explanations by
perturbing the input and calculating the certainty change over the human
response. We show that LERG adheres to desired properties of explanations for
text generation including unbiased approximation, consistency and cause
identification. Empirically, our results show that our method consistently
improves other widely used methods on proposed automatic- and human- evaluation
metrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can
extract both explicit and implicit relations between input and output segments.",2021-06-11
"AUGNLG: Few-shot Natural Language Generation using Self-trained Data
  Augmentation",2021-06-10 08:45:28+00:00,http://arxiv.org/abs/2106.05589v1,"Xinnuo Xu, Guoyin Wang, Young-Bum Kim, Sungjin Lee",cs.CL,dialogue,"Natural Language Generation (NLG) is a key component in a task-oriented
dialogue system, which converts the structured meaning representation (MR) to
the natural language. For large-scale conversational systems, where it is
common to have over hundreds of intents and thousands of slots, neither
template-based approaches nor model-based approaches are scalable. Recently,
neural NLGs started leveraging transfer learning and showed promising results
in few-shot settings. This paper proposes AUGNLG, a novel data augmentation
approach that combines a self-trained neural retrieval model with a few-shot
learned NLU model, to automatically create MR-to-Text data from open-domain
texts. The proposed system mostly outperforms the state-of-the-art methods on
the FewShotWOZ data in both BLEU and Slot Error Rate. We further confirm
improved results on the FewShotSGD data and provide comprehensive analysis
results on key components of our system. Our code and data are available at
https://github.com/XinnuoXu/AugNLG.",2021-06-10
Defending against Backdoor Attacks in Natural Language Generation,2021-06-03 13:00:28+00:00,http://arxiv.org/abs/2106.01810v1,"Chun Fan, Xiaoya Li, Yuxian Meng, Xiaofei Sun, Xiang Ao, Fei Wu, Jiwei Li, Tianwei Zhang",cs.CL,dialogue,"The frustratingly fragile nature of neural network models make current
natural language generation (NLG) systems prone to backdoor attacks and
generate malicious sequences that could be sexist or offensive. Unfortunately,
little effort has been invested to how backdoor attacks can affect current NLG
models and how to defend against these attacks. In this work, we investigate
this problem on two important NLG tasks, machine translation and dialogue
generation. By giving a formal definition for backdoor attack and defense, and
developing corresponding benchmarks, we design methods to attack NLG models,
which achieve high attack success to ask NLG models to generate malicious
sequences. To defend against these attacks, we propose to detect the attack
trigger by examining the effect of deleting or replacing certain words on the
generation outputs, which we find successful for certain types of attacks. We
will discuss the limitation of this work, and hope this work can raise the
awareness of backdoor risks concealed in deep NLG systems. (Code and data are
available at https://github.com/ShannonAI/backdoor_nlg.)",2021-06-03
"Generate, Prune, Select: A Pipeline for Counterspeech Generation against
  Online Hate Speech",2021-06-03 06:54:03+00:00,http://arxiv.org/abs/2106.01625v1,"Wanzheng Zhu, Suma Bhat",cs.CL,dialogue,"Countermeasures to effectively fight the ever increasing hate speech online
without blocking freedom of speech is of great social interest. Natural
Language Generation (NLG), is uniquely capable of developing scalable
solutions. However, off-the-shelf NLG methods are primarily
sequence-to-sequence neural models and they are limited in that they generate
commonplace, repetitive and safe responses regardless of the hate speech (e.g.,
""Please refrain from using such language."") or irrelevant responses, making
them ineffective for de-escalating hateful conversations. In this paper, we
design a three-module pipeline approach to effectively improve the diversity
and relevance. Our proposed pipeline first generates various counterspeech
candidates by a generative model to promote diversity, then filters the
ungrammatical ones using a BERT model, and finally selects the most relevant
counterspeech response using a novel retrieval-based method. Extensive
Experiments on three representative datasets demonstrate the efficacy of our
approach in generating diverse and relevant counterspeech.",2021-06-03
"Detecting Bot-Generated Text by Characterizing Linguistic Accommodation
  in Human-Bot Interactions",2021-06-02 14:10:28+00:00,http://arxiv.org/abs/2106.01170v1,"Paras Bhatt, Anthony Rios",cs.CL,dialogue,"Language generation models' democratization benefits many domains, from
answering health-related questions to enhancing education by providing
AI-driven tutoring services. However, language generation models'
democratization also makes it easier to generate human-like text at-scale for
nefarious activities, from spreading misinformation to targeting specific
groups with hate speech. Thus, it is essential to understand how people
interact with bots and develop methods to detect bot-generated text. This paper
shows that bot-generated text detection methods are more robust across datasets
and models if we use information about how people respond to it rather than
using the bot's text directly. We also analyze linguistic alignment, providing
insight into differences between human-human and human-bot conversations.",2021-06-02
"NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based
  Simulation",2021-05-30 07:54:54+00:00,http://arxiv.org/abs/2105.14454v1,"Sungdong Kim, Minsuk Chang, Sang-Woo Lee",cs.CL,dialogue,"We propose NeuralWOZ, a novel dialogue collection framework that uses
model-based dialogue simulation. NeuralWOZ has two pipelined models, Collector
and Labeler. Collector generates dialogues from (1) user's goal instructions,
which are the user context and task constraints in natural language, and (2)
system's API call results, which is a list of possible query responses for user
requests from the given knowledge base. Labeler annotates the generated
dialogue by formulating the annotation as a multiple-choice problem, in which
the candidate labels are extracted from goal instructions and API call results.
We demonstrate the effectiveness of the proposed method in the zero-shot domain
transfer learning for dialogue state tracking. In the evaluation, the synthetic
dialogue corpus generated from NeuralWOZ achieves a new state-of-the-art with
improvements of 4.4% point joint goal accuracy on average across domains, and
improvements of 5.7% point of zero-shot coverage against the MultiWOZ 2.1
dataset.",2021-05-30
