title,pubdate,id,authors,categories,search,abstract,displaydate
BERT Transformer model for Detecting Arabic GPT2 Auto-Generated Tweets,2021-01-22 21:50:38+00:00,http://arxiv.org/abs/2101.09345v1,"Fouzi Harrag, Maria Debbah, Kareem Darwish, Ahmed Abdelali",cs.CL,knowledge,"During the last two decades, we have progressively turned to the Internet and
social media to find news, entertain conversations and share opinion. Recently,
OpenAI has developed a ma-chine learning system called GPT-2 for Generative
Pre-trained Transformer-2, which can pro-duce deepfake texts. It can generate
blocks of text based on brief writing prompts that look like they were written
by humans, facilitating the spread false or auto-generated text. In line with
this progress, and in order to counteract potential dangers, several methods
have been pro-posed for detecting text written by these language models. In
this paper, we propose a transfer learning based model that will be able to
detect if an Arabic sentence is written by humans or automatically generated by
bots. Our dataset is based on tweets from a previous work, which we have
crawled and extended using the Twitter API. We used GPT2-Small-Arabic to
generate fake Arabic Sentences. For evaluation, we compared different recurrent
neural network (RNN) word embeddings based baseline models, namely: LSTM,
BI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new
transfer-learning model has obtained an accuracy up to 98%. To the best of our
knowledge, this work is the first study where ARABERT and GPT2 were combined to
detect and classify the Arabic auto-generated texts.",2021-01-22
What Makes Good In-Context Examples for GPT-$3$?,2021-01-17 23:38:40+00:00,http://arxiv.org/abs/2101.06804v1,"Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen",cs.CL,knowledge,"GPT-$3$ has attracted lots of attention due to its superior performance
across a wide range of NLP tasks, especially with its powerful and versatile
in-context few-shot learning ability. Despite its success, we found that the
empirical results of GPT-$3$ depend heavily on the choice of in-context
examples. In this work, we investigate whether there are more effective
strategies for judiciously selecting in-context examples (relative to random
sampling) that better leverage GPT-$3$'s few-shot capabilities. Inspired by the
recent success of leveraging a retrieval module to augment large-scale neural
network models, we propose to retrieve examples that are semantically-similar
to a test sample to formulate its corresponding prompt. Intuitively, the
in-context examples selected with such a strategy may serve as more informative
inputs to unleash GPT-$3$'s extensive knowledge. We evaluate the proposed
approach on several natural language understanding and generation benchmarks,
where the retrieval-based prompt selection approach consistently outperforms
the random baseline. Moreover, it is observed that the sentence encoders
fine-tuned on task-related datasets yield even more helpful retrieval results.
Notably, significant gains are observed on tasks such as table-to-text
generation (41.9% on the ToTTo dataset) and open-domain question answering
(45.5% on the NQ dataset). We hope our investigation could help understand the
behaviors of GPT-$3$ and large-scale pre-trained LMs in general and enhance
their few-shot capabilities.",2021-01-17
Persuasive Natural Language Generation -- A Literature Review,2021-01-14 18:44:51+00:00,http://arxiv.org/abs/2101.05786v1,"Sebastian Duerr, Peter A. Gloor","cs.CL, cs.AI, I.2.7; J.4",knowledge,"This literature review focuses on the use of Natural Language Generation
(NLG) to automatically detect and generate persuasive texts. Extending previous
research on automatic identification of persuasion in text, we concentrate on
generative aspects through conceptualizing determinants of persuasion in five
business-focused categories: benevolence, linguistic appropriacy, logical
argumentation, trustworthiness, tools and datasets. These allow NLG to increase
an existing message's persuasiveness. Previous research illustrates key aspects
in each of the above mentioned five categories. A research agenda to further
study persuasive NLG is developed. The review includes analysis of
seventy-seven articles, outlining the existing body of knowledge and showing
the steady progress in this research field.",2021-01-14
"Political Depolarization of News Articles Using Attribute-aware Word
  Embeddings",2021-01-05 07:39:12+00:00,http://arxiv.org/abs/2101.01391v1,"Ruibo Liu, Lili Wang, Chenyan Jia, Soroush Vosoughi","cs.CL, cs.AI",knowledge,"Political polarization in the US is on the rise. This polarization negatively
affects the public sphere by contributing to the creation of ideological echo
chambers. In this paper, we focus on addressing one of the factors that
contributes to this polarity, polarized media. We introduce a framework for
depolarizing news articles. Given an article on a certain topic with a
particular ideological slant (eg., liberal or conservative), the framework
first detects polar language in the article and then generates a new article
with the polar language replaced with neutral expressions. To detect polar
words, we train a multi-attribute-aware word embedding model that is aware of
ideology and topics on 360k full-length media articles. Then, for text
generation, we propose a new algorithm called Text Annealing Depolarization
Algorithm (TADA). TADA retrieves neutral expressions from the word embedding
model that not only decrease ideological polarity but also preserve the
original argument of the text, while maintaining grammatical correctness. We
evaluate our framework by comparing the depolarized output of our model in two
modes, fully-automatic and semi-automatic, on 99 stories spanning 11 topics.
Based on feedback from 161 human testers, our framework successfully
depolarized 90.1% of paragraphs in semi-automatic mode and 78.3% of paragraphs
in fully-automatic mode. Furthermore, 81.2% of the testers agree that the
non-polar content information is well-preserved and 79% agree that
depolarization does not harm semantic correctness when they compare the
original text and the depolarized text. Our work shows that data-driven methods
can help to locate political polarity and aid in the depolarization of
articles.",2021-01-05
"Outline to Story: Fine-grained Controllable Story Generation from
  Cascaded Events",2021-01-04 08:16:21+00:00,http://arxiv.org/abs/2101.00822v1,"Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong, Changyou Chen","cs.CL, cs.AI, cs.LG",knowledge,"Large-scale pretrained language models have shown thrilling generation
capabilities, especially when they generate consistent long text in thousands
of words with ease. However, users of these models can only control the prefix
of sentences or certain global aspects of generated text. It is challenging to
simultaneously achieve fine-grained controllability and preserve the
state-of-the-art unconditional text generation capability. In this paper, we
first propose a new task named ""Outline to Story"" (O2S) as a test bed for
fine-grained controllable generation of long text, which generates a
multi-paragraph story from cascaded events, i.e. a sequence of outline events
that guide subsequent paragraph generation. We then create dedicate datasets
for future benchmarks, built by state-of-the-art keyword extraction techniques.
Finally, we propose an extremely simple yet strong baseline method for the O2S
task, which fine tunes pre-trained language models on augmented sequences of
outline-story pairs with simple language modeling objective. Our method does
not introduce any new parameters or perform any architecture modification,
except several special tokens as delimiters to build augmented sequences.
Extensive experiments on various datasets demonstrate state-of-the-art
conditional story generation performance with our model, achieving better
fine-grained controllability and user flexibility. Our paper is among the first
ones by our knowledge to propose a model and to create datasets for the task of
""outline to story"". Our work also instantiates research interest of
fine-grained controllable generation of open-domain long text, where
controlling inputs are represented by short text.",2021-01-04
"DISCOS: Bridging the Gap between Discourse Knowledge and Commonsense
  Knowledge",2021-01-01 03:30:38+00:00,http://arxiv.org/abs/2101.00154v1,"Tianqing Fang, Hongming Zhang, Weiqi Wang, Yangqiu Song, Bin He","cs.CL, cs.AI",knowledge,"Commonsense knowledge is crucial for artificial intelligence systems to
understand natural language. Previous commonsense knowledge acquisition
approaches typically rely on human annotations (e.g., ATOMIC) or text
generation models (e.g., COMET). Human annotation could provide high-quality
commonsense knowledge, yet its high cost often results in relatively small
scale and low coverage. On the other hand, generation models have the potential
to automatically generate more knowledge. Nonetheless, machine learning models
often fit the training data too well to generate novel knowledge in high
quality, thus still suffering from coverage problems. To address the
limitations of previous approaches, in this paper, we propose an alternative
commonsense knowledge acquisition framework DISCOS (from DIScourse to
COmmonSense), which automatically mines expensive complex commonsense knowledge
from more affordable linguistic knowledge resources. Experiments demonstrate
that we can successfully convert discourse knowledge over eventualities from
ASER, a large-scale discourse knowledge graph, into inferential if-then
commonsense knowledge defined in ATOMIC without any additional annotation
effort. Further study suggests that DISCOS significantly outperforms previous
supervised approaches in terms of novelty and diversity with comparable
quality. In total, we can acquire 3.4M ATOMIC-like inferential commonsense
knowledge by populating ATOMIC on the core part of ASER. Codes and data are
available at https://github.com/HKUST-KnowComp/DISCOS-commonsense.",2021-01-01
A Graph Total Variation Regularized Softmax for Text Generation,2021-01-01 03:29:21+00:00,http://arxiv.org/abs/2101.00153v1,"Liu Bin, Wang Liang, Yin Guosheng",cs.CL,knowledge,"The softmax operator is one of the most important functions in machine
learning models. When applying neural networks to multi-category
classification, the correlations among different categories are often ignored.
For example, in text generation, a language model makes a choice of each new
word based only on the former selection of its context. In this scenario, the
link statistics information of concurrent words based on a corpus (an analogy
of the natural way of expression) is also valuable in choosing the next word,
which can help to improve the sentence's fluency and smoothness. To fully
explore such important information, we propose a graph softmax function for
text generation. It is expected that the final classification result would be
dominated by both the language model and graphical text relationships among
words. We use a graph total variation term to regularize softmax so as to
incorporate the concurrent relationship into the language model. The total
variation of the generated words should be small locally. We apply the proposed
graph softmax to GPT2 for the text generation task. Experimental results
demonstrate that the proposed graph softmax achieves better BLEU and perplexity
than softmax. Human testers can also easily distinguish the text generated by
the graph softmax or softmax.",2021-01-01
A Distributional Approach to Controlled Text Generation,2020-12-21 19:02:41+00:00,http://arxiv.org/abs/2012.11635v1,"Muhammad Khalifa, Hady Elsahar, Marc Dymetman","cs.CL, cs.AI, cs.LG",knowledge,"We propose a Distributional Approach to address Controlled Text Generation
from pre-trained Language Models (LMs). This view permits to define, in a
single formal framework, ""pointwise"" and ""distributional"" constraints over the
target LM -- to our knowledge, this is the first approach with such generality
-- while minimizing KL divergence with the initial LM distribution. The optimal
target distribution is then uniquely determined as an explicit EBM
(Energy-Based Model) representation. From that optimal representation we then
train the target controlled autoregressive LM through an adaptive
distributional variant of Policy Gradient. We conduct a first set of
experiments over pointwise constraints showing the advantages of our approach
over a set of baselines, in terms of obtaining a controlled LM balancing
constraint satisfaction with divergence from the initial LM (GPT-2). We then
perform experiments over distributional constraints, a unique feature of our
approach, demonstrating its potential as a remedy to the problem of Bias in
Language Models. Through an ablation study we show the effectiveness of our
adaptive technique for obtaining faster convergence.",2020-12-21
"Lexically-constrained Text Generation through Commonsense Knowledge
  Extraction and Injection",2020-12-19 23:23:40+00:00,http://arxiv.org/abs/2012.10813v1,"Yikang Li, Pulkit Goel, Varsha Kuppur Rajendra, Har Simrat Singh, Jonathan Francis, Kaixin Ma, Eric Nyberg, Alessandro Oltramari",cs.CL,knowledge,"Conditional text generation has been a challenging task that is yet to see
human-level performance from state-of-the-art models. In this work, we
specifically focus on the Commongen benchmark, wherein the aim is to generate a
plausible sentence for a given set of input concepts. Despite advances in other
tasks, large pre-trained language models that are fine-tuned on this dataset
often produce sentences that are syntactically correct but qualitatively
deviate from a human understanding of common sense. Furthermore, generated
sequences are unable to fulfill such lexical requirements as matching
part-of-speech and full concept coverage. In this paper, we explore how
commonsense knowledge graphs can enhance model performance, with respect to
commonsense reasoning and lexically-constrained decoding. We propose strategies
for enhancing the semantic correctness of the generated text, which we
accomplish through: extracting commonsense relations from Conceptnet, injecting
these relations into the Unified Language Model (UniLM) through attention
mechanisms, and enforcing the aforementioned lexical requirements through
output constraints. By performing several ablations, we find that commonsense
injection enables the generation of sentences that are more aligned with human
understanding, while remaining compliant with lexical requirements.",2020-12-19
Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings,2020-12-14 10:59:59+00:00,http://arxiv.org/abs/2012.07412v2,"Qipeng Guo, Zhijing Jin, Ziyu Wang, Xipeng Qiu, Weinan Zhang, Jun Zhu, Zheng Zhang, David Wipf","cs.LG, cs.AI, cs.CL",knowledge,"Cycle-consistent training is widely used for jointly learning a forward and
inverse mapping between two domains of interest without the cumbersome
requirement of collecting matched pairs within each domain. In this regard, the
implicit assumption is that there exists (at least approximately) a
ground-truth bijection such that a given input from either domain can be
accurately reconstructed from successive application of the respective
mappings. But in many applications no such bijection can be expected to exist
and large reconstruction errors can compromise the success of cycle-consistent
training. As one important instance of this limitation, we consider
practically-relevant situations where there exists a many-to-one or surjective
mapping between domains. To address this regime, we develop a conditional
variational autoencoder (CVAE) approach that can be viewed as converting
surjective mappings to implicit bijections whereby reconstruction errors in
both directions can be minimized, and as a natural byproduct, realistic output
diversity can be obtained in the one-to-many direction. As theoretical
motivation, we analyze a simplified scenario whereby minima of the proposed
CVAE-based energy function align with the recovery of ground-truth surjective
mappings. On the empirical side, we consider a synthetic image dataset with
known ground-truth, as well as a real-world application involving natural
language generation from knowledge graphs and vice versa, a prototypical
surjective case. For the latter, our CVAE pipeline can capture such many-to-one
mappings during cycle training while promoting textural diversity for
graph-to-text tasks. Our code is available at github.com/QipengGuo/CycleGT",2020-12-14
"Generating Math Word Problems from Equations with Topic Controlling and
  Commonsense Enforcement",2020-12-14 10:02:11+00:00,http://arxiv.org/abs/2012.07379v1,"Tianyang Cao, Shuang Zeng, Songge Zhao, Mairgup Mansur, Baobao Chang",cs.CL,knowledge,"Recent years have seen significant advancement in text generation tasks with
the help of neural language models. However, there exists a challenging task:
generating math problem text based on mathematical equations, which has made
little progress so far. In this paper, we present a novel equation-to-problem
text generation model. In our model, 1) we propose a flexible scheme to
effectively encode math equations, we then enhance the equation encoder by a
Varitional Autoen-coder (VAE) 2) given a math equation, we perform topic
selection, followed by which a dynamic topic memory mechanism is introduced to
restrict the topic distribution of the generator 3) to avoid commonsense
violation in traditional generation model, we pretrain word embedding with
background knowledge graph (KG), and we link decoded words to related words in
KG, targeted at injecting background knowledge into our model. We evaluate our
model through both automatic metrices and human evaluation, experiments
demonstrate our model outperforms baseline and previous models in both accuracy
and richness of generated problem text.",2020-12-14
"Denoising Pre-Training and Data Augmentation Strategies for Enhanced RDF
  Verbalization with Transformers",2020-12-01 15:25:47+00:00,http://arxiv.org/abs/2012.00571v1,"Sebastien Montella, Betty Fabre, Tanguy Urvoy, Johannes Heinecke, Lina Rojas-Barahona",cs.CL,knowledge,"The task of verbalization of RDF triples has known a growth in popularity due
to the rising ubiquity of Knowledge Bases (KBs). The formalism of RDF triples
is a simple and efficient way to store facts at a large scale. However, its
abstract representation makes it difficult for humans to interpret. For this
purpose, the WebNLG challenge aims at promoting automated RDF-to-text
generation. We propose to leverage pre-trainings from augmented data with the
Transformer model using a data augmentation strategy. Our experiment results
show a minimum relative increases of 3.73%, 126.05% and 88.16% in BLEU score
for seen categories, unseen entities and unseen categories respectively over
the standard training.",2020-12-01
Video Generative Adversarial Networks: A Review,2020-11-04 12:16:05+00:00,http://arxiv.org/abs/2011.02250v1,"Nuha Aldausari, Arcot Sowmya, Nadine Marcus, Gelareh Mohammadi","cs.CV, cs.LG, eess.IV",knowledge,"With the increasing interest in the content creation field in multiple
sectors such as media, education, and entertainment, there is an increasing
trend in the papers that uses AI algorithms to generate content such as images,
videos, audio, and text. Generative Adversarial Networks (GANs) in one of the
promising models that synthesizes data samples that are similar to real data
samples. While the variations of GANs models, in general, have been covered to
some extent in several survey papers, to the best of our knowledge, this is
among the first survey papers that reviews the state-of-the-art video GANs
models. This paper first categorized GANs review papers into general GANs
review papers, image GANs review papers, and special field GANs review papers
such as anomaly detection, medical imaging, or cybersecurity. The paper then
summarizes the main improvements in GANs frameworks that are not initially
developed for the video domain but have been adopted in multiple video GANs
variations. Then, a comprehensive review of video GANs models is provided under
two main divisions according to the presence or non-presence of a condition.
The conditional models then further grouped according to the type of condition
into audio, text, video, and image. The paper is concluded by highlighting the
main challenges and limitations of the current video GANs models. A
comprehensive list of datasets, applied loss functions, and evaluation metrics
is provided in the supplementary material.",2020-11-04
Data-to-Text Generation with Iterative Text Editing,2020-11-03 13:32:38+00:00,http://arxiv.org/abs/2011.01694v1,"Zdeněk Kasner, Ondřej Dušek","cs.CL, I.2.7",knowledge,"We present a novel approach to data-to-text generation based on iterative
text editing. Our approach maximizes the completeness and semantic accuracy of
the output text while leveraging the abilities of recent pre-trained models for
text editing (LaserTagger) and language modeling (GPT-2) to improve the text
fluency. To this end, we first transform data items to text using trivial
templates, and then we iteratively improve the resulting text by a neural model
trained for the sentence fusion task. The output of the model is filtered by a
simple heuristic and reranked with an off-the-shelf pre-trained language model.
We evaluate our approach on two major data-to-text datasets (WebNLG, Cleaned
E2E) and analyze its caveats and benefits. Furthermore, we show that our
formulation of data-to-text generation opens up the possibility for zero-shot
domain adaptation using a general-domain dataset for sentence fusion.",2020-11-03
"Topic-Centric Unsupervised Multi-Document Summarization of Scientific
  and News Articles",2020-11-03 04:04:21+00:00,http://arxiv.org/abs/2011.08072v1,"Amanuel Alambo, Cori Lohstroh, Erik Madaus, Swati Padhee, Brandy Foster, Tanvi Banerjee, Krishnaprasad Thirunarayan, Michael Raymer","cs.CL, cs.IR, cs.LG",knowledge,"Recent advances in natural language processing have enabled automation of a
wide range of tasks, including machine translation, named entity recognition,
and sentiment analysis. Automated summarization of documents, or groups of
documents, however, has remained elusive, with many efforts limited to
extraction of keywords, key phrases, or key sentences. Accurate abstractive
summarization has yet to be achieved due to the inherent difficulty of the
problem, and limited availability of training data. In this paper, we propose a
topic-centric unsupervised multi-document summarization framework to generate
extractive and abstractive summaries for groups of scientific articles across
20 Fields of Study (FoS) in Microsoft Academic Graph (MAG) and news articles
from DUC-2004 Task 2. The proposed algorithm generates an abstractive summary
by developing salient language unit selection and text generation techniques.
Our approach matches the state-of-the-art when evaluated on automated
extractive evaluation metrics and performs better for abstractive summarization
on five human evaluation metrics (entailment, coherence, conciseness,
readability, and grammar). We achieve a kappa score of 0.68 between two
co-author linguists who evaluated our results. We plan to publicly share
MAG-20, a human-validated gold standard dataset of topic-clustered research
articles and their summaries to promote research in abstractive summarization.",2020-11-03
Fusion Models for Improved Visual Captioning,2020-10-28 21:55:25+00:00,http://arxiv.org/abs/2010.15251v2,"Marimuthu Kalimuthu, Aditya Mogadala, Marius Mosbach, Dietrich Klakow","cs.CV, cs.AI, cs.CL, cs.LG",knowledge,"Visual captioning aims to generate textual descriptions given images or
videos. Traditionally, image captioning models are trained on human annotated
datasets such as Flickr30k and MS-COCO, which are limited in size and
diversity. This limitation hinders the generalization capabilities of these
models while also rendering them liable to making mistakes. Language models
can, however, be trained on vast amounts of freely available unlabelled data
and have recently emerged as successful language encoders and coherent text
generators. Meanwhile, several unimodal and multimodal fusion techniques have
been proven to work well for natural language generation and automatic speech
recognition. Building on these recent developments, and with the aim of
improving the quality of generated captions, the contribution of our work in
this paper is two-fold: First, we propose a generic multimodal model fusion
framework for caption generation as well as emendation where we utilize
different fusion strategies to integrate a pretrained Auxiliary Language Model
(AuxLM) within the traditional encoder-decoder visual captioning frameworks.
Next, we employ the same fusion strategies to integrate a pretrained Masked
Language Model (MLM), namely BERT, with a visual captioning model, viz. Show,
Attend, and Tell, for emending both syntactic and semantic errors in captions.
Our caption emendation experiments on three benchmark image captioning
datasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the
baseline, indicating the usefulness of our proposed multimodal fusion
strategies. Further, we perform a preliminary qualitative analysis on the
emended captions and identify error categories based on the type of
corrections.",2020-10-28
Go Figure! A Meta Evaluation of Factuality in Summarization,2020-10-24 08:30:20+00:00,http://arxiv.org/abs/2010.12834v1,"Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, Jianfeng Gao",cs.CL,knowledge,"Text generation models can generate factually inconsistent text containing
distorted or fabricated facts about the source text. Recent work has focused on
building evaluation models to verify the factual correctness of semantically
constrained text generation tasks such as document summarization. While the
field of factuality evaluation is growing fast, we don't have well-defined
criteria for measuring the effectiveness, generalizability, reliability, or
sensitivity of the factuality metrics. Focusing on these aspects, in this
paper, we introduce a meta-evaluation framework for evaluating factual
consistency metrics. We introduce five necessary, common-sense conditions for
effective factuality metrics and experiment with nine recent factuality metrics
using synthetic and human-labeled factuality data from short news, long news
and dialogue summarization domains. Our framework enables assessing the
efficiency of any new factual consistency metric on a variety of dimensions
over multiple summarization domains and can be easily extended with new
meta-evaluation criteria. We also present our conclusions towards standardizing
the factuality evaluation metrics.",2020-10-24
CaM-Gen:Causally-aware Metric-guided Text Generation,2020-10-24 06:17:35+00:00,http://arxiv.org/abs/2010.12795v1,"Navita Goyal, Roodram Paneri, Ayush Agarwal, Udit Kalani, Abhilasha Sancheti, Niyati Chhaya",cs.CL,knowledge,"Content is created for a well-defined purpose, often described by a metric or
a signal represented in the form of structured information. The relationship
between the metrics or the goal of a target content and the content itself are
non-trivial. While large scale language models show promising text generation
capabilities, guiding and informing the generated text with external metrics is
challenging. These metrics and the content tend to have inherent relationships
and not all of them may directly impact the content. We introduce a CaM-Gen:
Causally-aware Generative Networks guided by user-defined input metrics
incorporating the causal relationships between the metric and the content
features. We leverage causal inference techniques to identify the causally
significant aspects of text that leads to the target metric and then explicitly
guide the generative model towards these by a feedback mechanism. We propose
this mechanism for variational autoencoder-based and transformer-based
generative models. The proposed models beat baselines in terms of the target
metric accuracy while maintaining the fluency and the language quality of the
generated text. To the best of our knowledge, this is one of the early attempts
at incorporating a metric-guide using causal inference towards controlled
generation.",2020-10-24
"Large Scale Knowledge Graph Based Synthetic Corpus Generation for
  Knowledge-Enhanced Language Model Pre-training",2020-10-23 22:14:50+00:00,http://arxiv.org/abs/2010.12688v1,"Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami Al-Rfou",cs.CL,knowledge,"Generating natural sentences from Knowledge Graph (KG) triples, known as
Data-To-Text Generation, is a task with many datasets for which numerous
complex systems have been developed. However, no prior work has attempted to
perform this generation at scale by converting an entire KG into natural text.
In this paper, we verbalize the entire Wikidata KG, and create a KG-Text
aligned corpus in the training process. We discuss the challenges in
verbalizing an entire KG versus verbalizing smaller datasets. We further show
that verbalizing an entire KG can be used to integrate structured and natural
language data. In contrast to the many architectures that have been developed
to integrate the structural differences between these two sources, our approach
converts the KG into the same format as natural text allowing it to be
seamlessly plugged into existing natural language systems. We evaluate this
approach by augmenting the retrieval corpus in REALM and showing improvements,
both on the LAMA knowledge probe and open domain QA.",2020-10-23
AI-lead Court Debate Case Investigation,2020-10-22 11:05:14+00:00,http://arxiv.org/abs/2010.11604v2,"Changzhen Ji, Xin Zhou, Conghui Zhu, Tiejun Zhao",cs.CL,knowledge,"The multi-role judicial debate composed of the plaintiff, defendant, and
judge is an important part of the judicial trial. Different from other types of
dialogue, questions are raised by the judge, The plaintiff, plaintiff's agent
defendant, and defendant's agent would be to debating so that the trial can
proceed in an orderly manner. Question generation is an important task in
Natural Language Generation. In the judicial trial, it can help the judge raise
efficient questions so that the judge has a clearer understanding of the case.
In this work, we propose an innovative end-to-end question generation
model-Trial Brain Model (TBM) to build a Trial Brain, it can generate the
questions the judge wants to ask through the historical dialogue between the
plaintiff and the defendant. Unlike prior efforts in natural language
generation, our model can learn the judge's questioning intention through
predefined knowledge. We do experiments on real-world datasets, the
experimental results show that our model can provide a more accurate question
in the multi-role court debate scene.",2020-10-22
"Better Distractions: Transformer-based Distractor Generation and
  Multiple Choice Question Filtering",2020-10-19 15:23:24+00:00,http://arxiv.org/abs/2010.09598v1,"Jeroen Offerijns, Suzan Verberne, Tessa Verhoef",cs.CL,knowledge,"For the field of education, being able to generate semantically correct and
educationally relevant multiple choice questions (MCQs) could have a large
impact. While question generation itself is an active research topic,
generating distractors (the incorrect multiple choice options) receives much
less attention. A missed opportunity, since there is still a lot of room for
improvement in this area. In this work, we train a GPT-2 language model to
generate three distractors for a given question and text context, using the
RACE dataset. Next, we train a BERT language model to answer MCQs, and use this
model as a filter, to select only questions that can be answered and therefore
presumably make sense. To evaluate our work, we start by using text generation
metrics, which show that our model outperforms earlier work on distractor
generation (DG) and achieves state-of-the-art performance. Also, by calculating
the question answering ability, we show that larger base models lead to better
performance. Moreover, we conducted a human evaluation study, which confirmed
the quality of the generated questions, but showed no statistically significant
effect of the QA filter.",2020-10-19
"RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich
  Semantic Annotations for Task-Oriented Dialogue Modeling",2020-10-17 08:18:59+00:00,http://arxiv.org/abs/2010.08738v1,"Jun Quan, Shian Zhang, Qian Cao, Zizhong Li, Deyi Xiong",cs.CL,knowledge,"In order to alleviate the shortage of multi-domain data and to capture
discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a
large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic
Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn
semantically annotated dialogues, with more than 150K utterances spanning over
12 domains, which is larger than all previous annotated H2H conversational
datasets. Both single- and multi-domain dialogues are constructed, accounting
for 65% and 35%, respectively. Each dialogue is labeled with comprehensive
dialogue annotations, including dialogue goal in the form of natural language
description, domain, dialogue states and acts at both the user and system side.
In addition to traditional dialogue annotations, we especially provide
linguistic annotations on discourse phenomena, e.g., ellipsis and coreference,
in dialogues, which are useful for dialogue coreference and ellipsis resolution
tasks. Apart from the fully annotated dataset, we also present a detailed
description of the data collection procedure, statistics and analysis of the
dataset. A series of benchmark models and results are reported, including
natural language understanding (intent detection & slot filling), dialogue
state tracking and dialogue context-to-text generation, as well as coreference
and ellipsis resolution, which facilitate the baseline comparison for future
research on this corpus.",2020-10-17
"Incorporate Semantic Structures into Machine Translation Evaluation via
  UCCA",2020-10-17 06:47:58+00:00,http://arxiv.org/abs/2010.08728v2,"Jin Xu, Yinuo Guo, Junfeng Hu",cs.CL,knowledge,"Copying mechanism has been commonly used in neural paraphrasing networks and
other text generation tasks, in which some important words in the input
sequence are preserved in the output sequence. Similarly, in machine
translation, we notice that there are certain words or phrases appearing in all
good translations of one source text, and these words tend to convey important
semantic information. Therefore, in this work, we define words carrying
important semantic meanings in sentences as semantic core words. Moreover, we
propose an MT evaluation approach named Semantically Weighted Sentence
Similarity (SWSS). It leverages the power of UCCA to identify semantic core
words, and then calculates sentence similarity scores on the overlap of
semantic core words. Experimental results show that SWSS can consistently
improve the performance of popular MT evaluation metrics which are based on
lexical similarity.",2020-10-17
Reflective Decoding: Unsupervised Paraphrasing and Abductive Reasoning,2020-10-16 18:02:07+00:00,http://arxiv.org/abs/2010.08566v1,"Peter West, Ximing Lu, Ari Holtzman, Chandra Bhagavatula, Jena Hwang, Yejin Choi",cs.CL,knowledge,"Pretrained Language Models (LMs) generate text with remarkable quality,
novelty,and coherence. Yet applying LMs to the problems of paraphrasing and
infilling currently requires direct supervision, since these tasks break the
left-to-right generation setup of pretrained LMs. We present Reflective
Decoding, a novel unsupervised approach to apply the capabilities of pretrained
LMs to non-sequential tasks. Our approach is general and applicable to two
distant tasks - paraphrasing and abductive reasoning. It requires no
supervision or parallel corpora, only two pretrained language models: forward
and backward. Reflective Decoding operates in two intuitive steps. In the
contextualization step, we use LMs to generate many left and right contexts
which collectively capture the meaning of the input sentence. Then, in the
reflection step we decode in the semantic neighborhood of the input,
conditioning on an ensemble of generated contexts with the reverse direction
LM. We reflect through the generated contexts, effectively using them as an
intermediate meaning representation to generate conditional output. Empirical
results demonstrate that Reflective Decoding outperforms strong unsupervised
baselines on both paraphrasing and abductive text infilling, significantly
narrowing the gap between unsupervised and supervised methods.Reflective
Decoding introduces the concept of using generated contexts to represent
meaning, opening up new possibilities for unsupervised conditional text
generation.",2020-10-16
Neural Deepfake Detection with Factual Structure of Text,2020-10-15 02:35:31+00:00,http://arxiv.org/abs/2010.07475v1,"Wanjun Zhong, Duyu Tang, Zenan Xu, Ruize Wang, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin",cs.CL,knowledge,"Deepfake detection, the task of automatically discriminating
machine-generated text, is increasingly critical with recent advances in
natural language generative models. Existing approaches to deepfake detection
typically represent documents with coarse-grained representations. However,
they struggle to capture factual structures of documents, which is a
discriminative factor between machine-generated and human-written text
according to our statistical analysis. To address this, we propose a
graph-based model that utilizes the factual structure of a document for
deepfake detection of text. Our approach represents the factual structure of a
given document as an entity graph, which is further utilized to learn sentence
representations with a graph neural network. Sentence representations are then
composed to a document representation for making predictions, where consistent
relations between neighboring sentences are sequentially modeled. Results of
experiments on two public deepfake datasets show that our approach
significantly improves strong base models built with RoBERTa. Model analysis
further indicates that our model can distinguish the difference in the factual
structure between machine-generated text and human-written text.",2020-10-15
"ReviewRobot: Explainable Paper Review Generation based on Knowledge
  Synthesis",2020-10-13 02:17:58+00:00,http://arxiv.org/abs/2010.06119v3,"Qingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight, Heng Ji, Nazneen Fatema Rajani","cs.CL, cs.AI",knowledge,"To assist human review process, we build a novel ReviewRobot to automatically
assign a review score and write comments for multiple categories such as
novelty and meaningful comparison. A good review needs to be knowledgeable,
namely that the comments should be constructive and informative to help improve
the paper; and explainable by providing detailed evidence. ReviewRobot achieves
these goals via three steps: (1) We perform domain-specific Information
Extraction to construct a knowledge graph (KG) from the target paper under
review, a related work KG from the papers cited by the target paper, and a
background KG from a large collection of previous papers in the domain. (2) By
comparing these three KGs, we predict a review score and detailed structured
knowledge as evidence for each review category. (3) We carefully select and
generalize human review sentences into templates, and apply these templates to
transform the review scores and evidence into natural language comments.
Experimental results show that our review score predictor reaches 71.4%-100%
accuracy. Human assessment by domain experts shows that 41.7%-70.5% of the
comments generated by ReviewRobot are valid and constructive, and better than
human-written ones for 20% of the time. Thus, ReviewRobot can serve as an
assistant for paper reviewers, program chairs and authors.",2020-10-13
"A Sentiment-Controllable Topic-to-Essay Generator with Topic Knowledge
  Graph",2020-10-12 08:06:12+00:00,http://arxiv.org/abs/2010.05511v1,"Lin Qiao, Jianhao Yan, Fandong Meng, Zhendong Yang, Jie Zhou",cs.CL,knowledge,"Generating a vivid, novel, and diverse essay with only several given topic
words is a challenging task of natural language generation. In previous work,
there are two problems left unsolved: neglect of sentiment beneath the text and
insufficient utilization of topic-related knowledge. Therefore, we propose a
novel Sentiment-Controllable topic-to-essay generator with a Topic Knowledge
Graph enhanced decoder, named SCTKG, which is based on the conditional
variational autoencoder (CVAE) framework. We firstly inject the sentiment
information into the generator for controlling sentiment for each sentence,
which leads to various generated essays. Then we design a Topic Knowledge Graph
enhanced decoder. Unlike existing models that use knowledge entities
separately, our model treats the knowledge graph as a whole and encodes more
structured, connected semantic information in the graph to generate a more
relevant essay. Experimental results show that our SCTKG can generate sentiment
controllable essays and outperform the state-of-the-art approach in terms of
topic relevance, fluency, and diversity on both automatic and human evaluation.",2020-10-12
Evaluating Factuality in Generation with Dependency-level Entailment,2020-10-12 06:43:10+00:00,http://arxiv.org/abs/2010.05478v2,"Tanya Goyal, Greg Durrett",cs.CL,knowledge,"Despite significant progress in text generation models, a serious limitation
is their tendency to produce text that is factually inconsistent with
information in the input. Recent work has studied whether textual entailment
systems can be used to identify factual errors; however, these sentence-level
entailment models are trained to solve a different problem than generation
filtering and they do not localize which part of a generation is non-factual.
In this paper, we propose a new formulation of entailment that decomposes it at
the level of dependency arcs. Rather than focusing on aggregate decisions, we
instead ask whether the semantic relationship manifested by individual
dependency arcs in the generated output is supported by the input. Human
judgments on this task are difficult to obtain; we therefore propose a method
to automatically create data based on existing entailment or paraphrase
corpora. Experiments show that our dependency arc entailment model trained on
this data can identify factual inconsistencies in paraphrasing and
summarization better than sentence-level methods or those based on question
generation, while additionally localizing the erroneous parts of the
generation.",2020-10-12
Controllable Multi-Character Psychology-Oriented Story Generation,2020-10-11 12:05:00+00:00,http://arxiv.org/abs/2010.05230v1,"Feifei Xu, Xinpeng Wang, Yunpu Ma, Volker Tresp, Yuyi Wang, Shanlin Zhou, Haizhou Du",cs.CL,knowledge,"Story generation, which aims to generate a long and coherent story
automatically based on the title or an input sentence, is an important research
area in the field of natural language generation. There is relatively little
work on story generation with appointed emotions. Most existing works focus on
using only one specific emotion to control the generation of a whole story and
ignore the emotional changes in the characters in the course of the story. In
our work, we aim to design an emotional line for each character that considers
multiple emotions common in psychological theories, with the goal of generating
stories with richer emotional changes in the characters. To the best of our
knowledge, this work is first to focuses on characters' emotional lines in
story generation. We present a novel model-based attention mechanism that we
call SoCP (Storytelling of multi-Character Psychology). We show that the
proposed model can generate stories considering the changes in the
psychological state of different characters. To take into account the
particularity of the model, in addition to commonly used evaluation
indicators(BLEU, ROUGE, etc.), we introduce the accuracy rate of psychological
state control as a novel evaluation metric. The new indicator reflects the
effect of the model on the psychological state control of story characters.
Experiments show that with SoCP, the generated stories follow the psychological
state for each character according to both automatic and human evaluations.",2020-10-11
"Adversarial Self-Supervised Data-Free Distillation for Text
  Classification",2020-10-10 02:46:06+00:00,http://arxiv.org/abs/2010.04883v1,"Xinyin Ma, Yongliang Shen, Gongfan Fang, Chen Chen, Chenghao Jia, Weiming Lu","cs.CL, cs.AI",knowledge,"Large pre-trained transformer-based language models have achieved impressive
results on a wide range of NLP tasks. In the past few years, Knowledge
Distillation(KD) has become a popular paradigm to compress a computationally
expensive model to a resource-efficient lightweight model. However, most KD
algorithms, especially in NLP, rely on the accessibility of the original
training dataset, which may be unavailable due to privacy issues. To tackle
this problem, we propose a novel two-stage data-free distillation method, named
Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed
for compressing large-scale transformer-based models (e.g., BERT). To avoid
text generation in discrete space, we introduce a Plug & Play Embedding
Guessing method to craft pseudo embeddings from the teacher's hidden knowledge.
Meanwhile, with a self-supervised module to quantify the student's ability, we
adapt the difficulty of pseudo embeddings in an adversarial training manner. To
the best of our knowledge, our framework is the first data-free distillation
framework designed for NLP tasks. We verify the effectiveness of our method on
several text classification datasets.",2020-10-10
Online Back-Parsing for AMR-to-Text Generation,2020-10-09 12:08:14+00:00,http://arxiv.org/abs/2010.04520v1,"Xuefeng Bai, Linfeng Song, Yue Zhang",cs.CL,knowledge,"AMR-to-text generation aims to recover a text containing the same meaning as
an input AMR graph. Current research develops increasingly powerful graph
encoders to better represent AMR graphs, with decoders based on standard
language modeling being used to generate outputs. We propose a decoder that
back predicts projected AMR graphs on the target sentence during text
generation. As the result, our outputs can better preserve the input meaning
than standard decoders. Experiments on two AMR benchmarks show the superiority
of our model over the previous state-of-the-art system based on graph
Transformer.",2020-10-09
A Survey of Knowledge-Enhanced Text Generation,2020-10-09 06:46:46+00:00,http://arxiv.org/abs/2010.04389v1,"Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, Meng Jiang","cs.CL, cs.AI, cs.LG",knowledge,"The goal of text generation is to make machines express in human language. It
is one of the most important yet challenging tasks in natural language
processing (NLP). Since 2014, various neural encoder-decoder models pioneered
by Seq2Seq have been proposed to achieve the goal by learning to map input text
to output text. However, the input text alone often provides limited knowledge
to generate the desired output, so the performance of text generation is still
far from satisfaction in many real-world scenarios. To address this issue,
researchers have considered incorporating various forms of knowledge beyond the
input text into the generation models. This research direction is known as
knowledge-enhanced text generation. In this survey, we present a comprehensive
review of the research on knowledge enhanced text generation over the past five
years. The main content includes two parts: (i) general methods and
architectures for integrating knowledge into text generation; (ii) specific
techniques and applications according to different forms of knowledge data.
This survey can have broad audiences, researchers and practitioners, in
academia and industry.",2020-10-09
"Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text
  Generation",2020-10-09 06:03:46+00:00,http://arxiv.org/abs/2010.04383v1,"Yan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, Shay B. Cohen, Zuozhu Liu, Lidong Bing",cs.CL,knowledge,"AMR-to-text generation is used to transduce Abstract Meaning Representation
structures (AMR) into text. A key challenge in this task is to efficiently
learn effective graph representations. Previously, Graph Convolution Networks
(GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to
capture non-local information and additionally, they follow a local
(first-order) information aggregation scheme. To account for these issues,
larger and deeper GCN models are required to capture more complex interactions.
In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight
Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local
interactions by synthesizing higher order information from the input graphs. We
further develop two novel parameter saving strategies based on the group graph
convolutions and weight tied convolutions to reduce memory usage and model
complexity. With the help of these strategies, we are able to train a model
with fewer parameters while maintaining the model capacity. Experiments
demonstrate that LDGCNs outperform state-of-the-art models on two benchmark
datasets for AMR-to-text generation with significantly fewer parameters.",2020-10-09
Dual Inference for Improving Language Understanding and Generation,2020-10-08 20:14:41+00:00,http://arxiv.org/abs/2010.04246v2,"Shang-Yu Su, Yung-Sung Chuang, Yun-Nung Chen",cs.CL,knowledge,"Natural language understanding (NLU) and Natural language generation (NLG)
tasks hold a strong dual relationship, where NLU aims at predicting semantic
labels based on natural language utterances and NLG does the opposite. The
prior work mainly focused on exploiting the duality in model training in order
to obtain the models with better performance. However, regarding the
fast-growing scale of models in the current NLP area, sometimes we may have
difficulty retraining whole NLU and NLG models. To better address the issue,
this paper proposes to leverage the duality in the inference stage without the
need of retraining. The experiments on three benchmark datasets demonstrate the
effectiveness of the proposed method in both NLU and NLG, providing the great
potential of practical usage.",2020-10-08
Beyond [CLS] through Ranking by Generation,2020-10-06 22:56:31+00:00,http://arxiv.org/abs/2010.03073v1,"Cicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nallapati, Zhiheng Huang, Bing Xiang","cs.CL, cs.IR",knowledge,"Generative models for Information Retrieval, where ranking of documents is
viewed as the task of generating a query from a document's language model, were
very successful in various IR tasks in the past. However, with the advent of
modern deep neural networks, attention has shifted to discriminative ranking
functions that model the semantic similarity of documents and queries instead.
Recently, deep generative models such as GPT2 and BART have been shown to be
excellent text generators, but their effectiveness as rankers have not been
demonstrated yet. In this work, we revisit the generative framework for
information retrieval and show that our generative approaches are as effective
as state-of-the-art semantic similarity-based discriminative models for the
answer selection task. Additionally, we demonstrate the effectiveness of
unlikelihood losses for IR.",2020-10-06
"Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on
  Chest X-rays",2020-10-06 04:18:18+00:00,http://arxiv.org/abs/2010.02467v1,"Jianmo Ni, Chun-Nan Hsu, Amilcare Gentili, Julian McAuley","cs.CV, cs.CL",knowledge,"Automatic medical image report generation has drawn growing attention due to
its potential to alleviate radiologists' workload. Existing work on report
generation often trains encoder-decoder networks to generate complete reports.
However, such models are affected by data bias (e.g.~label imbalance) and face
common issues inherent in text generation models (e.g.~repetition). In this
work, we focus on reporting abnormal findings on radiology images; instead of
training on complete radiology reports, we propose a method to identify
abnormal findings from the reports in addition to grouping them with
unsupervised clustering and minimal rules. We formulate the task as cross-modal
retrieval and propose Conditional Visual-Semantic Embeddings to align images
and fine-grained abnormal findings in a joint embedding space. We demonstrate
that our method is able to retrieve abnormal findings and outperforms existing
generation models on both clinical correctness and text generation metrics.",2020-10-06
KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation,2020-10-05 19:59:05+00:00,http://arxiv.org/abs/2010.02307v2,"Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang","cs.CL, cs.AI",knowledge,"Data-to-text generation has recently attracted substantial interests due to
its wide applications. Existing methods have shown impressive performance on an
array of tasks. However, they rely on a significant amount of labeled data for
each task, which is costly to acquire and thus limits their application to new
tasks and domains. In this paper, we propose to leverage pre-training and
transfer learning to address this issue. We propose a knowledge-grounded
pre-training (KGPT), which consists of two parts, 1) a general
knowledge-grounded generation model to generate knowledge-enriched text. 2) a
pre-training paradigm on a massive knowledge-grounded text corpus crawled from
the web. The pre-trained model can be fine-tuned on various data-to-text
generation tasks to generate task-specific text. We adopt three settings,
namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness.
Under the fully-supervised setting, our model can achieve remarkable gains over
the known baselines. Under zero-shot setting, our model without seeing any
examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail.
Under the few-shot setting, our model only needs about one-fifteenth as many
labeled examples to achieve the same level of performance as baseline models.
These experiments consistently prove the strong generalization ability of our
proposed framework https://github.com/wenhuchen/KGPT.",2020-10-05
GenAug: Data Augmentation for Finetuning Text Generators,2020-10-05 05:46:39+00:00,http://arxiv.org/abs/2010.01794v2,"Steven Y. Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura, Eduard Hovy","cs.CL, cs.AI, cs.LG",knowledge,"In this paper, we investigate data augmentation for text generation, which we
call GenAug. Text generation and language modeling are important tasks within
natural language processing, and are especially challenging for low-data
regimes. We propose and evaluate various augmentation methods, including some
that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp
Reviews. We also examine the relationship between the amount of augmentation
and the quality of the generated text. We utilize several metrics that evaluate
important aspects of the generated text including its diversity and fluency.
Our experiments demonstrate that insertion of character-level synthetic noise
and keyword replacement with hypernyms are effective augmentation methods, and
that the quality of generations improves to a peak at approximately three times
the amount of original data.",2020-10-05
Transformer-Based Neural Text Generation with Syntactic Guidance,2020-10-05 01:33:58+00:00,http://arxiv.org/abs/2010.01737v1,"Yinghao Li, Rui Feng, Isaac Rehg, Chao Zhang",cs.CL,knowledge,"We study the problem of using (partial) constituency parse trees as syntactic
guidance for controlled text generation. Existing approaches to this problem
use recurrent structures, which not only suffer from the long-term dependency
problem but also falls short in modeling the tree structure of the syntactic
guidance. We propose to leverage the parallelism of Transformer to better
incorporate parse trees. Our method first expands a partial template
constituency parse tree to a full-fledged parse tree tailored for the input
source text, and then uses the expanded tree to guide text generation. The
effectiveness of our model in this process hinges upon two new attention
mechanisms: 1) a path attention mechanism that forces one node to attend to
only other nodes located in its path in the syntax tree to better incorporate
syntax guidance; 2) a multi-encoder attention mechanism that allows the decoder
to dynamically attend to information from multiple encoders. Our experiments in
the controlled paraphrasing task show that our method outperforms SOTA models
both semantically and syntactically, improving the best baseline's BLEU score
from 11.83 to 26.27.",2020-10-05
"Knowledge-Enhanced Personalized Review Generation with Capsule Graph
  Neural Network",2020-10-04 03:54:40+00:00,http://arxiv.org/abs/2010.01480v1,"Junyi Li, Siqing Li, Wayne Xin Zhao, Gaole He, Zhicheng Wei, Nicholas Jing Yuan, Ji-Rong Wen","cs.CL, cs.AI",knowledge,"Personalized review generation (PRG) aims to automatically produce review
text reflecting user preference, which is a challenging natural language
generation task. Most of previous studies do not explicitly model factual
description of products, tending to generate uninformative content. Moreover,
they mainly focus on word-level generation, but cannot accurately reflect more
abstractive user preference in multiple aspects. To address the above issues,
we propose a novel knowledge-enhanced PRG model based on capsule graph neural
network~(Caps-GNN). We first construct a heterogeneous knowledge graph (HKG)
for utilizing rich item attributes. We adopt Caps-GNN to learn graph capsules
for encoding underlying characteristics from the HKG. Our generation process
contains two major steps, namely aspect sequence generation and sentence
generation. First, based on graph capsules, we adaptively learn aspect capsules
for inferring the aspect sequence. Then, conditioned on the inferred aspect
label, we design a graph-based copy mechanism to generate sentences by
incorporating related entities or words from HKG. To our knowledge, we are the
first to utilize knowledge graph for the PRG task. The incorporated KG
information is able to enhance user preference at both aspect and word levels.
Extensive experiments on three real-world datasets have demonstrated the
effectiveness of our model on the PRG task.",2020-10-04
"Continual Learning for Natural Language Generation in Task-oriented
  Dialog Systems",2020-10-02 10:32:29+00:00,http://arxiv.org/abs/2010.00910v1,"Fei Mi, Liangwei Chen, Mengjie Zhao, Minlie Huang, Boi Faltings","cs.CL, cs.LG",knowledge,"Natural language generation (NLG) is an essential component of task-oriented
dialog systems. Despite the recent success of neural approaches for NLG, they
are typically developed in an offline manner for particular domains. To better
fit real-life applications where new data come in a stream, we study NLG in a
""continual learning"" setting to expand its knowledge to new domains or
functionalities incrementally. The major challenge towards this goal is
catastrophic forgetting, meaning that a continually trained model tends to
forget the knowledge it has learned before. To this end, we propose a method
called ARPER (Adaptively Regularized Prioritized Exemplar Replay) by replaying
prioritized historical exemplars, together with an adaptive regularization
technique based on ElasticWeight Consolidation. Extensive experiments to
continually learn new domains and intents are conducted on MultiWoZ-2.0 to
benchmark ARPER with a wide range of techniques. Empirical results demonstrate
that ARPER significantly outperforms other methods by effectively mitigating
the detrimental catastrophic forgetting issue.",2020-10-02
"MEGATRON-CNTRL: Controllable Story Generation with External Knowledge
  Using Large-Scale Language Models",2020-10-02 08:07:12+00:00,http://arxiv.org/abs/2010.00840v1,"Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung, Anima Anandkumar, Bryan Catanzaro",cs.CL,knowledge,"Existing pre-trained large language models have shown unparalleled generative
capabilities. However, they are not controllable. In this paper, we propose
MEGATRON-CNTRL, a novel framework that uses large-scale language models and
adds control to text generation by incorporating an external knowledge base.
Our framework consists of a keyword predictor, a knowledge retriever, a
contextual knowledge ranker, and a conditional text generator. As we do not
have access to ground-truth supervision for the knowledge ranker, we make use
of weak supervision from sentence embedding. The empirical results show that
our model generates more fluent, consistent, and coherent stories with less
repetition and higher diversity compared to prior work on the ROC story
dataset. We showcase the controllability of our model by replacing the keywords
used to generate stories and re-running the generation process. Human
evaluation results show that 77.5% of these stories are successfully controlled
by the new keywords. Furthermore, by scaling our model from 124 million to 8.3
billion parameters we demonstrate that larger models improve both the quality
of generation (from 74.5% to 93.0% for consistency) and controllability (from
77.5% to 91.5%).",2020-10-02
"Learning from Mistakes: Combining Ontologies via Self-Training for
  Dialogue Generation",2020-09-30 23:54:38+00:00,http://arxiv.org/abs/2010.00150v1,"Lena Reed, Vrindavan Harrison, Shereen Oraby, Dilek Hakkani-Tur, Marilyn Walker",cs.CL,knowledge,"Natural language generators (NLGs) for task-oriented dialogue typically take
a meaning representation (MR) as input. They are trained end-to-end with a
corpus of MR/utterance pairs, where the MRs cover a specific set of dialogue
acts and domain attributes. Creation of such datasets is labor-intensive and
time-consuming. Therefore, dialogue systems for new domain ontologies would
benefit from using data for pre-existing ontologies. Here we explore, for the
first time, whether it is possible to train an NLG for a new larger ontology
using existing training sets for the restaurant domain, where each set is based
on a different ontology. We create a new, larger combined ontology, and then
train an NLG to produce utterances covering it. For example, if one dataset has
attributes for family-friendly and rating information, and the other has
attributes for decor and service, our aim is an NLG for the combined ontology
that can produce utterances that realize values for family-friendly, rating,
decor and service. Initial experiments with a baseline neural
sequence-to-sequence model show that this task is surprisingly challenging. We
then develop a novel self-training method that identifies (errorful) model
outputs, automatically constructs a corrected MR input to form a new (MR,
utterance) training pair, and then repeatedly adds these new instances back
into the training data. We then test the resulting model on a new test set. The
result is a self-trained model whose performance is an absolute 75.4%
improvement over the baseline model. We also report a human qualitative
evaluation of the final model showing that it achieves high naturalness,
semantic coherence and grammaticality",2020-09-30
Graph-based Multi-hop Reasoning for Long Text Generation,2020-09-28 12:47:59+00:00,http://arxiv.org/abs/2009.13282v1,"Liang Zhao, Jingjing Xu, Junyang Lin, Yichang Zhang, Hongxia Yang, Xu Sun",cs.CL,knowledge,"Long text generation is an important but challenging task.The main problem
lies in learning sentence-level semantic dependencies which traditional
generative models often suffer from. To address this problem, we propose a
Multi-hop Reasoning Generation (MRG) approach that incorporates multi-hop
reasoning over a knowledge graph to learn semantic dependencies among
sentences. MRG consists of twoparts, a graph-based multi-hop reasoning module
and a path-aware sentence realization module. The reasoning module is
responsible for searching skeleton paths from a knowledge graph to imitate the
imagination process in the human writing for semantic transfer. Based on the
inferred paths, the sentence realization module then generates a complete
sentence. Unlike previous black-box models, MRG explicitly infers the skeleton
path, which provides explanatory views tounderstand how the proposed model
works. We conduct experiments on three representative tasks, including story
generation, review generation, and product description generation. Automatic
and manual evaluation show that our proposed method can generate more
informative and coherentlong text than strong baselines, such as pre-trained
models(e.g. GPT-2) and knowledge-enhanced models.",2020-09-28
"KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense
  Reasoning",2020-09-26 19:57:49+00:00,http://arxiv.org/abs/2009.12677v1,"Ye Liu, Yao Wan, Lifang He, Hao Peng, Philip S. Yu","cs.CL, cs.SC",knowledge,"Generative commonsense reasoning which aims to empower machines to generate
sentences with the capacity of reasoning over a set of concepts is a critical
bottleneck for text generation. Even the state-of-the-art pre-trained language
generation models struggle at this task and often produce implausible and
anomalous sentences. One reason is that they rarely consider incorporating the
knowledge graph which can provide rich relational information among the
commonsense concepts. To promote the ability of commonsense reasoning for text
generation, we propose a novel knowledge graphaugmented pre-trained language
generation model KG-BART, which encompasses the complex relations of concepts
through the knowledge graph and produces more logical and natural sentences as
output. Moreover, KG-BART can leverage the graph attention to aggregate the
rich concept semantics that enhances the model generalization on unseen concept
sets. Experiments on benchmark CommonGen dataset verify the effectiveness of
our proposed approach by comparing with several strong pre-trained language
generation models, particularly KG-BART outperforms BART by 15.98%, 17.49%, in
terms of BLEU-3, 4. Moreover, we also show that the generated context by our
model can work as background scenarios to benefit downstream commonsense QA
tasks.",2020-09-26
"Language Generation with Multi-Hop Reasoning on Commonsense Knowledge
  Graph",2020-09-24 13:55:32+00:00,http://arxiv.org/abs/2009.11692v1,"Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, Minlie Huang",cs.CL,knowledge,"Despite the success of generative pre-trained language models on a series of
text generation tasks, they still suffer in cases where reasoning over
underlying commonsense knowledge is required during generation. Existing
approaches that integrate commonsense knowledge into generative pre-trained
language models simply transfer relational knowledge by post-training on
individual knowledge triples while ignoring rich connections within the
knowledge graph. We argue that exploiting both the structural and semantic
information of the knowledge graph facilitates commonsense-aware text
generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow
(GRF) that enables pre-trained models with dynamic multi-hop reasoning on
multi-relational paths extracted from the external commonsense knowledge graph.
We empirically show that our model outperforms existing baselines on three text
generation tasks that require reasoning over commonsense knowledge. We also
demonstrate the effectiveness of the dynamic multi-hop reasoning module with
reasoning paths inferred by the model that provide rationale to the generation.",2020-09-24
Prior Art Search and Reranking for Generated Patent Text,2020-09-19 01:16:18+00:00,http://arxiv.org/abs/2009.09132v1,"Jieh-Sheng Lee, Jieh Hsiang",cs.CL,knowledge,"Generative models, such as GPT-2, have demonstrated impressive results
recently. A fundamental question we'd like to address is: where did the
generated text come from? This work is our initial effort toward answering the
question by using prior art search. The purpose of the prior art search is to
find the most similar prior text in the training data of GPT-2. We take a
reranking approach and apply it to the patent domain. Specifically, we
pre-train GPT-2 models from scratch by using the patent data from the USPTO.
The input for the prior art search is the patent text generated by the GPT-2
model. We also pre-trained BERT models from scratch for converting patent text
to embeddings. The steps of reranking are: (1) search the most similar text in
the training data of GPT-2 by taking a bag-of-word ranking approach (BM25), (2)
convert the search results in text format to BERT embeddings, and (3) provide
the final result by ranking the BERT embeddings based on their similarities
with the patent text generated by GPT-2. The experiments in this work show that
such reranking is better than ranking with embeddings alone. However, our mixed
results also indicate that calculating the semantic similarities among long
text spans is still challenging. To our knowledge, this work is the first to
implement a reranking system to identify retrospectively the most similar
inputs to a GPT model based on its output.",2020-09-19
Generation-Augmented Retrieval for Open-domain Question Answering,2020-09-17 23:08:01+00:00,http://arxiv.org/abs/2009.08553v2,"Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen","cs.CL, cs.IR",knowledge,"Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and
efficient, but solely rely on lexical overlap without semantic matching. Recent
dense retrieval methods learn latent representations to tackle the lexical
mismatch problem, while being more computationally expensive and insufficient
for exact matching as they embed the text sequence into a single vector with
limited capacity. In this paper, we present Generation-Augmented Retrieval
(GAR), a query expansion method that augments a query with relevant contexts
through text generation. We demonstrate on open-domain question answering that
the generated contexts significantly enrich the semantics of the queries and
thus GAR with sparse representations (BM25) achieves comparable or better
performance than the state-of-the-art dense methods such as DPR
\cite{karpukhin2020dense}. We show that generating various contexts of a query
is beneficial as fusing their results consistently yields better retrieval
accuracy. Moreover, as sparse and dense representations are often
complementary, GAR can be easily combined with DPR to achieve even better
performance. Furthermore, GAR achieves the state-of-the-art performance on the
Natural Questions and TriviaQA datasets under the extractive setting when
equipped with an extractive reader, and consistently outperforms other
retrieval methods when the same generative reader is used.",2020-09-17
Knowledge Graphs for Multilingual Language Translation and Generation,2020-09-16 14:36:41+00:00,http://arxiv.org/abs/2009.07715v1,Diego Moussallem,cs.CL,knowledge,"The Natural Language Processing (NLP) community has recently seen outstanding
progress, catalysed by the release of different Neural Network (NN)
architectures. Neural-based approaches have proven effective by significantly
increasing the output quality of a large number of automated solutions for NLP
tasks (Belinkov and Glass, 2019). Despite these notable advancements, dealing
with entities still poses a difficult challenge as they are rarely seen in
training data. Entities can be classified into two groups, i.e., proper nouns
and common nouns. Proper nouns are also known as Named Entities (NE) and
correspond to the name of people, organizations, or locations, e.g., John, WHO,
or Canada. Common nouns describe classes of objects, e.g., spoon or cancer.
Both types of entities can be found in a Knowledge Graph (KG). Recent work has
successfully exploited the contribution of KGs in NLP tasks, such as Natural
Language Inference (NLI) (KM et al.,2018) and Question Answering (QA) (Sorokin
and Gurevych, 2018). Only a few works had exploited the benefits of KGs in
Neural Machine Translation (NMT) when the work presented herein began.
Additionally, few works had studied the contribution of KGs to Natural Language
Generation (NLG) tasks. Moreover, the multilinguality also remained an open
research area in these respective tasks (Young et al., 2018). In this thesis,
we focus on the use of KGs for machine translation and the generation of texts
to deal with the problems caused by entities and consequently enhance the
quality of automatically generated texts.",2020-09-16
Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News,2020-09-16 14:13:15+00:00,http://arxiv.org/abs/2009.07698v5,"Reuben Tan, Bryan A. Plummer, Kate Saenko","cs.AI, cs.CL, cs.CV",knowledge,"Large-scale dissemination of disinformation online intended to mislead or
deceive the general population is a major societal problem. Rapid progression
in image, video, and natural language generative models has only exacerbated
this situation and intensified our need for an effective defense mechanism.
While existing approaches have been proposed to defend against neural fake
news, they are generally constrained to the very limited setting where articles
only have text and metadata such as the title and authors. In this paper, we
introduce the more realistic and challenging task of defending against
machine-generated news that also includes images and captions. To identify the
possible weaknesses that adversaries can exploit, we create a NeuralNews
dataset composed of 4 different types of generated articles as well as conduct
a series of human user study experiments based on this dataset. In addition to
the valuable insights gleaned from our user study experiments, we provide a
relatively effective approach based on detecting visual-semantic
inconsistencies, which will serve as an effective first line of defense and a
useful reference for future work in defending against machine-generated
disinformation.",2020-09-16
UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation,2020-09-16 11:01:46+00:00,http://arxiv.org/abs/2009.07602v1,"Jian Guan, Minlie Huang",cs.CL,knowledge,"Despite the success of existing referenced metrics (e.g., BLEU and
MoverScore), they correlate poorly with human judgments for open-ended text
generation including story or dialog generation because of the notorious
one-to-many issue: there are many plausible outputs for the same input, which
may differ substantially in literal or semantics from the limited number of
given references. To alleviate this issue, we propose UNION, a learnable
unreferenced metric for evaluating open-ended story generation, which measures
the quality of a generated story without any reference. Built on top of BERT,
UNION is trained to distinguish human-written stories from negative samples and
recover the perturbation in negative stories. We propose an approach of
constructing negative samples by mimicking the errors commonly observed in
existing NLG models, including repeated plots, conflicting logic, and
long-range incoherence. Experiments on two story datasets demonstrate that
UNION is a reliable measure for evaluating the quality of generated stories,
which correlates better with human judgments and is more generalizable than
existing state-of-the-art metrics.",2020-09-16
Autoregressive Knowledge Distillation through Imitation Learning,2020-09-15 17:43:02+00:00,http://arxiv.org/abs/2009.07253v2,"Alexander Lin, Jeremy Wohlwend, Howard Chen, Tao Lei","cs.CL, cs.LG",knowledge,"The performance of autoregressive models on natural language generation tasks
has dramatically improved due to the adoption of deep, self-attentive
architectures. However, these gains have come at the cost of hindering
inference speed, making state-of-the-art models cumbersome to deploy in
real-world, time-sensitive settings. We develop a compression technique for
autoregressive models that is driven by an imitation learning perspective on
knowledge distillation. The algorithm is designed to address the exposure bias
problem. On prototypical language generation tasks such as translation and
summarization, our method consistently outperforms other distillation
algorithms, such as sequence-level knowledge distillation. Student models
trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those
trained from scratch, while increasing inference speed by up to 14 times in
comparison to the teacher model.",2020-09-15
Robust Conversational AI with Grounded Text Generation,2020-09-07 23:49:28+00:00,http://arxiv.org/abs/2009.03457v1,"Jianfeng Gao, Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh, Lars Liden, Heung-Yeung Shum","cs.AI, cs.CL",knowledge,"This article presents a hybrid approach based on a Grounded Text Generation
(GTG) model to building robust task bots at scale. GTG is a hybrid model which
uses a large-scale Transformer neural network as its backbone, combined with
symbol-manipulation modules for knowledge base inference and prior knowledge
encoding, to generate responses grounded in dialog belief state and real-world
knowledge for task completion. GTG is pre-trained on large amounts of raw text
and human conversational data, and can be fine-tuned to complete a wide range
of tasks.
  The hybrid approach and its variants are being developed simultaneously by
multiple research teams. The primary results reported on task-oriented dialog
benchmarks are very promising, demonstrating the big potential of this
approach. This article provides an overview of this progress and discusses
related methods and technologies that can be incorporated for building robust
conversational AI systems.",2020-09-07
"Black Box to White Box: Discover Model Characteristics Based on
  Strategic Probing",2020-09-07 14:44:28+00:00,http://arxiv.org/abs/2009.03136v1,"Josh Kalin, Matthew Ciolino, David Noever, Gerry Dozier","cs.LG, stat.ML",knowledge,"In Machine Learning, White Box Adversarial Attacks rely on knowing underlying
knowledge about the model attributes. This works focuses on discovering to
distrinct pieces of model information: the underlying architecture and primary
training dataset. With the process in this paper, a structured set of input
probes and the output of the model become the training data for a deep
classifier. Two subdomains in Machine Learning are explored: image based
classifiers and text transformers with GPT-2. With image classification, the
focus is on exploring commonly deployed architectures and datasets available in
popular public libraries. Using a single transformer architecture with multiple
levels of parameters, text generation is explored by fine tuning off different
datasets. Each dataset explored in image and text are distinguishable from one
another. Diversity in text transformer outputs implies further research is
needed to successfully classify architecture attribution in text domain.",2020-09-07
"Adversarial Watermarking Transformer: Towards Tracing Text Provenance
  with Data Hiding",2020-09-07 11:01:24+00:00,http://arxiv.org/abs/2009.03015v1,"Sahar Abdelnabi, Mario Fritz","cs.CR, cs.CL, cs.CY, cs.LG, I.2.7",knowledge,"Recent advances in natural language generation have introduced powerful
language models with high-quality output text. However, this raises concerns
about the potential misuse of such models for malicious purposes. In this
paper, we study natural language watermarking as a defense to help better mark
and trace the provenance of text. We introduce the Adversarial Watermarking
Transformer (AWT) with a jointly trained encoder-decoder and adversarial
training that, given an input text and a binary message, generates an output
text that is unobtrusively encoded with the given message. We further study
different training and inference strategies to achieve minimal changes to the
semantics and correctness of the input text. AWT is the first end-to-end model
to hide data in text by automatically learning -- without ground truth -- word
substitutions along with their locations in order to encode the message. We
show that our model is effective in largely preserving text utility and
decoding the watermark while hiding its presence against adversaries.
Additionally, we demonstrate that our method is robust against a range of local
changes and denoising attacks.",2020-09-07
Navigating Human Language Models with Synthetic Agents,2020-08-10 14:39:53+00:00,http://arxiv.org/abs/2008.04162v7,"Philip Feldman, Antonio Bucchiarone","cs.AI, cs.CL, cs.MA, I.2; I.6; J.4",knowledge,"Modern natural language models such as the GPT-2/GPT-3 contain tremendous
amounts of information about human belief in a consistently testable form. If
these models could be shown to accurately reflect the underlying beliefs of the
human beings that produced the data used to train these models, then such
models become a powerful sociological tool in ways that are distinct from
traditional methods, such as interviews and surveys. In this study, We train a
version of the GPT-2 on a corpora of historical chess games, and then ""launch""
clusters of synthetic agents into the model, using text strings to create
context and orientation. We compare the trajectories contained in the text
generated by the agents/model and compare that to the known ground truth of the
chess board, move legality, and historical patterns of play. We find that the
percentages of moves by piece using the model are substantially similar from
human patterns. We further find that the model creates an accurate latent
representation of the chessboard, and that it is possible to plot trajectories
of legal moves across the board using this knowledge.",2020-08-10
Investigating Pretrained Language Models for Graph-to-Text Generation,2020-07-16 16:05:34+00:00,http://arxiv.org/abs/2007.08426v2,"Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Schütze, Iryna Gurevych",cs.CL,knowledge,"Graph-to-text generation aims to generate fluent texts from graph-based data.
In this paper, we investigate two recently proposed pretrained language models
(PLMs) and analyze the impact of different task-adaptive pretraining strategies
for PLMs in graph-to-text generation. We present a study across three graph
domains: meaning representations, Wikipedia knowledge graphs (KGs) and
scientific KGs. We show that the PLMs BART and T5 achieve new state-of-the-art
results and that task-adaptive pretraining strategies improve their performance
even further. In particular, we report new state-of-the-art BLEU scores of
49.72 on LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative
improvement of 31.8%, 4.5%, and 42.4%, respectively. In an extensive analysis,
we identify possible reasons for the PLMs' success on graph-to-text tasks. We
find evidence that their knowledge about true facts helps them perform well
even when the input graph representation is reduced to a simple bag of node and
edge labels.",2020-07-16
"Deep Transformer based Data Augmentation with Subword Units for
  Morphologically Rich Online ASR",2020-07-14 10:22:05+00:00,http://arxiv.org/abs/2007.06949v3,"Balázs Tarján, György Szaszák, Tibor Fegyó, Péter Mihajlik","eess.AS, cs.CL",knowledge,"Recently Deep Transformer models have proven to be particularly powerful in
language modeling tasks for ASR. Their high complexity, however, makes them
very difficult to apply in the first (single) pass of an online system. Recent
studies showed that a considerable part of the knowledge of neural network
Language Models (LM) can be transferred to traditional n-grams by using neural
text generation based data augmentation. In our paper, we pre-train a GPT-2
Transformer LM on a general text corpus and fine-tune it on our Hungarian
conversational call center ASR task. We show that although data augmentation
with Transformer-generated text works well for isolating languages, it causes a
vocabulary explosion in a morphologically rich language. Therefore, we propose
a new method called subword-based neural text augmentation, where we retokenize
the generated text into statistically derived subwords. We compare Morfessor
and BPE statistical subword tokenizers and show that both methods can
significantly improve the WER while greatly reducing vocabulary size and memory
requirements. Finally, we also demonstrate that subword-based neural text
augmentation outperforms the word-based approach not only in terms of overall
WER but also in recognition of OOV words.",2020-07-14
"Contextualized Code Representation Learning for Commit Message
  Generation",2020-07-14 09:43:26+00:00,http://arxiv.org/abs/2007.06934v1,"Lun Yiu Nie, Cuiyun Gao, Zhicong Zhong, Wai Lam, Yang Liu, Zenglin Xu","cs.CL, cs.LG, cs.SE",knowledge,"Automatic generation of high-quality commit messages for code commits can
substantially facilitate developers' works and coordination. However, the
semantic gap between source code and natural language poses a major challenge
for the task. Several studies have been proposed to alleviate the challenge but
none explicitly involves code contextual information during commit message
generation. Specifically, existing research adopts static embedding for code
tokens, which maps a token to the same vector regardless of its context. In
this paper, we propose a novel Contextualized code representation learning
method for commit message Generation (CoreGen). CoreGen first learns
contextualized code representation which exploits the contextual information
behind code commit sequences. The learned representations of code commits built
upon Transformer are then transferred for downstream commit message generation.
Experiments on the benchmark dataset demonstrate the superior effectiveness of
our model over the baseline models with an improvement of 28.18% in terms of
BLEU-4 score. Furthermore, we also highlight the future opportunities in
training contextualized code representations on larger code corpus as a
solution to low-resource settings and adapting the pretrained code
representations to other downstream code-to-text generation tasks.",2020-07-14
DART: Open-Domain Structured Data Record to Text Generation,2020-07-06 16:35:30+00:00,http://arxiv.org/abs/2007.02871v1,"Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Nazneen Fatema Rajani, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Murori Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher",cs.CL,knowledge,"We introduce DART, a large dataset for open-domain structured data record to
text generation. We consider the structured data record input as a set of RDF
entity-relation triples, a format widely used for knowledge representation and
semantics description. DART consists of 82,191 examples across different
domains with each input being a semantic RDF triple set derived from data
records in tables and the tree ontology of the schema, annotated with sentence
descriptions that cover all facts in the triple set. This hierarchical,
structured format with its open-domain nature differentiates DART from other
existing table-to-text corpora. We conduct an analysis of DART on several
state-of-the-art text generation models, showing that it introduces new and
interesting challenges compared to existing datasets. Furthermore, we
demonstrate that finetuning pretrained language models on DART facilitates
out-of-domain generalization on the WebNLG 2017 dataset. DART is available at
https://github.com/Yale-LILY/dart.",2020-07-06
Adversarial Mutual Information for Text Generation,2020-06-30 19:11:51+00:00,http://arxiv.org/abs/2007.00067v1,"Boyuan Pan, Yazheng Yang, Kaizhao Liang, Bhavya Kailkhura, Zhongming Jin, Xian-Sheng Hua, Deng Cai, Bo Li","cs.CL, cs.LG, stat.ML",knowledge,"Recent advances in maximizing mutual information (MI) between the source and
target have demonstrated its effectiveness in text generation. However,
previous works paid little attention to modeling the backward network of MI
(i.e., dependency from the target to the source), which is crucial to the
tightness of the variational information maximization lower bound. In this
paper, we propose Adversarial Mutual Information (AMI): a text generation
framework which is formed as a novel saddle point (min-max) optimization aiming
to identify joint interactions between the source and target. Within this
framework, the forward and backward networks are able to iteratively promote or
demote each other's generated instances by comparing the real and synthetic
data distributions. We also develop a latent noise sampling strategy that
leverages random variations at the high-level semantic space to enhance the
long term dependency in the generation process. Extensive experiments based on
different text generation tasks demonstrate that the proposed AMI framework can
significantly outperform several strong baselines, and we also show that AMI
has potential to lead to a tighter lower bound of maximum mutual information
for the variational information maximization problem.",2020-06-30
Learning Sparse Prototypes for Text Generation,2020-06-29 19:41:26+00:00,http://arxiv.org/abs/2006.16336v2,"Junxian He, Taylor Berg-Kirkpatrick, Graham Neubig","cs.CL, cs.LG",knowledge,"Prototype-driven text generation uses non-parametric models that first choose
from a library of sentence ""prototypes"" and then modify the prototype to
generate the output text. While effective, these methods are inefficient at
test time as a result of needing to store and index the entire training corpus.
Further, existing methods often require heuristics to identify which prototypes
to reference at training time. In this paper, we propose a novel generative
model that automatically learns a sparse prototype support set that,
nonetheless, achieves strong language modeling performance. This is achieved by
(1) imposing a sparsity-inducing prior on the prototype selection distribution,
and (2) utilizing amortized variational inference to learn a prototype
retrieval function. In experiments, our model outperforms previous
prototype-driven language models while achieving up to a 1000x memory
reduction, as well as a 1000x speed-up at test time. More interestingly, we
show that the learned prototypes are able to capture semantics and syntax at
different granularity as we vary the sparsity of prototype selection, and that
certain sentence attributes can be controlled by specifying the prototype for
generation.",2020-06-29
"Efficient text generation of user-defined topic using generative
  adversarial networks",2020-06-22 04:49:47+00:00,http://arxiv.org/abs/2006.12005v1,"Chenhan Yuan, Yi-chin Huang, Cheng-Hung Tsai",cs.CL,knowledge,"This study focused on efficient text generation using generative adversarial
networks (GAN). Assuming that the goal is to generate a paragraph of a
user-defined topic and sentimental tendency, conventionally the whole network
has to be re-trained to obtain new results each time when a user changes the
topic. This would be time-consuming and impractical. Therefore, we propose a
User-Defined GAN (UD-GAN) with two-level discriminators to solve this problem.
The first discriminator aims to guide the generator to learn paragraph-level
information and sentence syntactic structure, which is constructed by
multiple-LSTMs. The second one copes with higher-level information, such as the
user-defined sentiment and topic for text generation. The cosine similarity
based on TF-IDF and length penalty are adopted to determine the relevance of
the topic. Then, the second discriminator is re-trained with the generator if
the topic or sentiment for text generation is modified. The system evaluations
are conducted to compare the performance of the proposed method with other
GAN-based ones. The objective results showed that the proposed method is
capable of generating texts with less time than others and the generated text
is related to the user-defined topic and sentiment. We will further investigate
the possibility of incorporating more detailed paragraph information such as
semantics into text generation to enhance the result.",2020-06-22
Explainable and Discourse Topic-aware Neural Language Understanding,2020-06-18 15:53:58+00:00,http://arxiv.org/abs/2006.10632v2,"Yatin Chaudhary, Hinrich Schütze, Pankaj Gupta","cs.CL, cs.AI, cs.LG",knowledge,"Marrying topic models and language models exposes language understanding to a
broader source of document-level context beyond sentences via topics. While
introducing topical semantics in language models, existing approaches
incorporate latent document topic proportions and ignore topical discourse in
sentences of the document. This work extends the line of research by
additionally introducing an explainable topic representation in language
understanding, obtained from a set of key terms correspondingly for each latent
topic of the proportion. Moreover, we retain sentence-topic associations along
with document-topic association by modeling topical discourse for every
sentence in the document. We present a novel neural composite language model
that exploits both the latent and explainable topics along with topical
discourse at sentence-level in a joint learning framework of topic and language
models. Experiments over a range of tasks such as language modeling, word sense
disambiguation, document classification, retrieval and text generation
demonstrate ability of the proposed model in improving language understanding.",2020-06-18
"Modeling Graph Structure via Relative Position for Better Text
  Generation from Knowledge Graphs",2020-06-16 15:20:04+00:00,http://arxiv.org/abs/2006.09242v1,"Martin Schmitt, Leonardo F. R. Ribeiro, Philipp Dufter, Iryna Gurevych, Hinrich Schütze",cs.CL,knowledge,"We present a novel encoder-decoder architecture for graph-to-text generation
based on Transformer, called the Graformer. With our novel graph
self-attention, every node in the input graph is taken into account for the
encoding of every other node - not only direct neighbors, facilitating the
detection of global patterns. For this, the relation between any two nodes is
characterized by the length of the shortest path between them, including the
special case when there is no such path. The Graformer learns to weigh these
node-node relations differently for different attention heads, thus virtually
learning differently connected views of the input graph. We evaluate the
Graformer on two graph-to-text generation benchmarks, the AGENDA dataset and
the WebNLG challenge dataset, where it achieves strong performance while using
significantly less parameters than other approaches.",2020-06-16
"Modelling Hierarchical Structure between Dialogue Policy and Natural
  Language Generator with Option Framework for Task-oriented Dialogue System",2020-06-11 20:55:28+00:00,http://arxiv.org/abs/2006.06814v3,"Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, Yunjie Gu","cs.CL, cs.AI, cs.LG",knowledge,"Designing task-oriented dialogue systems is a challenging research topic,
since it needs not only to generate utterances fulfilling user requests but
also to guarantee the comprehensibility. Many previous works trained end-to-end
(E2E) models with supervised learning (SL), however, the bias in annotated
system utterances remains as a bottleneck. Reinforcement learning (RL) deals
with the problem through using non-differentiable evaluation metrics (e.g., the
success rate) as rewards. Nonetheless, existing works with RL showed that the
comprehensibility of generated system utterances could be corrupted when
improving the performance on fulfilling user requests. In o gur work, we (1)
propose modelling the hierarchical structure between dialogue policy and
natural language generator (NLG) with the option framework, called HDNO, where
the latent dialogue act is applied to avoid designing specific dialogue act
representations; (2) train HDNO via hierarchical reinforcement learning (HRL),
as well as suggest the asynchronous updates between dialogue policy and NLG
during training to theoretically guarantee their convergence to a local
maximizer; and (3) propose using a discriminator modelled with language models
as an additional reward to further improve the comprehensibility. We test HDNO
on MultiWoz 2.0 and MultiWoz 2.1, the datasets on multi-domain dialogues, in
comparison with word-level E2E model trained with RL, LaRL and HDSA, showing
improvements on the performance evaluated by automatic evaluation metrics and
human evaluation. Finally, we demonstrate the semantic meanings of latent
dialogue acts to show the ability of explanation.",2020-06-11
"On the Effectiveness of Neural Text Generation based Data Augmentation
  for Recognition of Morphologically Rich Speech",2020-06-09 09:01:04+00:00,http://arxiv.org/abs/2006.05129v1,"Balázs Tarján, György Szaszák, Tibor Fegyó, Péter Mihajlik","eess.AS, cs.CL, cs.SD",knowledge,"Advanced neural network models have penetrated Automatic Speech Recognition
(ASR) in recent years, however, in language modeling many systems still rely on
traditional Back-off N-gram Language Models (BNLM) partly or entirely. The
reason for this are the high cost and complexity of training and using neural
language models, mostly possible by adding a second decoding pass (rescoring).
In our recent work we have significantly improved the online performance of a
conversational speech transcription system by transferring knowledge from a
Recurrent Neural Network Language Model (RNNLM) to the single pass BNLM with
text generation based data augmentation. In the present paper we analyze the
amount of transferable knowledge and demonstrate that the neural augmented LM
(RNN-BNLM) can help to capture almost 50% of the knowledge of the RNNLM yet by
dropping the second decoding pass and making the system real-time capable. We
also systematically compare word and subword LMs and show that subword-based
neural text augmentation can be especially beneficial in under-resourced
conditions. In addition, we show that using the RNN-BNLM in the first pass
followed by a neural second pass, offline ASR results can be even significantly
improved.",2020-06-09
ColdGANs: Taming Language GANs with Cautious Sampling Strategies,2020-06-08 14:48:14+00:00,http://arxiv.org/abs/2006.04643v1,"Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano","cs.CL, cs.LG",knowledge,"Training regimes based on Maximum Likelihood Estimation (MLE) suffer from
known limitations, often leading to poorly generated text sequences. At the
root of these limitations is the mismatch between training and inference, i.e.
the so-called exposure bias, exacerbated by considering only the reference
texts as correct, while in practice several alternative formulations could be
as good. Generative Adversarial Networks (GANs) can mitigate those limitations
but the discrete nature of text has hindered their application to language
generation: the approaches proposed so far, based on Reinforcement Learning,
have been shown to underperform MLE. Departing from previous works, we analyze
the exploration step in GANs applied to text generation, and show how classical
sampling results in unstable training. We propose to consider alternative
exploration strategies in a GAN framework that we name ColdGANs, where we force
the sampling to be close to the distribution modes to get smoother learning
dynamics. For the first time, to the best of our knowledge, the proposed
language GANs compare favorably to MLE, and obtain improvements over the
state-of-the-art on three generative tasks, namely unconditional text
generation, question generation, and abstractive summarization.",2020-06-08
"M3P: Learning Universal Representations via Multitask Multilingual
  Multimodal Pre-training",2020-06-04 03:54:29+00:00,http://arxiv.org/abs/2006.02635v1,"Haoyang Huang, Lin Su, Di Qi, Nan Duan, Edward Cui, Taroon Bharti, Lei Zhang, Lijuan Wang, Jianfeng Gao, Bei Liu, Jianlong Fu, Dongdong Zhang, Xin Liu, Ming Zhou","cs.CL, cs.CV",knowledge,"This paper presents a Multitask Multilingual Multimodal Pre-trained model
(M3P) that combines multilingual-monomodal pre-training and
monolingual-multimodal pre-training into a unified framework via multitask
learning and weight sharing. The model learns universal representations that
can map objects that occurred in different modalities or expressed in different
languages to vectors in a common semantic space. To verify the generalization
capability of M3P, we fine-tune the pre-trained model for different types of
downstream tasks: multilingual image-text retrieval, multilingual image
captioning, multimodal machine translation, multilingual natural language
inference and multilingual text generation. Evaluation shows that M3P can (i)
achieve comparable results on multilingual tasks and English multimodal tasks,
compared to the state-of-the-art models pre-trained for these two types of
tasks separately, and (ii) obtain new state-of-the-art results on non-English
multimodal tasks in the zero-shot or few-shot setting. We also build a new
Multilingual Image-Language Dataset (MILD) by collecting large amounts of
(text-query, image, context) triplets in 8 languages from the logs of a
commercial search engine",2020-06-04
"Graph-Stega: Semantic Controllable Steganographic Text Generation Guided
  by Knowledge Graph",2020-06-02 06:53:21+00:00,http://arxiv.org/abs/2006.08339v1,"Zhongliang Yang, Baitao Gong, Yamin Li, Jinshuai Yang, Zhiwen Hu, Yongfeng Huang","cs.CL, cs.CR",knowledge,"Most of the existing text generative steganographic methods are based on
coding the conditional probability distribution of each word during the
generation process, and then selecting specific words according to the secret
information, so as to achieve information hiding. Such methods have their
limitations which may bring potential security risks. Firstly, with the
increase of embedding rate, these models will choose words with lower
conditional probability, which will reduce the quality of the generated
steganographic texts; secondly, they can not control the semantic expression of
the final generated steganographic text. This paper proposes a new text
generative steganography method which is quietly different from the existing
models. We use a Knowledge Graph (KG) to guide the generation of steganographic
sentences. On the one hand, we hide the secret information by coding the path
in the knowledge graph, but not the conditional probability of each generated
word; on the other hand, we can control the semantic expression of the
generated steganographic text to a certain extent. The experimental results
show that the proposed model can guarantee both the quality of the generated
text and its semantic expression, which is a supplement and improvement to the
current text generation steganography.",2020-06-02
"Improving Disentangled Text Representation Learning with
  Information-Theoretic Guidance",2020-06-01 03:36:01+00:00,http://arxiv.org/abs/2006.00693v2,"Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yitong Li, Lawrence Carin","cs.LG, stat.ML",knowledge,"Learning disentangled representations of natural language is essential for
many NLP tasks, e.g., conditional text generation, style transfer, personalized
dialogue systems, etc. Similar problems have been studied extensively for other
forms of data, such as images and videos. However, the discrete nature of
natural language makes the disentangling of textual representations more
challenging (e.g., the manipulation over the data space cannot be easily
achieved). Inspired by information theory, we propose a novel method that
effectively manifests disentangled representations of text, without any
supervision on semantics. A new mutual information upper bound is derived and
leveraged to measure dependence between style and content. By minimizing this
upper bound, the proposed method induces style and content embeddings into two
independent low-dimensional spaces. Experiments on both conditional text
generation and text-style transfer demonstrate the high quality of our
disentangled representation in terms of content and style preservation.",2020-06-01
"MixingBoard: a Knowledgeable Stylized Integrated Text Generation
  Platform",2020-05-17 20:29:27+00:00,http://arxiv.org/abs/2005.08365v2,"Xiang Gao, Michel Galley, Bill Dolan",cs.CL,knowledge,"We present MixingBoard, a platform for quickly building demos with a focus on
knowledge grounded stylized text generation. We unify existing text generation
algorithms in a shared codebase and further adapt earlier algorithms for
constrained generation. To borrow advantages from different models, we
implement strategies for cross-model integration, from the token probability
level to the latent space level. An interface to external knowledge is provided
via a module that retrieves on-the-fly relevant knowledge from passages on the
web or any document collection. A user interface for local development, remote
webpage access, and a RESTful API are provided to make it simple for users to
build their own demos.",2020-05-17
Schema-Guided Natural Language Generation,2020-05-11 23:01:22+00:00,http://arxiv.org/abs/2005.05480v2,"Yuheng Du, Shereen Oraby, Vittorio Perera, Minmin Shen, Anjali Narayan-Chen, Tagyoung Chung, Anu Venkatesh, Dilek Hakkani-Tur",cs.CL,knowledge,"Neural network based approaches to data-to-text natural language generation
(NLG) have gained popularity in recent years, with the goal of generating a
natural language prompt that accurately realizes an input meaning
representation. To facilitate the training of neural network models,
researchers created large datasets of paired utterances and their meaning
representations. However, the creation of such datasets is an arduous task and
they mostly consist of simple meaning representations composed of slot and
value tokens to be realized. These representations do not include any
contextual information that an NLG system can use when trying to generalize,
such as domain information and descriptions of slots and values. In this paper,
we present the novel task of Schema-Guided Natural Language Generation
(SG-NLG). Here, the goal is still to generate a natural language prompt, but in
SG-NLG, the input MRs are paired with rich schemata providing contextual
information. To generate a dataset for SG-NLG we re-purpose an existing dataset
for another task: dialog state tracking, which includes a large and rich schema
spanning multiple different attributes, including information about the domain,
user intent, and slot descriptions. We train different state-of-the-art models
for neural natural language generation on this dataset and show that in many
cases, including rich schema information allows our models to produce higher
quality outputs both in terms of semantics and diversity. We also conduct
experiments comparing model performance on seen versus unseen domains, and
present a human evaluation demonstrating high ratings for overall output
quality.",2020-05-11
Posterior Control of Blackbox Generation,2020-05-10 03:22:45+00:00,http://arxiv.org/abs/2005.04560v1,"Xiang Lisa Li, Alexander M. Rush","cs.CL, cs.AI, cs.LG",knowledge,"Text generation often requires high-precision output that obeys task-specific
rules. This fine-grained control is difficult to enforce with off-the-shelf
deep learning models. In this work, we consider augmenting neural generation
models with discrete control states learned through a structured
latent-variable approach. Under this formulation, task-specific knowledge can
be encoded through a range of rich, posterior constraints that are effectively
trained into the model. This approach allows users to ground internal model
decisions based on prior knowledge, without sacrificing the representational
power of neural generative models. Experiments consider applications of this
approach for text generation. We find that this method improves over standard
benchmarks, while also providing fine-grained control.",2020-05-10
Smart To-Do : Automatic Generation of To-Do Items from Emails,2020-05-05 02:21:40+00:00,http://arxiv.org/abs/2005.06282v1,"Sudipto Mukherjee, Subhabrata Mukherjee, Marcello Hasegawa, Ahmed Hassan Awadallah, Ryen White","cs.CL, cs.AI, cs.LG",knowledge,"Intelligent features in email service applications aim to increase
productivity by helping people organize their folders, compose their emails and
respond to pending tasks. In this work, we explore a new application,
Smart-To-Do, that helps users with task management over emails. We introduce a
new task and dataset for automatically generating To-Do items from emails where
the sender has promised to perform an action. We design a two-stage process
leveraging recent advances in neural text generation and sequence-to-sequence
learning, obtaining BLEU and ROUGE scores of 0:23 and 0:63 for this task. To
the best of our knowledge, this is the first work to address the problem of
composing To-Do items from emails.",2020-05-05
Improving Adversarial Text Generation by Modeling the Distant Future,2020-05-04 05:45:13+00:00,http://arxiv.org/abs/2005.01279v1,"Ruiyi Zhang, Changyou Chen, Zhe Gan, Wenlin Wang, Dinghan Shen, Guoyin Wang, Zheng Wen, Lawrence Carin","cs.CL, cs.LG",knowledge,"Auto-regressive text generation models usually focus on local fluency, and
may cause inconsistent semantic meaning in long text generation. Further,
automatically generating words with similar semantics is challenging, and
hand-crafted linguistic rules are difficult to apply. We consider a text
planning scheme and present a model-based imitation-learning approach to
alleviate the aforementioned issues. Specifically, we propose a novel guider
network to focus on the generative process over a longer horizon, which can
assist next-word prediction and provide intermediate rewards for generator
optimization. Extensive experiments demonstrate that the proposed method leads
to improved performance.",2020-05-04
"Towards Faithful Neural Table-to-Text Generation with Content-Matching
  Constraints",2020-05-03 02:54:26+00:00,http://arxiv.org/abs/2005.00969v1,"Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, Changyou Chen",cs.CL,knowledge,"Text generation from a knowledge base aims to translate knowledge triples to
natural language descriptions. Most existing methods ignore the faithfulness
between a generated text description and the original table, leading to
generated information that goes beyond the content of the table. In this paper,
for the first time, we propose a novel Transformer-based generation framework
to achieve the goal. The core techniques in our method to enforce faithfulness
include a new table-text optimal-transport matching loss and a table-text
embedding similarity loss based on the Transformer model. Furthermore, to
evaluate faithfulness, we propose a new automatic metric specialized to the
table-to-text generation problem. We also provide detailed analysis on each
component of our model in our experiments. Automatic and human evaluations show
that our framework can significantly outperform state-of-the-art by a large
margin.",2020-05-03
APo-VAE: Text Generation in Hyperbolic Space,2020-04-30 19:05:41+00:00,http://arxiv.org/abs/2005.00054v1,"Shuyang Dai, Zhe Gan, Yu Cheng, Chenyang Tao, Lawrence Carin, Jingjing Liu","cs.LG, stat.ML",knowledge,"Natural language often exhibits inherent hierarchical structure ingrained
with complex syntax and semantics. However, most state-of-the-art deep
generative models learn embeddings only in Euclidean vector space, without
accounting for this structural property of language. In this paper, we
investigate text generation in a hyperbolic latent space to learn continuous
hierarchical representations. An Adversarial Poincare Variational Autoencoder
(APo-VAE) is presented, where both the prior and variational posterior of
latent variables are defined over a Poincare ball via wrapped normal
distributions. By adopting the primal-dual formulation of KL divergence, an
adversarial learning procedure is introduced to empower robust model training.
Extensive experiments in language modeling and dialog-response generation tasks
demonstrate the winning effectiveness of the proposed APo-VAE model over VAEs
in Euclidean latent space, thanks to its superb capabilities in capturing
latent language hierarchies in hyperbolic space.",2020-04-30
Context based Text-generation using LSTM networks,2020-04-30 18:39:25+00:00,http://arxiv.org/abs/2005.00048v1,Sivasurya Santhanam,"cs.CL, cs.LG",knowledge,"Long short-term memory(LSTM) units on sequence-based models are being used in
translation, question-answering systems, classification tasks due to their
capability of learning long-term dependencies. In Natural language generation,
LSTM networks are providing impressive results on text generation models by
learning language models with grammatically stable syntaxes. But the downside
is that the network does not learn about the context. The network only learns
the input-output function and generates text given a set of input words
irrespective of pragmatics. As the model is trained without any such context,
there is no semantic consistency among the generated sentences. The proposed
model is trained to generate text for a given set of input words along with a
context vector. A context vector is similar to a paragraph vector that grasps
the semantic meaning(context) of the sentence. Several methods of extracting
the context vectors are proposed in this work. While training a language model,
in addition to the input-output sequences, context vectors are also trained
along with the inputs. Due to this structure, the model learns the relation
among the input words, context vector and the target word. Given a set of
context terms, a well trained model will generate text around the provided
context. Based on the nature of computing context vectors, the model has been
tried out with two variations (word importance and word clustering). In the
word clustering method, the suitable embeddings among various domains are also
explored. The results are evaluated based on the semantic closeness of the
generated text to the given context.",2020-04-30
Template Guided Text Generation for Task-Oriented Dialogue,2020-04-30 17:51:08+00:00,http://arxiv.org/abs/2004.15006v2,"Mihir Kale, Abhinav Rastogi",cs.CL,knowledge,"Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri
enable users to interact with a large number of services and APIs on the web
using natural language. In this work, we investigate two methods for Natural
Language Generation (NLG) using a single domain-independent model across a
large number of APIs. First, we propose a schema-guided approach which
conditions the generation on a schema describing the API in natural language.
Our second method investigates the use of a small number of templates, growing
linearly in number of slots, to convey the semantics of the API. To generate
utterances for an arbitrary slot combination, a few simple templates are first
concatenated to give a semantically correct, but possibly incoherent and
ungrammatical utterance. A pre-trained language model is subsequently employed
to rewrite it into coherent, natural sounding text. Through automatic metrics
and human evaluation, we show that our method improves over strong baselines,
is robust to out-of-domain inputs and shows improved sample efficiency.",2020-04-30
ENT-DESC: Entity Description Generation by Exploring Knowledge Graph,2020-04-30 14:16:19+00:00,http://arxiv.org/abs/2004.14813v2,"Liying Cheng, Dekun Wu, Lidong Bing, Yan Zhang, Zhanming Jie, Wei Lu, Luo Si",cs.CL,knowledge,"Previous works on knowledge-to-text generation take as input a few RDF
triples or key-value pairs conveying the knowledge of some entities to generate
a natural language description. Existing datasets, such as WIKIBIO, WebNLG, and
E2E, basically have a good alignment between an input triple/pair set and its
output text. However, in practice, the input knowledge could be more than
enough, since the output description may only cover the most significant
knowledge. In this paper, we introduce a large-scale and challenging dataset to
facilitate the study of such a practical scenario in KG-to-text. Our dataset
involves retrieving abundant knowledge of various types of main entities from a
large knowledge graph (KG), which makes the current graph-to-sequence models
severely suffer from the problems of information loss and parameter explosion
while generating the descriptions. We address these challenges by proposing a
multi-graph structure that is able to represent the original graph information
more comprehensively. Furthermore, we also incorporate aggregation methods that
learn to extract the rich graph information. Extensive experiments demonstrate
the effectiveness of our model architecture.",2020-04-30
"Towards Unsupervised Language Understanding and Generation by Joint Dual
  Learning",2020-04-30 12:02:33+00:00,http://arxiv.org/abs/2004.14710v1,"Shang-Yu Su, Chao-Wei Huang, Yun-Nung Chen",cs.CL,knowledge,"In modular dialogue systems, natural language understanding (NLU) and natural
language generation (NLG) are two critical components, where NLU extracts the
semantics from the given texts and NLG is to construct corresponding natural
language sentences based on the input semantic representations. However, the
dual property between understanding and generation has been rarely explored.
The prior work is the first attempt that utilized the duality between NLU and
NLG to improve the performance via a dual supervised learning framework.
However, the prior work still learned both components in a supervised manner,
instead, this paper introduces a general learning framework to effectively
exploit such duality, providing flexibility of incorporating both supervised
and unsupervised learning algorithms to train language understanding and
generation models in a joint fashion. The benchmark experiments demonstrate
that the proposed approach is capable of boosting the performance of both NLU
and NLG.",2020-04-30
Logic2Text: High-Fidelity Natural Language Generation from Logical Forms,2020-04-30 04:06:06+00:00,http://arxiv.org/abs/2004.14579v2,"Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, William Yang Wang",cs.CL,knowledge,"Previous works on Natural Language Generation (NLG) from structured data have
primarily focused on surface-level descriptions of record sequences. However,
for complex structured data, e.g., multi-row tables, it is often desirable for
an NLG system to describe interesting facts from logical inferences across
records. If only provided with the table, it is hard for existing models to
produce controllable and high-fidelity logical generations. In this work, we
formulate logical level NLG as generation from logical forms in order to obtain
controllable, high-fidelity, and faithful generations. We present a new
large-scale dataset, \textsc{Logic2Text}, with 10,753 descriptions involving
common logic types paired with the underlying logical forms. The logical forms
show diversified graph structure of free schema, which poses great challenges
on the model's ability to understand the semantics. We experiment on (1)
Fully-supervised training with the full datasets, and (2) Few-shot setting,
provided with hundreds of paired examples; We compare several popular
generation models and analyze their performances. We hope our dataset can
encourage research towards building an advanced NLG system capable of natural,
faithful, and human-like generation. The dataset and code are available at
https://github.com/czyssrs/Logic2Text.",2020-04-30
"Learning to Encode Evolutionary Knowledge for Automatic Commenting Long
  Novels",2020-04-21 13:09:50+00:00,http://arxiv.org/abs/2004.09974v1,"Canxiang Yan, Jianhao Yan, Yangyin Xu, Cheng Niu, Jie Zhou","cs.CL, cs.LG",knowledge,"Static knowledge graph has been incorporated extensively into
sequence-to-sequence framework for text generation. While effectively
representing structured context, static knowledge graph failed to represent
knowledge evolution, which is required in modeling dynamic events. In this
paper, an automatic commenting task is proposed for long novels, which involves
understanding context of more than tens of thousands of words. To model the
dynamic storyline, especially the transitions of the characters and their
relations, Evolutionary Knowledge Graph(EKG) is proposed and learned within a
multi-task framework. Given a specific passage to comment, sequential modeling
is used to incorporate historical and future embedding for context
representation. Further, a graph-to-sequence model is designed to utilize the
EKG for comment generation. Extensive experimental results show that our
EKG-based method is superior to several strong baselines on both automatic and
human evaluations.",2020-04-21
