title,pubdate,id,authors,categories,search,abstract,displaydate
"Permutation invariant graph-to-sequence model for template-free
  retrosynthesis and reaction prediction",2021-10-19 01:23:15+00:00,http://arxiv.org/abs/2110.09681v1,"Zhengkai Tu, Connor W. Coley",cs.LG,knowledge,"Synthesis planning and reaction outcome prediction are two fundamental
problems in computer-aided organic chemistry for which a variety of data-driven
approaches have emerged. Natural language approaches that model each problem as
a SMILES-to-SMILES translation lead to a simple end-to-end formulation, reduce
the need for data preprocessing, and enable the use of well-optimized machine
translation model architectures. However, SMILES representations are not an
efficient representation for capturing information about molecular structures,
as evidenced by the success of SMILES augmentation to boost empirical
performance. Here, we describe a novel Graph2SMILES model that combines the
power of Transformer models for text generation with the permutation invariance
of molecular graph encoders that mitigates the need for input data
augmentation. As an end-to-end architecture, Graph2SMILES can be used as a
drop-in replacement for the Transformer in any task involving
molecule(s)-to-molecule(s) transformations. In our encoder, an
attention-augmented directed message passing neural network (D-MPNN) captures
local chemical environments, and the global attention encoder allows for
long-range and intermolecular interactions, enhanced by graph-aware positional
embedding. Graph2SMILES improves the top-1 accuracy of the Transformer
baselines by $1.7\%$ and $1.9\%$ for reaction outcome prediction on USPTO_480k
and USPTO_STEREO datasets respectively, and by $9.8\%$ for one-step
retrosynthesis on the USPTO_50k dataset.",2021-10-19
"Improving Compositional Generalization with Self-Training for
  Data-to-Text Generation",2021-10-16 04:26:56+00:00,http://arxiv.org/abs/2110.08467v1,"Sanket Vaibhav Mehta, Jinfeng Rao, Yi Tay, Mihir Kale, Ankur Parikh, Hongtao Zhong, Emma Strubell","cs.CL, cs.AI",knowledge,"Data-to-text generation focuses on generating fluent natural language
responses from structured semantic representations. Such representations are
compositional, allowing for the combination of atomic meaning schemata in
various ways to express the rich semantics in natural language. Recently,
pretrained language models (LMs) have achieved impressive results on
data-to-text tasks, though it remains unclear the extent to which these LMs
generalize to new semantic representations. In this work, we systematically
study the compositional generalization of current state-of-the-art generation
models in data-to-text tasks. By simulating structural shifts in the
compositional Weather dataset, we show that T5 models fail to generalize to
unseen structures. Next, we show that template-based input representations
greatly improve the model performance and model scale does not trivially solve
the lack of generalization. To further improve the model's performance, we
propose an approach based on self-training using finetuned BLEURT for
pseudo-response selection. Extensive experiments on the few-shot Weather and
multi-domain SGD datasets demonstrate strong gains of our method.",2021-10-16
"Open Domain Question Answering over Virtual Documents: A Unified
  Approach for Data and Text",2021-10-16 00:11:21+00:00,http://arxiv.org/abs/2110.08417v1,"Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg, Jianfeng Gao","cs.CL, cs.AI",knowledge,"Due to its potential for a universal interface over both data and text,
data-to-text generation is becoming increasingly popular recently. However, few
previous work has focused on its application to downstream tasks, e.g. using
the converted data for grounding or reasoning. In this work, we aim to bridge
this gap and use the data-to-text method as a means for encoding structured
knowledge for knowledge-intensive applications, i.e. open-domain question
answering (QA). Specifically, we propose a verbalizer-retriever-reader
framework for open-domain QA over data and text where verbalized tables from
Wikipedia and triples from Wikidata are used as augmented knowledge sources. We
show that our Unified Data and Text QA, UDT-QA, can effectively benefit from
the expanded knowledge index, leading to large gains over text-only baselines.
Notably, our approach sets the single-model state-of-the-art on Natural
Questions. Furthermore, our analyses indicate that verbalized knowledge is
preferred for answer reasoning for both adapted and hot-swap settings.",2021-10-16
"Cross-Domain Data Integration for Named Entity Disambiguation in
  Biomedical Text",2021-10-15 17:38:16+00:00,http://arxiv.org/abs/2110.08228v1,"Maya Varma, Laurel Orr, Sen Wu, Megan Leszczynski, Xiao Ling, Christopher Ré","cs.CL, cs.AI",knowledge,"Named entity disambiguation (NED), which involves mapping textual mentions to
structured entities, is particularly challenging in the medical domain due to
the presence of rare entities. Existing approaches are limited by the presence
of coarse-grained structural resources in biomedical knowledge bases as well as
the use of training datasets that provide low coverage over uncommon resources.
In this work, we address these issues by proposing a cross-domain data
integration method that transfers structural knowledge from a general text
knowledge base to the medical domain. We utilize our integration scheme to
augment structural resources and generate a large biomedical NED dataset for
pretraining. Our pretrained model with injected structural knowledge achieves
state-of-the-art performance on two benchmark medical NED datasets: MedMentions
and BC5CDR. Furthermore, we improve disambiguation of rare entities by up to 57
accuracy points.",2021-10-15
"Hindsight: Posterior-guided training of retrievers for improved
  open-ended generation",2021-10-14 22:24:57+00:00,http://arxiv.org/abs/2110.07752v2,"Ashwin Paranjape, Omar Khattab, Christopher Potts, Matei Zaharia, Christopher D. Manning","cs.CL, cs.IR",knowledge,"Many text generation systems benefit from using a retriever to retrieve
passages from a textual knowledge corpus (e.g., Wikipedia) which are then
provided as additional context to the generator. For open-ended generation
tasks (like generating informative utterances in conversations) many varied
passages may be equally relevant and we find that existing methods that jointly
train the retriever and generator underperform: the retriever may not find
relevant passages even amongst the top-10 and hence the generator may not learn
a preference to ground its generated output in them. We propose using an
additional guide retriever that is allowed to use the target output and ""in
hindsight"" retrieve relevant passages during training. We model the guide
retriever after the posterior distribution Q of passages given the input and
the target output and train it jointly with the standard retriever and the
generator by maximizing the evidence lower bound (ELBo) in expectation over Q.
For informative conversations from the Wizard of Wikipedia dataset, with
posterior-guided training, the retriever finds passages with higher relevance
in the top-10 (23% relative improvement), the generator's responses are more
grounded in the retrieved passage (19% relative improvement) and the end-to-end
system produces better overall output (6.4% relative improvement).",2021-10-14
MReD: A Meta-Review Dataset for Controllable Text Generation,2021-10-14 15:48:03+00:00,http://arxiv.org/abs/2110.07474v1,"Chenhui Shen, Liying Cheng, Ran Zhou, Lidong Bing, Yang You, Luo Si",cs.CL,knowledge,"When directly using existing text generation datasets for controllable
generation, we are facing the problem of not having the domain knowledge and
thus the aspects that could be controlled are limited.A typical example is when
using CNN/Daily Mail dataset for controllable text summarization, there is no
guided information on the emphasis of summary sentences. A more useful text
generator should leverage both the input text and control variables to guide
the generation, which can only be built with deep understanding of the domain
knowledge. Motivated by this vi-sion, our paper introduces a new text
generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews
and all its 45k meta-review sentences are manually annotated as one of the
carefully defined 9 categories, including abstract, strength, decision, etc. We
present experimental results on start-of-the-art summarization models, and
propose methods for controlled generation on both extractive and abstractive
models using our annotated data. By exploring various settings and analaysing
the model behavior with respect to the control inputs, we demonstrate the
challenges and values of our dataset. MReD allows us to have a better
understanding of the meta-review corpora and enlarge the research room for
controllable text generation.",2021-10-14
Solving Aspect Category Sentiment Analysis as a Text Generation Task,2021-10-14 12:25:21+00:00,http://arxiv.org/abs/2110.07310v1,"Jian Liu, Zhiyang Teng, Leyang Cui, Hanmeng Liu, Yue Zhang",cs.CL,knowledge,"Aspect category sentiment analysis has attracted increasing research
attention. The dominant methods make use of pre-trained language models by
learning effective aspect category-specific representations, and adding
specific output layers to its pre-trained representation. We consider a more
direct way of making use of pre-trained language models, by casting the ACSA
tasks into natural language generation tasks, using natural language sentences
to represent the output. Our method allows more direct use of pre-trained
knowledge in seq2seq language models by directly following the task setting
during pre-training. Experiments on several benchmarks show that our method
gives the best reported results, having large advantages in few-shot and
zero-shot settings.",2021-10-14
Learning Compact Metrics for MT,2021-10-12 20:39:35+00:00,http://arxiv.org/abs/2110.06341v1,"Amy Pu, Hyung Won Chung, Ankur P. Parikh, Sebastian Gehrmann, Thibault Sellam",cs.CL,knowledge,"Recent developments in machine translation and multilingual text generation
have led researchers to adopt trained metrics such as COMET or BLEURT, which
treat evaluation as a regression problem and use representations from
multilingual pre-trained models such as XLM-RoBERTa or mBERT. Yet studies on
related tasks suggest that these models are most efficient when they are large,
which is costly and impractical for evaluation. We investigate the trade-off
between multilinguality and model capacity with RemBERT, a state-of-the-art
multilingual language model, using data from the WMT Metrics Shared Task. We
present a series of experiments which show that model size is indeed a
bottleneck for cross-lingual transfer, then demonstrate how distillation can
help addressing this bottleneck, by leveraging synthetic data generation and
transferring knowledge from one teacher to multiple students trained on related
languages. Our method yields up to 10.5% improvement over vanilla fine-tuning
and reaches 92.6% of RemBERT's performance using only a third of its
parameters.",2021-10-12
"Global Explainability of BERT-Based Evaluation Metrics by Disentangling
  along Linguistic Factors",2021-10-08 22:40:33+00:00,http://arxiv.org/abs/2110.04399v1,"Marvin Kaster, Wei Zhao, Steffen Eger",cs.CL,knowledge,"Evaluation metrics are a key ingredient for progress of text generation
systems. In recent years, several BERT-based evaluation metrics have been
proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much
better with human assessment of text generation quality than BLEU or ROUGE,
invented two decades ago. However, little is known what these metrics, which
are based on black-box language model representations, actually capture (it is
typically assumed they model semantic similarity). In this work, we \wei{use a
simple regression based global explainability technique to} disentangle metric
scores along linguistic factors, including semantics, syntax, morphology, and
lexical overlap. We show that the different metrics capture all aspects to some
degree, but that they are all substantially sensitive to lexical overlap, just
like BLEU and ROUGE. This exposes limitations of these novelly proposed
metrics, which we also highlight in an adversarial test scenario.",2021-10-08
Simulated annealing for optimization of graphs and sequences,2021-10-01 01:12:19+00:00,http://arxiv.org/abs/2110.01384v1,"Xianggen Liu, Pengyong Li, Fandong Meng, Hao Zhou, Huasong Zhong, Jie Zhou, Lili Mou, Sen Song","cs.LG, cs.AI",knowledge,"Optimization of discrete structures aims at generating a new structure with
the better property given an existing one, which is a fundamental problem in
machine learning. Different from the continuous optimization, the realistic
applications of discrete optimization (e.g., text generation) are very
challenging due to the complex and long-range constraints, including both
syntax and semantics, in discrete structures. In this work, we present SAGS, a
novel Simulated Annealing framework for Graph and Sequence optimization. The
key idea is to integrate powerful neural networks into metaheuristics (e.g.,
simulated annealing, SA) to restrict the search space in discrete optimization.
We start by defining a sophisticated objective function, involving the property
of interest and pre-defined constraints (e.g., grammar validity). SAGS searches
from the discrete space towards this objective by performing a sequence of
local edits, where deep generative neural networks propose the editing content
and thus can control the quality of editing. We evaluate SAGS on paraphrase
generation and molecule generation for sequence optimization and graph
optimization, respectively. Extensive results show that our approach achieves
state-of-the-art performance compared with existing paraphrase generation
methods in terms of both automatic and human evaluations. Further, SAGS also
significantly outperforms all the previous methods in molecule generation.",2021-10-01
Self-conditioning pre-trained language models,2021-09-30 11:18:19+00:00,http://arxiv.org/abs/2110.02802v1,"Xavier Suau, Luca Zappella, Nicholas Apostoloff",cs.CL,knowledge,"We study the presence of expert units in pre-trained Transformer-based
Language Models (TLMs), and how they can be used to condition text generation
to contain specific concepts. We define expert units to be neurons that are
able to detect a concept in the input with a given average precision. A concept
is represented with a set of sentences that either do or do not contain the
concept. Leveraging the OneSec dataset, we compile a dataset of 1344 concepts
that allows diverse expert units in TLMs to be discovered. Our experiments
demonstrate that off-the-shelf pre-trained TLMs can be conditioned on their own
knowledge (self-conditioning) to generate text that contains a given concept.
To this end, we intervene on the top expert units by fixing their output during
inference, and we show experimentally that this is an effective method to
condition TLMs. Our method does not require fine-tuning the model or using
additional parameters, which allows conditioning large TLM with minimal compute
resources. Furthermore, by intervening on a small number of experts in GPT2, we
can achieve parity with respect to two concepts at generation time. The
specific case of gender bias is explored, and we show that, for given contexts,
gender parity is achieved while maintaining the model's perplexity.",2021-09-30
"TURINGBENCH: A Benchmark Environment for Turing Test in the Age of
  Neural Text Generation",2021-09-27 18:35:33+00:00,http://arxiv.org/abs/2109.13296v1,"Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, Dongwon Lee",cs.CL,knowledge,"Recent progress in generative language models has enabled machines to
generate astonishingly realistic texts. While there are many legitimate
applications of such models, there is also a rising need to distinguish
machine-generated texts from human-written ones (e.g., fake news detection).
However, to our best knowledge, there is currently no benchmark environment
with datasets and tasks to systematically study the so-called ""Turing Test""
problem for neural text generation methods. In this work, we present the
TuringBench benchmark environment, which is comprised of (1) a dataset with
200K human- or machine-generated samples across 20 labels {Human, GPT-1,
GPT-2_small, GPT-2_medium, GPT-2_large, GPT-2_xl, GPT-2_PyTorch, GPT-3,
GROVER_base, GROVER_large, GROVER_mega, CTRL, XLM, XLNET_base, XLNET_large,
FAIR_wmt19, FAIR_wmt20, TRANSFORMER_XL, PPLM_distil, PPLM_gpt2}, (2) two
benchmark tasks -- i.e., Turing Test (TT) and Authorship Attribution (AA), and
(3) a website with leaderboards. Our preliminary experimental results using
TuringBench show that FAIR_wmt20 and GPT-3 are the current winners, among all
language models tested, in generating the most human-like indistinguishable
texts with the lowest F1 score by five state-of-the-art TT detection models.
The TuringBench is available at: https://turingbench.ist.psu.edu/",2021-09-27
Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation,2021-09-25 00:06:23+00:00,http://arxiv.org/abs/2109.12242v1,"An Yan, Zexue He, Xing Lu, Jiang Du, Eric Chang, Amilcare Gentili, Julian McAuley, Chun-Nan Hsu",cs.CL,knowledge,"Radiology report generation aims at generating descriptive text from
radiology images automatically, which may present an opportunity to improve
radiology reporting and interpretation. A typical setting consists of training
encoder-decoder models on image-report pairs with a cross entropy loss, which
struggles to generate informative sentences for clinical diagnoses since normal
findings dominate the datasets. To tackle this challenge and encourage more
clinically-accurate text outputs, we propose a novel weakly supervised
contrastive loss for medical report generation. Experimental results
demonstrate that our method benefits from contrasting target reports with
incorrect but semantically-close ones. It outperforms previous work on both
clinical correctness and text generation metrics for two public benchmarks.",2021-09-25
Style Control for Schema-Guided Natural Language Generation,2021-09-24 21:47:58+00:00,http://arxiv.org/abs/2109.12211v1,"Alicia Y. Tsai, Shereen Oraby, Vittorio Perera, Jiun-Yu Kao, Yuheng Du, Anjali Narayan-Chen, Tagyoung Chung, Dilek Hakkani-Tur",cs.CL,knowledge,"Natural Language Generation (NLG) for task-oriented dialogue systems focuses
on communicating specific content accurately, fluently, and coherently. While
these attributes are crucial for a successful dialogue, it is also desirable to
simultaneously accomplish specific stylistic goals, such as response length,
point-of-view, descriptiveness, sentiment, formality, and empathy. In this
work, we focus on stylistic control and evaluation for schema-guided NLG, with
joint goals of achieving both semantic and stylistic control. We experiment in
detail with various controlled generation methods for large pretrained language
models: specifically, conditional training, guided fine-tuning, and guided
decoding. We discuss their advantages and limitations, and evaluate them with a
broad range of automatic and human evaluation metrics. Our results show that
while high style accuracy and semantic correctness are easier to achieve for
more lexically-defined styles with conditional training, stylistic control is
also achievable for more semantically complex styles using discriminator-based
guided decoding methods. The results also suggest that methods that are more
scalable (with less hyper-parameters tuning) and that disentangle content
generation and stylistic variations are more effective at achieving semantic
correctness and style accuracy.",2021-09-24
"Incorporating Linguistic Knowledge for Abstractive Multi-document
  Summarization",2021-09-23 08:13:35+00:00,http://arxiv.org/abs/2109.11199v1,"Congbo Ma, Wei Emma Zhang, Hu Wang, Shubham Gupta, Mingyu Guo",cs.CL,knowledge,"Within natural language processing tasks, linguistic knowledge can always
serve an important role in assisting the model to learn excel representations
and better guide the natural language generation. In this work, we develop a
neural network based abstractive multi-document summarization (MDS) model which
leverages dependency parsing to capture cross-positional dependencies and
grammatical structures. More concretely, we process the dependency information
into the linguistic-guided attention mechanism and further fuse it with the
multi-head attention for better feature representation. With the help of
linguistic signals, sentence-level relations can be correctly captured, thus
improving MDS performance. Our model has two versions based on Flat-Transformer
and Hierarchical Transformer respectively. Empirical studies on both versions
demonstrate that this simple but effective method outperforms existing works on
the benchmark dataset. Extensive analyses examine different settings and
configurations of the proposed model which provide a good reference to the
community.",2021-09-23
Enriching and Controlling Global Semantics for Text Summarization,2021-09-22 09:31:50+00:00,http://arxiv.org/abs/2109.10616v1,"Thong Nguyen, Anh Tuan Luu, Truc Lu, Tho Quan",cs.CL,knowledge,"Recently, Transformer-based models have been proven effective in the
abstractive summarization task by creating fluent and informative summaries.
Nevertheless, these models still suffer from the short-range dependency
problem, causing them to produce summaries that miss the key points of
document. In this paper, we attempt to address this issue by introducing a
neural topic model empowered with normalizing flow to capture the global
semantics of the document, which are then integrated into the summarization
model. In addition, to avoid the overwhelming effect of global semantics on
contextualized representation, we introduce a mechanism to control the amount
of global semantics supplied to the text generation module. Our method
outperforms state-of-the-art summarization models on five common text
summarization datasets, namely CNN/DailyMail, XSum, Reddit TIFU, arXiv, and
PubMed.",2021-09-22
A Plug-and-Play Method for Controlled Text Generation,2021-09-20 17:27:03+00:00,http://arxiv.org/abs/2109.09707v1,"Damian Pascual, Beni Egressy, Clara Meister, Ryan Cotterell, Roger Wattenhofer","cs.CL, cs.AI",knowledge,"Large pre-trained language models have repeatedly shown their ability to
produce fluent text. Yet even when starting from a prompt, generation can
continue in many plausible directions. Current decoding methods with the goal
of controlling generation, e.g., to ensure specific words are included, either
require additional models or fine-tuning, or work poorly when the task at hand
is semantically unconstrained, e.g., story generation. In this work, we present
a plug-and-play decoding method for controlled language generation that is so
simple and intuitive, it can be described in a single sentence: given a topic
or keyword, we add a shift to the probability distribution over our vocabulary
towards semantically similar words. We show how annealing this distribution can
be used to impose hard constraints on language generation, something no other
plug-and-play method is currently able to do with SOTA language generators.
Despite the simplicity of this approach, we see it works incredibly well in
practice: decoding from GPT-2 leads to diverse and fluent sentences while
guaranteeing the appearance of given guide words. We perform two user studies,
revealing that (1) our method outperforms competing methods in human
evaluations; and (2) forcing the guide words to appear in the generated text
has no impact on the fluency of the generated text.",2021-09-20
Relating Neural Text Degeneration to Exposure Bias,2021-09-17 18:11:03+00:00,http://arxiv.org/abs/2109.08705v1,"Ting-Rui Chiang, Yun-Nung Chen","cs.CL, cs.LG",knowledge,"This work focuses on relating two mysteries in neural-based text generation:
exposure bias, and text degeneration. Despite the long time since exposure bias
was mentioned and the numerous studies for its remedy, to our knowledge, its
impact on text generation has not yet been verified. Text degeneration is a
problem that the widely-used pre-trained language model GPT-2 was recently
found to suffer from (Holtzman et al., 2020). Motivated by the unknown
causation of the text degeneration, in this paper we attempt to relate these
two mysteries. Specifically, we first qualitatively quantitatively identify
mistakes made before text degeneration occurs. Then we investigate the
significance of the mistakes by inspecting the hidden states in GPT-2. Our
results show that text degeneration is likely to be partly caused by exposure
bias. We also study the self-reinforcing mechanism of text degeneration,
explaining why the mistakes amplify. In sum, our study provides a more concrete
foundation for further investigation on exposure bias and text degeneration
problems.",2021-09-17
Let the CAT out of the bag: Contrastive Attributed explanations for Text,2021-09-16 13:44:55+00:00,http://arxiv.org/abs/2109.07983v1,"Saneem Chemmengath, Amar Prakash Azad, Ronny Luss, Amit Dhurandhar","cs.CL, cs.AI",knowledge,"Contrastive explanations for understanding the behavior of black box models
has gained a lot of attention recently as they provide potential for recourse.
In this paper, we propose a method Contrastive Attributed explanations for Text
(CAT) which provides contrastive explanations for natural language text data
with a novel twist as we build and exploit attribute classifiers leading to
more semantically meaningful explanations. To ensure that our contrastive
generated text has the fewest possible edits with respect to the original text,
while also being fluent and close to a human generated contrastive, we resort
to a minimal perturbation approach regularized using a BERT language model and
attribute classifiers trained on available attributes. We show through
qualitative examples and a user study that our method not only conveys more
insight because of these attributes, but also leads to better quality
(contrastive) text. Moreover, quantitatively we show that our method is more
efficient than other state-of-the-art methods with it also scoring higher on
benchmark metrics such as flip rate, (normalized) Levenstein distance, fluency
and content preservation.",2021-09-16
"Attention Is Indeed All You Need: Semantically Attention-Guided Decoding
  for Data-to-Text NLG",2021-09-15 01:42:51+00:00,http://arxiv.org/abs/2109.07043v1,"Juraj Juraska, Marilyn Walker","cs.CL, cs.LG",knowledge,"Ever since neural models were adopted in data-to-text language generation,
they have invariably been reliant on extrinsic components to improve their
semantic accuracy, because the models normally do not exhibit the ability to
generate text that reliably mentions all of the information provided in the
input. In this paper, we propose a novel decoding method that extracts
interpretable information from encoder-decoder models' cross-attention, and
uses it to infer which attributes are mentioned in the generated text, which is
subsequently used to rescore beam hypotheses. Using this decoding method with
T5 and BART, we show on three datasets its ability to dramatically reduce
semantic errors in the generated outputs, while maintaining their
state-of-the-art quality.",2021-09-15
"PETGEN: Personalized Text Generation Attack on Deep Sequence
  Embedding-based Classification Models",2021-09-14 15:48:07+00:00,http://arxiv.org/abs/2109.06777v1,"Bing He, Mustaque Ahamad, Srijan Kumar","cs.LG, cs.SI",knowledge,"\textit{What should a malicious user write next to fool a detection model?}
Identifying malicious users is critical to ensure the safety and integrity of
internet platforms. Several deep learning based detection models have been
created. However, malicious users can evade deep detection models by
manipulating their behavior, rendering these models of little use. The
vulnerability of such deep detection models against adversarial attacks is
unknown. Here we create a novel adversarial attack model against deep user
sequence embedding-based classification models, which use the sequence of user
posts to generate user embeddings and detect malicious users. In the attack,
the adversary generates a new post to fool the classifier. We propose a novel
end-to-end Personalized Text Generation Attack model, called \texttt{PETGEN},
that simultaneously reduces the efficacy of the detection model and generates
posts that have several key desirable properties. Specifically, \texttt{PETGEN}
generates posts that are personalized to the user's writing style, have
knowledge about a given target context, are aware of the user's historical
posts on the target context, and encapsulate the user's recent topical
interests. We conduct extensive experiments on two real-world datasets (Yelp
and Wikipedia, both with ground-truth of malicious users) to show that
\texttt{PETGEN} significantly reduces the performance of popular deep user
sequence embedding-based classification models. \texttt{PETGEN} outperforms
five attack baselines in terms of text quality and attack efficacy in both
white-box and black-box classifier settings. Overall, this work paves the path
towards the next generation of adversary-aware sequence classification models.",2021-09-14
"Controllable Dialogue Generation with Disentangled Multi-grained Style
  Specification and Attribute Consistency Reward",2021-09-14 14:29:38+00:00,http://arxiv.org/abs/2109.06717v1,"Zhe Hu, Zhiwei Cao, Hou Pong Chan, Jiachen Liu, Xinyan Xiao, Jinsong Su, Hua Wu","cs.CL, cs.AI",knowledge,"Controllable text generation is an appealing but challenging task, which
allows users to specify particular attributes of the generated outputs. In this
paper, we propose a controllable dialogue generation model to steer response
generation under multi-attribute constraints. Specifically, we define and
categorize the commonly used control attributes into global and local ones,
which possess different granularities of effects on response generation. Then,
we significantly extend the conventional seq2seq framework by introducing a
novel two-stage decoder, which first uses a multi-grained style specification
layer to impose the stylistic constraints and determine word-level control
states of responses based on the attributes, and then employs a response
generation layer to generate final responses maintaining both semantic
relevancy to the contexts and fidelity to the attributes. Furthermore, we train
our model with an attribute consistency reward to promote response control with
explicit supervision signals. Extensive experiments and in-depth analyses on
two datasets indicate that our model can significantly outperform competitive
baselines in terms of response quality, content diversity and controllability.",2021-09-14
"KFCNet: Knowledge Filtering and Contrastive Learning Network for
  Generative Commonsense Reasoning",2021-09-14 14:10:37+00:00,http://arxiv.org/abs/2109.06704v1,"Haonan Li, Yeyun Gong, Jian Jiao, Ruofei Zhang, Timothy Baldwin, Nan Duan",cs.CL,knowledge,"Pre-trained language models have led to substantial gains over a broad range
of natural language processing (NLP) tasks, but have been shown to have
limitations for natural language generation tasks with high-quality
requirements on the output, such as commonsense generation and ad keyword
generation. In this work, we present a novel Knowledge Filtering and
Contrastive learning Network (KFCNet) which references external knowledge and
achieves better generation performance. Specifically, we propose a BERT-based
filter model to remove low-quality candidates, and apply contrastive learning
separately to each of the encoder and decoder, within a general
encoder--decoder architecture. The encoder contrastive module helps to capture
global target semantics during encoding, and the decoder contrastive module
enhances the utility of retrieved prototypes while learning general features.
Extensive experiments on the CommonGen benchmark show that our model
outperforms the previous state of the art by a large margin: +6.6 points (42.5
vs. 35.9) for BLEU-4, +3.7 points (33.3 vs. 29.6) for SPICE, and +1.3 points
(18.3 vs. 17.0) for CIDEr. We further verify the effectiveness of the proposed
contrastive module on ad keyword generation, and show that our model has
potential commercial value.",2021-09-14
"Improving Gradient-based Adversarial Training for Text Classification by
  Contrastive Learning and Auto-Encoder",2021-09-14 09:08:58+00:00,http://arxiv.org/abs/2109.06536v1,"Yao Qiu, Jinchao Zhang, Jie Zhou",cs.CL,knowledge,"Recent work has proposed several efficient approaches for generating
gradient-based adversarial perturbations on embeddings and proved that the
model's performance and robustness can be improved when they are trained with
these contaminated embeddings. While they paid little attention to how to help
the model to learn these adversarial samples more efficiently. In this work, we
focus on enhancing the model's ability to defend gradient-based adversarial
attack during the model's training process and propose two novel adversarial
training approaches: (1) CARL narrows the original sample and its adversarial
sample in the representation space while enlarging their distance from
different labeled samples. (2) RAR forces the model to reconstruct the original
sample from its adversarial representation. Experiments show that the proposed
two approaches outperform strong baselines on various text classification
datasets. Analysis experiments find that when using our approaches, the
semantic representation of the input sentence won't be significantly affected
by adversarial perturbations, and the model's performance drops less under
adversarial attack. That is to say, our approaches can effectively improve the
robustness of the model. Besides, RAR can also be used to generate text-form
adversarial samples.",2021-09-14
"Style Pooling: Automatic Text Style Obfuscation for Improved
  Classification Fairness",2021-09-10 02:17:21+00:00,http://arxiv.org/abs/2109.04624v1,"Fatemehsadat Mireshghallah, Taylor Berg-Kirkpatrick",cs.LG,knowledge,"Text style can reveal sensitive attributes of the author (e.g. race or age)
to the reader, which can, in turn, lead to privacy violations and bias in both
human and algorithmic decisions based on text. For example, the style of
writing in job applications might reveal protected attributes of the candidate
which could lead to bias in hiring decisions, regardless of whether hiring
decisions are made algorithmically or by humans. We propose a VAE-based
framework that obfuscates stylistic features of human-generated text through
style transfer by automatically re-writing the text itself. Our framework
operationalizes the notion of obfuscated style in a flexible way that enables
two distinct notions of obfuscated style: (1) a minimal notion that effectively
intersects the various styles seen in training, and (2) a maximal notion that
seeks to obfuscate by adding stylistic features of all sensitive attributes to
text, in effect, computing a union of styles. Our style-obfuscation framework
can be used for multiple purposes, however, we demonstrate its effectiveness in
improving the fairness of downstream classifiers. We also conduct a
comprehensive study on style pooling's effect on fluency, semantic consistency,
and attribute removal from text, in two and three domain style obfuscation.",2021-09-10
Graphine: A Dataset for Graph-aware Terminology Definition Generation,2021-09-09 03:29:23+00:00,http://arxiv.org/abs/2109.04018v1,"Zequn Liu, Shukai Wang, Yiyang Gu, Ruiyi Zhang, Ming Zhang, Sheng Wang",cs.CL,knowledge,"Precisely defining the terminology is the first step in scientific
communication. Developing neural text generation models for definition
generation can circumvent the labor-intensity curation, further accelerating
scientific discovery. Unfortunately, the lack of large-scale terminology
definition dataset hinders the process toward definition generation. In this
paper, we present a large-scale terminology definition dataset Graphine
covering 2,010,648 terminology definition pairs, spanning 227 biomedical
subdisciplines. Terminologies in each subdiscipline further form a directed
acyclic graph, opening up new avenues for developing graph-aware text
generation models. We then proposed a novel graph-aware definition generation
model Graphex that integrates transformer with graph neural network. Our model
outperforms existing text generation models by exploiting the graph structure
of terminologies. We further demonstrated how Graphine can be used to evaluate
pretrained language models, compare graph representation learning methods and
predict sentence granularity. We envision Graphine to be a unique resource for
definition generation and many other NLP tasks in biomedicine.",2021-09-09
NumGPT: Improving Numeracy Ability of Generative Pre-trained Models,2021-09-07 15:06:12+00:00,http://arxiv.org/abs/2109.03137v1,"Zhihua Jin, Xin Jiang, Xingbo Wang, Qun Liu, Yong Wang, Xiaozhe Ren, Huamin Qu","cs.CL, cs.LG",knowledge,"Existing generative pre-trained language models (e.g., GPT) focus on modeling
the language structure and semantics of general texts. However, those models do
not consider the numerical properties of numbers and cannot perform robustly on
numerical reasoning tasks (e.g., math word problems and measurement
estimation). In this paper, we propose NumGPT, a generative pre-trained model
that explicitly models the numerical properties of numbers in texts.
Specifically, it leverages a prototype-based numeral embedding to encode the
mantissa of the number and an individual embedding to encode the exponent of
the number. A numeral-aware loss function is designed to integrate numerals
into the pre-training objective of NumGPT. We conduct extensive experiments on
four different datasets to evaluate the numeracy ability of NumGPT. The
experiment results show that NumGPT outperforms baseline models (e.g., GPT and
GPT with DICE) on a range of numerical reasoning tasks such as measurement
estimation, number comparison, math word problems, and magnitude
classification. Ablation studies are also conducted to evaluate the impact of
pre-training and model hyperparameters on the performance.",2021-09-07
"Naturalness Evaluation of Natural Language Generation in Task-oriented
  Dialogues using BERT",2021-09-07 08:40:14+00:00,http://arxiv.org/abs/2109.02938v1,"Ye Liu, Wolfgang Maier, Wolfgang Minker, Stefan Ultes","cs.CL, cs.AI",knowledge,"This paper presents an automatic method to evaluate the naturalness of
natural language generation in dialogue systems. While this task was previously
rendered through expensive and time-consuming human labor, we present this
novel task of automatic naturalness evaluation of generated language. By
fine-tuning the BERT model, our proposed naturalness evaluation method shows
robust results and outperforms the baselines: support vector machines,
bi-directional LSTMs, and BLEURT. In addition, the training speed and
evaluation performance of naturalness model are improved by transfer learning
from quality and informativeness linguistic knowledge.",2021-09-07
"ConQX: Semantic Expansion of Spoken Queries for Intent Detection based
  on Conditioned Text Generation",2021-09-02 05:57:07+00:00,http://arxiv.org/abs/2109.00729v1,"Eyup Halit Yilmaz, Cagri Toraman","cs.CL, cs.AI",knowledge,"Intent detection of spoken queries is a challenging task due to their noisy
structure and short length. To provide additional information regarding the
query and enhance the performance of intent detection, we propose a method for
semantic expansion of spoken queries, called ConQX, which utilizes the text
generation ability of an auto-regressive language model, GPT-2. To avoid
off-topic text generation, we condition the input query to a structured context
with prompt mining. We then apply zero-shot, one-shot, and few-shot learning.
We lastly use the expanded queries to fine-tune BERT and RoBERTa for intent
detection. The experimental results show that the performance of intent
detection can be improved by our semantic expansion method.",2021-09-02
Event Extraction as Natural Language Generation,2021-08-29 00:27:31+00:00,http://arxiv.org/abs/2108.12724v1,"I-Hung Hsu, Kuan-Hao Huang, Elizabeth Boschee, Scott Miller, Prem Natarajan, Kai-Wei Chang, Nanyun Peng","cs.CL, cs.AI",knowledge,"Event extraction (EE), the task that identifies event triggers and their
arguments in text, is usually formulated as a classification or structured
prediction problem. Such models usually reduce labels to numeric identifiers,
making them unable to take advantage of label semantics (e.g. an event type
named Arrest is related to words like arrest, detain, or apprehend). This
prevents the generalization to new event types. In this work, we formulate EE
as a natural language generation task and propose GenEE, a model that not only
captures complex dependencies within an event but also generalizes well to
unseen or rare event types. Given a passage and an event type, GenEE is trained
to generate a natural sentence following a predefined template for that event
type. The generated output is then decoded into trigger and argument
predictions. The autoregressive generation process naturally models the
dependencies among the predictions -- each new word predicted depends on those
already generated in the output sentence. Using carefully designed input
prompts during generation, GenEE is able to capture label semantics, which
enables the generalization to new event types. Empirical results show that our
model achieves strong performance on event extraction tasks under all
zero-shot, few-shot, and high-resource scenarios. Especially, in the
high-resource setting, GenEE outperforms the state-of-the-art model on argument
extraction and gets competitive results with the current best on end-to-end EE
tasks.",2021-08-29
"ReGen: Reinforcement Learning for Text and Knowledge Base Generation
  using Pretrained Language Models",2021-08-27 19:37:12+00:00,http://arxiv.org/abs/2108.12472v1,"Pierre L. Dognin, Inkit Padhi, Igor Melnyk, Payel Das","cs.CL, cs.LG",knowledge,"Automatic construction of relevant Knowledge Bases (KBs) from text, and
generation of semantically meaningful text from KBs are both long-standing
goals in Machine Learning. In this paper, we present ReGen, a bidirectional
generation of text and graph leveraging Reinforcement Learning (RL) to improve
performance. Graph linearization enables us to re-frame both tasks as a
sequence to sequence generation problem regardless of the generative direction,
which in turn allows the use of Reinforcement Learning for sequence training
where the model itself is employed as its own critic leading to Self-Critical
Sequence Training (SCST). We present an extensive investigation demonstrating
that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020
and TekGen datasets. Our system provides state-of-the-art results on WebNLG+
2020 by significantly improving upon published results from the WebNLG 2020+
Challenge for both text-to-graph and graph-to-text generation tasks.",2021-08-27
Latent Tree Decomposition Parsers for AMR-to-Text Generation,2021-08-27 14:30:35+00:00,http://arxiv.org/abs/2108.12304v2,"Lisa Jin, Daniel Gildea",cs.CL,knowledge,"Graph encoders in AMR-to-text generation models often rely on neighborhood
convolutions or global vertex attention. While these approaches apply to
general graphs, AMRs may be amenable to encoders that target their tree-like
structure. By clustering edges into a hierarchy, a tree decomposition
summarizes graph structure. Our model encodes a derivation forest of tree
decompositions and extracts an expected tree. From tree node embeddings, it
builds graph edge features used in vertex attention of the graph encoder.
Encoding TD forests instead of shortest-pairwise paths in a self-attentive
baseline raises BLEU by 0.7 and chrF++ by 0.3. The forest encoder also
surpasses a convolutional baseline for molecular property prediction by 1.92%
ROC-AUC.",2021-08-27
Tree Decomposition Attention for AMR-to-Text Generation,2021-08-27 14:24:25+00:00,http://arxiv.org/abs/2108.12300v2,"Lisa Jin, Daniel Gildea",cs.CL,knowledge,"Text generation from AMR requires mapping a semantic graph to a string that
it annotates. Transformer-based graph encoders, however, poorly capture vertex
dependencies that may benefit sequence prediction. To impose order on an
encoder, we locally constrain vertex self-attention using a graph's tree
decomposition. Instead of forming a full query-key bipartite graph, we restrict
attention to vertices in parent, subtree, and same-depth bags of a vertex. This
hierarchical context lends both sparsity and structure to vertex state updates.
We apply dynamic programming to derive a forest of tree decompositions,
choosing the most structurally similar tree to the AMR. Our system outperforms
a self-attentive baseline by 1.6 BLEU and 1.8 chrF++.",2021-08-27
Lingxi: A Diversity-aware Chinese Modern Poetry Generation System,2021-08-27 03:33:28+00:00,http://arxiv.org/abs/2108.12108v1,"Xinran Zhang, Maosong Sun, Jiafeng Liu, Xiaobing Li",cs.CL,knowledge,"Poetry generation has been a difficult task in natural language processing.
Unlike plain neural text generation tasks, poetry has a high requirement for
novelty, since an easily-understood sentence with too many high frequency words
might not be considered as poetic, while adequately ambiguous sentences with
low frequency words can possibly be novel and creative. Inspired by this, we
present Lingxi, a diversity-aware Chinese modern poetry generation system. We
propose nucleus sampling with randomized head (NS-RH) algorithm, which
randomizes the high frequency part (""head"") of the predicted distribution, in
order to emphasize on the ""comparatively low frequency"" words. The proposed
algorithm can significantly increase the novelty of generated poetry compared
with traditional sampling methods. The permutation of distribution is
controllable by tuning the filtering parameter that determines the ""head"" to
permutate, achieving diversity-aware sampling. We find that even when a large
portion of filtered vocabulary is randomized, it can actually generate fluent
poetry but with notably higher novelty. We also propose a
semantic-similarity-based rejection sampling algorithm, which creates longer
and more informative context on the basis of the short input poetry title while
maintaining high semantic similarity to the title, alleviating the off-topic
issue.",2021-08-27
Semantic-based Self-Critical Training For Question Generation,2021-08-26 20:33:35+00:00,http://arxiv.org/abs/2108.12026v1,"Loïc, Kwate Dassi","cs.CL, cs.AI",knowledge,"We present in this work a fully Transformer-based reinforcement learning
generator-evaluator architecture for neural question generation. Question
generation is a task that consists in generating questions given a context and
answer. To improve the quality of the generated question, we came up with a
semantic-based self-critical training layout in generator-evaluator
architecture, which goes beyond typical maximum likelihood training. Evaluation
metrics for language modeling only based on n-gram overlapping do not consider
semantic relations between reference and candidate strings. To improve the
evaluation step, we assess our model for both n-gram overlap using BLEU and
semantically using BERTScore and NUBIA, a novel state-of-the-art evaluation
metric for text generation. Question generation could be used in many
downstream applications, including in extending question answering datasets,
conversational systems, and educational assessment systems.",2021-08-26
"A Neural Conversation Generation Model via Equivalent Shared Memory
  Investigation",2021-08-20 13:20:14+00:00,http://arxiv.org/abs/2108.09164v1,"Changzhen Ji, Yating Zhang, Xiaozhong Liu, Adam Jatowt, Changlong Sun, Conghui Zhu, Tiejun Zhao",cs.CL,knowledge,"Conversation generation as a challenging task in Natural Language Generation
(NLG) has been increasingly attracting attention over the last years. A number
of recent works adopted sequence-to-sequence structures along with external
knowledge, which successfully enhanced the quality of generated conversations.
Nevertheless, few works utilized the knowledge extracted from similar
conversations for utterance generation. Taking conversations in customer
service and court debate domains as examples, it is evident that essential
entities/phrases, as well as their associated logic and inter-relationships can
be extracted and borrowed from similar conversation instances. Such information
could provide useful signals for improving conversation generation. In this
paper, we propose a novel reading and memory framework called Deep Reading
Memory Network (DRMN) which is capable of remembering useful information of
similar conversations for improving utterance generation. We apply our model to
two large-scale conversation datasets of justice and e-commerce fields.
Experiments prove that the proposed model outperforms the state-of-the-art
approaches.",2021-08-20
"GGP: A Graph-based Grouping Planner for Explicit Control of Long Text
  Generation",2021-08-18 06:55:55+00:00,http://arxiv.org/abs/2108.07998v1,"Xuming Lin, Shaobo Cui, Zhongzhou Zhao, Wei Zhou, Ji Zhang, Haiqing Chen",cs.CL,knowledge,"Existing data-driven methods can well handle short text generation. However,
when applied to the long-text generation scenarios such as story generation or
advertising text generation in the commercial scenario, these methods may
generate illogical and uncontrollable texts. To address these aforementioned
issues, we propose a graph-based grouping planner(GGP) following the idea of
first-plan-then-generate. Specifically, given a collection of key phrases, GGP
firstly encodes these phrases into an instance-level sequential representation
and a corpus-level graph-based representation separately. With these two
synergic representations, we then regroup these phrases into a fine-grained
plan, based on which we generate the final long text. We conduct our
experiments on three long text generation datasets and the experimental results
reveal that GGP significantly outperforms baselines, which proves that GGP can
control the long text generation by knowing how to say and in what order.",2021-08-18
"HiTab: A Hierarchical Table Dataset for Question Answering and Natural
  Language Generation",2021-08-15 10:14:21+00:00,http://arxiv.org/abs/2108.06712v1,"Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou, Dongmei Zhang","cs.CL, cs.IR",knowledge,"Tables are often created with hierarchies, but existing works on table
reasoning mainly focus on flat tables and neglect hierarchical tables.
Hierarchical tables challenge existing methods by hierarchical indexing, as
well as implicit relationships of calculation and semantics. This work presents
HiTab, a free and open dataset for the research community to study question
answering (QA) and natural language generation (NLG) over hierarchical tables.
HiTab is a cross-domain dataset constructed from a wealth of statistical
reports and Wikipedia pages, and has unique characteristics: (1) nearly all
tables are hierarchical, and (2) both target sentences for NLG and questions
for QA are revised from high-quality descriptions in statistical reports that
are meaningful and diverse. (3) HiTab provides fine-grained annotations on both
entity and quantity alignment. Targeting hierarchical structure, we devise a
novel hierarchy-aware logical form for symbolic reasoning over tables, which
shows high effectiveness. Then given annotations of entity and quantity
alignment, we propose partially supervised training, which helps models to
largely reduce spurious predictions in the QA task. In the NLG task, we find
that entity and quantity alignment also helps NLG models to generate better
results in a conditional generation setting. Experiment results of
state-of-the-art baselines suggest that this dataset presents a strong
challenge and a valuable benchmark for future research.",2021-08-15
Generating Diverse Descriptions from Semantic Graphs,2021-08-12 11:00:09+00:00,http://arxiv.org/abs/2108.05659v2,"Jiuzhou Han, Daniel Beck, Trevor Cohn",cs.CL,knowledge,"Text generation from semantic graphs is traditionally performed with
deterministic methods, which generate a unique description given an input
graph. However, the generation problem admits a range of acceptable textual
outputs, exhibiting lexical, syntactic and semantic variation. To address this
disconnect, we present two main contributions. First, we propose a stochastic
graph-to-text model, incorporating a latent variable in an encoder-decoder
model, and its use in an ensemble. Second, to assess the diversity of the
generated sentences, we propose a new automatic evaluation metric which jointly
evaluates output diversity and quality in a multi-reference setting. We
evaluate the models on WebNLG datasets in English and Russian, and show an
ensemble of stochastic models produces diverse sets of generated sentences,
while retaining similar quality to state-of-the-art models.",2021-08-12
Sentence Semantic Regression for Text Generation,2021-08-06 07:35:59+00:00,http://arxiv.org/abs/2108.02984v1,"Wei Wang, Piji Li, Hai-Tao Zheng",cs.CL,knowledge,"Recall the classical text generation works, the generation framework can be
briefly divided into two phases: \textbf{idea reasoning} and \textbf{surface
realization}. The target of idea reasoning is to figure out the main idea which
will be presented in the following talking/writing periods. Surface realization
aims to arrange the most appropriate sentence to depict and convey the
information distilled from the main idea. However, the current popular
token-by-token text generation methods ignore this crucial process and suffer
from many serious issues, such as idea/topic drift. To tackle the problems and
realize this two-phase paradigm, we propose a new framework named Sentence
Semantic Regression (\textbf{SSR}) based on sentence-level language modeling.
For idea reasoning, two architectures \textbf{SSR-AR} and \textbf{SSR-NonAR}
are designed to conduct sentence semantic regression autoregressively (like
GPT2/3) and bidirectionally (like BERT). In the phase of surface realization, a
mixed-granularity sentence decoder is designed to generate text with better
consistency by jointly incorporating the predicted sentence-level main idea as
well as the preceding contextual token-level information. We conduct
experiments on four tasks of story ending prediction, story ending generation,
dialogue generation, and sentence infilling. The results show that SSR can
obtain better performance in terms of automatic metrics and human evaluation.",2021-08-06
Logic-Consistency Text Generation from Semantic Parses,2021-08-02 01:12:18+00:00,http://arxiv.org/abs/2108.00577v1,"Chang Shu, Yusen Zhang, Xiangyu Dong, Peng Shi, Tao Yu, Rui Zhang",cs.CL,knowledge,"Text generation from semantic parses is to generate textual descriptions for
formal representation inputs such as logic forms and SQL queries. This is
challenging due to two reasons: (1) the complex and intensive inner logic with
the data scarcity constraint, (2) the lack of automatic evaluation metrics for
logic consistency. To address these two challenges, this paper first proposes
SNOWBALL, a framework for logic consistent text generation from semantic parses
that employs an iterative training procedure by recursively augmenting the
training set with quality control. Second, we propose a novel automatic metric,
BLEC, for evaluating the logical consistency between the semantic parses and
generated texts. The experimental results on two benchmark datasets, Logic2Text
and Spider, demonstrate the SNOWBALL framework enhances the logic consistency
on both BLEC and human evaluation. Furthermore, our statistical analysis
reveals that BLEC is more logically consistent with human evaluation than
general-purpose automatic metrics including BLEU, ROUGE and, BLEURT. Our data
and code are available at https://github.com/Ciaranshu/relogic.",2021-08-02
"Neural Rule-Execution Tracking Machine For Transformer-Based Text
  Generation",2021-07-27 20:41:05+00:00,http://arxiv.org/abs/2107.13077v1,"Yufei Wang, Can Xu, Huang Hu, Chongyang Tao, Stephen Wan, Mark Dras, Mark Johnson, Daxin Jiang",cs.CL,knowledge,"Sequence-to-Sequence (S2S) neural text generation models, especially the
pre-trained ones (e.g., BART and T5), have exhibited compelling performance on
various natural language generation tasks. However, the black-box nature of
these models limits their application in tasks where specific rules (e.g.,
controllable constraints, prior knowledge) need to be executed. Previous works
either design specific model structure (e.g., Copy Mechanism corresponding to
the rule ""the generated output should include certain words in the source
input"") or implement specialized inference algorithm (e.g., Constrained Beam
Search) to execute particular rules through the text generation. These methods
require careful design case-by-case and are difficult to support multiple rules
concurrently. In this paper, we propose a novel module named Neural
Rule-Execution Tracking Machine that can be equipped into various
transformer-based generators to leverage multiple rules simultaneously to guide
the neural generation model for superior generation performance in a unified
and scalable way. Extensive experimental results on several benchmarks verify
the effectiveness of our proposed model in both controllable and general text
generation.",2021-07-27
"Exploiting Language Model for Efficient Linguistic Steganalysis: An
  Empirical Study",2021-07-26 12:37:18+00:00,http://arxiv.org/abs/2107.12168v1,"Biao Yi, Hanzhou Wu, Guorui Feng, Xinpeng Zhang","cs.CL, cs.MM",knowledge,"Recent advances in linguistic steganalysis have successively applied CNNs,
RNNs, GNNs and other deep learning models for detecting secret information in
generative texts. These methods tend to seek stronger feature extractors to
achieve higher steganalysis effects. However, we have found through experiments
that there actually exists significant difference between automatically
generated steganographic texts and carrier texts in terms of the conditional
probability distribution of individual words. Such kind of statistical
difference can be naturally captured by the language model used for generating
steganographic texts, which drives us to give the classifier a priori knowledge
of the language model to enhance the steganalysis ability. To this end, we
present two methods to efficient linguistic steganalysis in this paper. One is
to pre-train a language model based on RNN, and the other is to pre-train a
sequence autoencoder. Experimental results show that the two methods have
different degrees of performance improvement when compared to the randomly
initialized RNN classifier, and the convergence speed is significantly
accelerated. Moreover, our methods have achieved the best detection results.",2021-07-26
Guided Generation of Cause and Effect,2021-07-21 02:32:47+00:00,http://arxiv.org/abs/2107.09846v1,"Zhongyang Li, Xiao Ding, Ting Liu, J. Edward Hu, Benjamin Van Durme",cs.CL,knowledge,"We present a conditional text generation framework that posits sentential
expressions of possible causes and effects. This framework depends on two novel
resources we develop in the course of this work: a very large-scale collection
of English sentences expressing causal patterns CausalBank; and a refinement
over previous work on constructing large lexical causal knowledge graphs Cause
Effect Graph. Further, we extend prior work in lexically-constrained decoding
to support disjunctive positive constraints. Human assessment confirms that our
approach gives high-quality and diverse outputs. Finally, we use CausalBank to
perform continued training of an encoder supporting a recent state-of-the-art
model for causal reasoning, leading to a 3-point improvement on the COPA
challenge set, with no change in model architecture.",2021-07-21
WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset,2021-07-20 15:18:30+00:00,http://arxiv.org/abs/2107.09556v1,"Luyu Wang, Yujia Li, Ozlem Aslan, Oriol Vinyals","cs.CL, cs.AI",knowledge,"We present a new dataset of Wikipedia articles each paired with a knowledge
graph, to facilitate the research in conditional text generation, graph
generation and graph representation learning. Existing graph-text paired
datasets typically contain small graphs and short text (1 or few sentences),
thus limiting the capabilities of the models that can be learned on the data.
Our new dataset WikiGraphs is collected by pairing each Wikipedia article from
the established WikiText-103 benchmark (Merity et al., 2016) with a subgraph
from the Freebase knowledge graph (Bollacker et al., 2008). This makes it easy
to benchmark against other state-of-the-art text generative models that are
capable of generating long paragraphs of coherent text. Both the graphs and the
text data are of significantly larger scale compared to prior graph-text paired
datasets. We present baseline graph neural network and transformer model
results on our dataset for 3 tasks: graph -> text generation, graph -> text
retrieval and text -> graph retrieval. We show that better conditioning on the
graph provides gains in generation and retrieval quality but there is still
large room for improvement.",2021-07-20
From Show to Tell: A Survey on Image Captioning,2021-07-14 18:00:54+00:00,http://arxiv.org/abs/2107.06912v1,"Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli, Giuseppe Fiameni, Rita Cucchiara","cs.CV, cs.CL",knowledge,"Connecting Vision and Language plays an essential role in Generative
Intelligence. For this reason, in the last few years, a large research effort
has been devoted to image captioning, i.e. the task of describing images with
syntactically and semantically meaningful sentences. Starting from 2015 the
task has generally been addressed with pipelines composed of a visual encoding
step and a language model for text generation. During these years, both
components have evolved considerably through the exploitation of object
regions, attributes, and relationships and the introduction of multi-modal
connections, fully-attentive approaches, and BERT-like early-fusion strategies.
However, regardless of the impressive results obtained, research in image
captioning has not reached a conclusive answer yet. This work aims at providing
a comprehensive overview and categorization of image captioning approaches,
from visual encoding and text generation to training strategies, used datasets,
and evaluation metrics. In this respect, we quantitatively compare many
relevant state-of-the-art approaches to identify the most impactful technical
innovations in image captioning architectures and training strategies.
Moreover, many variants of the problem and its open challenges are analyzed and
discussed. The final goal of this work is to serve as a tool for understanding
the existing state-of-the-art and highlighting the future directions for an
area of research where Computer Vision and Natural Language Processing can find
an optimal synergy.",2021-07-14
"Document Embedding for Scientific Articles: Efficacy of Word Embeddings
  vs TFIDF",2021-07-11 23:58:39+00:00,http://arxiv.org/abs/2107.05151v1,"H. J. Meijer, J. Truong, R. Karimi",cs.AI,knowledge,"Over the last few years, neural network derived word embeddings became
popular in the natural language processing literature. Studies conducted have
mostly focused on the quality and application of word embeddings trained on
public available corpuses such as Wikipedia or other news and social media
sources. However, these studies are limited to generic text and thus lack
technical and scientific nuances such as domain specific vocabulary,
abbreviations, or scientific formulas which are commonly used in academic
context. This research focuses on the performance of word embeddings applied to
a large scale academic corpus. More specifically, we compare quality and
efficiency of trained word embeddings to TFIDF representations in modeling
content of scientific articles. We use a word2vec skip-gram model trained on
titles and abstracts of about 70 million scientific articles. Furthermore, we
have developed a benchmark to evaluate content models in a scientific context.
The benchmark is based on a categorization task that matches articles to
journals for about 1.3 million articles published in 2017. Our results show
that content models based on word embeddings are better for titles (short text)
while TFIDF works better for abstracts (longer text). However, the slight
improvement of TFIDF for larger text comes at the expense of 3.7 times more
memory requirement as well as up to 184 times higher computation times which
may make it inefficient for online applications. In addition, we have created a
2-dimensional visualization of the journals modeled via embeddings to
qualitatively inspect embedding model. This graph shows useful insights and can
be used to find competitive journals or gaps to propose new journals.",2021-07-11
"Inspiration through Observation: Demonstrating the Influence of
  Automatically Generated Text on Creative Writing",2021-07-08 17:53:22+00:00,http://arxiv.org/abs/2107.04007v1,Melissa Roemmele,"cs.CL, cs.AI, cs.HC",knowledge,"Getting machines to generate text perceived as creative is a long-pursued
goal. A growing body of research directs this goal towards augmenting the
creative writing abilities of human authors. In this paper, we pursue this
objective by analyzing how observing examples of automatically generated text
influences writing. In particular, we examine a task referred to as sentence
infilling, which involves transforming a list of words into a complete
sentence. We emphasize ""storiability"" as a desirable feature of the resulting
sentences, where ""storiable"" sentences are those that suggest a story a reader
would be curious to hear about. Both humans and an automated system (based on a
neural language model) performed this sentence infilling task. In one setting,
people wrote sentences on their own; in a different setting, people observed
the sentences produced by the model while writing their own sentences. Readers
then assigned storiability preferences to the resulting sentences in a
subsequent evaluation. We find that human-authored sentences were judged as
more storiable when authors observed the generated examples, and that
storiability increased as authors derived more semantic content from the
examples. This result gives evidence of an ""inspiration through observation""
paradigm for human-computer collaborative writing, through which human writing
can be enhanced by text generation models without directly copying their
output.",2021-07-08
Scarecrow: A Framework for Scrutinizing Machine Text,2021-07-02 22:37:03+00:00,http://arxiv.org/abs/2107.01294v2,"Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Choi",cs.CL,knowledge,"Modern neural text generation systems can produce remarkably fluent and
grammatical texts. While earlier language models suffered from repetition and
syntactic errors, the errors made by contemporary models are often semantic,
narrative, or discourse failures.
  To facilitate research of these complex error types, we introduce a new
structured, crowdsourced error annotation schema called Scarecrow. The error
categories used in Scarecrow -- such as redundancy, commonsense errors, and
incoherence -- were identified by combining expert analysis with several pilot
rounds of ontology-free crowd annotation to arrive at a schema which covers the
error phenomena found in real machine generated text.
  We use Scarecrow to collect 13k annotations of 1.3k human and machine
generate paragraphs of English language news text, amounting to over 41k spans
each labeled with its error category, severity, a natural language explanation,
and antecedent span (where relevant). We collect annotations for text generated
by state-of-the-art systems with varying known performance levels, from GPT-2
Small through the largest GPT-3. We isolate several factors for detailed
analysis, including parameter count, training data, and decoding technique. Our
results show both expected and surprising differences across these settings.
These findings demonstrate the value of Scarecrow annotations in the assessment
of current and future text generation systems. We release our complete
annotation toolkit and dataset at https://yao-dou.github.io/scarecrow/.",2021-07-02
"Capturing Event Argument Interaction via A Bi-Directional Entity-Level
  Recurrent Decoder",2021-07-01 02:55:12+00:00,http://arxiv.org/abs/2107.00189v1,"Xiangyu Xi, Wei Ye, Shikun Zhang, Quanxiu Wang, Huixing Jiang, Wei Wu",cs.CL,knowledge,"Capturing interactions among event arguments is an essential step towards
robust event argument extraction (EAE). However, existing efforts in this
direction suffer from two limitations: 1) The argument role type information of
contextual entities is mainly utilized as training signals, ignoring the
potential merits of directly adopting it as semantically rich input features;
2) The argument-level sequential semantics, which implies the overall
distribution pattern of argument roles over an event mention, is not well
characterized. To tackle the above two bottlenecks, we formalize EAE as a
Seq2Seq-like learning problem for the first time, where a sentence with a
specific event trigger is mapped to a sequence of event argument roles. A
neural architecture with a novel Bi-directional Entity-level Recurrent Decoder
(BERD) is proposed to generate argument roles by incorporating contextual
entities' argument role predictions, like a word-by-word text generation
process, thereby distinguishing implicit argument distribution patterns within
an event more accurately.",2021-07-01
Topic-to-Essay Generation with Comprehensive Knowledge Enhancement,2021-06-29 08:01:42+00:00,http://arxiv.org/abs/2106.15142v1,"Zhiyue Liu, Jiahai Wang, Zhenghong Li",cs.CL,knowledge,"Generating high-quality and diverse essays with a set of topics is a
challenging task in natural language generation. Since several given topics
only provide limited source information, utilizing various topic-related
knowledge is essential for improving essay generation performance. However,
previous works cannot sufficiently use that knowledge to facilitate the
generation procedure. This paper aims to improve essay generation by extracting
information from both internal and external knowledge. Thus, a topic-to-essay
generation model with comprehensive knowledge enhancement, named TEGKE, is
proposed. For internal knowledge enhancement, both topics and related essays
are fed to a teacher network as source information. Then, informative features
would be obtained from the teacher network and transferred to a student network
which only takes topics as input but provides comparable information compared
with the teacher network. For external knowledge enhancement, a topic knowledge
graph encoder is proposed. Unlike the previous works only using the nearest
neighbors of topics in the commonsense base, our topic knowledge graph encoder
could exploit more structural and semantic information of the commonsense
knowledge graph to facilitate essay generation. Moreover, the adversarial
training based on the Wasserstein distance is proposed to improve generation
quality. Experimental results demonstrate that TEGKE could achieve
state-of-the-art performance on both automatic and human evaluation.",2021-06-29
Membership Inference on Word Embedding and Beyond,2021-06-21 19:37:06+00:00,http://arxiv.org/abs/2106.11384v1,"Saeed Mahloujifar, Huseyin A. Inan, Melissa Chase, Esha Ghosh, Marcello Hasegawa","cs.CL, cs.AI, cs.CR, cs.LG",knowledge,"In the text processing context, most ML models are built on word embeddings.
These embeddings are themselves trained on some datasets, potentially
containing sensitive data. In some cases this training is done independently,
in other cases, it occurs as part of training a larger, task-specific model. In
either case, it is of interest to consider membership inference attacks based
on the embedding layer as a way of understanding sensitive information leakage.
But, somewhat surprisingly, membership inference attacks on word embeddings and
their effect in other natural language processing (NLP) tasks that use these
embeddings, have remained relatively unexplored.
  In this work, we show that word embeddings are vulnerable to black-box
membership inference attacks under realistic assumptions. Furthermore, we show
that this leakage persists through two other major NLP applications:
classification and text-generation, even when the embedding layer is not
exposed to the attacker. We show that our MI attack achieves high attack
accuracy against a classifier model and an LSTM-based language model. Indeed,
our attack is a cheaper membership inference attack on text-generative models,
which does not require the knowledge of the target model or any expensive
training of text-generative models as shadow models.",2021-06-21
"Do Encoder Representations of Generative Dialogue Models Encode
  Sufficient Information about the Task ?",2021-06-20 04:52:37+00:00,http://arxiv.org/abs/2106.10622v1,"Prasanna Parthasarathi, Joelle Pineau, Sarath Chandar",cs.CL,knowledge,"Predicting the next utterance in dialogue is contingent on encoding of users'
input text to generate appropriate and relevant response in data-driven
approaches. Although the semantic and syntactic quality of the language
generated is evaluated, more often than not, the encoded representation of
input is not evaluated. As the representation of the encoder is essential for
predicting the appropriate response, evaluation of encoder representation is a
challenging yet important problem. In this work, we showcase evaluating the
text generated through human or automatic metrics is not sufficient to
appropriately evaluate soundness of the language understanding of dialogue
models and, to that end, propose a set of probe tasks to evaluate encoder
representation of different language encoders commonly used in dialogue models.
From experiments, we observe that some of the probe tasks are easier and some
are harder for even sophisticated model architectures to learn. And, through
experiments we observe that RNN based architectures have lower performance on
automatic metrics on text generation than transformer model but perform better
than the transformer model on the probe tasks indicating that RNNs might
preserve task information better than the Transformers.",2021-06-20
"JointGT: Graph-Text Joint Representation Learning for Text Generation
  from Knowledge Graphs",2021-06-19 14:10:10+00:00,http://arxiv.org/abs/2106.10502v1,"Pei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Linfeng Song, Xiaoyan Zhu, Minlie Huang","cs.CL, cs.AI",knowledge,"Existing pre-trained models for knowledge-graph-to-text (KG-to-text)
generation simply fine-tune text-to-text pre-trained models such as BART or T5
on KG-to-text datasets, which largely ignore the graph structure during
encoding and lack elaborate pre-training tasks to explicitly model graph-text
alignments. To tackle these problems, we propose a graph-text joint
representation learning model called JointGT. During encoding, we devise a
structure-aware semantic aggregation module which is plugged into each
Transformer layer to preserve the graph structure. Furthermore, we propose
three new pre-training tasks to explicitly enhance the graph-text alignment
including respective text / graph reconstruction, and graph-text alignment in
the embedding space via Optimal Transport. Experiments show that JointGT
obtains new state-of-the-art performance on various KG-to-text datasets.",2021-06-19
Zero-Shot Controlled Generation with Encoder-Decoder Transformers,2021-06-11 14:07:19+00:00,http://arxiv.org/abs/2106.06411v2,"Devamanyu Hazarika, Mahdi Namazifar, Dilek Hakkani-Tür","cs.CL, cs.AI",knowledge,"Controlling neural network-based models for natural language generation (NLG)
has broad applications in numerous areas such as machine translation, document
summarization, and dialog systems. Approaches that enable such control in a
zero-shot manner would be of great importance as, among other reasons, they
remove the need for additional annotated data and training. In this work, we
propose novel approaches for controlling encoder-decoder transformer-based NLG
models in zero-shot. This is done by introducing three control knobs, namely,
attention biasing, decoder mixing, and context augmentation, that are applied
to these models at generation time. These knobs control the generation process
by directly manipulating trained NLG models (e.g., biasing cross-attention
layers) to realize the desired attributes in the generated outputs. We show
that not only are these NLG models robust to such manipulations, but also their
behavior could be controlled without an impact on their generation performance.
These results, to the best of our knowledge, are the first of their kind.
Through these control knobs, we also investigate the role of transformer
decoder's self-attention module and show strong evidence that its primary role
is maintaining fluency of sentences generated by these models. Based on this
hypothesis, we show that alternative architectures for transformer decoders
could be viable options. We also study how this hypothesis could lead to more
efficient ways for training encoder-decoder transformer models.",2021-06-11
AGGGEN: Ordering and Aggregating while Generating,2021-06-10 08:14:59+00:00,http://arxiv.org/abs/2106.05580v1,"Xinnuo Xu, Ondřej Dušek, Verena Rieser, Ioannis Konstas",cs.CL,knowledge,"We present AGGGEN (pronounced 'again'), a data-to-text model which
re-introduces two explicit sentence planning stages into neural data-to-text
systems: input ordering and input aggregation. In contrast to previous work
using sentence planning, our model is still end-to-end: AGGGEN performs
sentence planning at the same time as generating text by learning latent
alignments (via semantic facts) between input representation and target text.
Experiments on the WebNLG and E2E challenge data show that by using fact-based
alignments our approach is more interpretable, expressive, robust to noise, and
easier to control, while retaining the advantages of end-to-end systems in
terms of fluency. Our code is available at https://github.com/XinnuoXu/AggGen.",2021-06-10
"Turing: an Accurate and Interpretable Multi-Hypothesis Cross-Domain
  Natural Language Database Interface",2021-06-08 17:46:20+00:00,http://arxiv.org/abs/2106.04559v1,"Peng Xu, Wenjie Zi, Hamidreza Shahidi, Ákos Kádár, Keyi Tang, Wei Yang, Jawad Ateeq, Harsh Barot, Meidan Alon, Yanshuai Cao",cs.CL,knowledge,"A natural language database interface (NLDB) can democratize data-driven
insights for non-technical users. However, existing Text-to-SQL semantic
parsers cannot achieve high enough accuracy in the cross-database setting to
allow good usability in practice. This work presents Turing, a NLDB system
toward bridging this gap. The cross-domain semantic parser of Turing with our
novel value prediction method achieves $75.1\%$ execution accuracy, and
$78.3\%$ top-5 beam execution accuracy on the Spider validation set. To benefit
from the higher beam accuracy, we design an interactive system where the SQL
hypotheses in the beam are explained step-by-step in natural language, with
their differences highlighted. The user can then compare and judge the
hypotheses to select which one reflects their intention if any. The English
explanations of SQL queries in Turing are produced by our high-precision
natural language generation system based on synchronous grammars.",2021-06-08
"Few-shot Knowledge Graph-to-Text Generation with Pretrained Language
  Models",2021-06-03 06:48:00+00:00,http://arxiv.org/abs/2106.01623v1,"Junyi Li, Tianyi Tang, Wayne Xin Zhao, Zhicheng Wei, Nicholas Jing Yuan, Ji-Rong Wen",cs.CL,knowledge,"This paper studies how to automatically generate a natural language text that
describes the facts in knowledge graph (KG). Considering the few-shot setting,
we leverage the excellent capacities of pretrained language models (PLMs) in
language understanding and generation. We make three major technical
contributions, namely representation alignment for bridging the semantic gap
between KG encodings and PLMs, relation-biased KG linearization for deriving
better input representations, and multi-task learning for learning the
correspondence between KG and text. Extensive experiments on three benchmark
datasets have demonstrated the effectiveness of our model on KG-to-text
generation task. In particular, our model outperforms all comparison methods on
both fully-supervised and few-shot settings. Our code and datasets are
available at https://github.com/RUCAIBox/Few-Shot-KG2Text.",2021-06-03
Inspecting the concept knowledge graph encoded by modern language models,2021-05-27 22:19:19+00:00,http://arxiv.org/abs/2105.13471v2,"Carlos Aspillaga, Marcelo Mendoza, Alvaro Soto","cs.AI, cs.CL",knowledge,"The field of natural language understanding has experienced exponential
progress in the last few years, with impressive results in several tasks. This
success has motivated researchers to study the underlying knowledge encoded by
these models. Despite this, attempts to understand their semantic capabilities
have not been successful, often leading to non-conclusive, or contradictory
conclusions among different works. Via a probing classifier, we extract the
underlying knowledge graph of nine of the most influential language models of
the last years, including word embeddings, text generators, and context
encoders. This probe is based on concept relatedness, grounded on WordNet. Our
results reveal that all the models encode this knowledge, but suffer from
several inaccuracies. Furthermore, we show that the different architectures and
training strategies lead to different model biases. We conduct a systematic
evaluation to discover specific factors that explain why some concepts are
challenging. We hope our insights will motivate the development of models that
capture concepts more precisely.",2021-05-27
"Read, Listen, and See: Leveraging Multimodal Information Helps Chinese
  Spell Checking",2021-05-26 02:38:11+00:00,http://arxiv.org/abs/2105.12306v1,"Heng-Da Xu, Zhongli Li, Qingyu Zhou, Chao Li, Zizhen Wang, Yunbo Cao, Heyan Huang, Xian-Ling Mao",cs.CL,knowledge,"Chinese Spell Checking (CSC) aims to detect and correct erroneous characters
for user-generated text in the Chinese language. Most of the Chinese spelling
errors are misused semantically, phonetically or graphically similar
characters. Previous attempts noticed this phenomenon and try to use the
similarity for this task. However, these methods use either heuristics or
handcrafted confusion sets to predict the correct character. In this paper, we
propose a Chinese spell checker called ReaLiSe, by directly leveraging the
multimodal information of the Chinese characters. The ReaLiSe model tackles the
CSC task by (1) capturing the semantic, phonetic and graphic information of the
input characters, and (2) selectively mixing the information in these
modalities to predict the correct output. Experiments on the SIGHAN benchmarks
show that the proposed model outperforms strong baselines by a large margin.",2021-05-26
Personalized Transformer for Explainable Recommendation,2021-05-25 01:42:47+00:00,http://arxiv.org/abs/2105.11601v1,"Lei Li, Yongfeng Zhang, Li Chen","cs.IR, cs.AI",knowledge,"Personalization of natural language generation plays a vital role in a large
spectrum of tasks, such as explainable recommendation, review summarization and
dialog systems. In these tasks, user and item IDs are important identifiers for
personalization. Transformer, which is demonstrated with strong language
modeling capability, however, is not personalized and fails to make use of the
user and item IDs since the ID tokens are not even in the same semantic space
as the words. To address this problem, we present a PErsonalized Transformer
for Explainable Recommendation (PETER), on which we design a simple and
effective learning objective that utilizes the IDs to predict the words in the
target explanation, so as to endow the IDs with linguistic meanings and to
achieve personalized Transformer. Besides generating explanations, PETER can
also make recommendations, which makes it a unified model for the whole
recommendation-explanation pipeline. Extensive experiments show that our small
unpretrained model outperforms fine-tuned BERT on the generation task, in terms
of both effectiveness and efficiency, which highlights the importance and the
nice utility of our design.",2021-05-25
VANiLLa : Verbalized Answers in Natural Language at Large Scale,2021-05-24 16:57:54+00:00,http://arxiv.org/abs/2105.11407v1,"Debanjali Biswas, Mohnish Dubey, Md Rashad Al Hasan Rony, Jens Lehmann",cs.CL,knowledge,"In the last years, there have been significant developments in the area of
Question Answering over Knowledge Graphs (KGQA). Despite all the notable
advancements, current KGQA datasets only provide the answers as the direct
output result of the formal query, rather than full sentences incorporating
question context. For achieving coherent answers sentence with the question's
vocabulary, template-based verbalization so are usually employed for a better
representation of answers, which in turn require extensive expert intervention.
Thus, making way for machine learning approaches; however, there is a scarcity
of datasets that empower machine learning models in this area. Hence, we
provide the VANiLLa dataset which aims at reducing this gap by offering answers
in natural language sentences. The answer sentences in this dataset are
syntactically and semantically closer to the question than to the triple fact.
Our dataset consists of over 100k simple questions adapted from the CSQA and
SimpleQuestionsWikidata datasets and generated using a semi-automatic
framework. We also present results of training our dataset on multiple baseline
models adapted from current state-of-the-art Natural Language Generation (NLG)
architectures. We believe that this dataset will allow researchers to focus on
finding suitable methodologies and architectures for answer verbalization.",2021-05-24
"Long Text Generation by Modeling Sentence-Level and Discourse-Level
  Coherence",2021-05-19 07:29:08+00:00,http://arxiv.org/abs/2105.08963v1,"Jian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wenbiao Ding, Minlie Huang",cs.CL,knowledge,"Generating long and coherent text is an important but challenging task,
particularly for open-ended language generation tasks such as story generation.
Despite the success in modeling intra-sentence coherence, existing generation
models (e.g., BART) still struggle to maintain a coherent event sequence
throughout the generated text. We conjecture that this is because of the
difficulty for the decoder to capture the high-level semantics and discourse
structures in the context beyond token-level co-occurrence. In this paper, we
propose a long text generation model, which can represent the prefix sentences
at sentence level and discourse level in the decoding process. To this end, we
propose two pretraining objectives to learn the representations by predicting
inter-sentence semantic similarity and distinguishing between normal and
shuffled sentence orders. Extensive experiments show that our model can
generate more coherent texts than state-of-the-art baselines.",2021-05-19
OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics,2021-05-19 04:45:07+00:00,http://arxiv.org/abs/2105.08920v1,"Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, Minlie Huang",cs.CL,knowledge,"Automatic metrics are essential for developing natural language generation
(NLG) models, particularly for open-ended language generation tasks such as
story generation. However, existing automatic metrics are observed to correlate
poorly with human evaluation. The lack of standardized benchmark datasets makes
it difficult to fully evaluate the capabilities of a metric and fairly compare
different metrics. Therefore, we propose OpenMEVA, a benchmark for evaluating
open-ended story generation metrics. OpenMEVA provides a comprehensive test
suite to assess the capabilities of metrics, including (a) the correlation with
human judgments, (b) the generalization to different model outputs and
datasets, (c) the ability to judge story coherence, and (d) the robustness to
perturbations. To this end, OpenMEVA includes both manually annotated stories
and auto-constructed test examples. We evaluate existing metrics on OpenMEVA
and observe that they have poor correlation with human judgments, fail to
recognize discourse-level incoherence, and lack inferential knowledge (e.g.,
causal order between events), the generalization ability and robustness. Our
study presents insights for developing NLG models and metrics in further
research.",2021-05-19
Stage-wise Fine-tuning for Graph-to-Text Generation,2021-05-17 17:15:29+00:00,http://arxiv.org/abs/2105.08021v1,"Qingyun Wang, Semih Yavuz, Victoria Lin, Heng Ji, Nazneen Rajani","cs.CL, cs.AI",knowledge,"Graph-to-text generation has benefited from pre-trained language models
(PLMs) in achieving better performance than structured graph encoders. However,
they fail to fully utilize the structure information of the input graph. In
this paper, we aim to further improve the performance of the pre-trained
language model by proposing a structured graph-to-text model with a two-step
fine-tuning mechanism which first fine-tunes model on Wikipedia before adapting
to the graph-to-text generation. In addition to using the traditional token and
position embeddings to encode the knowledge graph (KG), we propose a novel
tree-level embedding method to capture the inter-dependency structures of the
input graph. This new approach has significantly improved the performance of
all text generation metrics for the English WebNLG 2017 dataset.",2021-05-17
R2D2: Relational Text Decoding with Transformers,2021-05-10 19:59:11+00:00,http://arxiv.org/abs/2105.04645v1,"Aryan Arbabi, Mingqiu Wang, Laurent El Shafey, Nan Du, Izhak Shafran",cs.CL,knowledge,"We propose a novel framework for modeling the interaction between graphical
structures and the natural language text associated with their nodes and edges.
Existing approaches typically fall into two categories. On group ignores the
relational structure by converting them into linear sequences and then utilize
the highly successful Seq2Seq models. The other side ignores the sequential
nature of the text by representing them as fixed-dimensional vectors and apply
graph neural networks. Both simplifications lead to information loss.
  Our proposed method utilizes both the graphical structure as well as the
sequential nature of the texts. The input to our model is a set of text
segments associated with the nodes and edges of the graph, which are then
processed with a transformer encoder-decoder model, equipped with a
self-attention mechanism that is aware of the graphical relations between the
nodes containing the segments. This also allows us to use BERT-like models that
are already trained on large amounts of text.
  While the proposed model has wide applications, we demonstrate its
capabilities on data-to-text generation tasks. Our approach compares favorably
against state-of-the-art methods in four tasks without tailoring the model
architecture. We also provide an early demonstration in a novel practical
application -- generating clinical notes from the medical entities mentioned
during clinical visits.",2021-05-10
Knowledge-based Review Generation by Coherence Enhanced Text Planning,2021-05-09 02:12:05+00:00,http://arxiv.org/abs/2105.03815v1,"Junyi Li, Wayne Xin Zhao, Zhicheng Wei, Nicholas Jing Yuan, Ji-Rong Wen",cs.CL,knowledge,"As a natural language generation task, it is challenging to generate
informative and coherent review text. In order to enhance the informativeness
of the generated text, existing solutions typically learn to copy entities or
triples from knowledge graphs (KGs). However, they lack overall consideration
to select and arrange the incorporated knowledge, which tends to cause text
incoherence.
  To address the above issue, we focus on improving entity-centric coherence of
the generated reviews by leveraging the semantic structure of KGs. In this
paper, we propose a novel Coherence Enhanced Text Planning model (CETP) based
on knowledge graphs (KGs) to improve both global and local coherence for review
generation. The proposed model learns a two-level text plan for generating a
document: (1) the document plan is modeled as a sequence of sentence plans in
order, and (2) the sentence plan is modeled as an entity-based subgraph from
KG. Local coherence can be naturally enforced by KG subgraphs through
intra-sentence correlations between entities. For global coherence, we design a
hierarchical self-attentive architecture with both subgraph- and node-level
attention to enhance the correlations between subgraphs. To our knowledge, we
are the first to utilize a KG-based text planning model to enhance text
coherence for review generation. Extensive experiments on three datasets
confirm the effectiveness of our model on improving the content coherence of
generated texts.",2021-05-09
"SGG: Learning to Select, Guide, and Generate for Keyphrase Generation",2021-05-06 09:43:33+00:00,http://arxiv.org/abs/2105.02544v1,"Jing Zhao, Junwei Bao, Yifan Wang, Youzheng Wu, Xiaodong He, Bowen Zhou","cs.CL, cs.AI",knowledge,"Keyphrases, that concisely summarize the high-level topics discussed in a
document, can be categorized into present keyphrase which explicitly appears in
the source text, and absent keyphrase which does not match any contiguous
subsequence but is highly semantically related to the source. Most existing
keyphrase generation approaches synchronously generate present and absent
keyphrases without explicitly distinguishing these two categories. In this
paper, a Select-Guide-Generate (SGG) approach is proposed to deal with present
and absent keyphrase generation separately with different mechanisms.
Specifically, SGG is a hierarchical neural network which consists of a
pointing-based selector at low layer concentrated on present keyphrase
generation, a selection-guided generator at high layer dedicated to absent
keyphrase generation, and a guider in the middle to transfer information from
selector to generator. Experimental results on four keyphrase generation
benchmarks demonstrate the effectiveness of our model, which significantly
outperforms the strong baselines for both present and absent keyphrases
generation. Furthermore, we extend SGG to a title generation task which
indicates its extensibility in natural language generation tasks.",2021-05-06
"Mitigating Political Bias in Language Models Through Reinforced
  Calibration",2021-04-30 07:21:30+00:00,http://arxiv.org/abs/2104.14795v1,"Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, Lili Wang, Soroush Vosoughi","cs.CL, cs.AI",knowledge,"Current large-scale language models can be politically biased as a result of
the data they are trained on, potentially causing serious problems when they
are deployed in real-world settings. In this paper, we describe metrics for
measuring political bias in GPT-2 generation and propose a reinforcement
learning (RL) framework for mitigating political biases in generated text. By
using rewards from word embeddings or a classifier, our RL framework guides
debiased generation without having access to the training data or requiring the
model to be retrained. In empirical experiments on three attributes sensitive
to political bias (gender, location, and topic), our methods reduced bias
according to both our metrics and human evaluation, while maintaining
readability and semantic coherence.",2021-04-30
"Imaginative Walks: Generative Random Walk Deviation Loss for Improved
  Unseen Learning Representation",2021-04-20 04:34:28+00:00,http://arxiv.org/abs/2104.09757v1,"Mohamed Elhoseiny, Divyansh Jha, Kai Yi, Ivan Skorokhodov","cs.CV, cs.AI",knowledge,"We propose a novel loss for generative models, dubbed as GRaWD (Generative
Random Walk Deviation), to improve learning representations of unexplored
visual spaces. Quality learning representation of unseen classes (or styles) is
crucial to facilitate novel image generation and better generative
understanding of unseen visual classes (a.k.a. Zero-Shot Learning, ZSL). By
generating representations of unseen classes from their semantic descriptions,
such as attributes or text, Generative ZSL aims at identifying unseen
categories discriminatively from seen ones. We define GRaWD by constructing a
dynamic graph, including the seen class/style centers and generated samples in
the current mini-batch. Our loss starts a random walk probability from each
center through visual generations produced from hallucinated unseen classes. As
a deviation signal, we encourage the random walk to eventually land after t
steps in a feature representation that is hard to classify to any of the seen
classes. We show that our loss can improve unseen class representation quality
on four text-based ZSL benchmarks on CUB and NABirds datasets and three
attribute-based ZSL benchmarks on AWA2, SUN, and aPY datasets. We also study
our loss's ability to produce meaningful novel visual art generations on
WikiArt dataset. Our experiments and human studies show that our loss can
improve StyleGAN1 and StyleGAN2 generation quality, creating novel art that is
significantly more preferred. Code will be made available.",2021-04-20
"ExplaGraphs: An Explanation Graph Generation Task for Structured
  Commonsense Reasoning",2021-04-15 17:51:36+00:00,http://arxiv.org/abs/2104.07644v1,"Swarnadeep Saha, Prateek Yadav, Lisa Bauer, Mohit Bansal","cs.CL, cs.AI",knowledge,"Recent commonsense-reasoning tasks are typically discriminative in nature,
where a model answers a multiple-choice question for a certain context.
Discriminative tasks are limiting because they fail to adequately evaluate the
model's ability to reason and explain predictions with underlying commonsense
knowledge. They also allow such models to use reasoning shortcuts and not be
""right for the right reasons"". In this work, we present ExplaGraphs, a new
generative and structured commonsense-reasoning task (and an associated
dataset) of explanation graph generation for stance prediction. Specifically,
given a belief and an argument, a model has to predict whether the argument
supports or counters the belief and also generate a commonsense-augmented graph
that serves as non-trivial, complete, and unambiguous explanation for the
predicted stance. The explanation graphs for our dataset are collected via
crowdsourcing through a novel Collect-Judge-And-Refine graph collection
framework that improves the graph quality via multiple rounds of verification
and refinement. A significant 83% of our graphs contain external commonsense
nodes with diverse structures and reasoning depths. We also propose a
multi-level evaluation framework that checks for the structural and semantic
correctness of the generated graphs and their plausibility with human-written
graphs. We experiment with state-of-the-art text generation models like BART
and T5 to generate explanation graphs and observe that there is a large gap
with human performance, thereby encouraging useful future work for this new
commonsense graph-based explanation generation task.",2021-04-15
"Span Pointer Networks for Non-Autoregressive Task-Oriented Semantic
  Parsing",2021-04-15 07:02:35+00:00,http://arxiv.org/abs/2104.07275v1,"Akshat Shrivastava, Pierce Chuang, Arun Babu, Shrey Desai, Abhinav Arora, Alexander Zotov, Ahmed Aly",cs.CL,knowledge,"An effective recipe for building seq2seq, non-autoregressive, task-oriented
parsers to map utterances to semantic frames proceeds in three steps: encoding
an utterance $x$, predicting a frame's length |y|, and decoding a |y|-sized
frame with utterance and ontology tokens. Though empirically strong, these
models are typically bottle necked by length prediction, as even small
inaccuracies change the syntactic and semantic characteristics of resulting
frames. In our work, we propose span pointer networks, non-autoregressive
parsers which shift the decoding task from text generation to span prediction;
that is, when imputing utterance spans into frame slots, our model produces
endpoints (e.g., [i, j]) as opposed to text (e.g., ""6pm""). This natural
quantization of the output space reduces the variability of gold frames,
therefore improving length prediction and, ultimately, exact match.
Furthermore, length prediction is now responsible for frame syntax and the
decoder is responsible for frame semantics, resulting in a coarse-to-fine
model. We evaluate our approach on several task-oriented semantic parsing
datasets. Notably, we bridge the quality gap between non-autogressive and
autoregressive parsers, achieving 87 EM on TOPv2 (Chen et al. 2020).
Furthermore,due to our more consistent gold frames, we show strong improvements
in model generalization in both cross-domain and cross-lingual transfer in
low-resource settings. Finally, due to our diminished output vocabulary, we
observe 70% reduction in latency and 83% reduction in memory at beam size 5
compared to prior non-autoregressive parsers.",2021-04-15
"StylePTB: A Compositional Benchmark for Fine-grained Controllable Text
  Style Transfer",2021-04-12 04:25:09+00:00,http://arxiv.org/abs/2104.05196v1,"Yiwei Lyu, Paul Pu Liang, Hai Pham, Eduard Hovy, Barnabás Póczos, Ruslan Salakhutdinov, Louis-Philippe Morency","cs.CL, cs.AI, cs.LG",knowledge,"Text style transfer aims to controllably generate text with targeted
stylistic changes while maintaining core meaning from the source sentence
constant. Many of the existing style transfer benchmarks primarily focus on
individual high-level semantic changes (e.g. positive to negative), which
enable controllability at a high level but do not offer fine-grained control
involving sentence structure, emphasis, and content of the sentence. In this
paper, we introduce a large-scale benchmark, StylePTB, with (1) paired
sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical,
syntactic, semantic, and thematic transfers of text, as well as (2)
compositions of multiple transfers which allow modeling of fine-grained
stylistic changes as building blocks for more complex, high-level transfers. By
benchmarking existing methods on StylePTB, we find that they struggle to model
fine-grained changes and have an even more difficult time composing multiple
styles. As a result, StylePTB brings novel challenges that we hope will
encourage future research in controllable text style transfer, compositional
models, and learning disentangled representations. Solving these challenges
would present important steps towards controllable text generation.",2021-04-12
"What's the best place for an AI conference, Vancouver or ______: Why
  completing comparative questions is difficult",2021-04-05 14:56:09+00:00,http://arxiv.org/abs/2104.01940v1,"Avishai Zagoury, Einat Minkov, Idan Szpektor, William W. Cohen",cs.CL,knowledge,"Although large neural language models (LMs) like BERT can be finetuned to
yield state-of-the-art results on many NLP tasks, it is often unclear what
these models actually learn. Here we study using such LMs to fill in entities
in human-authored comparative questions, like ``Which country is older, India
or ______?'' -- i.e., we study the ability of neural LMs to ask (not answer)
reasonable questions. We show that accuracy in this fill-in-the-blank task is
well-correlated with human judgements of whether a question is reasonable, and
that these models can be trained to achieve nearly human-level performance in
completing comparative questions in three different subdomains. However,
analysis shows that what they learn fails to model any sort of broad notion of
which entities are semantically comparable or similar -- instead the trained
models are very domain-specific, and performance is highly correlated with
co-occurrences between specific entities observed in the training set. This is
true both for models that are pretrained on general text corpora, as well as
models trained on a large corpus of comparison questions. Our study thus
reinforces recent results on the difficulty of making claims about a deep
model's world knowledge or linguistic competence based on performance on
specific benchmark problems. We make our evaluation datasets publicly available
to foster future research on complex understanding and reasoning in such models
at standards of human interaction.",2021-04-05
WakaVT: A Sequential Variational Transformer for Waka Generation,2021-04-01 12:14:41+00:00,http://arxiv.org/abs/2104.00426v1,"Yuka Takeishi, Mingxuan Niu, Jing Luo, Zhong Jin, Xinyu Yang","cs.CL, cs.AI",knowledge,"Poetry generation has long been a challenge for artificial intelligence. In
the scope of Japanese poetry generation, many researchers have paid attention
to Haiku generation, but few have focused on Waka generation. To further
explore the creative potential of natural language generation systems in
Japanese poetry creation, we propose a novel Waka generation model, WakaVT,
which automatically produces Waka poems given user-specified keywords. Firstly,
an additive mask-based approach is presented to satisfy the form constraint.
Secondly, the structures of Transformer and variational autoencoder are
integrated to enhance the quality of generated content. Specifically, to obtain
novelty and diversity, WakaVT employs a sequence of latent variables, which
effectively captures word-level variability in Waka data. To improve linguistic
quality in terms of fluency, coherence, and meaningfulness, we further propose
the fused multilevel self-attention mechanism, which properly models the
hierarchical linguistic structure of Waka. To the best of our knowledge, we are
the first to investigate Waka generation with models based on Transformer
and/or variational autoencoder. Both objective and subjective evaluation
results demonstrate that our model outperforms baselines significantly.",2021-04-01
FeTaQA: Free-form Table Question Answering,2021-04-01 09:59:40+00:00,http://arxiv.org/abs/2104.00369v1,"Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryściński, Nick Schoelkopf, Riley Kong, Xiangru Tang, Murori Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, Dragomir Radev",cs.CL,knowledge,"Existing table question answering datasets contain abundant factual questions
that primarily evaluate the query and schema comprehension capability of a
system, but they fail to include questions that require complex reasoning and
integration of information due to the constraint of the associated short-form
answers. To address these issues and to demonstrate the full challenge of table
question answering, we introduce FeTaQA, a new dataset with 10K Wikipedia-based
{table, question, free-form answer, supporting table cells} pairs. FeTaQA
yields a more challenging table question answering setting because it requires
generating free-form text answers after retrieval, inference, and integration
of multiple discontinuous facts from a structured knowledge source. Unlike
datasets of generative QA over text in which answers are prevalent with copies
of short text spans from the source, answers in our dataset are human-generated
explanations involving entities and their high-level relations. We provide two
benchmark methods for the proposed task: a pipeline method based on
semantic-parsing-based QA systems and an end-to-end method based on large
pretrained text generation models, and show that FeTaQA poses a challenge for
both methods.",2021-04-01
AfriKI: Machine-in-the-Loop Afrikaans Poetry Generation,2021-03-30 09:17:56+00:00,http://arxiv.org/abs/2103.16190v1,"Imke van Heerden, Anil Bas","cs.CL, cs.LG",knowledge,"This paper proposes a generative language model called AfriKI. Our approach
is based on an LSTM architecture trained on a small corpus of contemporary
fiction. With the aim of promoting human creativity, we use the model as an
authoring tool to explore machine-in-the-loop Afrikaans poetry generation. To
our knowledge, this is the first study to attempt creative text generation in
Afrikaans.",2021-03-30
SparseGAN: Sparse Generative Adversarial Network for Text Generation,2021-03-22 04:44:43+00:00,http://arxiv.org/abs/2103.11578v1,"Liping Yuan, Jiehang Zeng, Xiaoqing Zheng",cs.CL,knowledge,"It is still a challenging task to learn a neural text generation model under
the framework of generative adversarial networks (GANs) since the entire
training process is not differentiable. The existing training strategies either
suffer from unreliable gradient estimations or imprecise sentence
representations. Inspired by the principle of sparse coding, we propose a
SparseGAN that generates semantic-interpretable, but sparse sentence
representations as inputs to the discriminator. The key idea is that we treat
an embedding matrix as an over-complete dictionary, and use a linear
combination of very few selected word embeddings to approximate the output
feature representation of the generator at each time step. With such
semantic-rich representations, we not only reduce unnecessary noises for
efficient adversarial training, but also make the entire training process fully
differentiable. Experiments on multiple text generation datasets yield
performance improvements, especially in sequence-level metrics, such as BLEU.",2021-03-22
"Model Extraction and Adversarial Transferability, Your BERT is
  Vulnerable!",2021-03-18 04:23:21+00:00,http://arxiv.org/abs/2103.10013v1,"Xuanli He, Lingjuan Lyu, Qiongkai Xu, Lichao Sun",cs.CL,knowledge,"Natural language processing (NLP) tasks, ranging from text classification to
text generation, have been revolutionised by the pre-trained language models,
such as BERT. This allows corporations to easily build powerful APIs by
encapsulating fine-tuned BERT models for downstream tasks. However, when a
fine-tuned BERT model is deployed as a service, it may suffer from different
attacks launched by malicious users. In this work, we first present how an
adversary can steal a BERT-based API service (the victim/target model) on
multiple benchmark datasets with limited prior knowledge and queries. We
further show that the extracted model can lead to highly transferable
adversarial attacks against the victim model. Our studies indicate that the
potential vulnerabilities of BERT-based API services still hold, even when
there is an architectural mismatch between the victim model and the attack
model. Finally, we investigate two defence strategies to protect the victim
model and find that unless the performance of the victim model is sacrificed,
both model ex-traction and adversarial transferability can effectively
compromise the target models",2021-03-18
"Structural Adapters in Pretrained Language Models for AMR-to-text
  Generation",2021-03-16 15:06:50+00:00,http://arxiv.org/abs/2103.09120v1,"Leonardo F. R. Ribeiro, Yue Zhang, Iryna Gurevych",cs.CL,knowledge,"Previous work on text generation from graph-structured data relies on
pretrained language models (PLMs) and utilizes graph linearization heuristics
rather than explicitly considering the graph structure. Efficiently encoding
the graph structure in PLMs is challenging because they were pretrained on
natural language, and modeling structured data may lead to catastrophic
forgetting of distributional knowledge. In this paper, we propose StructAdapt,
an adapter method to encode graph structure into PLMs. Contrary to prior work,
StructAdapt effectively models interactions among the nodes based on the graph
connectivity, only training graph structure-aware adapter parameters. In this
way, we avoid catastrophic forgetting while maintaining the topological
structure of the graph. We empirically show the benefits of explicitly encoding
graph structure into PLMs using adapters and achieve state-of-the-art results
on two AMR-to-text datasets, training only 5.1% of the PLM parameters.",2021-03-16
InFillmore: Neural Frame Lexicalization for Narrative Text Infilling,2021-03-08 17:59:41+00:00,http://arxiv.org/abs/2103.04941v1,"Jiefu Ou, Nathaniel Weir, Anton Belyy, Felix Yu, Benjamin Van Durme",cs.CL,knowledge,"We propose a structured extension to bidirectional-context conditional
language generation, or ""infilling,"" inspired by Frame Semantic theory
(Fillmore, 1976). Guidance is provided through two approaches: (1) model
fine-tuning, conditioning directly on observed symbolic frames, and (2) a novel
extension to disjunctive lexically constrained decoding that leverages frame
semantic lexical units. Automatic and human evaluations confirm that
frame-guided generation allows for explicit manipulation of intended infill
semantics, with minimal loss of indistinguishability from the human-generated
text. Our methods flexibly apply to a variety of use scenarios, and we provide
an interactive web demo available at https://nlp.jhu.edu/demos.",2021-03-08
"Empathetic BERT2BERT Conversational Model: Learning Arabic Language
  Generation with Little Data",2021-03-07 13:23:51+00:00,http://arxiv.org/abs/2103.04353v1,"Tarek Naous, Wissam Antoun, Reem A. Mahmoud, Hazem Hajj",cs.CL,knowledge,"Enabling empathetic behavior in Arabic dialogue agents is an important aspect
of building human-like conversational models. While Arabic Natural Language
Processing has seen significant advances in Natural Language Understanding
(NLU) with language models such as AraBERT, Natural Language Generation (NLG)
remains a challenge. The shortcomings of NLG encoder-decoder models are
primarily due to the lack of Arabic datasets suitable to train NLG models such
as conversational agents. To overcome this issue, we propose a
transformer-based encoder-decoder initialized with AraBERT parameters. By
initializing the weights of the encoder and decoder with AraBERT pre-trained
weights, our model was able to leverage knowledge transfer and boost
performance in response generation. To enable empathy in our conversational
model, we train it using the ArabicEmpatheticDialogues dataset and achieve high
performance in empathetic response generation. Specifically, our model achieved
a low perplexity value of 17.0 and an increase in 5 BLEU points compared to the
previous state-of-the-art model. Also, our proposed model was rated highly by
85 human evaluators, validating its high capability in exhibiting empathy while
generating relevant and fluent responses in open-domain settings.",2021-03-07
Topic Modelling Meets Deep Neural Networks: A Survey,2021-02-28 12:59:28+00:00,http://arxiv.org/abs/2103.00498v1,"He Zhao, Dinh Phung, Viet Huynh, Yuan Jin, Lan Du, Wray Buntine","cs.LG, cs.CL, cs.IR",knowledge,"Topic modelling has been a successful technique for text analysis for almost
twenty years. When topic modelling met deep neural networks, there emerged a
new and increasingly popular research area, neural topic models, with over a
hundred models developed and a wide range of applications in neural language
understanding such as text generation, summarisation and language models. There
is a need to summarise research developments and discuss open problems and
future directions. In this paper, we provide a focused yet comprehensive
overview of neural topic models for interested researchers in the AI community,
so as to facilitate them to navigate and innovate in this fast-growing research
area. To the best of our knowledge, ours is the first review focusing on this
specific topic.",2021-02-28
Structural Information Preserving for Graph-to-Text Generation,2021-02-12 20:09:01+00:00,http://arxiv.org/abs/2102.06749v1,"Linfeng Song, Ante Wang, Jinsong Su, Yue Zhang, Kun Xu, Yubin Ge, Dong Yu",cs.CL,knowledge,"The task of graph-to-text generation aims at producing sentences that
preserve the meaning of input graphs. As a crucial defect, the current
state-of-the-art models may mess up or even drop the core structural
information of input graphs when generating outputs. We propose to tackle this
problem by leveraging richer training signals that can guide our model for
preserving input information. In particular, we introduce two types of
autoencoding losses, each individually focusing on different aspects (a.k.a.
views) of input graphs. The losses are then back-propagated to better calibrate
our model via multi-task training. Experiments on two benchmarks for
graph-to-text generation show the effectiveness of our approach over a
state-of-the-art baseline. Our code is available at
\url{http://github.com/Soistesimmer/AMR-multiview}.",2021-02-12
GraphPlan: Story Generation by Planning with Event Graph,2021-02-05 03:18:55+00:00,http://arxiv.org/abs/2102.02977v1,"Hong Chen, Raphael Shu, Hiroya Takamura, Hideki Nakayama",cs.CL,knowledge,"Story generation is a task that aims to automatically produce multiple
sentences to make up a meaningful story. This task is challenging because it
requires high-level understanding of semantic meaning of sentences and
causality of story events. Naive sequence-to-sequence models generally fail to
acquire such knowledge, as the logical correctness can hardly be guaranteed in
a text generation model without the strategic planning. In this paper, we focus
on planning a sequence of events assisted by event graphs, and use the events
to guide the generator. Instead of using a sequence-to-sequence model to output
a storyline as in some existing works, we propose to generate an event sequence
by walking on an event graph. The event graphs are built automatically based on
the corpus. To evaluate the proposed approach, we conduct human evaluation both
on event planning and story generation. Based on large-scale human annotation
results, our proposed approach is shown to produce more logically correct event
sequences and stories.",2021-02-05
BERT Transformer model for Detecting Arabic GPT2 Auto-Generated Tweets,2021-01-22 21:50:38+00:00,http://arxiv.org/abs/2101.09345v1,"Fouzi Harrag, Maria Debbah, Kareem Darwish, Ahmed Abdelali",cs.CL,knowledge,"During the last two decades, we have progressively turned to the Internet and
social media to find news, entertain conversations and share opinion. Recently,
OpenAI has developed a ma-chine learning system called GPT-2 for Generative
Pre-trained Transformer-2, which can pro-duce deepfake texts. It can generate
blocks of text based on brief writing prompts that look like they were written
by humans, facilitating the spread false or auto-generated text. In line with
this progress, and in order to counteract potential dangers, several methods
have been pro-posed for detecting text written by these language models. In
this paper, we propose a transfer learning based model that will be able to
detect if an Arabic sentence is written by humans or automatically generated by
bots. Our dataset is based on tweets from a previous work, which we have
crawled and extended using the Twitter API. We used GPT2-Small-Arabic to
generate fake Arabic Sentences. For evaluation, we compared different recurrent
neural network (RNN) word embeddings based baseline models, namely: LSTM,
BI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new
transfer-learning model has obtained an accuracy up to 98%. To the best of our
knowledge, this work is the first study where ARABERT and GPT2 were combined to
detect and classify the Arabic auto-generated texts.",2021-01-22
What Makes Good In-Context Examples for GPT-$3$?,2021-01-17 23:38:40+00:00,http://arxiv.org/abs/2101.06804v1,"Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen",cs.CL,knowledge,"GPT-$3$ has attracted lots of attention due to its superior performance
across a wide range of NLP tasks, especially with its powerful and versatile
in-context few-shot learning ability. Despite its success, we found that the
empirical results of GPT-$3$ depend heavily on the choice of in-context
examples. In this work, we investigate whether there are more effective
strategies for judiciously selecting in-context examples (relative to random
sampling) that better leverage GPT-$3$'s few-shot capabilities. Inspired by the
recent success of leveraging a retrieval module to augment large-scale neural
network models, we propose to retrieve examples that are semantically-similar
to a test sample to formulate its corresponding prompt. Intuitively, the
in-context examples selected with such a strategy may serve as more informative
inputs to unleash GPT-$3$'s extensive knowledge. We evaluate the proposed
approach on several natural language understanding and generation benchmarks,
where the retrieval-based prompt selection approach consistently outperforms
the random baseline. Moreover, it is observed that the sentence encoders
fine-tuned on task-related datasets yield even more helpful retrieval results.
Notably, significant gains are observed on tasks such as table-to-text
generation (41.9% on the ToTTo dataset) and open-domain question answering
(45.5% on the NQ dataset). We hope our investigation could help understand the
behaviors of GPT-$3$ and large-scale pre-trained LMs in general and enhance
their few-shot capabilities.",2021-01-17
Persuasive Natural Language Generation -- A Literature Review,2021-01-14 18:44:51+00:00,http://arxiv.org/abs/2101.05786v1,"Sebastian Duerr, Peter A. Gloor","cs.CL, cs.AI, I.2.7; J.4",knowledge,"This literature review focuses on the use of Natural Language Generation
(NLG) to automatically detect and generate persuasive texts. Extending previous
research on automatic identification of persuasion in text, we concentrate on
generative aspects through conceptualizing determinants of persuasion in five
business-focused categories: benevolence, linguistic appropriacy, logical
argumentation, trustworthiness, tools and datasets. These allow NLG to increase
an existing message's persuasiveness. Previous research illustrates key aspects
in each of the above mentioned five categories. A research agenda to further
study persuasive NLG is developed. The review includes analysis of
seventy-seven articles, outlining the existing body of knowledge and showing
the steady progress in this research field.",2021-01-14
"Political Depolarization of News Articles Using Attribute-aware Word
  Embeddings",2021-01-05 07:39:12+00:00,http://arxiv.org/abs/2101.01391v1,"Ruibo Liu, Lili Wang, Chenyan Jia, Soroush Vosoughi","cs.CL, cs.AI",knowledge,"Political polarization in the US is on the rise. This polarization negatively
affects the public sphere by contributing to the creation of ideological echo
chambers. In this paper, we focus on addressing one of the factors that
contributes to this polarity, polarized media. We introduce a framework for
depolarizing news articles. Given an article on a certain topic with a
particular ideological slant (eg., liberal or conservative), the framework
first detects polar language in the article and then generates a new article
with the polar language replaced with neutral expressions. To detect polar
words, we train a multi-attribute-aware word embedding model that is aware of
ideology and topics on 360k full-length media articles. Then, for text
generation, we propose a new algorithm called Text Annealing Depolarization
Algorithm (TADA). TADA retrieves neutral expressions from the word embedding
model that not only decrease ideological polarity but also preserve the
original argument of the text, while maintaining grammatical correctness. We
evaluate our framework by comparing the depolarized output of our model in two
modes, fully-automatic and semi-automatic, on 99 stories spanning 11 topics.
Based on feedback from 161 human testers, our framework successfully
depolarized 90.1% of paragraphs in semi-automatic mode and 78.3% of paragraphs
in fully-automatic mode. Furthermore, 81.2% of the testers agree that the
non-polar content information is well-preserved and 79% agree that
depolarization does not harm semantic correctness when they compare the
original text and the depolarized text. Our work shows that data-driven methods
can help to locate political polarity and aid in the depolarization of
articles.",2021-01-05
"Outline to Story: Fine-grained Controllable Story Generation from
  Cascaded Events",2021-01-04 08:16:21+00:00,http://arxiv.org/abs/2101.00822v1,"Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong, Changyou Chen","cs.CL, cs.AI, cs.LG",knowledge,"Large-scale pretrained language models have shown thrilling generation
capabilities, especially when they generate consistent long text in thousands
of words with ease. However, users of these models can only control the prefix
of sentences or certain global aspects of generated text. It is challenging to
simultaneously achieve fine-grained controllability and preserve the
state-of-the-art unconditional text generation capability. In this paper, we
first propose a new task named ""Outline to Story"" (O2S) as a test bed for
fine-grained controllable generation of long text, which generates a
multi-paragraph story from cascaded events, i.e. a sequence of outline events
that guide subsequent paragraph generation. We then create dedicate datasets
for future benchmarks, built by state-of-the-art keyword extraction techniques.
Finally, we propose an extremely simple yet strong baseline method for the O2S
task, which fine tunes pre-trained language models on augmented sequences of
outline-story pairs with simple language modeling objective. Our method does
not introduce any new parameters or perform any architecture modification,
except several special tokens as delimiters to build augmented sequences.
Extensive experiments on various datasets demonstrate state-of-the-art
conditional story generation performance with our model, achieving better
fine-grained controllability and user flexibility. Our paper is among the first
ones by our knowledge to propose a model and to create datasets for the task of
""outline to story"". Our work also instantiates research interest of
fine-grained controllable generation of open-domain long text, where
controlling inputs are represented by short text.",2021-01-04
"DISCOS: Bridging the Gap between Discourse Knowledge and Commonsense
  Knowledge",2021-01-01 03:30:38+00:00,http://arxiv.org/abs/2101.00154v1,"Tianqing Fang, Hongming Zhang, Weiqi Wang, Yangqiu Song, Bin He","cs.CL, cs.AI",knowledge,"Commonsense knowledge is crucial for artificial intelligence systems to
understand natural language. Previous commonsense knowledge acquisition
approaches typically rely on human annotations (e.g., ATOMIC) or text
generation models (e.g., COMET). Human annotation could provide high-quality
commonsense knowledge, yet its high cost often results in relatively small
scale and low coverage. On the other hand, generation models have the potential
to automatically generate more knowledge. Nonetheless, machine learning models
often fit the training data too well to generate novel knowledge in high
quality, thus still suffering from coverage problems. To address the
limitations of previous approaches, in this paper, we propose an alternative
commonsense knowledge acquisition framework DISCOS (from DIScourse to
COmmonSense), which automatically mines expensive complex commonsense knowledge
from more affordable linguistic knowledge resources. Experiments demonstrate
that we can successfully convert discourse knowledge over eventualities from
ASER, a large-scale discourse knowledge graph, into inferential if-then
commonsense knowledge defined in ATOMIC without any additional annotation
effort. Further study suggests that DISCOS significantly outperforms previous
supervised approaches in terms of novelty and diversity with comparable
quality. In total, we can acquire 3.4M ATOMIC-like inferential commonsense
knowledge by populating ATOMIC on the core part of ASER. Codes and data are
available at https://github.com/HKUST-KnowComp/DISCOS-commonsense.",2021-01-01
A Graph Total Variation Regularized Softmax for Text Generation,2021-01-01 03:29:21+00:00,http://arxiv.org/abs/2101.00153v1,"Liu Bin, Wang Liang, Yin Guosheng",cs.CL,knowledge,"The softmax operator is one of the most important functions in machine
learning models. When applying neural networks to multi-category
classification, the correlations among different categories are often ignored.
For example, in text generation, a language model makes a choice of each new
word based only on the former selection of its context. In this scenario, the
link statistics information of concurrent words based on a corpus (an analogy
of the natural way of expression) is also valuable in choosing the next word,
which can help to improve the sentence's fluency and smoothness. To fully
explore such important information, we propose a graph softmax function for
text generation. It is expected that the final classification result would be
dominated by both the language model and graphical text relationships among
words. We use a graph total variation term to regularize softmax so as to
incorporate the concurrent relationship into the language model. The total
variation of the generated words should be small locally. We apply the proposed
graph softmax to GPT2 for the text generation task. Experimental results
demonstrate that the proposed graph softmax achieves better BLEU and perplexity
than softmax. Human testers can also easily distinguish the text generated by
the graph softmax or softmax.",2021-01-01
A Distributional Approach to Controlled Text Generation,2020-12-21 19:02:41+00:00,http://arxiv.org/abs/2012.11635v1,"Muhammad Khalifa, Hady Elsahar, Marc Dymetman","cs.CL, cs.AI, cs.LG",knowledge,"We propose a Distributional Approach to address Controlled Text Generation
from pre-trained Language Models (LMs). This view permits to define, in a
single formal framework, ""pointwise"" and ""distributional"" constraints over the
target LM -- to our knowledge, this is the first approach with such generality
-- while minimizing KL divergence with the initial LM distribution. The optimal
target distribution is then uniquely determined as an explicit EBM
(Energy-Based Model) representation. From that optimal representation we then
train the target controlled autoregressive LM through an adaptive
distributional variant of Policy Gradient. We conduct a first set of
experiments over pointwise constraints showing the advantages of our approach
over a set of baselines, in terms of obtaining a controlled LM balancing
constraint satisfaction with divergence from the initial LM (GPT-2). We then
perform experiments over distributional constraints, a unique feature of our
approach, demonstrating its potential as a remedy to the problem of Bias in
Language Models. Through an ablation study we show the effectiveness of our
adaptive technique for obtaining faster convergence.",2020-12-21
"Lexically-constrained Text Generation through Commonsense Knowledge
  Extraction and Injection",2020-12-19 23:23:40+00:00,http://arxiv.org/abs/2012.10813v1,"Yikang Li, Pulkit Goel, Varsha Kuppur Rajendra, Har Simrat Singh, Jonathan Francis, Kaixin Ma, Eric Nyberg, Alessandro Oltramari",cs.CL,knowledge,"Conditional text generation has been a challenging task that is yet to see
human-level performance from state-of-the-art models. In this work, we
specifically focus on the Commongen benchmark, wherein the aim is to generate a
plausible sentence for a given set of input concepts. Despite advances in other
tasks, large pre-trained language models that are fine-tuned on this dataset
often produce sentences that are syntactically correct but qualitatively
deviate from a human understanding of common sense. Furthermore, generated
sequences are unable to fulfill such lexical requirements as matching
part-of-speech and full concept coverage. In this paper, we explore how
commonsense knowledge graphs can enhance model performance, with respect to
commonsense reasoning and lexically-constrained decoding. We propose strategies
for enhancing the semantic correctness of the generated text, which we
accomplish through: extracting commonsense relations from Conceptnet, injecting
these relations into the Unified Language Model (UniLM) through attention
mechanisms, and enforcing the aforementioned lexical requirements through
output constraints. By performing several ablations, we find that commonsense
injection enables the generation of sentences that are more aligned with human
understanding, while remaining compliant with lexical requirements.",2020-12-19
Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings,2020-12-14 10:59:59+00:00,http://arxiv.org/abs/2012.07412v2,"Qipeng Guo, Zhijing Jin, Ziyu Wang, Xipeng Qiu, Weinan Zhang, Jun Zhu, Zheng Zhang, David Wipf","cs.LG, cs.AI, cs.CL",knowledge,"Cycle-consistent training is widely used for jointly learning a forward and
inverse mapping between two domains of interest without the cumbersome
requirement of collecting matched pairs within each domain. In this regard, the
implicit assumption is that there exists (at least approximately) a
ground-truth bijection such that a given input from either domain can be
accurately reconstructed from successive application of the respective
mappings. But in many applications no such bijection can be expected to exist
and large reconstruction errors can compromise the success of cycle-consistent
training. As one important instance of this limitation, we consider
practically-relevant situations where there exists a many-to-one or surjective
mapping between domains. To address this regime, we develop a conditional
variational autoencoder (CVAE) approach that can be viewed as converting
surjective mappings to implicit bijections whereby reconstruction errors in
both directions can be minimized, and as a natural byproduct, realistic output
diversity can be obtained in the one-to-many direction. As theoretical
motivation, we analyze a simplified scenario whereby minima of the proposed
CVAE-based energy function align with the recovery of ground-truth surjective
mappings. On the empirical side, we consider a synthetic image dataset with
known ground-truth, as well as a real-world application involving natural
language generation from knowledge graphs and vice versa, a prototypical
surjective case. For the latter, our CVAE pipeline can capture such many-to-one
mappings during cycle training while promoting textural diversity for
graph-to-text tasks. Our code is available at github.com/QipengGuo/CycleGT",2020-12-14
"Generating Math Word Problems from Equations with Topic Controlling and
  Commonsense Enforcement",2020-12-14 10:02:11+00:00,http://arxiv.org/abs/2012.07379v1,"Tianyang Cao, Shuang Zeng, Songge Zhao, Mairgup Mansur, Baobao Chang",cs.CL,knowledge,"Recent years have seen significant advancement in text generation tasks with
the help of neural language models. However, there exists a challenging task:
generating math problem text based on mathematical equations, which has made
little progress so far. In this paper, we present a novel equation-to-problem
text generation model. In our model, 1) we propose a flexible scheme to
effectively encode math equations, we then enhance the equation encoder by a
Varitional Autoen-coder (VAE) 2) given a math equation, we perform topic
selection, followed by which a dynamic topic memory mechanism is introduced to
restrict the topic distribution of the generator 3) to avoid commonsense
violation in traditional generation model, we pretrain word embedding with
background knowledge graph (KG), and we link decoded words to related words in
KG, targeted at injecting background knowledge into our model. We evaluate our
model through both automatic metrices and human evaluation, experiments
demonstrate our model outperforms baseline and previous models in both accuracy
and richness of generated problem text.",2020-12-14
"Denoising Pre-Training and Data Augmentation Strategies for Enhanced RDF
  Verbalization with Transformers",2020-12-01 15:25:47+00:00,http://arxiv.org/abs/2012.00571v1,"Sebastien Montella, Betty Fabre, Tanguy Urvoy, Johannes Heinecke, Lina Rojas-Barahona",cs.CL,knowledge,"The task of verbalization of RDF triples has known a growth in popularity due
to the rising ubiquity of Knowledge Bases (KBs). The formalism of RDF triples
is a simple and efficient way to store facts at a large scale. However, its
abstract representation makes it difficult for humans to interpret. For this
purpose, the WebNLG challenge aims at promoting automated RDF-to-text
generation. We propose to leverage pre-trainings from augmented data with the
Transformer model using a data augmentation strategy. Our experiment results
show a minimum relative increases of 3.73%, 126.05% and 88.16% in BLEU score
for seen categories, unseen entities and unseen categories respectively over
the standard training.",2020-12-01
Video Generative Adversarial Networks: A Review,2020-11-04 12:16:05+00:00,http://arxiv.org/abs/2011.02250v1,"Nuha Aldausari, Arcot Sowmya, Nadine Marcus, Gelareh Mohammadi","cs.CV, cs.LG, eess.IV",knowledge,"With the increasing interest in the content creation field in multiple
sectors such as media, education, and entertainment, there is an increasing
trend in the papers that uses AI algorithms to generate content such as images,
videos, audio, and text. Generative Adversarial Networks (GANs) in one of the
promising models that synthesizes data samples that are similar to real data
samples. While the variations of GANs models, in general, have been covered to
some extent in several survey papers, to the best of our knowledge, this is
among the first survey papers that reviews the state-of-the-art video GANs
models. This paper first categorized GANs review papers into general GANs
review papers, image GANs review papers, and special field GANs review papers
such as anomaly detection, medical imaging, or cybersecurity. The paper then
summarizes the main improvements in GANs frameworks that are not initially
developed for the video domain but have been adopted in multiple video GANs
variations. Then, a comprehensive review of video GANs models is provided under
two main divisions according to the presence or non-presence of a condition.
The conditional models then further grouped according to the type of condition
into audio, text, video, and image. The paper is concluded by highlighting the
main challenges and limitations of the current video GANs models. A
comprehensive list of datasets, applied loss functions, and evaluation metrics
is provided in the supplementary material.",2020-11-04
Data-to-Text Generation with Iterative Text Editing,2020-11-03 13:32:38+00:00,http://arxiv.org/abs/2011.01694v1,"Zdeněk Kasner, Ondřej Dušek","cs.CL, I.2.7",knowledge,"We present a novel approach to data-to-text generation based on iterative
text editing. Our approach maximizes the completeness and semantic accuracy of
the output text while leveraging the abilities of recent pre-trained models for
text editing (LaserTagger) and language modeling (GPT-2) to improve the text
fluency. To this end, we first transform data items to text using trivial
templates, and then we iteratively improve the resulting text by a neural model
trained for the sentence fusion task. The output of the model is filtered by a
simple heuristic and reranked with an off-the-shelf pre-trained language model.
We evaluate our approach on two major data-to-text datasets (WebNLG, Cleaned
E2E) and analyze its caveats and benefits. Furthermore, we show that our
formulation of data-to-text generation opens up the possibility for zero-shot
domain adaptation using a general-domain dataset for sentence fusion.",2020-11-03
"Topic-Centric Unsupervised Multi-Document Summarization of Scientific
  and News Articles",2020-11-03 04:04:21+00:00,http://arxiv.org/abs/2011.08072v1,"Amanuel Alambo, Cori Lohstroh, Erik Madaus, Swati Padhee, Brandy Foster, Tanvi Banerjee, Krishnaprasad Thirunarayan, Michael Raymer","cs.CL, cs.IR, cs.LG",knowledge,"Recent advances in natural language processing have enabled automation of a
wide range of tasks, including machine translation, named entity recognition,
and sentiment analysis. Automated summarization of documents, or groups of
documents, however, has remained elusive, with many efforts limited to
extraction of keywords, key phrases, or key sentences. Accurate abstractive
summarization has yet to be achieved due to the inherent difficulty of the
problem, and limited availability of training data. In this paper, we propose a
topic-centric unsupervised multi-document summarization framework to generate
extractive and abstractive summaries for groups of scientific articles across
20 Fields of Study (FoS) in Microsoft Academic Graph (MAG) and news articles
from DUC-2004 Task 2. The proposed algorithm generates an abstractive summary
by developing salient language unit selection and text generation techniques.
Our approach matches the state-of-the-art when evaluated on automated
extractive evaluation metrics and performs better for abstractive summarization
on five human evaluation metrics (entailment, coherence, conciseness,
readability, and grammar). We achieve a kappa score of 0.68 between two
co-author linguists who evaluated our results. We plan to publicly share
MAG-20, a human-validated gold standard dataset of topic-clustered research
articles and their summaries to promote research in abstractive summarization.",2020-11-03
Fusion Models for Improved Visual Captioning,2020-10-28 21:55:25+00:00,http://arxiv.org/abs/2010.15251v2,"Marimuthu Kalimuthu, Aditya Mogadala, Marius Mosbach, Dietrich Klakow","cs.CV, cs.AI, cs.CL, cs.LG",knowledge,"Visual captioning aims to generate textual descriptions given images or
videos. Traditionally, image captioning models are trained on human annotated
datasets such as Flickr30k and MS-COCO, which are limited in size and
diversity. This limitation hinders the generalization capabilities of these
models while also rendering them liable to making mistakes. Language models
can, however, be trained on vast amounts of freely available unlabelled data
and have recently emerged as successful language encoders and coherent text
generators. Meanwhile, several unimodal and multimodal fusion techniques have
been proven to work well for natural language generation and automatic speech
recognition. Building on these recent developments, and with the aim of
improving the quality of generated captions, the contribution of our work in
this paper is two-fold: First, we propose a generic multimodal model fusion
framework for caption generation as well as emendation where we utilize
different fusion strategies to integrate a pretrained Auxiliary Language Model
(AuxLM) within the traditional encoder-decoder visual captioning frameworks.
Next, we employ the same fusion strategies to integrate a pretrained Masked
Language Model (MLM), namely BERT, with a visual captioning model, viz. Show,
Attend, and Tell, for emending both syntactic and semantic errors in captions.
Our caption emendation experiments on three benchmark image captioning
datasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the
baseline, indicating the usefulness of our proposed multimodal fusion
strategies. Further, we perform a preliminary qualitative analysis on the
emended captions and identify error categories based on the type of
corrections.",2020-10-28
Go Figure! A Meta Evaluation of Factuality in Summarization,2020-10-24 08:30:20+00:00,http://arxiv.org/abs/2010.12834v1,"Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, Jianfeng Gao",cs.CL,knowledge,"Text generation models can generate factually inconsistent text containing
distorted or fabricated facts about the source text. Recent work has focused on
building evaluation models to verify the factual correctness of semantically
constrained text generation tasks such as document summarization. While the
field of factuality evaluation is growing fast, we don't have well-defined
criteria for measuring the effectiveness, generalizability, reliability, or
sensitivity of the factuality metrics. Focusing on these aspects, in this
paper, we introduce a meta-evaluation framework for evaluating factual
consistency metrics. We introduce five necessary, common-sense conditions for
effective factuality metrics and experiment with nine recent factuality metrics
using synthetic and human-labeled factuality data from short news, long news
and dialogue summarization domains. Our framework enables assessing the
efficiency of any new factual consistency metric on a variety of dimensions
over multiple summarization domains and can be easily extended with new
meta-evaluation criteria. We also present our conclusions towards standardizing
the factuality evaluation metrics.",2020-10-24
CaM-Gen:Causally-aware Metric-guided Text Generation,2020-10-24 06:17:35+00:00,http://arxiv.org/abs/2010.12795v1,"Navita Goyal, Roodram Paneri, Ayush Agarwal, Udit Kalani, Abhilasha Sancheti, Niyati Chhaya",cs.CL,knowledge,"Content is created for a well-defined purpose, often described by a metric or
a signal represented in the form of structured information. The relationship
between the metrics or the goal of a target content and the content itself are
non-trivial. While large scale language models show promising text generation
capabilities, guiding and informing the generated text with external metrics is
challenging. These metrics and the content tend to have inherent relationships
and not all of them may directly impact the content. We introduce a CaM-Gen:
Causally-aware Generative Networks guided by user-defined input metrics
incorporating the causal relationships between the metric and the content
features. We leverage causal inference techniques to identify the causally
significant aspects of text that leads to the target metric and then explicitly
guide the generative model towards these by a feedback mechanism. We propose
this mechanism for variational autoencoder-based and transformer-based
generative models. The proposed models beat baselines in terms of the target
metric accuracy while maintaining the fluency and the language quality of the
generated text. To the best of our knowledge, this is one of the early attempts
at incorporating a metric-guide using causal inference towards controlled
generation.",2020-10-24
"Large Scale Knowledge Graph Based Synthetic Corpus Generation for
  Knowledge-Enhanced Language Model Pre-training",2020-10-23 22:14:50+00:00,http://arxiv.org/abs/2010.12688v1,"Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami Al-Rfou",cs.CL,knowledge,"Generating natural sentences from Knowledge Graph (KG) triples, known as
Data-To-Text Generation, is a task with many datasets for which numerous
complex systems have been developed. However, no prior work has attempted to
perform this generation at scale by converting an entire KG into natural text.
In this paper, we verbalize the entire Wikidata KG, and create a KG-Text
aligned corpus in the training process. We discuss the challenges in
verbalizing an entire KG versus verbalizing smaller datasets. We further show
that verbalizing an entire KG can be used to integrate structured and natural
language data. In contrast to the many architectures that have been developed
to integrate the structural differences between these two sources, our approach
converts the KG into the same format as natural text allowing it to be
seamlessly plugged into existing natural language systems. We evaluate this
approach by augmenting the retrieval corpus in REALM and showing improvements,
both on the LAMA knowledge probe and open domain QA.",2020-10-23
AI-lead Court Debate Case Investigation,2020-10-22 11:05:14+00:00,http://arxiv.org/abs/2010.11604v2,"Changzhen Ji, Xin Zhou, Conghui Zhu, Tiejun Zhao",cs.CL,knowledge,"The multi-role judicial debate composed of the plaintiff, defendant, and
judge is an important part of the judicial trial. Different from other types of
dialogue, questions are raised by the judge, The plaintiff, plaintiff's agent
defendant, and defendant's agent would be to debating so that the trial can
proceed in an orderly manner. Question generation is an important task in
Natural Language Generation. In the judicial trial, it can help the judge raise
efficient questions so that the judge has a clearer understanding of the case.
In this work, we propose an innovative end-to-end question generation
model-Trial Brain Model (TBM) to build a Trial Brain, it can generate the
questions the judge wants to ask through the historical dialogue between the
plaintiff and the defendant. Unlike prior efforts in natural language
generation, our model can learn the judge's questioning intention through
predefined knowledge. We do experiments on real-world datasets, the
experimental results show that our model can provide a more accurate question
in the multi-role court debate scene.",2020-10-22
"Better Distractions: Transformer-based Distractor Generation and
  Multiple Choice Question Filtering",2020-10-19 15:23:24+00:00,http://arxiv.org/abs/2010.09598v1,"Jeroen Offerijns, Suzan Verberne, Tessa Verhoef",cs.CL,knowledge,"For the field of education, being able to generate semantically correct and
educationally relevant multiple choice questions (MCQs) could have a large
impact. While question generation itself is an active research topic,
generating distractors (the incorrect multiple choice options) receives much
less attention. A missed opportunity, since there is still a lot of room for
improvement in this area. In this work, we train a GPT-2 language model to
generate three distractors for a given question and text context, using the
RACE dataset. Next, we train a BERT language model to answer MCQs, and use this
model as a filter, to select only questions that can be answered and therefore
presumably make sense. To evaluate our work, we start by using text generation
metrics, which show that our model outperforms earlier work on distractor
generation (DG) and achieves state-of-the-art performance. Also, by calculating
the question answering ability, we show that larger base models lead to better
performance. Moreover, we conducted a human evaluation study, which confirmed
the quality of the generated questions, but showed no statistically significant
effect of the QA filter.",2020-10-19
"RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich
  Semantic Annotations for Task-Oriented Dialogue Modeling",2020-10-17 08:18:59+00:00,http://arxiv.org/abs/2010.08738v1,"Jun Quan, Shian Zhang, Qian Cao, Zizhong Li, Deyi Xiong",cs.CL,knowledge,"In order to alleviate the shortage of multi-domain data and to capture
discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a
large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic
Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn
semantically annotated dialogues, with more than 150K utterances spanning over
12 domains, which is larger than all previous annotated H2H conversational
datasets. Both single- and multi-domain dialogues are constructed, accounting
for 65% and 35%, respectively. Each dialogue is labeled with comprehensive
dialogue annotations, including dialogue goal in the form of natural language
description, domain, dialogue states and acts at both the user and system side.
In addition to traditional dialogue annotations, we especially provide
linguistic annotations on discourse phenomena, e.g., ellipsis and coreference,
in dialogues, which are useful for dialogue coreference and ellipsis resolution
tasks. Apart from the fully annotated dataset, we also present a detailed
description of the data collection procedure, statistics and analysis of the
dataset. A series of benchmark models and results are reported, including
natural language understanding (intent detection & slot filling), dialogue
state tracking and dialogue context-to-text generation, as well as coreference
and ellipsis resolution, which facilitate the baseline comparison for future
research on this corpus.",2020-10-17
"Incorporate Semantic Structures into Machine Translation Evaluation via
  UCCA",2020-10-17 06:47:58+00:00,http://arxiv.org/abs/2010.08728v2,"Jin Xu, Yinuo Guo, Junfeng Hu",cs.CL,knowledge,"Copying mechanism has been commonly used in neural paraphrasing networks and
other text generation tasks, in which some important words in the input
sequence are preserved in the output sequence. Similarly, in machine
translation, we notice that there are certain words or phrases appearing in all
good translations of one source text, and these words tend to convey important
semantic information. Therefore, in this work, we define words carrying
important semantic meanings in sentences as semantic core words. Moreover, we
propose an MT evaluation approach named Semantically Weighted Sentence
Similarity (SWSS). It leverages the power of UCCA to identify semantic core
words, and then calculates sentence similarity scores on the overlap of
semantic core words. Experimental results show that SWSS can consistently
improve the performance of popular MT evaluation metrics which are based on
lexical similarity.",2020-10-17
Reflective Decoding: Unsupervised Paraphrasing and Abductive Reasoning,2020-10-16 18:02:07+00:00,http://arxiv.org/abs/2010.08566v1,"Peter West, Ximing Lu, Ari Holtzman, Chandra Bhagavatula, Jena Hwang, Yejin Choi",cs.CL,knowledge,"Pretrained Language Models (LMs) generate text with remarkable quality,
novelty,and coherence. Yet applying LMs to the problems of paraphrasing and
infilling currently requires direct supervision, since these tasks break the
left-to-right generation setup of pretrained LMs. We present Reflective
Decoding, a novel unsupervised approach to apply the capabilities of pretrained
LMs to non-sequential tasks. Our approach is general and applicable to two
distant tasks - paraphrasing and abductive reasoning. It requires no
supervision or parallel corpora, only two pretrained language models: forward
and backward. Reflective Decoding operates in two intuitive steps. In the
contextualization step, we use LMs to generate many left and right contexts
which collectively capture the meaning of the input sentence. Then, in the
reflection step we decode in the semantic neighborhood of the input,
conditioning on an ensemble of generated contexts with the reverse direction
LM. We reflect through the generated contexts, effectively using them as an
intermediate meaning representation to generate conditional output. Empirical
results demonstrate that Reflective Decoding outperforms strong unsupervised
baselines on both paraphrasing and abductive text infilling, significantly
narrowing the gap between unsupervised and supervised methods.Reflective
Decoding introduces the concept of using generated contexts to represent
meaning, opening up new possibilities for unsupervised conditional text
generation.",2020-10-16
Neural Deepfake Detection with Factual Structure of Text,2020-10-15 02:35:31+00:00,http://arxiv.org/abs/2010.07475v1,"Wanjun Zhong, Duyu Tang, Zenan Xu, Ruize Wang, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin",cs.CL,knowledge,"Deepfake detection, the task of automatically discriminating
machine-generated text, is increasingly critical with recent advances in
natural language generative models. Existing approaches to deepfake detection
typically represent documents with coarse-grained representations. However,
they struggle to capture factual structures of documents, which is a
discriminative factor between machine-generated and human-written text
according to our statistical analysis. To address this, we propose a
graph-based model that utilizes the factual structure of a document for
deepfake detection of text. Our approach represents the factual structure of a
given document as an entity graph, which is further utilized to learn sentence
representations with a graph neural network. Sentence representations are then
composed to a document representation for making predictions, where consistent
relations between neighboring sentences are sequentially modeled. Results of
experiments on two public deepfake datasets show that our approach
significantly improves strong base models built with RoBERTa. Model analysis
further indicates that our model can distinguish the difference in the factual
structure between machine-generated text and human-written text.",2020-10-15
"ReviewRobot: Explainable Paper Review Generation based on Knowledge
  Synthesis",2020-10-13 02:17:58+00:00,http://arxiv.org/abs/2010.06119v3,"Qingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight, Heng Ji, Nazneen Fatema Rajani","cs.CL, cs.AI",knowledge,"To assist human review process, we build a novel ReviewRobot to automatically
assign a review score and write comments for multiple categories such as
novelty and meaningful comparison. A good review needs to be knowledgeable,
namely that the comments should be constructive and informative to help improve
the paper; and explainable by providing detailed evidence. ReviewRobot achieves
these goals via three steps: (1) We perform domain-specific Information
Extraction to construct a knowledge graph (KG) from the target paper under
review, a related work KG from the papers cited by the target paper, and a
background KG from a large collection of previous papers in the domain. (2) By
comparing these three KGs, we predict a review score and detailed structured
knowledge as evidence for each review category. (3) We carefully select and
generalize human review sentences into templates, and apply these templates to
transform the review scores and evidence into natural language comments.
Experimental results show that our review score predictor reaches 71.4%-100%
accuracy. Human assessment by domain experts shows that 41.7%-70.5% of the
comments generated by ReviewRobot are valid and constructive, and better than
human-written ones for 20% of the time. Thus, ReviewRobot can serve as an
assistant for paper reviewers, program chairs and authors.",2020-10-13
"A Sentiment-Controllable Topic-to-Essay Generator with Topic Knowledge
  Graph",2020-10-12 08:06:12+00:00,http://arxiv.org/abs/2010.05511v1,"Lin Qiao, Jianhao Yan, Fandong Meng, Zhendong Yang, Jie Zhou",cs.CL,knowledge,"Generating a vivid, novel, and diverse essay with only several given topic
words is a challenging task of natural language generation. In previous work,
there are two problems left unsolved: neglect of sentiment beneath the text and
insufficient utilization of topic-related knowledge. Therefore, we propose a
novel Sentiment-Controllable topic-to-essay generator with a Topic Knowledge
Graph enhanced decoder, named SCTKG, which is based on the conditional
variational autoencoder (CVAE) framework. We firstly inject the sentiment
information into the generator for controlling sentiment for each sentence,
which leads to various generated essays. Then we design a Topic Knowledge Graph
enhanced decoder. Unlike existing models that use knowledge entities
separately, our model treats the knowledge graph as a whole and encodes more
structured, connected semantic information in the graph to generate a more
relevant essay. Experimental results show that our SCTKG can generate sentiment
controllable essays and outperform the state-of-the-art approach in terms of
topic relevance, fluency, and diversity on both automatic and human evaluation.",2020-10-12
Evaluating Factuality in Generation with Dependency-level Entailment,2020-10-12 06:43:10+00:00,http://arxiv.org/abs/2010.05478v2,"Tanya Goyal, Greg Durrett",cs.CL,knowledge,"Despite significant progress in text generation models, a serious limitation
is their tendency to produce text that is factually inconsistent with
information in the input. Recent work has studied whether textual entailment
systems can be used to identify factual errors; however, these sentence-level
entailment models are trained to solve a different problem than generation
filtering and they do not localize which part of a generation is non-factual.
In this paper, we propose a new formulation of entailment that decomposes it at
the level of dependency arcs. Rather than focusing on aggregate decisions, we
instead ask whether the semantic relationship manifested by individual
dependency arcs in the generated output is supported by the input. Human
judgments on this task are difficult to obtain; we therefore propose a method
to automatically create data based on existing entailment or paraphrase
corpora. Experiments show that our dependency arc entailment model trained on
this data can identify factual inconsistencies in paraphrasing and
summarization better than sentence-level methods or those based on question
generation, while additionally localizing the erroneous parts of the
generation.",2020-10-12
Controllable Multi-Character Psychology-Oriented Story Generation,2020-10-11 12:05:00+00:00,http://arxiv.org/abs/2010.05230v1,"Feifei Xu, Xinpeng Wang, Yunpu Ma, Volker Tresp, Yuyi Wang, Shanlin Zhou, Haizhou Du",cs.CL,knowledge,"Story generation, which aims to generate a long and coherent story
automatically based on the title or an input sentence, is an important research
area in the field of natural language generation. There is relatively little
work on story generation with appointed emotions. Most existing works focus on
using only one specific emotion to control the generation of a whole story and
ignore the emotional changes in the characters in the course of the story. In
our work, we aim to design an emotional line for each character that considers
multiple emotions common in psychological theories, with the goal of generating
stories with richer emotional changes in the characters. To the best of our
knowledge, this work is first to focuses on characters' emotional lines in
story generation. We present a novel model-based attention mechanism that we
call SoCP (Storytelling of multi-Character Psychology). We show that the
proposed model can generate stories considering the changes in the
psychological state of different characters. To take into account the
particularity of the model, in addition to commonly used evaluation
indicators(BLEU, ROUGE, etc.), we introduce the accuracy rate of psychological
state control as a novel evaluation metric. The new indicator reflects the
effect of the model on the psychological state control of story characters.
Experiments show that with SoCP, the generated stories follow the psychological
state for each character according to both automatic and human evaluations.",2020-10-11
"Adversarial Self-Supervised Data-Free Distillation for Text
  Classification",2020-10-10 02:46:06+00:00,http://arxiv.org/abs/2010.04883v1,"Xinyin Ma, Yongliang Shen, Gongfan Fang, Chen Chen, Chenghao Jia, Weiming Lu","cs.CL, cs.AI",knowledge,"Large pre-trained transformer-based language models have achieved impressive
results on a wide range of NLP tasks. In the past few years, Knowledge
Distillation(KD) has become a popular paradigm to compress a computationally
expensive model to a resource-efficient lightweight model. However, most KD
algorithms, especially in NLP, rely on the accessibility of the original
training dataset, which may be unavailable due to privacy issues. To tackle
this problem, we propose a novel two-stage data-free distillation method, named
Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed
for compressing large-scale transformer-based models (e.g., BERT). To avoid
text generation in discrete space, we introduce a Plug & Play Embedding
Guessing method to craft pseudo embeddings from the teacher's hidden knowledge.
Meanwhile, with a self-supervised module to quantify the student's ability, we
adapt the difficulty of pseudo embeddings in an adversarial training manner. To
the best of our knowledge, our framework is the first data-free distillation
framework designed for NLP tasks. We verify the effectiveness of our method on
several text classification datasets.",2020-10-10
Online Back-Parsing for AMR-to-Text Generation,2020-10-09 12:08:14+00:00,http://arxiv.org/abs/2010.04520v1,"Xuefeng Bai, Linfeng Song, Yue Zhang",cs.CL,knowledge,"AMR-to-text generation aims to recover a text containing the same meaning as
an input AMR graph. Current research develops increasingly powerful graph
encoders to better represent AMR graphs, with decoders based on standard
language modeling being used to generate outputs. We propose a decoder that
back predicts projected AMR graphs on the target sentence during text
generation. As the result, our outputs can better preserve the input meaning
than standard decoders. Experiments on two AMR benchmarks show the superiority
of our model over the previous state-of-the-art system based on graph
Transformer.",2020-10-09
A Survey of Knowledge-Enhanced Text Generation,2020-10-09 06:46:46+00:00,http://arxiv.org/abs/2010.04389v1,"Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, Meng Jiang","cs.CL, cs.AI, cs.LG",knowledge,"The goal of text generation is to make machines express in human language. It
is one of the most important yet challenging tasks in natural language
processing (NLP). Since 2014, various neural encoder-decoder models pioneered
by Seq2Seq have been proposed to achieve the goal by learning to map input text
to output text. However, the input text alone often provides limited knowledge
to generate the desired output, so the performance of text generation is still
far from satisfaction in many real-world scenarios. To address this issue,
researchers have considered incorporating various forms of knowledge beyond the
input text into the generation models. This research direction is known as
knowledge-enhanced text generation. In this survey, we present a comprehensive
review of the research on knowledge enhanced text generation over the past five
years. The main content includes two parts: (i) general methods and
architectures for integrating knowledge into text generation; (ii) specific
techniques and applications according to different forms of knowledge data.
This survey can have broad audiences, researchers and practitioners, in
academia and industry.",2020-10-09
"Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text
  Generation",2020-10-09 06:03:46+00:00,http://arxiv.org/abs/2010.04383v1,"Yan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, Shay B. Cohen, Zuozhu Liu, Lidong Bing",cs.CL,knowledge,"AMR-to-text generation is used to transduce Abstract Meaning Representation
structures (AMR) into text. A key challenge in this task is to efficiently
learn effective graph representations. Previously, Graph Convolution Networks
(GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to
capture non-local information and additionally, they follow a local
(first-order) information aggregation scheme. To account for these issues,
larger and deeper GCN models are required to capture more complex interactions.
In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight
Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local
interactions by synthesizing higher order information from the input graphs. We
further develop two novel parameter saving strategies based on the group graph
convolutions and weight tied convolutions to reduce memory usage and model
complexity. With the help of these strategies, we are able to train a model
with fewer parameters while maintaining the model capacity. Experiments
demonstrate that LDGCNs outperform state-of-the-art models on two benchmark
datasets for AMR-to-text generation with significantly fewer parameters.",2020-10-09
Dual Inference for Improving Language Understanding and Generation,2020-10-08 20:14:41+00:00,http://arxiv.org/abs/2010.04246v2,"Shang-Yu Su, Yung-Sung Chuang, Yun-Nung Chen",cs.CL,knowledge,"Natural language understanding (NLU) and Natural language generation (NLG)
tasks hold a strong dual relationship, where NLU aims at predicting semantic
labels based on natural language utterances and NLG does the opposite. The
prior work mainly focused on exploiting the duality in model training in order
to obtain the models with better performance. However, regarding the
fast-growing scale of models in the current NLP area, sometimes we may have
difficulty retraining whole NLU and NLG models. To better address the issue,
this paper proposes to leverage the duality in the inference stage without the
need of retraining. The experiments on three benchmark datasets demonstrate the
effectiveness of the proposed method in both NLU and NLG, providing the great
potential of practical usage.",2020-10-08
Beyond [CLS] through Ranking by Generation,2020-10-06 22:56:31+00:00,http://arxiv.org/abs/2010.03073v1,"Cicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nallapati, Zhiheng Huang, Bing Xiang","cs.CL, cs.IR",knowledge,"Generative models for Information Retrieval, where ranking of documents is
viewed as the task of generating a query from a document's language model, were
very successful in various IR tasks in the past. However, with the advent of
modern deep neural networks, attention has shifted to discriminative ranking
functions that model the semantic similarity of documents and queries instead.
Recently, deep generative models such as GPT2 and BART have been shown to be
excellent text generators, but their effectiveness as rankers have not been
demonstrated yet. In this work, we revisit the generative framework for
information retrieval and show that our generative approaches are as effective
as state-of-the-art semantic similarity-based discriminative models for the
answer selection task. Additionally, we demonstrate the effectiveness of
unlikelihood losses for IR.",2020-10-06
"Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on
  Chest X-rays",2020-10-06 04:18:18+00:00,http://arxiv.org/abs/2010.02467v1,"Jianmo Ni, Chun-Nan Hsu, Amilcare Gentili, Julian McAuley","cs.CV, cs.CL",knowledge,"Automatic medical image report generation has drawn growing attention due to
its potential to alleviate radiologists' workload. Existing work on report
generation often trains encoder-decoder networks to generate complete reports.
However, such models are affected by data bias (e.g.~label imbalance) and face
common issues inherent in text generation models (e.g.~repetition). In this
work, we focus on reporting abnormal findings on radiology images; instead of
training on complete radiology reports, we propose a method to identify
abnormal findings from the reports in addition to grouping them with
unsupervised clustering and minimal rules. We formulate the task as cross-modal
retrieval and propose Conditional Visual-Semantic Embeddings to align images
and fine-grained abnormal findings in a joint embedding space. We demonstrate
that our method is able to retrieve abnormal findings and outperforms existing
generation models on both clinical correctness and text generation metrics.",2020-10-06
KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation,2020-10-05 19:59:05+00:00,http://arxiv.org/abs/2010.02307v2,"Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang","cs.CL, cs.AI",knowledge,"Data-to-text generation has recently attracted substantial interests due to
its wide applications. Existing methods have shown impressive performance on an
array of tasks. However, they rely on a significant amount of labeled data for
each task, which is costly to acquire and thus limits their application to new
tasks and domains. In this paper, we propose to leverage pre-training and
transfer learning to address this issue. We propose a knowledge-grounded
pre-training (KGPT), which consists of two parts, 1) a general
knowledge-grounded generation model to generate knowledge-enriched text. 2) a
pre-training paradigm on a massive knowledge-grounded text corpus crawled from
the web. The pre-trained model can be fine-tuned on various data-to-text
generation tasks to generate task-specific text. We adopt three settings,
namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness.
Under the fully-supervised setting, our model can achieve remarkable gains over
the known baselines. Under zero-shot setting, our model without seeing any
examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail.
Under the few-shot setting, our model only needs about one-fifteenth as many
labeled examples to achieve the same level of performance as baseline models.
These experiments consistently prove the strong generalization ability of our
proposed framework https://github.com/wenhuchen/KGPT.",2020-10-05
GenAug: Data Augmentation for Finetuning Text Generators,2020-10-05 05:46:39+00:00,http://arxiv.org/abs/2010.01794v2,"Steven Y. Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura, Eduard Hovy","cs.CL, cs.AI, cs.LG",knowledge,"In this paper, we investigate data augmentation for text generation, which we
call GenAug. Text generation and language modeling are important tasks within
natural language processing, and are especially challenging for low-data
regimes. We propose and evaluate various augmentation methods, including some
that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp
Reviews. We also examine the relationship between the amount of augmentation
and the quality of the generated text. We utilize several metrics that evaluate
important aspects of the generated text including its diversity and fluency.
Our experiments demonstrate that insertion of character-level synthetic noise
and keyword replacement with hypernyms are effective augmentation methods, and
that the quality of generations improves to a peak at approximately three times
the amount of original data.",2020-10-05
Transformer-Based Neural Text Generation with Syntactic Guidance,2020-10-05 01:33:58+00:00,http://arxiv.org/abs/2010.01737v1,"Yinghao Li, Rui Feng, Isaac Rehg, Chao Zhang",cs.CL,knowledge,"We study the problem of using (partial) constituency parse trees as syntactic
guidance for controlled text generation. Existing approaches to this problem
use recurrent structures, which not only suffer from the long-term dependency
problem but also falls short in modeling the tree structure of the syntactic
guidance. We propose to leverage the parallelism of Transformer to better
incorporate parse trees. Our method first expands a partial template
constituency parse tree to a full-fledged parse tree tailored for the input
source text, and then uses the expanded tree to guide text generation. The
effectiveness of our model in this process hinges upon two new attention
mechanisms: 1) a path attention mechanism that forces one node to attend to
only other nodes located in its path in the syntax tree to better incorporate
syntax guidance; 2) a multi-encoder attention mechanism that allows the decoder
to dynamically attend to information from multiple encoders. Our experiments in
the controlled paraphrasing task show that our method outperforms SOTA models
both semantically and syntactically, improving the best baseline's BLEU score
from 11.83 to 26.27.",2020-10-05
"Knowledge-Enhanced Personalized Review Generation with Capsule Graph
  Neural Network",2020-10-04 03:54:40+00:00,http://arxiv.org/abs/2010.01480v1,"Junyi Li, Siqing Li, Wayne Xin Zhao, Gaole He, Zhicheng Wei, Nicholas Jing Yuan, Ji-Rong Wen","cs.CL, cs.AI",knowledge,"Personalized review generation (PRG) aims to automatically produce review
text reflecting user preference, which is a challenging natural language
generation task. Most of previous studies do not explicitly model factual
description of products, tending to generate uninformative content. Moreover,
they mainly focus on word-level generation, but cannot accurately reflect more
abstractive user preference in multiple aspects. To address the above issues,
we propose a novel knowledge-enhanced PRG model based on capsule graph neural
network~(Caps-GNN). We first construct a heterogeneous knowledge graph (HKG)
for utilizing rich item attributes. We adopt Caps-GNN to learn graph capsules
for encoding underlying characteristics from the HKG. Our generation process
contains two major steps, namely aspect sequence generation and sentence
generation. First, based on graph capsules, we adaptively learn aspect capsules
for inferring the aspect sequence. Then, conditioned on the inferred aspect
label, we design a graph-based copy mechanism to generate sentences by
incorporating related entities or words from HKG. To our knowledge, we are the
first to utilize knowledge graph for the PRG task. The incorporated KG
information is able to enhance user preference at both aspect and word levels.
Extensive experiments on three real-world datasets have demonstrated the
effectiveness of our model on the PRG task.",2020-10-04
"Continual Learning for Natural Language Generation in Task-oriented
  Dialog Systems",2020-10-02 10:32:29+00:00,http://arxiv.org/abs/2010.00910v1,"Fei Mi, Liangwei Chen, Mengjie Zhao, Minlie Huang, Boi Faltings","cs.CL, cs.LG",knowledge,"Natural language generation (NLG) is an essential component of task-oriented
dialog systems. Despite the recent success of neural approaches for NLG, they
are typically developed in an offline manner for particular domains. To better
fit real-life applications where new data come in a stream, we study NLG in a
""continual learning"" setting to expand its knowledge to new domains or
functionalities incrementally. The major challenge towards this goal is
catastrophic forgetting, meaning that a continually trained model tends to
forget the knowledge it has learned before. To this end, we propose a method
called ARPER (Adaptively Regularized Prioritized Exemplar Replay) by replaying
prioritized historical exemplars, together with an adaptive regularization
technique based on ElasticWeight Consolidation. Extensive experiments to
continually learn new domains and intents are conducted on MultiWoZ-2.0 to
benchmark ARPER with a wide range of techniques. Empirical results demonstrate
that ARPER significantly outperforms other methods by effectively mitigating
the detrimental catastrophic forgetting issue.",2020-10-02
"MEGATRON-CNTRL: Controllable Story Generation with External Knowledge
  Using Large-Scale Language Models",2020-10-02 08:07:12+00:00,http://arxiv.org/abs/2010.00840v1,"Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung, Anima Anandkumar, Bryan Catanzaro",cs.CL,knowledge,"Existing pre-trained large language models have shown unparalleled generative
capabilities. However, they are not controllable. In this paper, we propose
MEGATRON-CNTRL, a novel framework that uses large-scale language models and
adds control to text generation by incorporating an external knowledge base.
Our framework consists of a keyword predictor, a knowledge retriever, a
contextual knowledge ranker, and a conditional text generator. As we do not
have access to ground-truth supervision for the knowledge ranker, we make use
of weak supervision from sentence embedding. The empirical results show that
our model generates more fluent, consistent, and coherent stories with less
repetition and higher diversity compared to prior work on the ROC story
dataset. We showcase the controllability of our model by replacing the keywords
used to generate stories and re-running the generation process. Human
evaluation results show that 77.5% of these stories are successfully controlled
by the new keywords. Furthermore, by scaling our model from 124 million to 8.3
billion parameters we demonstrate that larger models improve both the quality
of generation (from 74.5% to 93.0% for consistency) and controllability (from
77.5% to 91.5%).",2020-10-02
"Learning from Mistakes: Combining Ontologies via Self-Training for
  Dialogue Generation",2020-09-30 23:54:38+00:00,http://arxiv.org/abs/2010.00150v1,"Lena Reed, Vrindavan Harrison, Shereen Oraby, Dilek Hakkani-Tur, Marilyn Walker",cs.CL,knowledge,"Natural language generators (NLGs) for task-oriented dialogue typically take
a meaning representation (MR) as input. They are trained end-to-end with a
corpus of MR/utterance pairs, where the MRs cover a specific set of dialogue
acts and domain attributes. Creation of such datasets is labor-intensive and
time-consuming. Therefore, dialogue systems for new domain ontologies would
benefit from using data for pre-existing ontologies. Here we explore, for the
first time, whether it is possible to train an NLG for a new larger ontology
using existing training sets for the restaurant domain, where each set is based
on a different ontology. We create a new, larger combined ontology, and then
train an NLG to produce utterances covering it. For example, if one dataset has
attributes for family-friendly and rating information, and the other has
attributes for decor and service, our aim is an NLG for the combined ontology
that can produce utterances that realize values for family-friendly, rating,
decor and service. Initial experiments with a baseline neural
sequence-to-sequence model show that this task is surprisingly challenging. We
then develop a novel self-training method that identifies (errorful) model
outputs, automatically constructs a corrected MR input to form a new (MR,
utterance) training pair, and then repeatedly adds these new instances back
into the training data. We then test the resulting model on a new test set. The
result is a self-trained model whose performance is an absolute 75.4%
improvement over the baseline model. We also report a human qualitative
evaluation of the final model showing that it achieves high naturalness,
semantic coherence and grammaticality",2020-09-30
Graph-based Multi-hop Reasoning for Long Text Generation,2020-09-28 12:47:59+00:00,http://arxiv.org/abs/2009.13282v1,"Liang Zhao, Jingjing Xu, Junyang Lin, Yichang Zhang, Hongxia Yang, Xu Sun",cs.CL,knowledge,"Long text generation is an important but challenging task.The main problem
lies in learning sentence-level semantic dependencies which traditional
generative models often suffer from. To address this problem, we propose a
Multi-hop Reasoning Generation (MRG) approach that incorporates multi-hop
reasoning over a knowledge graph to learn semantic dependencies among
sentences. MRG consists of twoparts, a graph-based multi-hop reasoning module
and a path-aware sentence realization module. The reasoning module is
responsible for searching skeleton paths from a knowledge graph to imitate the
imagination process in the human writing for semantic transfer. Based on the
inferred paths, the sentence realization module then generates a complete
sentence. Unlike previous black-box models, MRG explicitly infers the skeleton
path, which provides explanatory views tounderstand how the proposed model
works. We conduct experiments on three representative tasks, including story
generation, review generation, and product description generation. Automatic
and manual evaluation show that our proposed method can generate more
informative and coherentlong text than strong baselines, such as pre-trained
models(e.g. GPT-2) and knowledge-enhanced models.",2020-09-28
"KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense
  Reasoning",2020-09-26 19:57:49+00:00,http://arxiv.org/abs/2009.12677v1,"Ye Liu, Yao Wan, Lifang He, Hao Peng, Philip S. Yu","cs.CL, cs.SC",knowledge,"Generative commonsense reasoning which aims to empower machines to generate
sentences with the capacity of reasoning over a set of concepts is a critical
bottleneck for text generation. Even the state-of-the-art pre-trained language
generation models struggle at this task and often produce implausible and
anomalous sentences. One reason is that they rarely consider incorporating the
knowledge graph which can provide rich relational information among the
commonsense concepts. To promote the ability of commonsense reasoning for text
generation, we propose a novel knowledge graphaugmented pre-trained language
generation model KG-BART, which encompasses the complex relations of concepts
through the knowledge graph and produces more logical and natural sentences as
output. Moreover, KG-BART can leverage the graph attention to aggregate the
rich concept semantics that enhances the model generalization on unseen concept
sets. Experiments on benchmark CommonGen dataset verify the effectiveness of
our proposed approach by comparing with several strong pre-trained language
generation models, particularly KG-BART outperforms BART by 15.98%, 17.49%, in
terms of BLEU-3, 4. Moreover, we also show that the generated context by our
model can work as background scenarios to benefit downstream commonsense QA
tasks.",2020-09-26
"Language Generation with Multi-Hop Reasoning on Commonsense Knowledge
  Graph",2020-09-24 13:55:32+00:00,http://arxiv.org/abs/2009.11692v1,"Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, Minlie Huang",cs.CL,knowledge,"Despite the success of generative pre-trained language models on a series of
text generation tasks, they still suffer in cases where reasoning over
underlying commonsense knowledge is required during generation. Existing
approaches that integrate commonsense knowledge into generative pre-trained
language models simply transfer relational knowledge by post-training on
individual knowledge triples while ignoring rich connections within the
knowledge graph. We argue that exploiting both the structural and semantic
information of the knowledge graph facilitates commonsense-aware text
generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow
(GRF) that enables pre-trained models with dynamic multi-hop reasoning on
multi-relational paths extracted from the external commonsense knowledge graph.
We empirically show that our model outperforms existing baselines on three text
generation tasks that require reasoning over commonsense knowledge. We also
demonstrate the effectiveness of the dynamic multi-hop reasoning module with
reasoning paths inferred by the model that provide rationale to the generation.",2020-09-24
Prior Art Search and Reranking for Generated Patent Text,2020-09-19 01:16:18+00:00,http://arxiv.org/abs/2009.09132v1,"Jieh-Sheng Lee, Jieh Hsiang",cs.CL,knowledge,"Generative models, such as GPT-2, have demonstrated impressive results
recently. A fundamental question we'd like to address is: where did the
generated text come from? This work is our initial effort toward answering the
question by using prior art search. The purpose of the prior art search is to
find the most similar prior text in the training data of GPT-2. We take a
reranking approach and apply it to the patent domain. Specifically, we
pre-train GPT-2 models from scratch by using the patent data from the USPTO.
The input for the prior art search is the patent text generated by the GPT-2
model. We also pre-trained BERT models from scratch for converting patent text
to embeddings. The steps of reranking are: (1) search the most similar text in
the training data of GPT-2 by taking a bag-of-word ranking approach (BM25), (2)
convert the search results in text format to BERT embeddings, and (3) provide
the final result by ranking the BERT embeddings based on their similarities
with the patent text generated by GPT-2. The experiments in this work show that
such reranking is better than ranking with embeddings alone. However, our mixed
results also indicate that calculating the semantic similarities among long
text spans is still challenging. To our knowledge, this work is the first to
implement a reranking system to identify retrospectively the most similar
inputs to a GPT model based on its output.",2020-09-19
Generation-Augmented Retrieval for Open-domain Question Answering,2020-09-17 23:08:01+00:00,http://arxiv.org/abs/2009.08553v2,"Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen","cs.CL, cs.IR",knowledge,"Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and
efficient, but solely rely on lexical overlap without semantic matching. Recent
dense retrieval methods learn latent representations to tackle the lexical
mismatch problem, while being more computationally expensive and insufficient
for exact matching as they embed the text sequence into a single vector with
limited capacity. In this paper, we present Generation-Augmented Retrieval
(GAR), a query expansion method that augments a query with relevant contexts
through text generation. We demonstrate on open-domain question answering that
the generated contexts significantly enrich the semantics of the queries and
thus GAR with sparse representations (BM25) achieves comparable or better
performance than the state-of-the-art dense methods such as DPR
\cite{karpukhin2020dense}. We show that generating various contexts of a query
is beneficial as fusing their results consistently yields better retrieval
accuracy. Moreover, as sparse and dense representations are often
complementary, GAR can be easily combined with DPR to achieve even better
performance. Furthermore, GAR achieves the state-of-the-art performance on the
Natural Questions and TriviaQA datasets under the extractive setting when
equipped with an extractive reader, and consistently outperforms other
retrieval methods when the same generative reader is used.",2020-09-17
Knowledge Graphs for Multilingual Language Translation and Generation,2020-09-16 14:36:41+00:00,http://arxiv.org/abs/2009.07715v1,Diego Moussallem,cs.CL,knowledge,"The Natural Language Processing (NLP) community has recently seen outstanding
progress, catalysed by the release of different Neural Network (NN)
architectures. Neural-based approaches have proven effective by significantly
increasing the output quality of a large number of automated solutions for NLP
tasks (Belinkov and Glass, 2019). Despite these notable advancements, dealing
with entities still poses a difficult challenge as they are rarely seen in
training data. Entities can be classified into two groups, i.e., proper nouns
and common nouns. Proper nouns are also known as Named Entities (NE) and
correspond to the name of people, organizations, or locations, e.g., John, WHO,
or Canada. Common nouns describe classes of objects, e.g., spoon or cancer.
Both types of entities can be found in a Knowledge Graph (KG). Recent work has
successfully exploited the contribution of KGs in NLP tasks, such as Natural
Language Inference (NLI) (KM et al.,2018) and Question Answering (QA) (Sorokin
and Gurevych, 2018). Only a few works had exploited the benefits of KGs in
Neural Machine Translation (NMT) when the work presented herein began.
Additionally, few works had studied the contribution of KGs to Natural Language
Generation (NLG) tasks. Moreover, the multilinguality also remained an open
research area in these respective tasks (Young et al., 2018). In this thesis,
we focus on the use of KGs for machine translation and the generation of texts
to deal with the problems caused by entities and consequently enhance the
quality of automatically generated texts.",2020-09-16
Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News,2020-09-16 14:13:15+00:00,http://arxiv.org/abs/2009.07698v5,"Reuben Tan, Bryan A. Plummer, Kate Saenko","cs.AI, cs.CL, cs.CV",knowledge,"Large-scale dissemination of disinformation online intended to mislead or
deceive the general population is a major societal problem. Rapid progression
in image, video, and natural language generative models has only exacerbated
this situation and intensified our need for an effective defense mechanism.
While existing approaches have been proposed to defend against neural fake
news, they are generally constrained to the very limited setting where articles
only have text and metadata such as the title and authors. In this paper, we
introduce the more realistic and challenging task of defending against
machine-generated news that also includes images and captions. To identify the
possible weaknesses that adversaries can exploit, we create a NeuralNews
dataset composed of 4 different types of generated articles as well as conduct
a series of human user study experiments based on this dataset. In addition to
the valuable insights gleaned from our user study experiments, we provide a
relatively effective approach based on detecting visual-semantic
inconsistencies, which will serve as an effective first line of defense and a
useful reference for future work in defending against machine-generated
disinformation.",2020-09-16
UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation,2020-09-16 11:01:46+00:00,http://arxiv.org/abs/2009.07602v1,"Jian Guan, Minlie Huang",cs.CL,knowledge,"Despite the success of existing referenced metrics (e.g., BLEU and
MoverScore), they correlate poorly with human judgments for open-ended text
generation including story or dialog generation because of the notorious
one-to-many issue: there are many plausible outputs for the same input, which
may differ substantially in literal or semantics from the limited number of
given references. To alleviate this issue, we propose UNION, a learnable
unreferenced metric for evaluating open-ended story generation, which measures
the quality of a generated story without any reference. Built on top of BERT,
UNION is trained to distinguish human-written stories from negative samples and
recover the perturbation in negative stories. We propose an approach of
constructing negative samples by mimicking the errors commonly observed in
existing NLG models, including repeated plots, conflicting logic, and
long-range incoherence. Experiments on two story datasets demonstrate that
UNION is a reliable measure for evaluating the quality of generated stories,
which correlates better with human judgments and is more generalizable than
existing state-of-the-art metrics.",2020-09-16
Autoregressive Knowledge Distillation through Imitation Learning,2020-09-15 17:43:02+00:00,http://arxiv.org/abs/2009.07253v2,"Alexander Lin, Jeremy Wohlwend, Howard Chen, Tao Lei","cs.CL, cs.LG",knowledge,"The performance of autoregressive models on natural language generation tasks
has dramatically improved due to the adoption of deep, self-attentive
architectures. However, these gains have come at the cost of hindering
inference speed, making state-of-the-art models cumbersome to deploy in
real-world, time-sensitive settings. We develop a compression technique for
autoregressive models that is driven by an imitation learning perspective on
knowledge distillation. The algorithm is designed to address the exposure bias
problem. On prototypical language generation tasks such as translation and
summarization, our method consistently outperforms other distillation
algorithms, such as sequence-level knowledge distillation. Student models
trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those
trained from scratch, while increasing inference speed by up to 14 times in
comparison to the teacher model.",2020-09-15
Robust Conversational AI with Grounded Text Generation,2020-09-07 23:49:28+00:00,http://arxiv.org/abs/2009.03457v1,"Jianfeng Gao, Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh, Lars Liden, Heung-Yeung Shum","cs.AI, cs.CL",knowledge,"This article presents a hybrid approach based on a Grounded Text Generation
(GTG) model to building robust task bots at scale. GTG is a hybrid model which
uses a large-scale Transformer neural network as its backbone, combined with
symbol-manipulation modules for knowledge base inference and prior knowledge
encoding, to generate responses grounded in dialog belief state and real-world
knowledge for task completion. GTG is pre-trained on large amounts of raw text
and human conversational data, and can be fine-tuned to complete a wide range
of tasks.
  The hybrid approach and its variants are being developed simultaneously by
multiple research teams. The primary results reported on task-oriented dialog
benchmarks are very promising, demonstrating the big potential of this
approach. This article provides an overview of this progress and discusses
related methods and technologies that can be incorporated for building robust
conversational AI systems.",2020-09-07
"Black Box to White Box: Discover Model Characteristics Based on
  Strategic Probing",2020-09-07 14:44:28+00:00,http://arxiv.org/abs/2009.03136v1,"Josh Kalin, Matthew Ciolino, David Noever, Gerry Dozier","cs.LG, stat.ML",knowledge,"In Machine Learning, White Box Adversarial Attacks rely on knowing underlying
knowledge about the model attributes. This works focuses on discovering to
distrinct pieces of model information: the underlying architecture and primary
training dataset. With the process in this paper, a structured set of input
probes and the output of the model become the training data for a deep
classifier. Two subdomains in Machine Learning are explored: image based
classifiers and text transformers with GPT-2. With image classification, the
focus is on exploring commonly deployed architectures and datasets available in
popular public libraries. Using a single transformer architecture with multiple
levels of parameters, text generation is explored by fine tuning off different
datasets. Each dataset explored in image and text are distinguishable from one
another. Diversity in text transformer outputs implies further research is
needed to successfully classify architecture attribution in text domain.",2020-09-07
"Adversarial Watermarking Transformer: Towards Tracing Text Provenance
  with Data Hiding",2020-09-07 11:01:24+00:00,http://arxiv.org/abs/2009.03015v1,"Sahar Abdelnabi, Mario Fritz","cs.CR, cs.CL, cs.CY, cs.LG, I.2.7",knowledge,"Recent advances in natural language generation have introduced powerful
language models with high-quality output text. However, this raises concerns
about the potential misuse of such models for malicious purposes. In this
paper, we study natural language watermarking as a defense to help better mark
and trace the provenance of text. We introduce the Adversarial Watermarking
Transformer (AWT) with a jointly trained encoder-decoder and adversarial
training that, given an input text and a binary message, generates an output
text that is unobtrusively encoded with the given message. We further study
different training and inference strategies to achieve minimal changes to the
semantics and correctness of the input text. AWT is the first end-to-end model
to hide data in text by automatically learning -- without ground truth -- word
substitutions along with their locations in order to encode the message. We
show that our model is effective in largely preserving text utility and
decoding the watermark while hiding its presence against adversaries.
Additionally, we demonstrate that our method is robust against a range of local
changes and denoising attacks.",2020-09-07
Navigating Human Language Models with Synthetic Agents,2020-08-10 14:39:53+00:00,http://arxiv.org/abs/2008.04162v7,"Philip Feldman, Antonio Bucchiarone","cs.AI, cs.CL, cs.MA, I.2; I.6; J.4",knowledge,"Modern natural language models such as the GPT-2/GPT-3 contain tremendous
amounts of information about human belief in a consistently testable form. If
these models could be shown to accurately reflect the underlying beliefs of the
human beings that produced the data used to train these models, then such
models become a powerful sociological tool in ways that are distinct from
traditional methods, such as interviews and surveys. In this study, We train a
version of the GPT-2 on a corpora of historical chess games, and then ""launch""
clusters of synthetic agents into the model, using text strings to create
context and orientation. We compare the trajectories contained in the text
generated by the agents/model and compare that to the known ground truth of the
chess board, move legality, and historical patterns of play. We find that the
percentages of moves by piece using the model are substantially similar from
human patterns. We further find that the model creates an accurate latent
representation of the chessboard, and that it is possible to plot trajectories
of legal moves across the board using this knowledge.",2020-08-10
Investigating Pretrained Language Models for Graph-to-Text Generation,2020-07-16 16:05:34+00:00,http://arxiv.org/abs/2007.08426v2,"Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Schütze, Iryna Gurevych",cs.CL,knowledge,"Graph-to-text generation aims to generate fluent texts from graph-based data.
In this paper, we investigate two recently proposed pretrained language models
(PLMs) and analyze the impact of different task-adaptive pretraining strategies
for PLMs in graph-to-text generation. We present a study across three graph
domains: meaning representations, Wikipedia knowledge graphs (KGs) and
scientific KGs. We show that the PLMs BART and T5 achieve new state-of-the-art
results and that task-adaptive pretraining strategies improve their performance
even further. In particular, we report new state-of-the-art BLEU scores of
49.72 on LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative
improvement of 31.8%, 4.5%, and 42.4%, respectively. In an extensive analysis,
we identify possible reasons for the PLMs' success on graph-to-text tasks. We
find evidence that their knowledge about true facts helps them perform well
even when the input graph representation is reduced to a simple bag of node and
edge labels.",2020-07-16
"Deep Transformer based Data Augmentation with Subword Units for
  Morphologically Rich Online ASR",2020-07-14 10:22:05+00:00,http://arxiv.org/abs/2007.06949v3,"Balázs Tarján, György Szaszák, Tibor Fegyó, Péter Mihajlik","eess.AS, cs.CL",knowledge,"Recently Deep Transformer models have proven to be particularly powerful in
language modeling tasks for ASR. Their high complexity, however, makes them
very difficult to apply in the first (single) pass of an online system. Recent
studies showed that a considerable part of the knowledge of neural network
Language Models (LM) can be transferred to traditional n-grams by using neural
text generation based data augmentation. In our paper, we pre-train a GPT-2
Transformer LM on a general text corpus and fine-tune it on our Hungarian
conversational call center ASR task. We show that although data augmentation
with Transformer-generated text works well for isolating languages, it causes a
vocabulary explosion in a morphologically rich language. Therefore, we propose
a new method called subword-based neural text augmentation, where we retokenize
the generated text into statistically derived subwords. We compare Morfessor
and BPE statistical subword tokenizers and show that both methods can
significantly improve the WER while greatly reducing vocabulary size and memory
requirements. Finally, we also demonstrate that subword-based neural text
augmentation outperforms the word-based approach not only in terms of overall
WER but also in recognition of OOV words.",2020-07-14
"Contextualized Code Representation Learning for Commit Message
  Generation",2020-07-14 09:43:26+00:00,http://arxiv.org/abs/2007.06934v1,"Lun Yiu Nie, Cuiyun Gao, Zhicong Zhong, Wai Lam, Yang Liu, Zenglin Xu","cs.CL, cs.LG, cs.SE",knowledge,"Automatic generation of high-quality commit messages for code commits can
substantially facilitate developers' works and coordination. However, the
semantic gap between source code and natural language poses a major challenge
for the task. Several studies have been proposed to alleviate the challenge but
none explicitly involves code contextual information during commit message
generation. Specifically, existing research adopts static embedding for code
tokens, which maps a token to the same vector regardless of its context. In
this paper, we propose a novel Contextualized code representation learning
method for commit message Generation (CoreGen). CoreGen first learns
contextualized code representation which exploits the contextual information
behind code commit sequences. The learned representations of code commits built
upon Transformer are then transferred for downstream commit message generation.
Experiments on the benchmark dataset demonstrate the superior effectiveness of
our model over the baseline models with an improvement of 28.18% in terms of
BLEU-4 score. Furthermore, we also highlight the future opportunities in
training contextualized code representations on larger code corpus as a
solution to low-resource settings and adapting the pretrained code
representations to other downstream code-to-text generation tasks.",2020-07-14
DART: Open-Domain Structured Data Record to Text Generation,2020-07-06 16:35:30+00:00,http://arxiv.org/abs/2007.02871v1,"Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Nazneen Fatema Rajani, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Murori Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher",cs.CL,knowledge,"We introduce DART, a large dataset for open-domain structured data record to
text generation. We consider the structured data record input as a set of RDF
entity-relation triples, a format widely used for knowledge representation and
semantics description. DART consists of 82,191 examples across different
domains with each input being a semantic RDF triple set derived from data
records in tables and the tree ontology of the schema, annotated with sentence
descriptions that cover all facts in the triple set. This hierarchical,
structured format with its open-domain nature differentiates DART from other
existing table-to-text corpora. We conduct an analysis of DART on several
state-of-the-art text generation models, showing that it introduces new and
interesting challenges compared to existing datasets. Furthermore, we
demonstrate that finetuning pretrained language models on DART facilitates
out-of-domain generalization on the WebNLG 2017 dataset. DART is available at
https://github.com/Yale-LILY/dart.",2020-07-06
Adversarial Mutual Information for Text Generation,2020-06-30 19:11:51+00:00,http://arxiv.org/abs/2007.00067v1,"Boyuan Pan, Yazheng Yang, Kaizhao Liang, Bhavya Kailkhura, Zhongming Jin, Xian-Sheng Hua, Deng Cai, Bo Li","cs.CL, cs.LG, stat.ML",knowledge,"Recent advances in maximizing mutual information (MI) between the source and
target have demonstrated its effectiveness in text generation. However,
previous works paid little attention to modeling the backward network of MI
(i.e., dependency from the target to the source), which is crucial to the
tightness of the variational information maximization lower bound. In this
paper, we propose Adversarial Mutual Information (AMI): a text generation
framework which is formed as a novel saddle point (min-max) optimization aiming
to identify joint interactions between the source and target. Within this
framework, the forward and backward networks are able to iteratively promote or
demote each other's generated instances by comparing the real and synthetic
data distributions. We also develop a latent noise sampling strategy that
leverages random variations at the high-level semantic space to enhance the
long term dependency in the generation process. Extensive experiments based on
different text generation tasks demonstrate that the proposed AMI framework can
significantly outperform several strong baselines, and we also show that AMI
has potential to lead to a tighter lower bound of maximum mutual information
for the variational information maximization problem.",2020-06-30
Learning Sparse Prototypes for Text Generation,2020-06-29 19:41:26+00:00,http://arxiv.org/abs/2006.16336v2,"Junxian He, Taylor Berg-Kirkpatrick, Graham Neubig","cs.CL, cs.LG",knowledge,"Prototype-driven text generation uses non-parametric models that first choose
from a library of sentence ""prototypes"" and then modify the prototype to
generate the output text. While effective, these methods are inefficient at
test time as a result of needing to store and index the entire training corpus.
Further, existing methods often require heuristics to identify which prototypes
to reference at training time. In this paper, we propose a novel generative
model that automatically learns a sparse prototype support set that,
nonetheless, achieves strong language modeling performance. This is achieved by
(1) imposing a sparsity-inducing prior on the prototype selection distribution,
and (2) utilizing amortized variational inference to learn a prototype
retrieval function. In experiments, our model outperforms previous
prototype-driven language models while achieving up to a 1000x memory
reduction, as well as a 1000x speed-up at test time. More interestingly, we
show that the learned prototypes are able to capture semantics and syntax at
different granularity as we vary the sparsity of prototype selection, and that
certain sentence attributes can be controlled by specifying the prototype for
generation.",2020-06-29
"Efficient text generation of user-defined topic using generative
  adversarial networks",2020-06-22 04:49:47+00:00,http://arxiv.org/abs/2006.12005v1,"Chenhan Yuan, Yi-chin Huang, Cheng-Hung Tsai",cs.CL,knowledge,"This study focused on efficient text generation using generative adversarial
networks (GAN). Assuming that the goal is to generate a paragraph of a
user-defined topic and sentimental tendency, conventionally the whole network
has to be re-trained to obtain new results each time when a user changes the
topic. This would be time-consuming and impractical. Therefore, we propose a
User-Defined GAN (UD-GAN) with two-level discriminators to solve this problem.
The first discriminator aims to guide the generator to learn paragraph-level
information and sentence syntactic structure, which is constructed by
multiple-LSTMs. The second one copes with higher-level information, such as the
user-defined sentiment and topic for text generation. The cosine similarity
based on TF-IDF and length penalty are adopted to determine the relevance of
the topic. Then, the second discriminator is re-trained with the generator if
the topic or sentiment for text generation is modified. The system evaluations
are conducted to compare the performance of the proposed method with other
GAN-based ones. The objective results showed that the proposed method is
capable of generating texts with less time than others and the generated text
is related to the user-defined topic and sentiment. We will further investigate
the possibility of incorporating more detailed paragraph information such as
semantics into text generation to enhance the result.",2020-06-22
Explainable and Discourse Topic-aware Neural Language Understanding,2020-06-18 15:53:58+00:00,http://arxiv.org/abs/2006.10632v2,"Yatin Chaudhary, Hinrich Schütze, Pankaj Gupta","cs.CL, cs.AI, cs.LG",knowledge,"Marrying topic models and language models exposes language understanding to a
broader source of document-level context beyond sentences via topics. While
introducing topical semantics in language models, existing approaches
incorporate latent document topic proportions and ignore topical discourse in
sentences of the document. This work extends the line of research by
additionally introducing an explainable topic representation in language
understanding, obtained from a set of key terms correspondingly for each latent
topic of the proportion. Moreover, we retain sentence-topic associations along
with document-topic association by modeling topical discourse for every
sentence in the document. We present a novel neural composite language model
that exploits both the latent and explainable topics along with topical
discourse at sentence-level in a joint learning framework of topic and language
models. Experiments over a range of tasks such as language modeling, word sense
disambiguation, document classification, retrieval and text generation
demonstrate ability of the proposed model in improving language understanding.",2020-06-18
"Modeling Graph Structure via Relative Position for Better Text
  Generation from Knowledge Graphs",2020-06-16 15:20:04+00:00,http://arxiv.org/abs/2006.09242v1,"Martin Schmitt, Leonardo F. R. Ribeiro, Philipp Dufter, Iryna Gurevych, Hinrich Schütze",cs.CL,knowledge,"We present a novel encoder-decoder architecture for graph-to-text generation
based on Transformer, called the Graformer. With our novel graph
self-attention, every node in the input graph is taken into account for the
encoding of every other node - not only direct neighbors, facilitating the
detection of global patterns. For this, the relation between any two nodes is
characterized by the length of the shortest path between them, including the
special case when there is no such path. The Graformer learns to weigh these
node-node relations differently for different attention heads, thus virtually
learning differently connected views of the input graph. We evaluate the
Graformer on two graph-to-text generation benchmarks, the AGENDA dataset and
the WebNLG challenge dataset, where it achieves strong performance while using
significantly less parameters than other approaches.",2020-06-16
"Modelling Hierarchical Structure between Dialogue Policy and Natural
  Language Generator with Option Framework for Task-oriented Dialogue System",2020-06-11 20:55:28+00:00,http://arxiv.org/abs/2006.06814v3,"Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, Yunjie Gu","cs.CL, cs.AI, cs.LG",knowledge,"Designing task-oriented dialogue systems is a challenging research topic,
since it needs not only to generate utterances fulfilling user requests but
also to guarantee the comprehensibility. Many previous works trained end-to-end
(E2E) models with supervised learning (SL), however, the bias in annotated
system utterances remains as a bottleneck. Reinforcement learning (RL) deals
with the problem through using non-differentiable evaluation metrics (e.g., the
success rate) as rewards. Nonetheless, existing works with RL showed that the
comprehensibility of generated system utterances could be corrupted when
improving the performance on fulfilling user requests. In o gur work, we (1)
propose modelling the hierarchical structure between dialogue policy and
natural language generator (NLG) with the option framework, called HDNO, where
the latent dialogue act is applied to avoid designing specific dialogue act
representations; (2) train HDNO via hierarchical reinforcement learning (HRL),
as well as suggest the asynchronous updates between dialogue policy and NLG
during training to theoretically guarantee their convergence to a local
maximizer; and (3) propose using a discriminator modelled with language models
as an additional reward to further improve the comprehensibility. We test HDNO
on MultiWoz 2.0 and MultiWoz 2.1, the datasets on multi-domain dialogues, in
comparison with word-level E2E model trained with RL, LaRL and HDSA, showing
improvements on the performance evaluated by automatic evaluation metrics and
human evaluation. Finally, we demonstrate the semantic meanings of latent
dialogue acts to show the ability of explanation.",2020-06-11
"On the Effectiveness of Neural Text Generation based Data Augmentation
  for Recognition of Morphologically Rich Speech",2020-06-09 09:01:04+00:00,http://arxiv.org/abs/2006.05129v1,"Balázs Tarján, György Szaszák, Tibor Fegyó, Péter Mihajlik","eess.AS, cs.CL, cs.SD",knowledge,"Advanced neural network models have penetrated Automatic Speech Recognition
(ASR) in recent years, however, in language modeling many systems still rely on
traditional Back-off N-gram Language Models (BNLM) partly or entirely. The
reason for this are the high cost and complexity of training and using neural
language models, mostly possible by adding a second decoding pass (rescoring).
In our recent work we have significantly improved the online performance of a
conversational speech transcription system by transferring knowledge from a
Recurrent Neural Network Language Model (RNNLM) to the single pass BNLM with
text generation based data augmentation. In the present paper we analyze the
amount of transferable knowledge and demonstrate that the neural augmented LM
(RNN-BNLM) can help to capture almost 50% of the knowledge of the RNNLM yet by
dropping the second decoding pass and making the system real-time capable. We
also systematically compare word and subword LMs and show that subword-based
neural text augmentation can be especially beneficial in under-resourced
conditions. In addition, we show that using the RNN-BNLM in the first pass
followed by a neural second pass, offline ASR results can be even significantly
improved.",2020-06-09
ColdGANs: Taming Language GANs with Cautious Sampling Strategies,2020-06-08 14:48:14+00:00,http://arxiv.org/abs/2006.04643v1,"Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano","cs.CL, cs.LG",knowledge,"Training regimes based on Maximum Likelihood Estimation (MLE) suffer from
known limitations, often leading to poorly generated text sequences. At the
root of these limitations is the mismatch between training and inference, i.e.
the so-called exposure bias, exacerbated by considering only the reference
texts as correct, while in practice several alternative formulations could be
as good. Generative Adversarial Networks (GANs) can mitigate those limitations
but the discrete nature of text has hindered their application to language
generation: the approaches proposed so far, based on Reinforcement Learning,
have been shown to underperform MLE. Departing from previous works, we analyze
the exploration step in GANs applied to text generation, and show how classical
sampling results in unstable training. We propose to consider alternative
exploration strategies in a GAN framework that we name ColdGANs, where we force
the sampling to be close to the distribution modes to get smoother learning
dynamics. For the first time, to the best of our knowledge, the proposed
language GANs compare favorably to MLE, and obtain improvements over the
state-of-the-art on three generative tasks, namely unconditional text
generation, question generation, and abstractive summarization.",2020-06-08
"M3P: Learning Universal Representations via Multitask Multilingual
  Multimodal Pre-training",2020-06-04 03:54:29+00:00,http://arxiv.org/abs/2006.02635v1,"Haoyang Huang, Lin Su, Di Qi, Nan Duan, Edward Cui, Taroon Bharti, Lei Zhang, Lijuan Wang, Jianfeng Gao, Bei Liu, Jianlong Fu, Dongdong Zhang, Xin Liu, Ming Zhou","cs.CL, cs.CV",knowledge,"This paper presents a Multitask Multilingual Multimodal Pre-trained model
(M3P) that combines multilingual-monomodal pre-training and
monolingual-multimodal pre-training into a unified framework via multitask
learning and weight sharing. The model learns universal representations that
can map objects that occurred in different modalities or expressed in different
languages to vectors in a common semantic space. To verify the generalization
capability of M3P, we fine-tune the pre-trained model for different types of
downstream tasks: multilingual image-text retrieval, multilingual image
captioning, multimodal machine translation, multilingual natural language
inference and multilingual text generation. Evaluation shows that M3P can (i)
achieve comparable results on multilingual tasks and English multimodal tasks,
compared to the state-of-the-art models pre-trained for these two types of
tasks separately, and (ii) obtain new state-of-the-art results on non-English
multimodal tasks in the zero-shot or few-shot setting. We also build a new
Multilingual Image-Language Dataset (MILD) by collecting large amounts of
(text-query, image, context) triplets in 8 languages from the logs of a
commercial search engine",2020-06-04
"Graph-Stega: Semantic Controllable Steganographic Text Generation Guided
  by Knowledge Graph",2020-06-02 06:53:21+00:00,http://arxiv.org/abs/2006.08339v1,"Zhongliang Yang, Baitao Gong, Yamin Li, Jinshuai Yang, Zhiwen Hu, Yongfeng Huang","cs.CL, cs.CR",knowledge,"Most of the existing text generative steganographic methods are based on
coding the conditional probability distribution of each word during the
generation process, and then selecting specific words according to the secret
information, so as to achieve information hiding. Such methods have their
limitations which may bring potential security risks. Firstly, with the
increase of embedding rate, these models will choose words with lower
conditional probability, which will reduce the quality of the generated
steganographic texts; secondly, they can not control the semantic expression of
the final generated steganographic text. This paper proposes a new text
generative steganography method which is quietly different from the existing
models. We use a Knowledge Graph (KG) to guide the generation of steganographic
sentences. On the one hand, we hide the secret information by coding the path
in the knowledge graph, but not the conditional probability of each generated
word; on the other hand, we can control the semantic expression of the
generated steganographic text to a certain extent. The experimental results
show that the proposed model can guarantee both the quality of the generated
text and its semantic expression, which is a supplement and improvement to the
current text generation steganography.",2020-06-02
"Improving Disentangled Text Representation Learning with
  Information-Theoretic Guidance",2020-06-01 03:36:01+00:00,http://arxiv.org/abs/2006.00693v2,"Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yitong Li, Lawrence Carin","cs.LG, stat.ML",knowledge,"Learning disentangled representations of natural language is essential for
many NLP tasks, e.g., conditional text generation, style transfer, personalized
dialogue systems, etc. Similar problems have been studied extensively for other
forms of data, such as images and videos. However, the discrete nature of
natural language makes the disentangling of textual representations more
challenging (e.g., the manipulation over the data space cannot be easily
achieved). Inspired by information theory, we propose a novel method that
effectively manifests disentangled representations of text, without any
supervision on semantics. A new mutual information upper bound is derived and
leveraged to measure dependence between style and content. By minimizing this
upper bound, the proposed method induces style and content embeddings into two
independent low-dimensional spaces. Experiments on both conditional text
generation and text-style transfer demonstrate the high quality of our
disentangled representation in terms of content and style preservation.",2020-06-01
"MixingBoard: a Knowledgeable Stylized Integrated Text Generation
  Platform",2020-05-17 20:29:27+00:00,http://arxiv.org/abs/2005.08365v2,"Xiang Gao, Michel Galley, Bill Dolan",cs.CL,knowledge,"We present MixingBoard, a platform for quickly building demos with a focus on
knowledge grounded stylized text generation. We unify existing text generation
algorithms in a shared codebase and further adapt earlier algorithms for
constrained generation. To borrow advantages from different models, we
implement strategies for cross-model integration, from the token probability
level to the latent space level. An interface to external knowledge is provided
via a module that retrieves on-the-fly relevant knowledge from passages on the
web or any document collection. A user interface for local development, remote
webpage access, and a RESTful API are provided to make it simple for users to
build their own demos.",2020-05-17
Schema-Guided Natural Language Generation,2020-05-11 23:01:22+00:00,http://arxiv.org/abs/2005.05480v2,"Yuheng Du, Shereen Oraby, Vittorio Perera, Minmin Shen, Anjali Narayan-Chen, Tagyoung Chung, Anu Venkatesh, Dilek Hakkani-Tur",cs.CL,knowledge,"Neural network based approaches to data-to-text natural language generation
(NLG) have gained popularity in recent years, with the goal of generating a
natural language prompt that accurately realizes an input meaning
representation. To facilitate the training of neural network models,
researchers created large datasets of paired utterances and their meaning
representations. However, the creation of such datasets is an arduous task and
they mostly consist of simple meaning representations composed of slot and
value tokens to be realized. These representations do not include any
contextual information that an NLG system can use when trying to generalize,
such as domain information and descriptions of slots and values. In this paper,
we present the novel task of Schema-Guided Natural Language Generation
(SG-NLG). Here, the goal is still to generate a natural language prompt, but in
SG-NLG, the input MRs are paired with rich schemata providing contextual
information. To generate a dataset for SG-NLG we re-purpose an existing dataset
for another task: dialog state tracking, which includes a large and rich schema
spanning multiple different attributes, including information about the domain,
user intent, and slot descriptions. We train different state-of-the-art models
for neural natural language generation on this dataset and show that in many
cases, including rich schema information allows our models to produce higher
quality outputs both in terms of semantics and diversity. We also conduct
experiments comparing model performance on seen versus unseen domains, and
present a human evaluation demonstrating high ratings for overall output
quality.",2020-05-11
Posterior Control of Blackbox Generation,2020-05-10 03:22:45+00:00,http://arxiv.org/abs/2005.04560v1,"Xiang Lisa Li, Alexander M. Rush","cs.CL, cs.AI, cs.LG",knowledge,"Text generation often requires high-precision output that obeys task-specific
rules. This fine-grained control is difficult to enforce with off-the-shelf
deep learning models. In this work, we consider augmenting neural generation
models with discrete control states learned through a structured
latent-variable approach. Under this formulation, task-specific knowledge can
be encoded through a range of rich, posterior constraints that are effectively
trained into the model. This approach allows users to ground internal model
decisions based on prior knowledge, without sacrificing the representational
power of neural generative models. Experiments consider applications of this
approach for text generation. We find that this method improves over standard
benchmarks, while also providing fine-grained control.",2020-05-10
Smart To-Do : Automatic Generation of To-Do Items from Emails,2020-05-05 02:21:40+00:00,http://arxiv.org/abs/2005.06282v1,"Sudipto Mukherjee, Subhabrata Mukherjee, Marcello Hasegawa, Ahmed Hassan Awadallah, Ryen White","cs.CL, cs.AI, cs.LG",knowledge,"Intelligent features in email service applications aim to increase
productivity by helping people organize their folders, compose their emails and
respond to pending tasks. In this work, we explore a new application,
Smart-To-Do, that helps users with task management over emails. We introduce a
new task and dataset for automatically generating To-Do items from emails where
the sender has promised to perform an action. We design a two-stage process
leveraging recent advances in neural text generation and sequence-to-sequence
learning, obtaining BLEU and ROUGE scores of 0:23 and 0:63 for this task. To
the best of our knowledge, this is the first work to address the problem of
composing To-Do items from emails.",2020-05-05
Improving Adversarial Text Generation by Modeling the Distant Future,2020-05-04 05:45:13+00:00,http://arxiv.org/abs/2005.01279v1,"Ruiyi Zhang, Changyou Chen, Zhe Gan, Wenlin Wang, Dinghan Shen, Guoyin Wang, Zheng Wen, Lawrence Carin","cs.CL, cs.LG",knowledge,"Auto-regressive text generation models usually focus on local fluency, and
may cause inconsistent semantic meaning in long text generation. Further,
automatically generating words with similar semantics is challenging, and
hand-crafted linguistic rules are difficult to apply. We consider a text
planning scheme and present a model-based imitation-learning approach to
alleviate the aforementioned issues. Specifically, we propose a novel guider
network to focus on the generative process over a longer horizon, which can
assist next-word prediction and provide intermediate rewards for generator
optimization. Extensive experiments demonstrate that the proposed method leads
to improved performance.",2020-05-04
"Towards Faithful Neural Table-to-Text Generation with Content-Matching
  Constraints",2020-05-03 02:54:26+00:00,http://arxiv.org/abs/2005.00969v1,"Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, Changyou Chen",cs.CL,knowledge,"Text generation from a knowledge base aims to translate knowledge triples to
natural language descriptions. Most existing methods ignore the faithfulness
between a generated text description and the original table, leading to
generated information that goes beyond the content of the table. In this paper,
for the first time, we propose a novel Transformer-based generation framework
to achieve the goal. The core techniques in our method to enforce faithfulness
include a new table-text optimal-transport matching loss and a table-text
embedding similarity loss based on the Transformer model. Furthermore, to
evaluate faithfulness, we propose a new automatic metric specialized to the
table-to-text generation problem. We also provide detailed analysis on each
component of our model in our experiments. Automatic and human evaluations show
that our framework can significantly outperform state-of-the-art by a large
margin.",2020-05-03
APo-VAE: Text Generation in Hyperbolic Space,2020-04-30 19:05:41+00:00,http://arxiv.org/abs/2005.00054v1,"Shuyang Dai, Zhe Gan, Yu Cheng, Chenyang Tao, Lawrence Carin, Jingjing Liu","cs.LG, stat.ML",knowledge,"Natural language often exhibits inherent hierarchical structure ingrained
with complex syntax and semantics. However, most state-of-the-art deep
generative models learn embeddings only in Euclidean vector space, without
accounting for this structural property of language. In this paper, we
investigate text generation in a hyperbolic latent space to learn continuous
hierarchical representations. An Adversarial Poincare Variational Autoencoder
(APo-VAE) is presented, where both the prior and variational posterior of
latent variables are defined over a Poincare ball via wrapped normal
distributions. By adopting the primal-dual formulation of KL divergence, an
adversarial learning procedure is introduced to empower robust model training.
Extensive experiments in language modeling and dialog-response generation tasks
demonstrate the winning effectiveness of the proposed APo-VAE model over VAEs
in Euclidean latent space, thanks to its superb capabilities in capturing
latent language hierarchies in hyperbolic space.",2020-04-30
Context based Text-generation using LSTM networks,2020-04-30 18:39:25+00:00,http://arxiv.org/abs/2005.00048v1,Sivasurya Santhanam,"cs.CL, cs.LG",knowledge,"Long short-term memory(LSTM) units on sequence-based models are being used in
translation, question-answering systems, classification tasks due to their
capability of learning long-term dependencies. In Natural language generation,
LSTM networks are providing impressive results on text generation models by
learning language models with grammatically stable syntaxes. But the downside
is that the network does not learn about the context. The network only learns
the input-output function and generates text given a set of input words
irrespective of pragmatics. As the model is trained without any such context,
there is no semantic consistency among the generated sentences. The proposed
model is trained to generate text for a given set of input words along with a
context vector. A context vector is similar to a paragraph vector that grasps
the semantic meaning(context) of the sentence. Several methods of extracting
the context vectors are proposed in this work. While training a language model,
in addition to the input-output sequences, context vectors are also trained
along with the inputs. Due to this structure, the model learns the relation
among the input words, context vector and the target word. Given a set of
context terms, a well trained model will generate text around the provided
context. Based on the nature of computing context vectors, the model has been
tried out with two variations (word importance and word clustering). In the
word clustering method, the suitable embeddings among various domains are also
explored. The results are evaluated based on the semantic closeness of the
generated text to the given context.",2020-04-30
Template Guided Text Generation for Task-Oriented Dialogue,2020-04-30 17:51:08+00:00,http://arxiv.org/abs/2004.15006v2,"Mihir Kale, Abhinav Rastogi",cs.CL,knowledge,"Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri
enable users to interact with a large number of services and APIs on the web
using natural language. In this work, we investigate two methods for Natural
Language Generation (NLG) using a single domain-independent model across a
large number of APIs. First, we propose a schema-guided approach which
conditions the generation on a schema describing the API in natural language.
Our second method investigates the use of a small number of templates, growing
linearly in number of slots, to convey the semantics of the API. To generate
utterances for an arbitrary slot combination, a few simple templates are first
concatenated to give a semantically correct, but possibly incoherent and
ungrammatical utterance. A pre-trained language model is subsequently employed
to rewrite it into coherent, natural sounding text. Through automatic metrics
and human evaluation, we show that our method improves over strong baselines,
is robust to out-of-domain inputs and shows improved sample efficiency.",2020-04-30
ENT-DESC: Entity Description Generation by Exploring Knowledge Graph,2020-04-30 14:16:19+00:00,http://arxiv.org/abs/2004.14813v2,"Liying Cheng, Dekun Wu, Lidong Bing, Yan Zhang, Zhanming Jie, Wei Lu, Luo Si",cs.CL,knowledge,"Previous works on knowledge-to-text generation take as input a few RDF
triples or key-value pairs conveying the knowledge of some entities to generate
a natural language description. Existing datasets, such as WIKIBIO, WebNLG, and
E2E, basically have a good alignment between an input triple/pair set and its
output text. However, in practice, the input knowledge could be more than
enough, since the output description may only cover the most significant
knowledge. In this paper, we introduce a large-scale and challenging dataset to
facilitate the study of such a practical scenario in KG-to-text. Our dataset
involves retrieving abundant knowledge of various types of main entities from a
large knowledge graph (KG), which makes the current graph-to-sequence models
severely suffer from the problems of information loss and parameter explosion
while generating the descriptions. We address these challenges by proposing a
multi-graph structure that is able to represent the original graph information
more comprehensively. Furthermore, we also incorporate aggregation methods that
learn to extract the rich graph information. Extensive experiments demonstrate
the effectiveness of our model architecture.",2020-04-30
"Towards Unsupervised Language Understanding and Generation by Joint Dual
  Learning",2020-04-30 12:02:33+00:00,http://arxiv.org/abs/2004.14710v1,"Shang-Yu Su, Chao-Wei Huang, Yun-Nung Chen",cs.CL,knowledge,"In modular dialogue systems, natural language understanding (NLU) and natural
language generation (NLG) are two critical components, where NLU extracts the
semantics from the given texts and NLG is to construct corresponding natural
language sentences based on the input semantic representations. However, the
dual property between understanding and generation has been rarely explored.
The prior work is the first attempt that utilized the duality between NLU and
NLG to improve the performance via a dual supervised learning framework.
However, the prior work still learned both components in a supervised manner,
instead, this paper introduces a general learning framework to effectively
exploit such duality, providing flexibility of incorporating both supervised
and unsupervised learning algorithms to train language understanding and
generation models in a joint fashion. The benchmark experiments demonstrate
that the proposed approach is capable of boosting the performance of both NLU
and NLG.",2020-04-30
Logic2Text: High-Fidelity Natural Language Generation from Logical Forms,2020-04-30 04:06:06+00:00,http://arxiv.org/abs/2004.14579v2,"Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, William Yang Wang",cs.CL,knowledge,"Previous works on Natural Language Generation (NLG) from structured data have
primarily focused on surface-level descriptions of record sequences. However,
for complex structured data, e.g., multi-row tables, it is often desirable for
an NLG system to describe interesting facts from logical inferences across
records. If only provided with the table, it is hard for existing models to
produce controllable and high-fidelity logical generations. In this work, we
formulate logical level NLG as generation from logical forms in order to obtain
controllable, high-fidelity, and faithful generations. We present a new
large-scale dataset, \textsc{Logic2Text}, with 10,753 descriptions involving
common logic types paired with the underlying logical forms. The logical forms
show diversified graph structure of free schema, which poses great challenges
on the model's ability to understand the semantics. We experiment on (1)
Fully-supervised training with the full datasets, and (2) Few-shot setting,
provided with hundreds of paired examples; We compare several popular
generation models and analyze their performances. We hope our dataset can
encourage research towards building an advanced NLG system capable of natural,
faithful, and human-like generation. The dataset and code are available at
https://github.com/czyssrs/Logic2Text.",2020-04-30
"Learning to Encode Evolutionary Knowledge for Automatic Commenting Long
  Novels",2020-04-21 13:09:50+00:00,http://arxiv.org/abs/2004.09974v1,"Canxiang Yan, Jianhao Yan, Yangyin Xu, Cheng Niu, Jie Zhou","cs.CL, cs.LG",knowledge,"Static knowledge graph has been incorporated extensively into
sequence-to-sequence framework for text generation. While effectively
representing structured context, static knowledge graph failed to represent
knowledge evolution, which is required in modeling dynamic events. In this
paper, an automatic commenting task is proposed for long novels, which involves
understanding context of more than tens of thousands of words. To model the
dynamic storyline, especially the transitions of the characters and their
relations, Evolutionary Knowledge Graph(EKG) is proposed and learned within a
multi-task framework. Given a specific passage to comment, sequential modeling
is used to incorporate historical and future embedding for context
representation. Further, a graph-to-sequence model is designed to utilize the
EKG for comment generation. Extensive experimental results show that our
EKG-based method is superior to several strong baselines on both automatic and
human evaluations.",2020-04-21
