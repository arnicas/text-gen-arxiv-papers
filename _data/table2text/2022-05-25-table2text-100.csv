title,pubdate,id,authors,categories,search,abstract,displaydate
"Intermediate Training on Question Answering Datasets Improves Generative
  Data Augmentation",2022-05-25 09:28:21+00:00,http://arxiv.org/abs/2205.12604v1,"Dheeraj Mekala, Tu Vu, Jingbo Shang",cs.CL,table2text,"Manually annotating datasets requires domain experts to read through many
documents and carefully label them, which is often expensive. Recently,
pre-trained generative language models (GLMs) have demonstrated exceptional
abilities in generating text which motivates to leverage them for generative
data augmentation. We improve generative data augmentation by formulating the
data generation as context generation task and use question answering (QA)
datasets for intermediate training. Specifically, we view QA to be more as a
format than of a task and train GLMs as context generators for a given question
and its respective answer. Then, we cast downstream tasks into question
answering format and adapt the fine-tuned context generators to the target task
domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which
is further used as synthetic training data for their corresponding tasks. We
perform extensive experiments, case studies, and ablation studies on multiple
sentiment and topic classification datasets and demonstrate substantial
improvements in performance in few-shot, zero-shot settings. Remarkably, on the
SST-2 dataset, intermediate training on SocialIQA dataset achieves an
improvement of 40% on Macro-F1 score. Through thorough analyses, we observe
that QA datasets that requires high-level reasoning abilities (e.g.,
abstractive and common-sense QA datasets) tend to give the best boost in
performance in both few-shot and zero-shot settings.",2022-05-25
"RSTGen: Imbuing Fine-Grained Interpretable Control into Long-FormText
  Generators",2022-05-25 09:06:04+00:00,http://arxiv.org/abs/2205.12590v1,"Rilwan A. Adewoyin, Ritabrata Dutta, Yulan He",cs.CL,table2text,"In this paper, we study the task of improving the cohesion and coherence of
long-form text generated by language models. To this end, we propose RSTGen, a
framework that utilises Rhetorical Structure Theory (RST), a classical language
theory, to control the discourse structure, semantics and topics of generated
text. Firstly, we demonstrate our model's ability to control structural
discourse and semantic features of generated text in open generation
evaluation. Then we experiment on the two challenging long-form text tasks of
argument generation and story generation. Evaluation using automated metrics
and a metric with high correlation to human evaluation, shows that our model
performs competitively against existing models, while offering significantly
more controls over generated text than alternative methods.",2022-05-25
"The Dialog Must Go On: Improving Visual Dialog via Generative
  Self-Training",2022-05-25 05:40:00+00:00,http://arxiv.org/abs/2205.12502v1,"Gi-Cheon Kang, Sungdong Kim, Jin-Hwa Kim, Donghyun Kwak, Byoung-Tak Zhang","cs.CV, cs.CL, cs.LG",table2text,"Visual dialog (VisDial) is a task of answering a sequence of questions
grounded in an image, using the dialog history as context. Prior work has
trained the dialog agents solely on VisDial data via supervised learning or
leveraged pre-training on related vision-and-language datasets. This paper
presents a semi-supervised learning approach for visually-grounded dialog,
called Generative Self-Training (GST), to leverage unlabeled images on the Web.
Specifically, GST first retrieves in-domain images through out-of-distribution
detection and generates synthetic dialogs regarding the images via multimodal
conditional text generation. GST then trains a dialog agent on the synthetic
and the original VisDial data. As a result, GST scales the amount of training
data up to an order of magnitude that of VisDial (1.2M to 12.9M QA data). For
robust training of the generated dialogs, we also propose perplexity-based data
selection and multimodal consistency regularization. Evaluation on VisDial v1.0
and v0.9 datasets shows that GST achieves new state-of-the-art results on both
datasets. We further observe strong performance gains in the low-data regime
(up to 9.35 absolute points on NDCG).",2022-05-25
R2D2: Robust Data-to-Text with Replacement Detection,2022-05-25 03:29:25+00:00,http://arxiv.org/abs/2205.12467v1,"Linyong Nan, Lorenzo Jaime Yu Flores, Yilun Zhao, Yixin Liu, Luke Benson, Weijin Zou, Dragomir Radev",cs.CL,table2text,"Unfaithful text generation is a common problem for text generation systems.
In the case of Data-to-Text (D2T) systems, the factuality of the generated text
is particularly crucial for any real-world applications. We introduce R2D2, a
training framework that addresses unfaithful Data-to-Text generation by
training a system both as a generator and a faithfulness discriminator with
additional replacement detection and unlikelihood learning tasks. To facilitate
such training, we propose two methods for sampling unfaithful sentences. We
argue that the poor entity retrieval capability of D2T systems is one of the
primary sources of unfaithfulness, so in addition to the existing metrics, we
further propose NER-based metrics to evaluate the fidelity of D2T generations.
Our experimental results show that R2D2 systems could effectively mitigate the
unfaithful text generation, and they achieve new state-of-the-art results on
FeTaQA, LogicNLG, and ToTTo, all with significant improvements.",2022-05-25
"Medical Scientific Table-to-Text Generation with Human-in-the-Loop under
  the Data Sparsity Constraint",2022-05-24 21:10:57+00:00,http://arxiv.org/abs/2205.12368v1,"Heng-Yi Wu, Jingqing Zhang, Julia Ive, Tong Li, Narges Tabari, Bingyuan Chen, Vibhor Gupta, Yike Guo",cs.CL,table2text,"Structured (tabular) data in the preclinical and clinical domains contains
valuable information about individuals and an efficient table-to-text
summarization system can drastically reduce manual efforts to condense this
data into reports. However, in practice, the problem is heavily impeded by the
data paucity, data sparsity and inability of the state-of-the-art natural
language generation models (including T5, PEGASUS and GPT-Neo) to produce
accurate and reliable outputs. In this paper, we propose a novel table-to-text
approach and tackle these problems with a novel two-step architecture which is
enhanced by auto-correction, copy mechanism and synthetic data augmentation.
The study shows that the proposed approach selects salient biomedical entities
and values from structured data with improved precision (up to 0.13 absolute
increase) of copying the tabular values to generate coherent and accurate text
for assay validation reports and toxicology reports. Moreover, we also
demonstrate a light-weight adaptation of the proposed system to new datasets by
fine-tuning with as little as 40\% training examples. The outputs of our model
are validated by human experts in the Human-in-the-Loop scenario.",2022-05-24
"DivEMT: Neural Machine Translation Post-Editing Effort Across
  Typologically Diverse Languages",2022-05-24 17:22:52+00:00,http://arxiv.org/abs/2205.12215v1,"Gabriele Sarti, Arianna Bisazza, Ana Guerberof Arenas, Antonio Toral",cs.CL,table2text,"We introduce DivEMT, the first publicly available post-editing study of
Neural Machine Translation (NMT) over a typologically diverse set of target
languages. Using a strictly controlled setup, 18 professional translators were
instructed to translate or post-edit the same set of English documents into
Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process,
their edits, keystrokes, editing times, pauses, and perceived effort were
recorded, enabling an in-depth, cross-lingual evaluation of NMT quality and its
post-editing process. Using this new dataset, we assess the impact on
translation productivity of two state-of-the-art NMT systems, namely: Google
Translate and the open-source multilingual model mBART50. We find that, while
post-editing is consistently faster than translation from scratch, the
magnitude of its contribution varies largely across systems and languages,
ranging from doubled productivity in Dutch and Italian to marginal gains in
Arabic, Turkish and Ukrainian, for some of the evaluated modalities. Moreover,
the observed cross-language variability appears to partly reflect source-target
relatedness and type of target morphology, while remaining hard to predict even
based on state-of-the-art automatic MT quality metrics. We publicly release the
complete dataset, including all collected behavioural data, to foster new
research on the ability of state-of-the-art NMT systems to generate text in
typologically diverse languages.",2022-05-24
"Rethinking Evaluation Practices in Visual Question Answering: A Case
  Study on Out-of-Distribution Generalization",2022-05-24 16:44:45+00:00,http://arxiv.org/abs/2205.12191v1,"Aishwarya Agrawal, Ivana Kajić, Emanuele Bugliarello, Elnaz Davoodi, Anita Gergely, Phil Blunsom, Aida Nematzadeh","cs.CL, cs.AI, cs.CV, cs.LG",table2text,"Vision-and-language (V&L) models pretrained on large-scale multimodal data
have demonstrated strong performance on various tasks such as image captioning
and visual question answering (VQA). The quality of such models is commonly
assessed by measuring their performance on unseen data that typically comes
from the same distribution as the training data. However, we observe that these
models exhibit poor out-of-distribution (OOD) generalization on the task of
VQA. To better understand the underlying causes of poor generalization, we
comprehensively investigate performance of two pretrained V&L models under
different settings (i.e. classification and open-ended text generation) by
conducting cross-dataset evaluations. We find that these models tend to learn
to solve the benchmark, rather than learning the high-level skills required by
the VQA task. We also argue that in most cases generative models are less
susceptible to shifts in data distribution, while frequently performing better
on our tested benchmarks. Moreover, we find that multimodal pretraining
improves OOD performance in most settings. Finally, we revisit assumptions
underlying the use of automatic VQA evaluation metrics, and empirically show
that their stringent nature repeatedly penalizes models for correct responses.",2022-05-24
"On Advances in Text Generation from Images Beyond Captioning: A Case
  Study in Self-Rationalization",2022-05-24 00:52:40+00:00,http://arxiv.org/abs/2205.11686v1,"Shruti Palaskar, Akshita Bhagia, Yonatan Bisk, Florian Metze, Alan W Black, Ana Marasovic","cs.CL, cs.CV",table2text,"Integrating vision and language has gained notable attention following the
success of pretrained language models. Despite that, a fraction of emerging
multimodal models is suitable for text generation conditioned on images. This
minority is typically developed and evaluated for image captioning, a text
generation task conditioned solely on images with the goal to describe what is
explicitly visible in an image. In this paper, we take a step back and ask: How
do these models work for more complex generative tasks, conditioned on both
text and images? Are models based on joint multimodal pretraining, visually
adapted pretrained language models, or models that combine these two
approaches, more promising for such tasks? We address these questions in the
context of self-rationalization (jointly generating task labels/answers and
free-text explanations) of three tasks: (i) visual question answering in VQA-X,
(ii) visual commonsense reasoning in VCR, and (iii) visual-textual entailment
in E-SNLI-VE. We show that recent advances in each modality, CLIP image
representations and scaling of language models, do not consistently improve
multimodal self-rationalization of tasks with multimodal inputs. We also
observe that no model type works universally the best across tasks/datasets and
finetuning data sizes. Our findings call for a backbone modelling approach that
can be built on to advance text generation from images and text beyond image
captioning.",2022-05-24
What Makes Data-to-Text Generation Hard for Pretrained Language Models?,2022-05-23 17:58:39+00:00,http://arxiv.org/abs/2205.11505v1,"Moniba Keymanesh, Adrian Benton, Mark Dredze","cs.CL, cs.AI, cs.IR, cs.LG",table2text,"Expressing natural language descriptions of structured facts or relations --
data-to-text generation (D2T) -- increases the accessibility of structured
knowledge repositories. Previous work shows that pre-trained language
models(PLMs) perform remarkably well on this task after fine-tuning on a
significant amount of task-specific training data. On the other hand, while
auto-regressive PLMs can generalize from a few task examples, their efficacy at
D2T is largely unexplored. Furthermore, we have an incomplete understanding of
the limits of PLMs on D2T.
  In this work, we conduct an empirical study of both fine-tuned and
auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their
performance as a function of the amount of task-specific data and how these
data are incorporated into the models: zero and few-shot learning, and
fine-tuning of model weights. In addition, we probe the limits of PLMs by
measuring performance on subsets of the evaluation data: novel predicates and
abstractive test examples. To improve the performance on these subsets, we
investigate two techniques: providing predicate descriptions in the context and
re-ranking generated candidates by information reflected in the source.
Finally, we conduct a human evaluation of model errors and show that D2T
generation tasks would benefit from datasets with more careful manual curation.",2022-05-23
A Self-Paced Mixed Distillation Method for Non-Autoregressive Generation,2022-05-23 09:54:53+00:00,http://arxiv.org/abs/2205.11162v1,"Weizhen Qi, Yeyun Gong, Yelong Shen, Jian Jiao, Yu Yan, Houqiang Li, Ruofei Zhang, Weizhu Chen, Nan Duan",cs.CL,table2text,"Non-Autoregressive generation is a sequence generation paradigm, which
removes the dependency between target tokens. It could efficiently reduce the
text generation latency with parallel decoding in place of token-by-token
sequential decoding. However, due to the known multi-modality problem,
Non-Autoregressive (NAR) models significantly under-perform Auto-regressive
(AR) models on various language generation tasks. Among the NAR models, BANG is
the first large-scale pre-training model on English un-labeled raw text corpus.
It considers different generation paradigms as its pre-training tasks including
Auto-regressive (AR), Non-Autoregressive (NAR), and semi-Non-Autoregressive
(semi-NAR) information flow with multi-stream strategy. It achieves
state-of-the-art performance without any distillation techniques. However, AR
distillation has been shown to be a very effective solution for improving NAR
performance. In this paper, we propose a novel self-paced mixed distillation
method to further improve the generation quality of BANG. Firstly, we propose
the mixed distillation strategy based on the AR stream knowledge. Secondly, we
encourage the model to focus on the samples with the same modality by
self-paced learning. The proposed self-paced mixed distillation algorithm
improves the generation quality and has no influence on the inference latency.
We carry out extensive experiments on summarization and question generation
tasks to validate the effectiveness. To further illustrate the commercial value
of our approach, we conduct experiments on three generation tasks in real-world
advertisements applications. Experimental results on commercial data show the
effectiveness of the proposed model. Compared with BANG, it achieves
significant BLEU score improvement. On the other hand, compared with
auto-regressive generation method, it achieves more than 7x speedup.",2022-05-23
"BanglaNLG: Benchmarks and Resources for Evaluating Low-Resource Natural
  Language Generation in Bangla",2022-05-23 06:54:56+00:00,http://arxiv.org/abs/2205.11081v2,"Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",cs.CL,table2text,"This work presents BanglaNLG, a comprehensive benchmark for evaluating
natural language generation (NLG) models in Bangla, a widely spoken yet
low-resource language in the web domain. We aggregate three challenging
conditional text generation tasks under the BanglaNLG benchmark. Then, using a
clean corpus of 27.5 GB of Bangla data, we pretrain BanglaT5, a
sequence-to-sequence Transformer model for Bangla. BanglaT5 achieves
state-of-the-art performance in all of these tasks, outperforming mT5 (base) by
up to 5.4%. We are making the BanglaT5 language model and a leaderboard
publicly available in the hope of advancing future research and evaluation on
Bangla NLG. The resources can be found at
https://github.com/csebuetnlp/BanglaNLG.",2022-05-23
TempLM: Distilling Language Models into Template-Based Generators,2022-05-23 05:46:59+00:00,http://arxiv.org/abs/2205.11055v1,"Tianyi Zhang, Mina Lee, Lisa Li, Ende Shen, Tatsunori B. Hashimoto","cs.CL, cs.LG",table2text,"While pretrained language models (PLMs) have greatly improved text
generation, they have also been known to produce unfaithful or inappropriate
content. In contrast, classic template-based systems provide strong guarantees
of faithfulness at the cost of fluency. We propose TempLM, which achieves the
best of both worlds by distilling a PLM into a template-based generator. On the
E2E and SynthBio data-to-text datasets, we show that TempLM is more faithful
than the original PLM and is more fluent than prior template systems. Notably,
on an out-of-domain evaluation, TempLM reduces a finetuned BART model's
unfaithfulness rate from 83% to 0%. In a human study, we find that TempLM's
templates substantially improve upon human-written ones in BERTScore.",2022-05-23
Diversity Enhanced Table-to-Text Generation via Type Control,2022-05-22 22:05:21+00:00,http://arxiv.org/abs/2205.10938v1,"Yotam Perlitz, Liat Ein-Dot, Dafna Sheinwald, Noam Slonim, Michal Shmueli-Scheuer",cs.CL,table2text,"Generating natural language statements to convey information from tabular
data (i.e., Table-to-text) is a process with one input and a variety of valid
outputs. This characteristic underscores the abilities to control the
generation and produce a diverse set of outputs as two key assets. Thus, we
propose a diversity enhancing scheme that builds upon an inherent property of
the statements, namely, their logic-types, by using a type-controlled
Table-to-text generation model. Employing automatic and manual tests, we prove
its twofold advantage: users can effectively tune the generated statement type,
and, by sampling different types, can obtain a diverse set of statements for a
given table.",2022-05-22
"Language Models with Image Descriptors are Strong Few-Shot
  Video-Language Learners",2022-05-22 05:18:27+00:00,http://arxiv.org/abs/2205.10747v2,"Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal, Heng Ji","cs.CV, cs.AI",table2text,"The goal of this work is to build flexible video-language models that can
generalize to various video-to-text tasks from few examples, such as
domain-specific captioning, question answering, and future event prediction.
Existing few-shot video-language learners focus exclusively on the encoder,
resulting in the absence of a video-to-text decoder to handle generative tasks.
Video captioners have been pretrained on large-scale video-language datasets,
but they rely heavily on finetuning and lack the ability to generate text for
unseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language
Learner via Image and Language models, which demonstrates strong performance on
few-shot video-to-text tasks without the necessity of pretraining or finetuning
on any video datasets. We use the image-language models to translate the video
content into frame captions, object, attribute, and event phrases, and compose
them into a temporal structure template. We then instruct a language model,
with a prompt containing a few in-context examples, to generate a target output
from the composed content. The flexibility of prompting allows the model to
capture any form of text input, such as automatic speech recognition (ASR)
transcripts. Our experiments demonstrate the power of language models in
understanding videos on a wide variety of video-language tasks, including video
captioning, video question answering, video caption retrieval, and video future
event prediction. Especially, on video future event prediction, our few-shot
model significantly outperforms state-of-the-art supervised models trained on
large-scale video datasets. Code and resources are publicly available for
research purposes at https://github.com/MikeWangWZHL/VidIL .",2022-05-22
"ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language
  Generation",2022-05-13 06:08:35+00:00,http://arxiv.org/abs/2205.06457v2,"Long Phan, Hieu Tran, Hieu Nguyen, Trieu H. Trinh","cs.CL, cs.AI",table2text,"We present ViT5, a pretrained Transformer-based encoder-decoder model for the
Vietnamese language. With T5-style self-supervised pretraining, ViT5 is trained
on a large corpus of high-quality and diverse Vietnamese texts. We benchmark
ViT5 on two downstream text generation tasks, Abstractive Text Summarization
and Named Entity Recognition. Although Abstractive Text Summarization has been
widely studied for the English language thanks to its rich and large source of
data, there has been minimal research into the same task in Vietnamese, a much
lower resource language. In this work, we perform exhaustive experiments on
both Vietnamese Abstractive Summarization and Named Entity Recognition,
validating the performance of ViT5 against many other pretrained
Transformer-based encoder-decoder models. Our experiments show that ViT5
significantly outperforms existing models and achieves state-of-the-art results
on Vietnamese Text Summarization. On the task of Named Entity Recognition, ViT5
is competitive against previous best results from pretrained encoder-based
Transformer models. Further analysis shows the importance of context length
during the self-supervised pretraining on downstream performance across
different settings.",2022-05-13
"Analysing the Robustness of Dual Encoders for Dense Retrieval Against
  Misspellings",2022-05-04 19:48:13+00:00,http://arxiv.org/abs/2205.02303v1,"Georgios Sidiropoulos, Evangelos Kanoulas",cs.IR,table2text,"Dense retrieval is becoming one of the standard approaches for document and
passage ranking. The dual-encoder architecture is widely adopted for scoring
question-passage pairs due to its efficiency and high performance. Typically,
dense retrieval models are evaluated on clean and curated datasets. However,
when deployed in real-life applications, these models encounter noisy
user-generated text. That said, the performance of state-of-the-art dense
retrievers can substantially deteriorate when exposed to noisy text. In this
work, we study the robustness of dense retrievers against typos in the user
question. We observe a significant drop in the performance of the dual-encoder
model when encountering typos and explore ways to improve its robustness by
combining data augmentation with contrastive learning. Our experiments on two
large-scale passage ranking and open-domain question answering datasets show
that our proposed approach outperforms competing approaches. Additionally, we
perform a thorough analysis on robustness. Finally, we provide insights on how
different typos affect the robustness of embeddings differently and how our
method alleviates the effect of some typos but not of others.",2022-05-04
Learning to Transfer Prompts for Text Generation,2022-05-03 14:53:48+00:00,http://arxiv.org/abs/2205.01543v1,"Junyi Li, Tianyi Tang, Jian-Yun Nie, Ji-Rong Wen, Wayne Xin Zhao",cs.CL,table2text,"Pretrained language models (PLMs) have made remarkable progress in text
generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in
a data-scarce situation. Therefore, it is non-trivial to develop a general and
lightweight model that can adapt to various text generation tasks based on
PLMs. To fulfill this purpose, the recent prompt-based learning offers a
potential solution. In this paper, we improve this technique and propose a
novel prompt-based method (PTG) for text generation in a transferable setting.
First, PTG learns a set of source prompts for various source generation tasks
and then transfers these prompts as target prompts to perform target generation
tasks. To consider both task- and instance-level information, we design an
adaptive attention mechanism to derive the target prompts. For each data
instance, PTG learns a specific target prompt by attending to highly relevant
source prompts. In extensive experiments, PTG yields competitive or better
results than fine-tuning methods. We release our source prompts as an open
resource, where users can add or reuse them to improve new text generation
tasks for future research. Code and data can be available at
https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.",2022-05-03
"LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense
  Passage Retrieval",2022-05-02 15:15:41+00:00,http://arxiv.org/abs/2205.00970v1,"Yifan Wang, Haodi Ma, Daisy Zhe Wang","cs.IR, cs.DB",table2text,"Text retrieval using dense embeddings generated from deep neural models is
called ""dense passage retrieval"". Dense passage retrieval systems normally
deploy a deep neural model followed by an approximate nearest neighbor (ANN)
search module. The model generates text embeddings, which are then indexed by
the ANN module. With the increasing data scale, the ANN module unavoidably
becomes the bottleneck on efficiency, because of its linear or sublinear time
complexity with data scale. An alternative is the learned index which has a
theoretically constant time complexity. But most of the existing learned
indexes are designed for low dimensional data. Thus they are not suitable for
dense passage retrieval tasks with high-dimensional dense embeddings. We
propose LIDER, an efficient high-dimensional Learned Index for large-scale
DEnse passage Retrieval. LIDER has a clustering-based hierarchical architecture
formed by two layers of core models. As the basic unit of LIDER to index and
search data, each core model includes an adapted recursive model index (RMI)
and a dimension reduction component which consists of an extended
SortingKeys-LSH (SK-LSH) and a key re-scaling module. The dimension reduction
component reduces the high-dimensional dense embeddings into one-dimensional
keys and sorts them in a specific order, which are then used by the RMI. And
the RMI consists of multiple simple linear regression models that make fast
prediction in only O(1) time. We successfully optimize and combine SK-LSH and
RMI together into the core model, and organize multiple core models into a
two-layer structure based on a clustering-based partitioning of the whole data
space. Experiments show that LIDER has a higher search speed with high
retrieval quality comparing to the state-of-the-art ANN indexes commonly used
in dense passage retrieval. Furthermore, LIDER has a better capability of
speed-quality trade-off.",2022-05-02
"NMTScore: A Multilingual Analysis of Translation-based Text Similarity
  Measures",2022-04-28 17:57:17+00:00,http://arxiv.org/abs/2204.13692v1,"Jannis Vamvas, Rico Sennrich",cs.CL,table2text,"Being able to rank the similarity of short text segments is an interesting
bonus feature of neural machine translation. Translation-based similarity
measures include direct and pivot translation probability, as well as
translation cross-likelihood, which has not been studied so far. We analyze
these measures in the common framework of multilingual NMT, releasing the
NMTScore library (available at https://github.com/ZurichNLP/nmtscore). Compared
to baselines such as sentence embeddings, translation-based measures prove
competitive in paraphrase identification and are more robust against
adversarial or multilingual input, especially if proper normalization is
applied. When used for reference-based evaluation of data-to-text generation in
2 tasks and 17 languages, translation-based measures show a relatively high
correlation to human judgments.",2022-04-28
"Crystal Transformer: Self-learning neural language model for Generative
  and Tinkering Design of Materials",2022-04-25 20:20:26+00:00,http://arxiv.org/abs/2204.11953v1,"Lai Wei, Qinyang Li, Yuqi Song, Stanislav Stefanov, Edirisuriya M. D. Siriwardane, Fanglin Chen, Jianjun Hu","cond-mat.mtrl-sci, cs.LG",table2text,"Self-supervised neural language models have recently achieved unprecedented
success, from natural language processing to learning the languages of
biological sequences and organic molecules. These models have demonstrated
superior performance in the generation, structure classification, and
functional predictions for proteins and molecules with learned representations.
However, most of the masking-based pre-trained language models are not designed
for generative design, and their black-box nature makes it difficult to
interpret their design logic. Here we propose BLMM Crystal Transformer, a
neural network based probabilistic generative model for generative and
tinkering design of inorganic materials. Our model is built on the blank
filling language model for text generation and has demonstrated unique
advantages in learning the ""materials grammars"" together with high-quality
generation, interpretability, and data efficiency. It can generate chemically
valid materials compositions with as high as 89.7\% charge neutrality and
84.8\% balanced electronegativity, which are more than 4 and 8 times higher
compared to a pseudo random sampling baseline. The probabilistic generation
process of BLMM allows it to recommend tinkering operations based on learned
materials chemistry and makes it useful for materials doping. Combined with the
TCSP crysal structure prediction algorithm, We have applied our model to
discover a set of new materials as validated using DFT calculations. Our work
thus brings the unsupervised transformer language models based generative
artificial intelligence to inorganic materials. A user-friendly web app has
been developed for computational materials doping and can be accessed freely at
\url{www.materialsatlas.org/blmtinker}.",2022-04-25
"Recovering Patient Journeys: A Corpus of Biomedical Entities and
  Relations on Twitter (BEAR)",2022-04-21 08:18:44+00:00,http://arxiv.org/abs/2204.09952v1,"Amelie Wührl, Roman Klinger","cs.CL, cs.IR",table2text,"Text mining and information extraction for the medical domain has focused on
scientific text generated by researchers. However, their direct access to
individual patient experiences or patient-doctor interactions can be limited.
Information provided on social media, e.g., by patients and their relatives,
complements the knowledge in scientific text. It reflects the patient's journey
and their subjective perspective on the process of developing symptoms, being
diagnosed and offered a treatment, being cured or learning to live with a
medical condition. The value of this type of data is therefore twofold:
Firstly, it offers direct access to people's perspectives. Secondly, it might
cover information that is not available elsewhere, including self-treatment or
self-diagnoses. Named entity recognition and relation extraction are methods to
structure information that is available in unstructured text. However, existing
medical social media corpora focused on a comparably small set of entities and
relations and particular domains, rather than putting the patient into the
center of analyses. With this paper we contribute a corpus with a rich set of
annotation layers following the motivation to uncover and model patients'
journeys and experiences in more detail. We label 14 entity classes (incl.
environmental factors, diagnostics, biochemical processes, patients'
quality-of-life descriptions, pathogens, medical conditions, and treatments)
and 20 relation classes (e.g., prevents, influences, interactions, causes) most
of which have not been considered before for social media data. The publicly
available dataset consists of 2,100 tweets with approx. 6,000 entity and 3,000
relation annotations. In a corpus analysis we find that over 80 % of documents
contain relevant entities. Over 50 % of tweets express relations which we
consider essential for uncovering patients' narratives about their journeys.",2022-04-21
"A Survey on Non-Autoregressive Generation for Neural Machine Translation
  and Beyond",2022-04-20 07:25:22+00:00,http://arxiv.org/abs/2204.09269v1,"Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, Tie-yan Liu","cs.CL, cs.LG",table2text,"Non-autoregressive (NAR) generation, which is first proposed in neural
machine translation (NMT) to speed up inference, has attracted much attention
in both machine learning and natural language processing communities. While NAR
generation can significantly accelerate inference speed for machine
translation, the speedup comes at the cost of sacrificed translation accuracy
compared to its counterpart, auto-regressive (AR) generation. In recent years,
many new models and algorithms have been designed/proposed to bridge the
accuracy gap between NAR generation and AR generation. In this paper, we
conduct a systematic survey with comparisons and discussions of various
non-autoregressive translation (NAT) models from different aspects.
Specifically, we categorize the efforts of NAT into several groups, including
data manipulation, modeling methods, training criterion, decoding algorithms,
and the benefit from pre-trained models. Furthermore, we briefly review other
applications of NAR models beyond machine translation, such as dialogue
generation, text summarization, grammar error correction, semantic parsing,
speech synthesis, and automatic speech recognition. In addition, we also
discuss potential directions for future exploration, including releasing the
dependency of KD, dynamic length prediction, pre-training for NAR, and wider
applications, etc. We hope this survey can help researchers capture the latest
progress in NAR generation, inspire the design of advanced NAR models and
algorithms, and enable industry practitioners to choose appropriate solutions
for their applications. The web page of this survey is at
\url{https://github.com/LitterBrother-Xiao/Overview-of-Non-autoregressive-Applications}.",2022-04-20
"Evaluating Mixed-initiative Conversational Search Systems via User
  Simulation",2022-04-17 16:27:33+00:00,http://arxiv.org/abs/2204.08046v1,"Ivan Sekulić, Mohammad Aliannejadi, Fabio Crestani","cs.CL, cs.IR",table2text,"Clarifying the underlying user information need by asking clarifying
questions is an important feature of modern conversational search system.
However, evaluation of such systems through answering prompted clarifying
questions requires significant human effort, which can be time-consuming and
expensive. In this paper, we propose a conversational User Simulator, called
USi, for automatic evaluation of such conversational search systems. Given a
description of an information need, USi is capable of automatically answering
clarifying questions about the topic throughout the search session. Through a
set of experiments, including automated natural language generation metrics and
crowdsourcing studies, we show that responses generated by USi are both inline
with the underlying information need and comparable to human-generated answers.
Moreover, we make the first steps towards multi-turn interactions, where
conversational search systems asks multiple questions to the (simulated) user
with a goal of clarifying the user need. To this end, we expand on currently
available datasets for studying clarifying questions, i.e., Qulac and ClariQ,
by performing a crowdsourcing-based multi-turn data acquisition. We show that
our generative, GPT2-based model, is capable of providing accurate and natural
answers to unseen clarifying questions in the single-turn setting and discuss
capabilities of our model in the multi-turn setting. We provide the code, data,
and the pre-trained model to be used for further research on the topic.",2022-04-17
Learning to Adapt Domain Shifts of Moral Values via Instance Weighting,2022-04-15 18:15:41+00:00,http://arxiv.org/abs/2204.07603v1,"Xiaolei Huang, Alexandra Wormley, Adam Cohen","cs.CL, cs.SI",table2text,"Classifying moral values in user-generated text from social media is critical
in understanding community cultures and interpreting user behaviors of social
movements. Moral values and language usage can change across the social
movements; however, text classifiers are usually trained in source domains of
existing social movements and tested in target domains of new social issues
without considering the variations. In this study, we examine domain shifts of
moral values and language usage, quantify the effects of domain shifts on the
morality classification task, and propose a neural adaptation framework via
instance weighting to improve cross-domain classification tasks. The
quantification analysis suggests a strong correlation between morality shifts,
language usage, and classification performance. We evaluate the neural
adaptation framework on a public Twitter data across 7 social movements and
gain classification improvements up to 12.1\%. Finally, we release a new data
of the COVID-19 vaccine labeled with moral values and evaluate our approach on
the new target domain. For the case study of the COVID-19 vaccine, our
adaptation framework achieves up to 5.26\% improvements over neural baselines.",2022-04-15
Text Revision by On-the-Fly Representation Optimization,2022-04-15 07:38:08+00:00,http://arxiv.org/abs/2204.07359v1,"Jingjing Li, Zichao Li, Tao Ge, Irwin King, Michael R. Lyu",cs.CL,table2text,"Text revision refers to a family of natural language generation tasks, where
the source and target sequences share moderate resemblance in surface form but
differentiate in attributes, such as text formality and simplicity. Current
state-of-the-art methods formulate these tasks as sequence-to-sequence learning
problems, which rely on large-scale parallel training corpus. In this paper, we
present an iterative in-place editing approach for text revision, which
requires no parallel data. In this approach, we simply fine-tune a pre-trained
Transformer with masked language modeling and attribute classification. During
inference, the editing at each iteration is realized by two-step span
replacement. At the first step, the distributed representation of the text
optimizes on the fly towards an attribute function. At the second step, a text
span is masked and another new one is proposed conditioned on the optimized
representation. The empirical experiments on two typical and important text
revision tasks, text formalization and text simplification, show the
effectiveness of our approach. It achieves competitive and even better
performance than state-of-the-art supervised methods on text simplification,
and gains better performance than strong unsupervised methods on text
formalization \footnote{Code and model are available at
\url{https://github.com/jingjingli01/OREO}}.",2022-04-15
"Rows from Many Sources: Enriching row completions from Wikidata with a
  pre-trained Language Model",2022-04-14 15:11:52+00:00,http://arxiv.org/abs/2204.07014v1,"Carina Negreanu, Alperen Karaoglu, Jack Williams, Shuang Chen, Daniel Fabian, Andrew Gordon, Chin-Yew Lin","cs.CL, cs.AI",table2text,"Row completion is the task of augmenting a given table of text and numbers
with additional, relevant rows. The task divides into two steps: subject
suggestion, the task of populating the main column; and gap filling, the task
of populating the remaining columns. We present state-of-the-art results for
subject suggestion and gap filling measured on a standard benchmark
(WikiTables). Our idea is to solve this task by harmoniously combining
knowledge base table interpretation and free text generation. We interpret the
table using the knowledge base to suggest new rows and generate metadata like
headers through property linking. To improve candidate diversity, we synthesize
additional rows using free text generation via GPT-3, and crucially, we exploit
the metadata we interpret to produce better prompts for text generation.
Finally, we verify that the additional synthesized content can be linked to the
knowledge base or a trusted web source such as Wikipedia.",2022-04-14
"GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text
  Generation",2022-04-13 23:53:37+00:00,http://arxiv.org/abs/2204.06674v1,"Anthony Colas, Mehrdad Alvandipour, Daisy Zhe Wang",cs.CL,table2text,"Recent improvements in KG-to-text generation are due to additional auxiliary
pre-trained tasks designed to give the fine-tune task a boost in performance.
These tasks require extensive computational resources while only suggesting
marginal improvements. Here, we demonstrate that by fusing graph-aware elements
into existing pre-trained language models, we are able to outperform
state-of-the-art models and close the gap imposed by additional pre-train
tasks. We do so by proposing a mask structure to capture neighborhood
information and a novel type encoder that adds a bias to the graph-attention
weights depending on the connection type. Experiments on two KG-to-text
benchmark datasets show these models to be superior in quality while involving
fewer parameters and no additional pre-trained tasks. By formulating the
problem as a framework, we can interchange the various proposed components and
begin interpreting KG-to-text generative models based on the topological and
type information found in a graph.",2022-04-13
"Generating Full Length Wikipedia Biographies: The Impact of Gender Bias
  on the Retrieval-Based Generation of Women Biographies",2022-04-12 15:16:57+00:00,http://arxiv.org/abs/2204.05879v1,"Angela Fan, Claire Gardent",cs.CL,table2text,"Generating factual, long-form text such as Wikipedia articles raises three
key challenges: how to gather relevant evidence, how to structure information
into well-formed text, and how to ensure that the generated text is factually
correct. We address these by developing a model for English text that uses a
retrieval mechanism to identify relevant supporting information on the web and
a cache-based pre-trained encoder-decoder to generate long-form biographies
section by section, including citation information. To assess the impact of
available web evidence on the output text, we compare the performance of our
approach when generating biographies about women (for which less information is
available on the web) vs. biographies generally. To this end, we curate a
dataset of 1,500 biographies about women. We analyze our generated text to
understand how differences in available web evidence data affect generation. We
evaluate the factuality, fluency, and quality of the generated texts using
automatic metrics and human evaluation. We hope that these techniques can be
used as a starting point for human writers, to aid in reducing the complexity
inherent in the creation of long-form, factual text.",2022-04-12
Toward More Effective Human Evaluation for Machine Translation,2022-04-11 17:59:22+00:00,http://arxiv.org/abs/2204.05307v1,"Belén Saldías, George Foster, Markus Freitag, Qijun Tan","cs.CL, cs.LG",table2text,"Improvements in text generation technologies such as machine translation have
necessitated more costly and time-consuming human evaluation procedures to
ensure an accurate signal. We investigate a simple way to reduce cost by
reducing the number of text segments that must be annotated in order to
accurately predict a score for a complete test set. Using a sampling approach,
we demonstrate that information from document membership and automatic metrics
can help improve estimates compared to a pure random sampling baseline. We
achieve gains of up to 20% in average absolute error by leveraging stratified
sampling and control variates. Our techniques can improve estimates made from a
fixed annotation budget, are easy to implement, and can be applied to any
problem with structure similar to the one we study.",2022-04-11
TRUE: Re-evaluating Factual Consistency Evaluation,2022-04-11 10:14:35+00:00,http://arxiv.org/abs/2204.04991v1,"Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, Yossi Matias",cs.CL,table2text,"Grounded text generation systems often generate text that contains factual
inconsistencies, hindering their real-world applicability. Automatic factual
consistency evaluation may help alleviate this limitation by accelerating
evaluation cycles, filtering inconsistent outputs and augmenting training data.
While attracting increasing attention, such evaluation metrics are usually
developed and evaluated in silo for a single task or dataset, slowing their
adoption. Moreover, previous meta-evaluation protocols focused on system-level
correlations with human annotations, which leave the example-level accuracy of
such metrics unclear. In this work, we introduce TRUE: a comprehensive study of
factual consistency metrics on a standardized collection of existing texts from
diverse tasks, manually annotated for factual consistency. Our standardization
enables an example-level meta-evaluation protocol that is more actionable and
interpretable than previously reported correlations, yielding clearer quality
measures. Across diverse state-of-the-art metrics and 11 datasets we find that
large-scale NLI and question generation-and-answering-based approaches achieve
strong and complementary results. We recommend those methods as a starting
point for model and metric developers, and hope TRUE will foster progress
towards even better methods.",2022-04-11
"Explanation Graph Generation via Pre-trained Language Models: An
  Empirical Study with Contrastive Learning",2022-04-11 00:58:27+00:00,http://arxiv.org/abs/2204.04813v1,"Swarnadeep Saha, Prateek Yadav, Mohit Bansal","cs.CL, cs.AI, cs.LG",table2text,"Pre-trained sequence-to-sequence language models have led to widespread
success in many natural language generation tasks. However, there has been
relatively less work on analyzing their ability to generate structured outputs
such as graphs. Unlike natural language, graphs have distinct structural and
semantic properties in the context of a downstream NLP task, e.g., generating a
graph that is connected and acyclic can be attributed to its structural
constraints, while the semantics of a graph can refer to how meaningfully an
edge represents the relation between two node concepts. In this work, we study
pre-trained language models that generate explanation graphs in an end-to-end
manner and analyze their ability to learn the structural constraints and
semantics of such graphs. We first show that with limited supervision,
pre-trained language models often generate graphs that either violate these
constraints or are semantically incoherent. Since curating large amount of
human-annotated graphs is expensive and tedious, we propose simple yet
effective ways of graph perturbations via node and edge edit operations that
lead to structurally and semantically positive and negative graphs. Next, we
leverage these graphs in different contrastive learning models with Max-Margin
and InfoNCE losses. Our methods lead to significant improvements in both
structural and semantic accuracy of explanation graphs and also generalize to
other similar graph generation tasks. Lastly, we show that human errors are the
best negatives for contrastive learning and also that automatically generating
more such human-like negative graphs can lead to further improvements. Our code
and models are publicly available at https://github.com/swarnaHub/ExplagraphGen",2022-04-11
"Explaining Deep Convolutional Neural Networks via Latent Visual-Semantic
  Filter Attention",2022-04-10 04:57:56+00:00,http://arxiv.org/abs/2204.04601v1,"Yu Yang, Seungbae Kim, Jungseock Joo","cs.CV, cs.AI, cs.LG",table2text,"Interpretability is an important property for visual models as it helps
researchers and users understand the internal mechanism of a complex model.
However, generating semantic explanations about the learned representation is
challenging without direct supervision to produce such explanations. We propose
a general framework, Latent Visual Semantic Explainer (LaViSE), to teach any
existing convolutional neural network to generate text descriptions about its
own latent representations at the filter level. Our method constructs a mapping
between the visual and semantic spaces using generic image datasets, using
images and category names. It then transfers the mapping to the target domain
which does not have semantic labels. The proposed framework employs a modular
structure and enables to analyze any trained network whether or not its
original training data is available. We show that our method can generate novel
descriptions for learned filters beyond the set of categories defined in the
training dataset and perform an extensive evaluation on multiple datasets. We
also demonstrate a novel application of our method for unsupervised dataset
bias analysis which allows us to automatically discover hidden biases in
datasets or compare different subsets without using additional labels. The
dataset and code are made public to facilitate further research.",2022-04-10
PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization,2022-04-09 07:40:52+00:00,http://arxiv.org/abs/2204.04413v1,"Xiaochen Liu, Yu Bai, Jiawei Li, Yinan Hu, Yang Gao","cs.CL, cs.AI",table2text,"Few-shot abstractive summarization has become a challenging task in natural
language generation. To support it, we designed a novel soft prompts
architecture coupled with a prompt pre-training plus fine-tuning paradigm that
is effective and tunes only extremely light parameters. The soft prompts
include continuous input embeddings across an encoder and a decoder to fit the
structure of the generation models. Importantly, a novel inner-prompt placed in
the text is introduced to capture document-level information. The aim is to
devote attention to understanding the document that better prompts the model to
generate document-related content. The first step in the summarization
procedure is to conduct prompt pre-training with self-supervised pseudo-data.
This teaches the model basic summarizing capabilities. The model is then
fine-tuned with few-shot examples. Experimental results on the CNN/DailyMail
and XSum datasets show that our method, with only 0.1% of the parameters,
outperforms full-model tuning where all model parameters are tuned. It also
surpasses Prompt Tuning by a large margin and delivers competitive results
against Prefix-Tuning with 3% of the parameters.",2022-04-09
Language Model for Text Analytic in Cybersecurity,2022-04-06 09:17:21+00:00,http://arxiv.org/abs/2204.02685v1,"Ehsan Aghaei, Xi Niu, Waseem Shadid, Ehab Al-Shaer","cs.CL, cs.AI, cs.CR",table2text,"NLP is a form of artificial intelligence and machine learning concerned with
a computer or machine's ability to understand and interpret human language.
Language models are crucial in text analytics and NLP since they allow
computers to interpret qualitative input and convert it to quantitative data
that they can use in other tasks. In essence, in the context of transfer
learning, language models are typically trained on a large generic corpus,
referred to as the pre-training stage, and then fine-tuned to a specific
underlying task. As a result, pre-trained language models are mostly used as a
baseline model that incorporates a broad grasp of the context and may be
further customized to be used in a new NLP task.
  The majority of pre-trained models are trained on corpora from general
domains, such as Twitter, newswire, Wikipedia, and Web. Such off-the-shelf NLP
models trained on general text may be inefficient and inaccurate in specialized
fields. In this paper, we propose a cybersecurity language model called
SecureBERT, which is able to capture the text connotations in the cybersecurity
domain, and therefore could further be used in automation for many important
cybersecurity tasks that would otherwise rely on human expertise and tedious
manual efforts. SecureBERT is trained on a large corpus of cybersecurity text
collected and preprocessed by us from a variety of sources in cybersecurity and
the general computing domain. Using our proposed methods for tokenization and
model weights adjustment, SecureBERT is not only able to preserve the
understanding of general English as most pre-trained language models can do,
but also effective when applied to text that has cybersecurity implications.",2022-04-06
CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations,2022-04-05 17:38:04+00:00,http://arxiv.org/abs/2204.02380v1,"Leonard Salewski, A. Sophia Koepke, Hendrik P. A. Lensch, Zeynep Akata","cs.CV, cs.CL",table2text,"Providing explanations in the context of Visual Question Answering (VQA)
presents a fundamental problem in machine learning. To obtain detailed insights
into the process of generating natural language explanations for VQA, we
introduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with
natural language explanations. For each image-question pair in the CLEVR
dataset, CLEVR-X contains multiple structured textual explanations which are
derived from the original scene graphs. By construction, the CLEVR-X
explanations are correct and describe the reasoning and visual information that
is necessary to answer a given question. We conducted a user study to confirm
that the ground-truth explanations in our proposed dataset are indeed complete
and relevant. We present baseline results for generating natural language
explanations in the context of VQA using two state-of-the-art frameworks on the
CLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation
generation quality for different question and answer types. Additionally, we
study the influence of using different numbers of ground-truth explanations on
the convergence of natural language generation (NLG) metrics. The CLEVR-X
dataset is publicly available at
\url{https://explainableml.github.io/CLEVR-X/}.",2022-04-05
"Diverse Text Generation via Variational Encoder-Decoder Models with
  Gaussian Process Priors",2022-04-04 04:09:15+00:00,http://arxiv.org/abs/2204.01227v1,"Wanyu Du, Jianqiao Zhao, Liwei Wang, Yangfeng Ji",cs.CL,table2text,"Generating high quality texts with high diversity is important for many NLG
applications, but current methods mostly focus on building deterministic models
to generate higher quality texts and do not provide many options for promoting
diversity. In this work, we present a novel latent structured variable model to
generate high quality texts by enriching contextual representation learning of
encoder-decoder models. Specifically, we introduce a stochastic function to map
deterministic encoder hidden states into random context variables. The proposed
stochastic function is sampled from a Gaussian process prior to (1) provide
infinite number of joint Gaussian distributions of random context variables
(diversity-promoting) and (2) explicitly model dependency between context
variables (accurate-encoding). To address the learning challenge of Gaussian
processes, we propose an efficient variational inference approach to
approximate the posterior distribution of random context variables. We evaluate
our method in two typical text generation tasks: paraphrase generation and text
style transfer. Experimental results on benchmark datasets demonstrate that our
method improves the generation quality and diversity compared with other
baselines.",2022-04-04
"CTRLEval: An Unsupervised Reference-Free Metric for Evaluating
  Controlled Text Generation",2022-04-02 13:42:49+00:00,http://arxiv.org/abs/2204.00862v1,"Pei Ke, Hao Zhou, Yankai Lin, Peng Li, Jie Zhou, Xiaoyan Zhu, Minlie Huang","cs.CL, cs.AI",table2text,"Existing reference-free metrics have obvious limitations for evaluating
controlled text generation models. Unsupervised metrics can only provide a
task-agnostic evaluation result which correlates weakly with human judgments,
whereas supervised ones may overfit task-specific data with poor generalization
ability to other datasets. In this paper, we propose an unsupervised
reference-free metric called CTRLEval, which evaluates controlled text
generation from different aspects by formulating each aspect into multiple text
infilling tasks. On top of these tasks, the metric assembles the generation
probabilities from a pre-trained language model without any model training.
Experimental results show that our metric has higher correlations with human
judgments than other baselines, while obtaining better generalization of
evaluating generated texts from different models and with different qualities.",2022-04-02
A Roadmap for Big Model,2022-03-26 15:38:00+00:00,http://arxiv.org/abs/2203.14101v1,"Sha Yuan, Hanyu Zhao, Shuai Zhao, Jiahong Leng, Yangxiao Liang, Xiaozhi Wang, Jifan Yu, Xin Lv, Zhou Shao, Jiaao He, Yankai Lin, Xu Han, Zhenghao Liu, Ning Ding, Yongming Rao, Yizhao Gao, Liang Zhang, Ming Ding, Cong Fang, Yisen Wang, Mingsheng Long, Jing Zhang, Yinpeng Dong, Tianyu Pang, Peng Cui, Lingxiao Huang, Zheng Liang, Huawei Shen, Hui Zhang, Quanshi Zhang, Qingxiu Dong, Zhixing Tan, Mingxuan Wang, Shuo Wang, Long Zhou, Haoran Li, Junwei Bao, Yingwei Pan, Weinan Zhang, Zhou Yu, Rui Yan, Chence Shi, Minghao Xu, Zuobai Zhang, Guoqiang Wang, Xiang Pan, Mengjie Li, Xiaoyu Chu, Zijun Yao, Fangwei Zhu, Shulin Cao, Weicheng Xue, Zixuan Ma, Zhengyan Zhang, Shengding Hu, Yujia Qin, Chaojun Xiao, Zheni Zeng, Ganqu Cui, Weize Chen, Weilin Zhao, Yuan Yao, Peng Li, Wenzhao Zheng, Wenliang Zhao, Ziyi Wang, Borui Zhang, Nanyi Fei, Anwen Hu, Zenan Ling, Haoyang Li, Boxi Cao, Xianpei Han, Weidong Zhan, Baobao Chang, Hao Sun, Jiawen Deng, Juanzi Li, Lei Hou, Xigang Cao, Jidong Zhai, Zhiyuan Liu, Maosong Sun, Jiwen Lu, Zhiwu Lu, Qin Jin, Ruihua Song, Ji-Rong Wen, Zhouchen Lin, Liwei Wang, Hang Su, Jun Zhu, Zhifang Sui, Jiajun Zhang, Yang Liu, Xiaodong He, Minlie Huang, Jian Tang, Jie Tang","cs.LG, cs.AI, cs.CL",table2text,"With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view.",2022-03-26
"GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate
  Degradation of Artificial Neural Language Models",2022-03-25 00:25:42+00:00,http://arxiv.org/abs/2203.13397v1,"Changye Li, David Knopman, Weizhe Xu, Trevor Cohen, Serguei Pakhomov",cs.CL,table2text,"Deep learning (DL) techniques involving fine-tuning large numbers of model
parameters have delivered impressive performance on the task of discriminating
between language produced by cognitively healthy individuals, and those with
Alzheimer's disease (AD). However, questions remain about their ability to
generalize beyond the small reference sets that are publicly available for
research. As an alternative to fitting model parameters directly, we propose a
novel method by which a Transformer DL model (GPT-2) pre-trained on general
English text is paired with an artificially degraded version of itself (GPT-D),
to compute the ratio between these two models' \textit{perplexities} on
language from cognitively healthy and impaired individuals. This technique
approaches state-of-the-art performance on text data from a widely used ""Cookie
Theft"" picture description task, and unlike established alternatives also
generalizes well to spontaneous conversations. Furthermore, GPT-D generates
text with characteristics known to be associated with AD, demonstrating the
induction of dementia-related linguistic anomalies. Our study is a step toward
better understanding of the relationships between the inner workings of
generative neural language models, the language that they produce, and the
deleterious effects of dementia on human speech and language characteristics.",2022-03-25
"Mix and Match: Learning-free Controllable Text Generation using Energy
  Language Models",2022-03-24 18:52:09+00:00,http://arxiv.org/abs/2203.13299v1,"Fatemehsadat Mireshghallah, Kartik Goyal, Taylor Berg-Kirkpatrick","cs.CL, cs.LG",table2text,"Recent work on controlled text generation has either required attribute-based
fine-tuning of the base language model (LM), or has restricted the
parameterization of the attribute discriminator to be compatible with the base
autoregressive LM. In this work, we propose Mix and Match LM, a global
score-based alternative for controllable text generation that combines
arbitrary pre-trained black-box models for achieving the desired attributes in
the generated text without involving any fine-tuning or structural assumptions
about the black-box models. We interpret the task of controllable generation as
drawing samples from an energy-based model whose energy values are a linear
combination of scores from black-box models that are separately responsible for
fluency, the control attribute, and faithfulness to any conditioning context.
We use a Metropolis-Hastings sampling scheme to sample from this energy-based
model using bidirectional context and global attribute features. We validate
the effectiveness of our approach on various controlled generation and
style-based text revision tasks by outperforming recently proposed methods that
involve extra training, fine-tuning, or restrictive assumptions over the form
of models.",2022-03-24
Language modeling via stochastic processes,2022-03-21 22:13:53+00:00,http://arxiv.org/abs/2203.11370v1,"Rose E Wang, Esin Durmus, Noah Goodman, Tatsunori Hashimoto","cs.CL, cs.LG",table2text,"Modern language models can generate high-quality short texts. However, they
often meander or are incoherent when generating longer texts. These issues
arise from the next-token-only language modeling objective. To address these
issues, we introduce Time Control (TC), a language model that implicitly plans
via a latent stochastic process. TC does this by learning a representation
which maps the dynamics of how text changes in a document to the dynamics of a
stochastic process of interest. Using this representation, the language model
can generate text by first implicitly generating a document plan via a
stochastic process, and then generating text that is consistent with this
latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a
variety of text domains, TC improves performance on text infilling and
discourse coherence. On long text generation settings, TC preserves the text
structure both in terms of ordering (up to +40% better) and text length
consistency (up to +17% better). Human evaluators also prefer TC's output 28.6%
more than the baselines.",2022-03-21
Dependency-based Mixture Language Models,2022-03-19 06:28:30+00:00,http://arxiv.org/abs/2203.10256v1,"Zhixian Yang, Xiaojun Wan",cs.CL,table2text,"Various models have been proposed to incorporate knowledge of syntactic
structures into neural language models. However, previous works have relied
heavily on elaborate components for a specific language model, usually
recurrent neural network (RNN), which makes themselves unwieldy in practice to
fit into other neural language models, such as Transformer and GPT-2. In this
paper, we introduce the Dependency-based Mixture Language Models. In detail, we
first train neural language models with a novel dependency modeling objective
to learn the probability distribution of future dependent tokens given context.
We then formulate the next-token probability by mixing the previous dependency
modeling probability distributions with self-attention. Extensive experiments
and human evaluations show that our method can be easily and effectively
applied to different neural language models while improving neural text
generation on various tasks.",2022-03-19
"Are You Robert or RoBERTa? Deceiving Online Authorship Attribution
  Models Using Neural Text Generators",2022-03-18 09:19:14+00:00,http://arxiv.org/abs/2203.09813v1,"Keenan Jones, Jason R. C. Nurse, Shujun Li","cs.CL, cs.AI, cs.LG",table2text,"Recently, there has been a rise in the development of powerful pre-trained
natural language models, including GPT-2, Grover, and XLM. These models have
shown state-of-the-art capabilities towards a variety of different NLP tasks,
including question answering, content summarisation, and text generation.
Alongside this, there have been many studies focused on online authorship
attribution (AA). That is, the use of models to identify the authors of online
texts. Given the power of natural language models in generating convincing
texts, this paper examines the degree to which these language models can
generate texts capable of deceiving online AA models. Experimenting with both
blog and Twitter data, we utilise GPT-2 language models to generate texts using
the existing posts of online users. We then examine whether these AI-based text
generators are capable of mimicking authorial style to such a degree that they
can deceive typical AA models. From this, we find that current AI-based text
generators are able to successfully mimic authorship, showing capabilities
towards this on both datasets. Our findings, in turn, highlight the current
capacity of powerful natural language models to generate original online posts
capable of mimicking authorial style sufficiently to deceive popular AA
methods; a key finding given the proposed role of AA in real world applications
such as spam-detection and forensic investigation.",2022-03-18
"ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and
  Implicit Hate Speech Detection",2022-03-17 17:57:56+00:00,http://arxiv.org/abs/2203.09509v1,"Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, Ece Kamar",cs.CL,table2text,"Toxic language detection systems often falsely flag text that contains
minority group mentions as toxic, as those groups are often the targets of
online hate. Such over-reliance on spurious correlations also causes systems to
struggle with detecting implicitly toxic language. To help mitigate these
issues, we create ToxiGen, a new large-scale and machine-generated dataset of
274k toxic and benign statements about 13 minority groups. We develop a
demonstration-based prompting framework and an adversarial
classifier-in-the-loop decoding method to generate subtly toxic and benign text
with a massive pretrained language model. Controlling machine generation in
this way allows ToxiGen to cover implicitly toxic text at a larger scale, and
about more demographic groups, than previous resources of human-written text.
We conduct a human evaluation on a challenging subset of ToxiGen and find that
annotators struggle to distinguish machine-generated text from human-written
language. We also find that 94.5% of toxic examples are labeled as hate speech
by human annotators. Using three publicly-available datasets, we show that
finetuning a toxicity classifier on our data improves its performance on
human-written data substantially. We also demonstrate that ToxiGen can be used
to fight machine-generated toxicity as finetuning improves the classifier
significantly on our evaluation subset.",2022-03-17
"Time Dependency, Data Flow, and Competitive Advantage",2022-03-17 07:09:30+00:00,http://arxiv.org/abs/2203.09128v1,"Ehsan Valavi, Joel Hestness, Marco Iansiti, Newsha Ardalani, Feng Zhu, Karim R. Lakhani","cs.LG, cs.CL, econ.GN, q-fin.EC",table2text,"Data is fundamental to machine learning-based products and services and is
considered strategic due to its externalities for businesses, governments,
non-profits, and more generally for society. It is renowned that the value of
organizations (businesses, government agencies and programs, and even
industries) scales with the volume of available data. What is often less
appreciated is that the data value in making useful organizational predictions
will range widely and is prominently a function of data characteristics and
underlying algorithms.
  In this research, our goal is to study how the value of data changes over
time and how this change varies across contexts and business areas (e.g. next
word prediction in the context of history, sports, politics). We focus on data
from Reddit.com and compare the value's time-dependency across various Reddit
topics (Subreddits). We make this comparison by measuring the rate at which
user-generated text data loses its relevance to the algorithmic prediction of
conversations. We show that different subreddits have different rates of
relevance decline over time.
  Relating the text topics to various business areas of interest, we argue that
competing in a business area in which data value decays rapidly alters
strategies to acquire competitive advantage. When data value decays rapidly,
access to a continuous flow of data will be more valuable than access to a
fixed stock of data. In this kind of setting, improving user engagement and
increasing user-base help creating and maintaining a competitive advantage.",2022-03-17
Graph Pre-training for AMR Parsing and Generation,2022-03-15 12:47:00+00:00,http://arxiv.org/abs/2203.07836v1,"Xuefeng Bai, Yulong Chen, Yue Zhang",cs.CL,table2text,"Abstract meaning representation (AMR) highlights the core semantic
information of text in a graph structure. Recently, pre-trained language models
(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,
respectively. However, PLMs are typically pre-trained on textual data, thus are
sub-optimal for modeling structural knowledge. To this end, we investigate
graph self-supervised training to improve the structure awareness of PLMs over
AMR graphs. In particular, we introduce two graph auto-encoding strategies for
graph-to-graph pre-training and four tasks to integrate text and graph
information during pre-training. We further design a unified framework to
bridge the gap between pre-training and fine-tuning tasks. Experiments on both
AMR parsing and AMR-to-text generation show the superiority of our model. To
our knowledge, we are the first to consider pre-training on semantic graphs.",2022-03-15
Chart-to-Text: A Large-Scale Benchmark for Chart Summarization,2022-03-12 17:01:38+00:00,http://arxiv.org/abs/2203.06486v1,"Shankar Kanthara, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, Shafiq Joty",cs.CL,table2text,"Charts are commonly used for exploring data and communicating insights.
Generating natural language summaries from charts can be very helpful for
people in inferring key insights that would otherwise require a lot of
cognitive and perceptual efforts. We present Chart-to-text, a large-scale
benchmark with two datasets and a total of 44,096 charts covering a wide range
of topics and chart types. We explain the dataset construction process and
analyze the datasets. We also introduce a number of state-of-the-art neural
models as baselines that utilize image captioning and data-to-text generation
techniques to tackle two problem variations: one assumes the underlying data
table of the chart is available while the other needs to extract data from
chart images. Our analysis with automatic and human evaluation shows that while
our best models usually generate fluent summaries and yield reasonable BLEU
scores, they also suffer from hallucinations and factual errors as well as
difficulties in correctly explaining complex patterns and trends in charts.",2022-03-12
"IndicNLG Suite: Multilingual Datasets for Diverse NLG Tasks in Indic
  Languages",2022-03-10 15:53:58+00:00,http://arxiv.org/abs/2203.05437v1,"Aman Kumar, Himani Shrotriya, Prachi Sahu, Raj Dabre, Ratish Puduppully, Anoop Kunchukuttan, Amogh Mishra, Mitesh M. Khapra, Pratyush Kumar","cs.CL, cs.AI",table2text,"In this paper, we present the IndicNLG suite, a collection of datasets for
benchmarking Natural Language Generation (NLG) for 11 Indic languages. We focus
on five diverse tasks, namely, biography generation using Wikipedia infoboxes
(WikiBio), news headline generation, sentence summarization, question
generation and paraphrase generation. We describe the process of creating the
datasets and present statistics of the dataset, following which we train and
report a variety of strong monolingual and multilingual baselines that leverage
pre-trained sequence-to-sequence models and analyze the results to understand
the challenges involved in Indic language NLG. To the best of our knowledge,
this is the first NLG dataset for Indic languages and also the largest
multilingual NLG dataset. Our methods can also be easily applied to
modest-resource languages with reasonable monolingual and parallel corpora, as
well as corpora containing structured data like Wikipedia. We hope this dataset
spurs research in NLG on diverse languages and tasks, particularly for Indic
languages. The datasets and models are publicly available at
https://indicnlp.ai4bharat.org/indicnlg-suite.",2022-03-10
"Faithfulness in Natural Language Generation: A Systematic Survey of
  Analysis, Evaluation and Optimization Methods",2022-03-10 08:28:32+00:00,http://arxiv.org/abs/2203.05227v1,"Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, Hua Wu",cs.CL,table2text,"Natural Language Generation (NLG) has made great progress in recent years due
to the development of deep learning techniques such as pre-trained language
models. This advancement has resulted in more fluent, coherent and even
properties controllable (e.g. stylistic, sentiment, length etc.) generation,
naturally leading to development in downstream tasks such as abstractive
summarization, dialogue generation, machine translation, and data-to-text
generation. However, the faithfulness problem that the generated text usually
contains unfaithful or non-factual information has become the biggest
challenge, which makes the performance of text generation unsatisfactory for
practical applications in many real-world scenarios. Many studies on analysis,
evaluation, and optimization methods for faithfulness problems have been
proposed for various tasks, but have not been organized, compared and discussed
in a combined manner. In this survey, we provide a systematic overview of the
research progress on the faithfulness problem of NLG, including problem
analysis, evaluation metrics and optimization methods. We organize the
evaluation and optimization methods for different tasks into a unified taxonomy
to facilitate comparison and learning across tasks. Several research trends are
discussed further.",2022-03-10
Compilable Neural Code Generation with Compiler Feedback,2022-03-10 03:15:17+00:00,http://arxiv.org/abs/2203.05132v1,"Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, Qun Liu","cs.CL, cs.AI, cs.PL",table2text,"Automatically generating compilable programs with (or without) natural
language descriptions has always been a touchstone problem for computational
linguistics and automated software engineering. Existing deep-learning
approaches model code generation as text generation, either constrained by
grammar structures in decoder, or driven by pre-trained language models on
large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of
them account for compilability of the generated programs. To improve
compilability of the generated programs, this paper proposes COMPCODER, a
three-stage pipeline utilizing compiler feedback for compilable code
generation, including language model fine-tuning, compilability reinforcement,
and compilability discrimination. Comprehensive experiments on two code
generation tasks demonstrate the effectiveness of our proposed approach,
improving the success rate of compilation from 44.18 to 89.18 in code
completion on average and from 70.3 to 96.2 in text-to-code generation,
respectively, when comparing with the state-of-the-art CodeGPT.",2022-03-10
Recent Advances in Neural Text Generation: A Task-Agnostic Survey,2022-03-06 20:47:49+00:00,http://arxiv.org/abs/2203.03047v1,"Chen Tang, Frank Guerin, Yucheng Li, Chenghua Lin","cs.CL, cs.AI",table2text,"In recent years much effort has been devoted to applying neural models to the
task of natural language generation. The challenge is to generate natural
human-like text, and to control the generation process. This paper presents a
task-agnostic survey of recent advances in neural text generation. These
advances have been achieved by numerous developments, which we group under the
following four headings: data construction, neural frameworks, training and
inference strategies, and evaluation metrics. Finally we discuss the future
directions for the development of neural text generation including neural
pipelines and exploiting back-ground knowledge.",2022-03-06
Deep Latent-Variable Models for Text Generation,2022-03-03 23:06:39+00:00,http://arxiv.org/abs/2203.02055v1,Xiaoyu Shen,cs.CL,table2text,"Text generation aims to produce human-like natural language output for
down-stream tasks. It covers a wide range of applications like machine
translation, document summarization, dialogue generation and so on. Recently
deep neural network-based end-to-end architectures have been widely adopted.
The end-to-end approach conflates all sub-modules, which used to be designed by
complex handcrafted rules, into a holistic encode-decode architecture. Given
enough training data, it is able to achieve state-of-the-art performance yet
avoiding the need of language/domain-dependent knowledge. Nonetheless, deep
learning models are known to be extremely data-hungry, and text generated from
them usually suffer from low diversity, interpretability and controllability.
As a result, it is difficult to trust the output from them in real-life
applications. Deep latent-variable models, by specifying the probabilistic
distribution over an intermediate latent process, provide a potential way of
addressing these problems while maintaining the expressive power of deep neural
networks. This dissertation presents how deep latent-variable models can
improve over the standard encoder-decoder model for text generation.",2022-03-03
"Attend, Memorize and Generate: Towards Faithful Table-to-Text Generation
  in Few Shots",2022-03-01 20:37:20+00:00,http://arxiv.org/abs/2203.00732v1,"Wenting Zhao, Ye Liu, Yao Wan, Philip S. Yu",cs.CL,table2text,"Few-shot table-to-text generation is a task of composing fluent and faithful
sentences to convey table content using limited data. Despite many efforts
having been made towards generating impressive fluent sentences by fine-tuning
powerful pre-trained language models, the faithfulness of generated content
still needs to be improved. To this end, this paper proposes a novel approach
Attend, Memorize and Generate (called AMG), inspired by the text generation
process of humans. In particular, AMG (1) attends over the multi-granularity of
context using a novel strategy based on table slot level and traditional
token-by-token level attention to exploit both the table structure and natural
linguistic information; (2) dynamically memorizes the table slot allocation
states; and (3) generates faithful sentences according to both the context and
memory allocation states. Comprehensive experiments with human evaluation on
three domains (i.e., humans, songs, and books) of the Wiki dataset show that
our model can generate higher qualified texts when compared with several
state-of-the-art baselines, in both fluency and faithfulness.",2022-03-01
Data-to-text Generation with Variational Sequential Planning,2022-02-28 13:17:59+00:00,http://arxiv.org/abs/2202.13756v1,"Ratish Puduppully, Yao Fu, Mirella Lapata",cs.CL,table2text,"We consider the task of data-to-text generation, which aims to create textual
output from non-linguistic input. We focus on generating long-form text, i.e.,
documents with multiple paragraphs, and propose a neural model enhanced with a
planning component responsible for organizing high-level information in a
coherent and meaningful way. We infer latent plans sequentially with a
structured variational model, while interleaving the steps of planning and
generation. Text is generated by conditioning on previous variational decisions
and previously generated text. Experiments on two data-to-text benchmarks
(RotoWire and MLB) show that our model outperforms strong baselines and is
sample efficient in the face of limited training data (e.g., a few hundred
instances).",2022-02-28
"Variational Autoencoder with Disentanglement Priors for Low-Resource
  Task-Specific Natural Language Generation",2022-02-27 13:34:24+00:00,http://arxiv.org/abs/2202.13363v1,"Zhuang Li, Lizhen Qu, Qiongkai Xu, Tongtong Wu, Tianyang Zhan, Gholamreza Haffari",cs.CL,table2text,"In this paper, we propose a variational autoencoder with disentanglement
priors, VAE-DPRIOR, for conditional natural language generation with none or a
handful of task-specific labeled examples. In order to improve compositional
generalization, our model performs disentangled representation learning by
introducing a prior for the latent content space and another prior for the
latent label space. We show both empirically and theoretically that the
conditional priors can already disentangle representations even without
specific regularizations as in the prior work. We can also sample diverse
content representations from the content space without accessing data of the
seen tasks, and fuse them with the representations of novel tasks for
generating diverse texts in the low-resource settings. Our extensive
experiments demonstrate the superior performance of our model over competitive
baselines in terms of i) data augmentation in continuous zero/few-shot
learning, and ii) text style transfer in both zero/few-shot settings.",2022-02-27
ZeroGen: Efficient Zero-shot Learning via Dataset Generation,2022-02-16 08:18:02+00:00,http://arxiv.org/abs/2202.07922v1,"Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, Lingpeng Kong","cs.CL, cs.AI",table2text,"There is a growing interest in dataset generation recently due to the
superior generative capacity of large pre-trained language models (PLMs). In
this paper, we study a flexible and efficient zero-short learning method,
ZeroGen. Given a zero-shot task, we first generate a dataset from scratch using
PLMs in an unsupervised manner. Then, we train a tiny task model (e.g., LSTM)
under the supervision of the synthesized dataset. This approach allows highly
efficient inference as the final task model only has orders of magnitude fewer
parameters comparing to PLMs (e.g., GPT2-XL). Apart from being annotation-free
and efficient, we argue that ZeroGen can also provide useful insights from the
perspective of data-free model-agnostic knowledge distillation, and
unreferenced text generation evaluation. Experiments and analysis on different
NLP tasks, namely, text classification, question answering, and natural
language inference), show the effectiveness of ZeroGen.",2022-02-16
"Integrating AI Planning with Natural Language Processing: A Combination
  of Explicit and Tacit Knowledge",2022-02-15 02:19:09+00:00,http://arxiv.org/abs/2202.07138v1,"Kebing Jin, Hankz Hankui Zhuo","cs.AI, cs.CL",table2text,"Automated planning focuses on strategies, building domain models and
synthesizing plans to transit initial states to goals. Natural language
processing concerns with the interactions between agents and human language,
especially processing and analyzing large amounts of natural language data.
These two fields have abilities to generate explicit knowledge, e.g.,
preconditions and effects of action models, and learn from tacit knowledge,
e.g., neural models, respectively. Integrating AI planning and natural language
processing effectively improves the communication between human and intelligent
agents. This paper outlines the commons and relations between AI planning and
natural language processing, argues that each of them can effectively impact on
the other one by four areas: (1) planning-based text understanding, (2)
planning-based text generation, (3) text-based human-robot interaction, and (4)
text-based explainable planning. We also explore some potential future issues
between AI planning and natural language processing.",2022-02-15
"Research on Dual Channel News Headline Classification Based on ERNIE
  Pre-training Model",2022-02-14 10:44:12+00:00,http://arxiv.org/abs/2202.06600v1,"Junjie Li, Hui Cao","cs.CL, cs.AI",table2text,"The classification of news headlines is an important direction in the field
of NLP, and its data has the characteristics of compactness, uniqueness and
various forms. Aiming at the problem that the traditional neural network model
cannot adequately capture the underlying feature information of the data and
cannot jointly extract key global features and deep local features, a
dual-channel network model DC-EBAD based on the ERNIE pre-training model is
proposed. Use ERNIE to extract the lexical, semantic and contextual feature
information at the bottom of the text, generate dynamic word vector
representations fused with context, and then use the BiLSTM-AT network channel
to secondary extract the global features of the data and use the attention
mechanism to give key parts higher The weight of the DPCNN channel is used to
overcome the long-distance text dependence problem and obtain deep local
features. The local and global feature vectors are spliced, and finally passed
to the fully connected layer, and the final classification result is output
through Softmax. The experimental results show that the proposed model improves
the accuracy, precision and F1-score of news headline classification compared
with the traditional neural network model and the single-channel model under
the same conditions. It can be seen that it can perform well in the
multi-classification application of news headline text under large data volume.",2022-02-14
A multi-task semi-supervised framework for Text2Graph & Graph2Text,2022-02-12 11:02:17+00:00,http://arxiv.org/abs/2202.06041v1,"Oriol Domingo, Marta R. Costa-jussà, Carlos Escolano","cs.CL, cs.IR",table2text,"The Artificial Intelligence industry regularly develops applications that
mostly rely on Knowledge Bases, a data repository about specific, or general,
domains, usually represented in a graph shape. Similar to other databases, they
face two main challenges: information ingestion and information retrieval. We
approach these challenges by jointly learning graph extraction from text and
text generation from graphs. The proposed solution, a T5 architecture, is
trained in a multi-task semi-supervised environment, with our collected
non-parallel data, following a cycle training regime. Experiments on WebNLG
dataset show that our approach surpasses unsupervised state-of-the-art results
in text-to-graph and graph-to-text. More relevantly, our framework is more
consistent across seen and unseen domains than supervised models. The resulting
model can be easily trained in any new domain with non-parallel data, by simply
adding text and graphs about it, in our cycle framework.",2022-02-12
"Generating Training Data with Language Models: Towards Zero-Shot
  Language Understanding",2022-02-09 16:02:18+00:00,http://arxiv.org/abs/2202.04538v1,"Yu Meng, Jiaxin Huang, Yu Zhang, Jiawei Han","cs.CL, cs.LG",table2text,"Pretrained language models (PLMs) have demonstrated remarkable performance in
various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are
well known for their superior text generation capabilities; bidirectional PLMs
(e.g., BERT) have been the prominent choice for natural language understanding
(NLU) tasks. While both types of models have achieved promising few-shot
learning performance, their potential for zero-shot learning has been
underexplored. In this paper, we present a simple approach that uses both types
of PLMs for fully zero-shot learning of NLU tasks without requiring any
task-specific data: A unidirectional PLM generates class-conditioned texts
guided by prompts, which are used as the training data for fine-tuning a
bidirectional PLM. With quality training data selected based on the generation
probability and regularization techniques (label smoothing and temporal
ensembling) applied to the fine-tuning stage for better generalization and
stability, our approach demonstrates strong performance across seven
classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and
92.8 on SST-2), significantly outperforming zero-shot prompting methods and
achieving even comparable results to strong few-shot approaches using 32
training samples per class.",2022-02-09
Survey of Hallucination in Natural Language Generation,2022-02-08 03:55:01+00:00,http://arxiv.org/abs/2202.03629v1,"Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, Pascale Fung","cs.CL, A.1",table2text,"Natural Language Generation (NLG) has improved exponentially in recent years
thanks to the development of deep learning technologies such as
Transformer-based language models. This advancement has led to more fluent and
coherent natural language generation, naturally leading to development in
downstream tasks such as abstractive summarization, dialogue generation and
data-to-text generation. However, it is also investigated that such generation
includes hallucinated texts, which makes the performances of text generation
fail to meet users' expectations in many real-world scenarios. In order to
address this issue, studies in evaluation and mitigation methods of
hallucinations have been presented in various tasks, but have not been reviewed
in a combined manner. In this survey, we provide a broad overview of the
research progress and challenges in the hallucination problem of NLG. The
survey is organized into two big divisions: (i) a general overview of metrics,
mitigation methods, and future directions; (ii) task-specific research progress
for hallucinations in a large set of downstream tasks: abstractive
summarization, dialogue generation, generative question answering, data-to-text
generation, and machine translation. This survey could facilitate collaborative
efforts among researchers in these tasks.",2022-02-08
Red Teaming Language Models with Language Models,2022-02-07 15:22:17+00:00,http://arxiv.org/abs/2202.03286v1,"Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, Geoffrey Irving","cs.CL, cs.AI, cs.CR, cs.LG",table2text,"Language Models (LMs) often cannot be deployed because of their potential to
harm users in hard-to-predict ways. Prior work identifies harmful behaviors
before deployment by using human annotators to hand-write test cases. However,
human annotation is expensive, limiting the number and diversity of test cases.
In this work, we automatically find cases where a target LM behaves in a
harmful way, by generating test cases (""red teaming"") using another LM. We
evaluate the target LM's replies to generated test questions using a classifier
trained to detect offensive content, uncovering tens of thousands of offensive
replies in a 280B parameter LM chatbot. We explore several methods, from
zero-shot generation to reinforcement learning, for generating test cases with
varying levels of diversity and difficulty. Furthermore, we use prompt
engineering to control LM-generated test cases to uncover a variety of other
harms, automatically finding groups of people that the chatbot discusses in
offensive ways, personal and hospital phone numbers generated as the chatbot's
own contact info, leakage of private training data in generated text, and harms
that occur over the course of a conversation. Overall, LM-based red teaming is
one promising tool (among many needed) for finding and fixing diverse,
undesirable LM behaviors before impacting users.",2022-02-07
"XAlign: Cross-lingual Fact-to-Text Alignment and Generation for
  Low-Resource Languages",2022-02-01 09:41:59+00:00,http://arxiv.org/abs/2202.00291v1,"Tushar Abhishek, Shivprasad Sagare, Bhavyajeet Singh, Anubhav Sharma, Manish Gupta, Vasudeva Varma",cs.CL,table2text,"Multiple critical scenarios (like Wikipedia text generation given English
Infoboxes) need automated generation of descriptive text in low resource (LR)
languages from English fact triples. Previous work has focused on English
fact-to-text (F2T) generation. To the best of our knowledge, there has been no
previous attempt on cross-lingual alignment or generation for LR languages.
Building an effective cross-lingual F2T (XF2T) system requires alignment
between English structured facts and LR sentences. We propose two unsupervised
methods for cross-lingual alignment. We contribute XALIGN, an XF2T dataset with
0.45M pairs across 8 languages, of which 5402 pairs have been manually
annotated. We also train strong baseline XF2T generation models on the XAlign
dataset.",2022-02-01
"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A
  Large-Scale Generative Language Model",2022-01-28 08:59:57+00:00,http://arxiv.org/abs/2201.11990v3,"Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro",cs.CL,table2text,"Pretrained general-purpose language models can achieve state-of-the-art
accuracies in various natural language processing domains by adapting to
downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of
their success, the size of these models has increased rapidly, requiring
high-performance hardware, software, and algorithmic techniques to enable
training such large models. As the result of a joint effort between Microsoft
and NVIDIA, we present details on the training of the largest monolithic
transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530
billion parameters. In this paper, we first focus on the infrastructure as well
as the 3D parallelism methodology used to train this model using DeepSpeed and
Megatron. Next, we detail the training process, the design of our training
corpus, and our data curation techniques, which we believe is a key ingredient
to the success of the model. Finally, we discuss various evaluation results, as
well as other interesting observations and new properties exhibited by MT-NLG.
We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning
accuracies on several NLP benchmarks and establishes new state-of-the-art
results. We believe that our contributions will help further the development of
large-scale training infrastructures, large-scale language models, and natural
language generations.",2022-01-28
Pre-Trained Language Transformers are Universal Image Classifiers,2022-01-25 08:56:14+00:00,http://arxiv.org/abs/2201.10182v1,"Rahul Goel, Modar Sulaiman, Kimia Noorbakhsh, Mahdi Sharifi, Rajesh Sharma, Pooyan Jamshidi, Kallol Roy","cs.CV, cs.AI",table2text,"Facial images disclose many hidden personal traits such as age, gender, race,
health, emotion, and psychology. Understanding these traits will help to
classify the people in different attributes. In this paper, we have presented a
novel method for classifying images using a pretrained transformer model. We
apply the pretrained transformer for the binary classification of facial images
in criminal and non-criminal classes. The pretrained transformer of GPT-2 is
trained to generate text and then fine-tuned to classify facial images. During
the finetuning process with images, most of the layers of GT-2 are frozen
during backpropagation and the model is frozen pretrained transformer (FPT).
The FPT acts as a universal image classifier, and this paper shows the
application of FPT on facial images. We also use our FPT on encrypted images
for classification. Our FPT shows high accuracy on both raw facial images and
encrypted images. We hypothesize the meta-learning capacity FPT gained because
of its large size and trained on a large size with theory and experiments. The
GPT-2 trained to generate a single word token at a time, through the
autoregressive process, forced to heavy-tail distribution. Then the FPT uses
the heavy-tail property as its meta-learning capacity for classifying images.
Our work shows one way to avoid bias during the machine classification of
images.The FPT encodes worldly knowledge because of the pretraining of one
text, which it uses during the classification. The statistical error of
classification is reduced because of the added context gained from the text.Our
paper shows the ethical dimension of using encrypted data for
classification.Criminal images are sensitive to share across the boundary but
encrypted largely evades ethical concern.FPT showing good classification
accuracy on encrypted images shows promise for further research on
privacy-preserving machine learning.",2022-01-25
An Application of Pseudo-Log-Likelihoods to Natural Language Scoring,2022-01-23 22:00:54+00:00,http://arxiv.org/abs/2201.09377v1,"Darren Abramson, Ali Emami","cs.CL, cs.AI, cs.LG",table2text,"Language models built using semi-supervised machine learning on large corpora
of natural language have very quickly enveloped the fields of natural language
generation and understanding. In this paper we apply a zero-shot approach
independently developed by a number of researchers now gaining recognition as a
significant alternative to fine-tuning for evaluation on common sense tasks. A
language model with relatively few parameters and training steps compared to a
more recent language model (T5) can outperform it on a recent large data set
(TimeDial), while displaying robustness in its performance across a similar
class of language tasks. Surprisingly, this result is achieved by using a
hyperparameter-free zero-shot method with the smaller model, compared to
fine-tuning to the larger model. We argue that robustness of the smaller model
ought to be understood in terms of compositionality, in a sense that we draw
from recent literature on a class of similar models. We identify a practical
cost for our method and model: high GPU-time for natural language evaluation.
The zero-shot measurement technique that produces remarkable stability, both
for ALBERT and other BERT variants, is an application of pseudo-log-likelihoods
to masked language models for the relative measurement of probability for
substitution alternatives in forced choice language tasks such as the Winograd
Schema Challenge, Winogrande, and others. One contribution of this paper is to
bring together a number of similar, but independent strands of research. We
produce some absolute state-of-the-art results for common sense reasoning in
binary choice tasks, performing better than any published result in the
literature, including fine-tuned efforts. We show a remarkable consistency of
the model's performance under adversarial settings, which we argue is best
explained by the model's compositionality of representations.",2022-01-23
"Gradient-guided Unsupervised Text Style Transfer via Contrastive
  Learning",2022-01-23 12:45:00+00:00,http://arxiv.org/abs/2202.00469v1,"Chenghao Fan, Ziao Li, Wei wei","cs.CL, cs.AI",table2text,"Text style transfer is a challenging text generation problem, which aims at
altering the style of a given sentence to a target one while keeping its
content unchanged. Since there is a natural scarcity of parallel datasets,
recent works mainly focus on solving the problem in an unsupervised manner.
However, previous gradient-based works generally suffer from the deficiencies
as follows, namely: (1) Content migration. Previous approaches lack explicit
modeling of content invariance and are thus susceptible to content shift
between the original sentence and the transferred one. (2) Style
misclassification. A natural drawback of the gradient-guided approaches is that
the inference process is homogeneous with a line of adversarial attack, making
latent optimization easily becomes an attack to the classifier due to
misclassification. This leads to difficulties in achieving high transfer
accuracy. To address the problems, we propose a novel gradient-guided model
through a contrastive paradigm for text style transfer, to explicitly gather
similar semantic sentences, and to design a siamese-structure based style
classifier for alleviating such two issues, respectively. Experiments on two
datasets show the effectiveness of our proposed approach, as compared to the
state-of-the-arts.",2022-01-23
A Causal Lens for Controllable Text Generation,2022-01-22 19:31:43+00:00,http://arxiv.org/abs/2201.09119v1,"Zhiting Hu, Li Erran Li","cs.CL, cs.AI, cs.LG, stat.ML",table2text,"Controllable text generation concerns two fundamental tasks of wide
applications, namely generating text of given attributes (i.e.,
attribute-conditional generation), and minimally editing existing text to
possess desired attributes (i.e., text attribute transfer). Extensive prior
work has largely studied the two problems separately, and developed different
conditional models which, however, are prone to producing biased text (e.g.,
various gender stereotypes). This paper proposes to formulate controllable text
generation from a principled causal perspective which models the two tasks with
a unified framework. A direct advantage of the causal formulation is the use of
rich causality tools to mitigate generation biases and improve control. We
treat the two tasks as interventional and counterfactual causal inference based
on a structural causal model, respectively. We then apply the framework to the
challenging practical setting where confounding factors (that induce spurious
correlations) are observable only on a small fraction of data. Experiments show
significant superiority of the causal approach over previous conditional models
for improved control accuracy and reduced bias.",2022-01-22
A Survey of Pretrained Language Models Based Text Generation,2022-01-14 01:44:58+00:00,http://arxiv.org/abs/2201.05273v1,"Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen",cs.CL,table2text,"Text Generation aims to produce plausible and readable text in human language
from input data. The resurgence of deep learning has greatly advanced this
field by neural generation models, especially the paradigm of pretrained
language models (PLMs). Grounding text generation on PLMs is seen as a
promising direction in both academia and industry. In this survey, we present
the recent advances achieved in the topic of PLMs for text generation. In
detail, we begin with introducing three key points of applying PLMs to text
generation: 1) how to encode the input data as representations preserving input
semantics which can be fused into PLMs; 2) how to design a universal and
performant architecture of PLMs served as generation models; and 3) how to
optimize PLMs given the reference text and ensure the generated text satisfying
special text properties. Then, we figure out several challenges and future
directions within each key point. Next, we present a summary of various useful
resources and typical text generation applications to work with PLMs. Finally,
we conclude and summarize the contribution of this survey.",2022-01-14
YACLC: A Chinese Learner Corpus with Multidimensional Annotation,2021-12-30 13:07:08+00:00,http://arxiv.org/abs/2112.15043v1,"Yingying Wang, Cunliang Kong, Liner Yang, Yijun Wang, Xiaorong Lu, Renfen Hu, Shan He, Zhenghao Liu, Yun Chen, Erhong Yang, Maosong Sun",cs.CL,table2text,"Learner corpus collects language data produced by L2 learners, that is second
or foreign-language learners. This resource is of great relevance for second
language acquisition research, foreign-language teaching, and automatic
grammatical error correction. However, there is little focus on learner corpus
for Chinese as Foreign Language (CFL) learners. Therefore, we propose to
construct a large-scale, multidimensional annotated Chinese learner corpus. To
construct the corpus, we first obtain a large number of topic-rich texts
generated by CFL learners. Then we design an annotation scheme including a
sentence acceptability score as well as grammatical error and fluency-based
corrections. We build a crowdsourcing platform to perform the annotation
effectively (https://yaclc.wenmind.net). We name the corpus YACLC (Yet Another
Chinese Learner Corpus) and release it as part of the CUGE benchmark
(http://cuge.baai.ac.cn). By analyzing the original sentences and annotations
in the corpus, we found that YACLC has a considerable size and very high
annotation quality. We hope this corpus can further enhance the studies on
Chinese International Education and Chinese automatic grammatical error
correction.",2021-12-30
"A Preliminary Study for Literary Rhyme Generation based on Neuronal
  Representation, Semantics and Shallow Parsing",2021-12-25 14:40:09+00:00,http://arxiv.org/abs/2112.13241v1,"Luis-Gil Moreno-Jiménez, Juan-Manuel Torres-Moreno, Roseli S. Wedemann","cs.CL, cs.AI",table2text,"In recent years, researchers in the area of Computational Creativity have
studied the human creative process proposing different approaches to reproduce
it with a formal procedure. In this paper, we introduce a model for the
generation of literary rhymes in Spanish, combining structures of language and
neural network models %(\textit{Word2vec}).%, into a structure for semantic
assimilation. The results obtained with a manual evaluation of the texts
generated by our algorithm are encouraging.",2021-12-25
Counterfactual Memorization in Neural Language Models,2021-12-24 04:20:57+00:00,http://arxiv.org/abs/2112.12938v1,"Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, Nicholas Carlini","cs.CL, cs.AI, cs.LG",table2text,"Modern neural language models widely used in tasks across NLP risk memorizing
sensitive information from their training data. As models continue to scale up
in parameters, training data, and compute, understanding memorization in
language models is both important from a learning-theoretical point of view,
and is practically crucial in real world applications. An open question in
previous studies of memorization in language models is how to filter out
""common"" memorization. In fact, most memorization criteria strongly correlate
with the number of occurrences in the training set, capturing ""common""
memorization such as familiar phrases, public knowledge or templated texts. In
this paper, we provide a principled perspective inspired by a taxonomy of human
memory in Psychology. From this perspective, we formulate a notion of
counterfactual memorization, which characterizes how a model's predictions
change if a particular document is omitted during training. We identify and
study counterfactually-memorized training examples in standard text datasets.
We further estimate the influence of each training example on the validation
set and on generated texts, and show that this can provide direct evidence of
the source of memorization at test time.",2021-12-24
A Survey of Natural Language Generation,2021-12-22 09:08:00+00:00,http://arxiv.org/abs/2112.11739v1,"Chenhe Dong, Yinghui Li, Haifan Gong, Miaoxin Chen, Junxin Li, Ying Shen, Min Yang","cs.CL, cs.AI, cs.LG",table2text,"This paper offers a comprehensive review of the research on Natural Language
Generation (NLG) over the past two decades, especially in relation to
data-to-text generation and text-to-text generation deep learning methods, as
well as new applications of NLG technology. This survey aims to (a) give the
latest synthesis of deep learning research on the NLG core tasks, as well as
the architectures adopted in the field; (b) detail meticulously and
comprehensively various NLG tasks and datasets, and draw attention to the
challenges in NLG evaluation, focusing on different evaluation methods and
their relationships; (c) highlight some future emphasis and relatively recent
research issues that arise due to the increasing synergy between NLG and other
artificial intelligence areas, such as computer vision, text and computational
creativity.",2021-12-22
"May the Force Be with Your Copy Mechanism: Enhanced Supervised-Copy
  Method for Natural Language Generation",2021-12-20 06:54:28+00:00,http://arxiv.org/abs/2112.10360v1,"Sanghyuk Choi, Jeong-in Hwang, Hyungjong Noh, Yeonsoo Lee",cs.CL,table2text,"Recent neural sequence-to-sequence models with a copy mechanism have achieved
remarkable progress in various text generation tasks. These models addressed
out-of-vocabulary problems and facilitated the generation of rare words.
However, the identification of the word which needs to be copied is difficult,
as observed by prior copy models, which suffer from incorrect generation and
lacking abstractness. In this paper, we propose a novel supervised approach of
a copy network that helps the model decide which words need to be copied and
which need to be generated. Specifically, we re-define the objective function,
which leverages source sequences and target vocabularies as guidance for
copying. The experimental results on data-to-text generation and abstractive
summarization tasks verify that our approach enhances the copying quality and
improves the degree of abstractness.",2021-12-20
Data Augmentation for Mental Health Classification on Social Media,2021-12-19 05:09:01+00:00,http://arxiv.org/abs/2112.10064v1,"Gunjan Ansari, Muskan Garg, Chandni Saxena",cs.CL,table2text,"The mental disorder of online users is determined using social media posts.
The major challenge in this domain is to avail the ethical clearance for using
the user generated text on social media platforms. Academic re searchers
identified the problem of insufficient and unlabeled data for mental health
classification. To handle this issue, we have studied the effect of data
augmentation techniques on domain specific user generated text for mental
health classification. Among the existing well established data augmentation
techniques, we have identified Easy Data Augmentation (EDA), conditional BERT,
and Back Translation (BT) as the potential techniques for generating additional
text to improve the performance of classifiers. Further, three different
classifiers Random Forest (RF), Support Vector Machine (SVM) and Logistic
Regression (LR) are employed for analyzing the impact of data augmentation on
two publicly available social media datasets. The experiments mental results
show significant improvements in classifiers performance when trained on the
augmented data.",2021-12-19
"NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead
  Heuristics",2021-12-16 09:22:54+00:00,http://arxiv.org/abs/2112.08726v1,"Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, Yejin Choi",cs.CL,table2text,"The dominant paradigm for neural text generation is left-to-right decoding
from autoregressive language models. Constrained or controllable generation
under complex lexical constraints, however, requires foresight to plan ahead
feasible future paths.
  Drawing inspiration from the A* search algorithm, we propose NeuroLogic
A*esque, a decoding algorithm that incorporates heuristic estimates of future
cost. We develop efficient lookahead heuristics that are efficient for
large-scale language models, making our method a drop-in replacement for common
techniques such as beam search and top-k sampling. To enable constrained
generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its
flexibility in incorporating logical constraints with A*esque estimates of
future constraint satisfaction.
  Our approach outperforms competitive baselines on five generation tasks, and
achieves new state-of-the-art performance on table-to-text generation,
constrained machine translation, and keyword-constrained generation. The
improvements are particularly notable on tasks that require complex constraint
satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque
illustrates the power of decoding for improving and enabling new capabilities
of large-scale language models.",2021-12-16
"DSGPT: Domain-Specific Generative Pre-Training of Transformers for Text
  Generation in E-commerce Title and Review Summarization",2021-12-15 19:02:49+00:00,http://arxiv.org/abs/2112.08414v1,"Xueying Zhang, Yunjiang Jiang, Yue Shang, Zhaomeng Cheng, Chi Zhang, Xiaochuan Fan, Yun Xiao, Bo Long","cs.CL, cs.AI",table2text,"We propose a novel domain-specific generative pre-training (DS-GPT) method
for text generation and apply it to the product titleand review summarization
problems on E-commerce mobile display.First, we adopt a decoder-only
transformer architecture, which fitswell for fine-tuning tasks by combining
input and output all to-gether. Second, we demonstrate utilizing only small
amount of pre-training data in related domains is powerful. Pre-training a
languagemodel from a general corpus such as Wikipedia or the CommonCrawl
requires tremendous time and resource commitment, andcan be wasteful if the
downstream tasks are limited in variety. OurDSGPT is pre-trained on a limited
dataset, the Chinese short textsummarization dataset (LCSTS). Third, our model
does not requireproduct-related human-labeled data. For title summarization
task,the state of art explicitly uses additional background knowledgein
training and predicting stages. In contrast, our model implic-itly captures
this knowledge and achieves significant improvementover other methods, after
fine-tuning on the public Taobao.comdataset. For review summarization task, we
utilize JD.com in-housedataset, and observe similar improvement over standard
machinetranslation methods which lack the flexibility of fine-tuning.
Ourproposed work can be simply extended to other domains for a widerange of
text generation tasks.",2021-12-15
"Simple Text Detoxification by Identifying a Linear Toxic Subspace in
  Language Model Embeddings",2021-12-15 18:54:34+00:00,http://arxiv.org/abs/2112.08346v1,"Andrew Wang, Mohit Sudhakar, Yangfeng Ji","cs.CL, cs.LG",table2text,"Large pre-trained language models are often trained on large volumes of
internet data, some of which may contain toxic or abusive language.
Consequently, language models encode toxic information, which makes the
real-world usage of these language models limited. Current methods aim to
prevent toxic features from appearing generated text. We hypothesize the
existence of a low-dimensional toxic subspace in the latent space of
pre-trained language models, the existence of which suggests that toxic
features follow some underlying pattern and are thus removable. To construct
this toxic subspace, we propose a method to generalize toxic directions in the
latent space. We also provide a methodology for constructing parallel datasets
using a context based word masking system. Through our experiments, we show
that when the toxic subspace is removed from a set of sentence representations,
almost no toxic representations remain in the result. We demonstrate
empirically that the subspace found using our method generalizes to multiple
toxicity corpora, indicating the existence of a low-dimensional toxic subspace.",2021-12-15
"Improving Logical-Level Natural Language Generation with
  Topic-Conditioned Data Augmentation and Logical Form Generation",2021-12-12 13:50:18+00:00,http://arxiv.org/abs/2112.06240v1,"Ao Liu, Congjian Luo, Naoaki Okazaki",cs.CL,table2text,"Logical Natural Language Generation, i.e., generating textual descriptions
that can be logically entailed by a structured table, has been a challenge due
to the low fidelity of the generation. \citet{chen2020logic2text} have
addressed this problem by annotating interim logical programs to control the
generation contents and semantics, and presented the task of table-aware
logical form to text (Logic2text) generation. However, although table instances
are abundant in the real world, logical forms paired with textual descriptions
require costly human annotation work, which limits the performance of neural
models. To mitigate this, we propose topic-conditioned data augmentation
(TopicDA), which utilizes GPT-2 to generate unpaired logical forms and textual
descriptions directly from tables. We further introduce logical form generation
(LG), a dual task of Logic2text that requires generating a valid logical form
based on a text description of a table. We also propose a semi-supervised
learning approach to jointly train a Logic2text and an LG model with both
labeled and augmented data. The two models benefit from each other by providing
extra supervision signals through back-translation. Experimental results on the
Logic2text dataset and the LG task demonstrate that our approach can
effectively utilize the augmented data and outperform supervised baselines by a
substantial margin.",2021-12-12
Discourse-Aware Prompt Design for Text Generation,2021-12-10 18:15:44+00:00,http://arxiv.org/abs/2112.05717v1,"Marjan Ghazvininejad, Vladimir Karpukhin, Asli Celikyilmaz","cs.CL, cs.LG, stat.ML",table2text,"Current efficient fine-tuning methods (e.g., adapters, prefix-tuning, etc.)
have optimized conditional text generation via training a small set of extra
parameters of the neural language model, while freezing the rest for
efficiency. While showing strong performance on some generation tasks, they
don't generalize across all generation tasks. In this work, we show that prompt
based conditional text generation can be improved with simple and efficient
methods that simulate modeling the discourse structure of human written text.
We introduce two key design choices: First we show that a higher-level
discourse structure of human written text can be modelled with
\textit{hierarchical blocking} on prefix parameters that enable spanning
different parts of the input and output text and yield more coherent output
generations. Second, we propose sparse prefix tuning by introducing
\textit{attention sparsity} on the prefix parameters at different layers of the
network and learn sparse transformations on the softmax-function, respectively.
We find that sparse attention enables the prefix-tuning to better control of
the input contents (salient facts) yielding more efficient tuning of the
prefix-parameters. Experiments on a wide-variety of text generation tasks show
that structured design of prefix parameters can achieve comparable results to
fine-tuning all parameters while outperforming standard prefix-tuning on all
generation tasks even in low-resource settings.",2021-12-10
"Unified Multimodal Pre-training and Prompt-based Tuning for
  Vision-Language Understanding and Generation",2021-12-10 14:59:06+00:00,http://arxiv.org/abs/2112.05587v2,"Tianyi Liu, Zuxuan Wu, Wenhan Xiong, Jingjing Chen, Yu-Gang Jiang","cs.CV, cs.CL, cs.LG",table2text,"Most existing vision-language pre-training methods focus on understanding
tasks and use BERT-like objectives (masked language modeling and image-text
matching) during pretraining. Although they perform well in many understanding
downstream tasks, e.g., visual question answering, image-text retrieval and
visual entailment, they do not possess the ability to generate. To tackle this
problem, we propose Unified multimodal pre-training for both Vision-Language
understanding and generation (UniVL). The proposed UniVL is capable of handling
both understanding tasks and generative tasks. We augment existing pretraining
paradigms that only use random masks with causal masks, i.e., triangular masks
that mask out future tokens, such that the pre-trained models can have
autoregressive generation abilities by design. We formulate several previous
understanding tasks as a text generation task and propose to use prompt-based
method for fine-tuning on different downstream tasks. Our experiments show that
there is a trade-off between understanding tasks and generation tasks while
using the same model, and a feasible way to improve both tasks is to use more
data. Our UniVL framework attains comparable performance to recent
vision-language pre-training methods on both understanding tasks and generation
tasks. Moreover, we demostrate that prompt-based finetuning is more
data-efficient - it outperforms discriminative methods in few-shot scenarios.",2021-12-10
"MAGMA -- Multimodal Augmentation of Generative Models through
  Adapter-based Finetuning",2021-12-09 23:58:45+00:00,http://arxiv.org/abs/2112.05253v1,"Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, Anette Frank","cs.CV, cs.CL, I.2.7; I.4.8; I.5.1",table2text,"Large-scale pretraining is fast becoming the norm in Vision-Language (VL)
modeling. However, prevailing VL approaches are limited by the requirement for
labeled data and the use of complex multi-step pretraining objectives. We
present MAGMA - a simple method for augmenting generative language models with
additional modalities using adapter-based finetuning. Building on Frozen, we
train a series of VL models that autoregressively generate text from arbitrary
combinations of visual and textual input. The pretraining is entirely
end-to-end using a single language modeling objective, simplifying optimization
compared to previous approaches. Importantly, the language model weights remain
unchanged during training, allowing for transfer of encyclopedic knowledge and
in-context learning abilities from language pretraining. MAGMA outperforms
Frozen on open-ended generative tasks, achieving state of the art results on
the OKVQA benchmark and competitive results on a range of other popular VL
benchmarks, while pretraining on 0.2% of the number of samples used to train
SimVLM.",2021-12-09
Self-Supervised Image-to-Text and Text-to-Image Synthesis,2021-12-09 13:54:56+00:00,http://arxiv.org/abs/2112.04928v1,"Anindya Sundar Das, Sriparna Saha","cs.CV, cs.CL, cs.LG",table2text,"A comprehensive understanding of vision and language and their interrelation
are crucial to realize the underlying similarities and differences between
these modalities and to learn more generalized, meaningful representations. In
recent years, most of the works related to Text-to-Image synthesis and
Image-to-Text generation, focused on supervised generative deep architectures
to solve the problems, where very little interest was placed on learning the
similarities between the embedding spaces across modalities. In this paper, we
propose a novel self-supervised deep learning based approach towards learning
the cross-modal embedding spaces; for both image to text and text to image
generations. In our approach, we first obtain dense vector representations of
images using StackGAN-based autoencoder model and also dense vector
representations on sentence-level utilizing LSTM based text-autoencoder; then
we study the mapping from embedding space of one modality to embedding space of
the other modality utilizing GAN and maximum mean discrepancy based generative
networks. We, also demonstrate that our model learns to generate textual
description from image data as well as images from textual data both
qualitatively and quantitatively.",2021-12-09
"Does Structure Matter? Leveraging Data-to-Text Generation for Answering
  Complex Information Needs",2021-12-08 15:51:27+00:00,http://arxiv.org/abs/2112.04344v1,"Hanane Djeddal, Thomas Gerald, Laure Soulier, Karen Pinel-Sauvagnat, Lynda Tamine","cs.CL, cs.IR, cs.LG",table2text,"In this work, our aim is to provide a structured answer in natural language
to a complex information need. Particularly, we envision using generative
models from the perspective of data-to-text generation. We propose the use of a
content selection and planning pipeline which aims at structuring the answer by
generating intermediate plans. The experimental evaluation is performed using
the TREC Complex Answer Retrieval (CAR) dataset. We evaluate both the generated
answer and its corresponding structure and show the effectiveness of
planning-based models in comparison to a text-to-text model.",2021-12-08
"Search and Learn: Improving Semantic Coverage for Data-to-Text
  Generation",2021-12-06 03:51:56+00:00,http://arxiv.org/abs/2112.02770v1,"Shailza Jolly, Zi Xuan Zhang, Andreas Dengel, Lili Mou",cs.CL,table2text,"Data-to-text generation systems aim to generate text descriptions based on
input data (often represented in the tabular form). A typical system uses huge
training samples for learning the correspondence between tables and texts.
However, large training sets are expensive to obtain, limiting the
applicability of these approaches in real-world scenarios. In this work, we
focus on few-shot data-to-text generation. We observe that, while fine-tuned
pretrained language models may generate plausible sentences, they suffer from
the low semantic coverage problem in the few-shot setting. In other words,
important input slots tend to be missing in the generated text. To this end, we
propose a search-and-learning approach that leverages pretrained language
models but inserts the missing slots to improve the semantic coverage. We
further fine-tune our system based on the search results to smooth out the
search noise, yielding better-quality text and improving inference efficiency
to a large extent. Experiments show that our model achieves high performance on
E2E and WikiBio datasets. Especially, we cover 98.35% of input slots on E2E,
largely alleviating the low coverage problem.",2021-12-06
"Representation Learning for Conversational Data using Discourse Mutual
  Information Maximization",2021-12-04 13:17:07+00:00,http://arxiv.org/abs/2112.05787v1,"Bishal Santra, Sumegh Roychowdhury, Aishik Mandal, Vasu Gurram, Atharva Naik, Manish Gupta, Pawan Goyal",cs.CL,table2text,"Although many pretrained models exist for text or images, there have been
relatively fewer attempts to train representations specifically for dialog
understanding. Prior works usually relied on finetuned representations based on
generic text representation models like BERT or GPT-2. But, existing
pretraining objectives do not take the structural information of text into
consideration. Although generative dialog models can learn structural features
too, we argue that the structure-unaware word-by-word generation is not
suitable for effective conversation modeling. We empirically demonstrate that
such representations do not perform consistently across various dialog
understanding tasks. Hence, we propose a structure-aware Mutual Information
based loss-function DMI (Discourse Mutual Information) for training
dialog-representation models, that additionally captures the inherent
uncertainty in response prediction. Extensive evaluation on nine diverse dialog
modeling tasks shows that our proposed DMI-based models outperform strong
baselines by significant margins, even with small-scale pretraining. Our models
show the most promising performance on the dialog evaluation task
DailyDialog++, in both random and adversarial negative scenarios.",2021-12-04
"LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with
  Self-training",2021-12-02 16:49:41+00:00,http://arxiv.org/abs/2112.01404v1,"Ningyu Zhang, Hongbin Ye, Jiacheng Yang, Shumin Deng, Chuanqi Tan, Mosha Chen, Songfang Huang, Fei Huang, Huajun Chen","cs.CL, cs.AI",table2text,"Natural language generation from structured data mainly focuses on
surface-level descriptions, suffering from uncontrollable content selection and
low fidelity. Previous works leverage logical forms to facilitate logical
knowledge-conditioned text generation. Though achieving remarkable progress,
they are data-hungry, which makes the adoption for real-world applications
challenging with limited data. To this end, this paper proposes a unified
framework for logical knowledge-conditioned text generation in the few-shot
setting. With only a few seeds logical forms (e.g., 20/100 shot), our approach
leverages self-training and samples pseudo logical forms based on content and
structure consistency. Experimental results demonstrate that our approach can
obtain better few-shot performance than baselines.",2021-12-02
LAFITE: Towards Language-Free Training for Text-to-Image Generation,2021-11-27 01:54:45+00:00,http://arxiv.org/abs/2111.13792v1,"Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, Tong Sun","cs.CV, cs.LG",table2text,"One of the major challenges in training text-to-image generation models is
the need of a large number of high-quality image-text pairs. While image
samples are often easily accessible, the associated text descriptions typically
require careful human captioning, which is particularly time- and
cost-consuming. In this paper, we propose the first work to train text-to-image
generation models without any text data. Our method leverages the well-aligned
multi-modal semantic space of the powerful pre-trained CLIP model: the
requirement of text-conditioning is seamlessly alleviated via generating text
features from image features. Extensive experiments are conducted to illustrate
the effectiveness of the proposed method. We obtain state-of-the-art results in
the standard text-to-image generation tasks. Importantly, the proposed
language-free model outperforms most existing models trained with full
image-text pairs. Furthermore, our method can be applied in fine-tuning
pre-trained models, which saves both training time and cost in training
text-to-image generation models. Our pre-trained model obtains competitive
results in zero-shot text-to-image generation on the MS-COCO dataset, yet with
around only 1% of the model size and training data size relative to the
recently proposed large DALL-E model.",2021-11-27
"Octree Transformer: Autoregressive 3D Shape Generation on Hierarchically
  Structured Sequences",2021-11-24 13:17:16+00:00,http://arxiv.org/abs/2111.12480v1,"Moritz Ibing, Gregor Kobsik, Leif Kobbelt","cs.CV, cs.GR, cs.LG",table2text,"Autoregressive models have proven to be very powerful in NLP text generation
tasks and lately have gained popularity for image generation as well. However,
they have seen limited use for the synthesis of 3D shapes so far. This is
mainly due to the lack of a straightforward way to linearize 3D data as well as
to scaling problems with the length of the resulting sequences when describing
complex shapes. In this work we address both of these problems. We use octrees
as a compact hierarchical shape representation that can be sequentialized by
traversal ordering. Moreover, we introduce an adaptive compression scheme, that
significantly reduces sequence lengths and thus enables their effective
generation with a transformer, while still allowing fully autoregressive
sampling and parallel training. We demonstrate the performance of our model by
comparing against the state-of-the-art in shape generation.",2021-11-24
"Selection of pseudo-annotated data for adverse drug reaction
  classification across drug groups",2021-11-24 13:11:05+00:00,http://arxiv.org/abs/2111.12477v1,"Ilseyar Alimova, Elena Tutubalina","cs.CL, cs.LG",table2text,"Automatic monitoring of adverse drug events (ADEs) or reactions (ADRs) is
currently receiving significant attention from the biomedical community. In
recent years, user-generated data on social media has become a valuable
resource for this task. Neural models have achieved impressive performance on
automatic text classification for ADR detection. Yet, training and evaluation
of these methods are carried out on user-generated texts about a targeted drug.
In this paper, we assess the robustness of state-of-the-art neural
architectures across different drug groups. We investigate several strategies
to use pseudo-labeled data in addition to a manually annotated train set.
Out-of-dataset experiments diagnose the bottleneck of supervised models in
terms of breakdown performance, while additional pseudo-labeled data improves
overall results regardless of the text selection strategy.",2021-11-24
NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,2021-11-24 11:02:12+00:00,http://arxiv.org/abs/2111.12417v1,"Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan","cs.CV, cs.AI",table2text,"This paper presents a unified multimodal pre-trained model called N\""UWA that
can generate new or manipulate existing visual data (i.e., images and videos)
for various visual synthesis tasks. To cover language, image, and video at the
same time for different scenarios, a 3D transformer encoder-decoder framework
is designed, which can not only deal with videos as 3D data but also adapt to
texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA)
mechanism is also proposed to consider the nature of the visual data and reduce
the computational complexity. We evaluate N\""UWA on 8 downstream tasks.
Compared to several strong baselines, N\""UWA achieves state-of-the-art results
on text-to-image generation, text-to-video generation, video prediction, etc.
Furthermore, it also shows surprisingly good zero-shot capabilities on
text-guided image and video manipulation tasks. Project repo is
https://github.com/microsoft/NUWA.",2021-11-24
Reinforcement Learning for Few-Shot Text Generation Adaptation,2021-11-22 07:33:40+00:00,http://arxiv.org/abs/2111.11030v1,"Cheng Pengsen, Dai Jinqiao, Liu Jiayong",cs.CL,table2text,"Controlling the generative model to adapt a new domain with limited samples
is a difficult challenge and it is receiving increasing attention. Recently,
few-shot learning has shown promising process in domain adaptation. However,
the texts generated by few-shot learning are typically devoid of linguistic
diversity. To address this shortcoming, we frame the adaptation of text
generation systems as a reinforcement learning problem and provide a new
approach to make text generation models easily adaptable to target domain with
the minimal amount of in-domain data. Experimental results on five target
domains in two few-shot configurations demonstrate that our method
significantly outperforms domain adaptation when very few in-domain samples are
available.",2021-11-22
"RDF-to-Text Generation with Reinforcement Learning Based Graph-augmented
  Structural Neural Encoders",2021-11-20 08:41:54+00:00,http://arxiv.org/abs/2111.10545v1,"Hanning Gao, Lingfei Wu, Po Hu, Zhihua Wei, Fangli Xu, Bo Long","cs.CL, cs.AI",table2text,"Considering a collection of RDF triples, the RDF-to-text generation task aims
to generate a text description. Most previous methods solve this task using a
sequence-to-sequence model or using a graph-based model to encode RDF triples
and to generate a text sequence. Nevertheless, these approaches fail to clearly
model the local and global structural information between and within RDF
triples. Moreover, the previous methods also face the non-negligible problem of
low faithfulness of the generated text, which seriously affects the overall
performance of these models. To solve these problems, we propose a model
combining two new graph-augmented structural neural encoders to jointly learn
both local and global structural information in the input RDF triples. To
further improve text faithfulness, we innovatively introduce a reinforcement
learning (RL) reward based on information extraction (IE). We first extract
triples from the generated text using a pretrained IE model and regard the
correct number of the extracted triples as the additional RL reward.
Experimental results on two benchmark datasets demonstrate that our proposed
model outperforms the state-of-the-art baselines, and the additional
reinforcement learning reward does help to improve the faithfulness of the
generated text.",2021-11-20
"How much do language models copy from their training data? Evaluating
  linguistic novelty in text generation using RAVEN",2021-11-18 04:07:09+00:00,http://arxiv.org/abs/2111.09509v1,"R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, Asli Celikyilmaz",cs.CL,table2text,"Current language models can generate high-quality text. Are they simply
copying text they have seen before, or have they learned generalizable
linguistic abstractions? To tease apart these possibilities, we introduce
RAVEN, a suite of analyses for assessing the novelty of generated text,
focusing on sequential structure (n-grams) and syntactic structure. We apply
these analyses to four neural language models (an LSTM, a Transformer,
Transformer-XL, and GPT-2). For local structure - e.g., individual dependencies
- model-generated text is substantially less novel than our baseline of
human-generated text from each model's test set. For larger-scale structure -
e.g., overall sentence structure - model-generated text is as novel or even
more novel than the human-generated baseline, but models still sometimes copy
substantially, in some cases duplicating passages over 1,000 words long from
the training set. We also perform extensive manual analysis showing that
GPT-2's novel text is usually well-formed morphologically and syntactically but
has reasonably frequent semantic issues (e.g., being self-contradictory).",2021-11-18
"MEDCOD: A Medically-Accurate, Emotive, Diverse, and Controllable Dialog
  System",2021-11-17 20:31:16+00:00,http://arxiv.org/abs/2111.09381v1,"Rhys Compton, Ilya Valmianski, Li Deng, Costa Huang, Namit Katariya, Xavier Amatriain, Anitha Kannan","cs.CL, cs.AI, cs.LG",table2text,"We present MEDCOD, a Medically-Accurate, Emotive, Diverse, and Controllable
Dialog system with a unique approach to the natural language generator module.
MEDCOD has been developed and evaluated specifically for the history taking
task. It integrates the advantage of a traditional modular approach to
incorporate (medical) domain knowledge with modern deep learning techniques to
generate flexible, human-like natural language expressions. Two key aspects of
MEDCOD's natural language output are described in detail. First, the generated
sentences are emotive and empathetic, similar to how a doctor would communicate
to the patient. Second, the generated sentence structures and phrasings are
varied and diverse while maintaining medical consistency with the desired
medical concept (provided by the dialogue manager module of MEDCOD).
Experimental results demonstrate the effectiveness of our approach in creating
a human-like medical dialogue system. Relevant code is available at
https://github.com/curai/curai-research/tree/main/MEDCOD",2021-11-17
"Entropy optimized semi-supervised decomposed vector-quantized
  variational autoencoder model based on transfer learning for multiclass text
  classification and generation",2021-11-10 07:07:54+00:00,http://arxiv.org/abs/2111.08453v1,"Shivani Malhotra, Vinay Kumar, Alpana Agarwal","cs.LG, cs.IT, math.IT",table2text,"Semisupervised text classification has become a major focus of research over
the past few years. Hitherto, most of the research has been based on supervised
learning, but its main drawback is the unavailability of labeled data samples
in practical applications. It is still a key challenge to train the deep
generative models and learn comprehensive representations without supervision.
Even though continuous latent variables are employed primarily in deep latent
variable models, discrete latent variables, with their enhanced
understandability and better compressed representations, are effectively used
by researchers. In this paper, we propose a semisupervised discrete latent
variable model for multi-class text classification and text generation. The
proposed model employs the concept of transfer learning for training a
quantized transformer model, which is able to learn competently using fewer
labeled instances. The model applies decomposed vector quantization technique
to overcome problems like posterior collapse and index collapse. Shannon
entropy is used for the decomposed sub-encoders, on which a variable
DropConnect is applied, to retain maximum information. Moreover, gradients of
the Loss function are adaptively modified during backpropagation from decoder
to encoder to enhance the performance of the model. Three conventional datasets
of diversified range have been used for validating the proposed model on a
variable number of labeled instances. Experimental results indicate that the
proposed model has surpassed the state-of-the-art models remarkably.",2021-11-10
Explaining Face Presentation Attack Detection Using Natural Language,2021-11-08 22:55:55+00:00,http://arxiv.org/abs/2111.04862v1,"Hengameh Mirzaalian, Mohamed E. Hussein, Leonidas Spinoulas, Jonathan May, Wael Abd-Almageed","cs.CV, cs.AI, cs.CL, cs.CR",table2text,"A large number of deep neural network based techniques have been developed to
address the challenging problem of face presentation attack detection (PAD).
Whereas such techniques' focus has been on improving PAD performance in terms
of classification accuracy and robustness against unseen attacks and
environmental conditions, there exists little attention on the explainability
of PAD predictions. In this paper, we tackle the problem of explaining PAD
predictions through natural language. Our approach passes feature
representations of a deep layer of the PAD model to a language model to
generate text describing the reasoning behind the PAD prediction. Due to the
limited amount of annotated data in our study, we apply a light-weight LSTM
network as our natural language generation model. We investigate how the
quality of the generated explanations is affected by different loss functions,
including the commonly used word-wise cross entropy loss, a sentence
discriminative loss, and a sentence semantic loss. We perform our experiments
using face images from a dataset consisting of 1,105 bona-fide and 924
presentation attack samples. Our quantitative and qualitative results show the
effectiveness of our model for generating proper PAD explanations through text
as well as the power of the sentence-wise losses. To the best of our knowledge,
this is the first introduction of a joint biometrics-NLP task. Our dataset can
be obtained through our GitHub page.",2021-11-08
Unsupervised and Distributional Detection of Machine-Generated Text,2021-11-04 14:07:46+00:00,http://arxiv.org/abs/2111.02878v1,"Matthias Gallé, Jos Rozen, Germán Kruszewski, Hady Elsahar","cs.CL, cs.IR",table2text,"The power of natural language generation models has provoked a flurry of
interest in automatic methods to detect if a piece of text is human or
machine-authored. The problem so far has been framed in a standard supervised
way and consists in training a classifier on annotated data to predict the
origin of one given new document. In this paper, we frame the problem in an
unsupervised and distributional way: we assume that we have access to a large
collection of unannotated documents, a big fraction of which is
machine-generated. We propose a method to detect those machine-generated
documents leveraging repeated higher-order n-grams, which we show over-appear
in machine-generated text as compared to human ones. That weak signal is the
starting point of a self-training setting where pseudo-labelled documents are
used to train an ensemble of classifiers. Our experiments show that leveraging
that signal allows us to rank suspicious documents accurately. Precision at
5000 is over 90% for top-k sampling strategies, and over 80% for nucleus
sampling for the largest model we used (GPT2-large). The drop with increased
size of model is small, which could indicate that the results hold for other
current and future large language models.",2021-11-04
"Recent Advances in Natural Language Processing via Large Pre-Trained
  Language Models: A Survey",2021-11-01 20:08:05+00:00,http://arxiv.org/abs/2111.01243v1,"Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heinz, Dan Roth","cs.CL, cs.AI, cs.LG",table2text,"Large, pre-trained transformer-based language models such as BERT have
drastically changed the Natural Language Processing (NLP) field. We present a
survey of recent work that uses these large language models to solve NLP tasks
via pre-training then fine-tuning, prompting, or text generation approaches. We
also present approaches that use pre-trained language models to generate data
for training augmentation or other purposes. We conclude with discussions on
limitations and suggested directions for future research.",2021-11-01
"Generating artificial texts as substitution or complement of training
  data",2021-10-25 14:53:42+00:00,http://arxiv.org/abs/2110.13016v1,"Vincent Claveau, Antoine Chaffin, Ewa Kijak","cs.CL, cs.IR",table2text,"The quality of artificially generated texts has considerably improved with
the advent of transformers. The question of using these models to generate
learning data for supervised learning tasks naturally arises. In this article,
this question is explored under 3 aspects: (i) are artificial data an efficient
complement? (ii) can they replace the original data when those are not
available or cannot be distributed for confidentiality reasons? (iii) can they
improve the explainability of classifiers? Different experiments are carried
out on Web-related classification tasks -- namely sentiment analysis on product
reviews and Fake News detection -- using artificially generated data by
fine-tuned GPT-2 models. The results show that such artificial data can be used
in a certain extend but require pre-processing to significantly improve
performance. We show that bag-of-word approaches benefit the most from such
data augmentation.",2021-10-25
