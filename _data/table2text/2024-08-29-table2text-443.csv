title,pubdate,id,authors,categories,search,abstract,displaydate
"The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text
  Memorization",2024-08-29 08:30:33+00:00,http://arxiv.org/abs/2408.16345v1,"Luka Borec, Philipp Sadler, David Schlangen",cs.CL,table2text,"This work analyses the text memorization behavior of large language models
(LLMs) when subjected to nucleus sampling. Stochastic decoding methods like
nucleus sampling are typically applied to overcome issues such as monotonous
and repetitive text generation, which are often observed with
maximization-based decoding techniques. We hypothesize that nucleus sampling
might also reduce the occurrence of memorization patterns, because it could
lead to the selection of tokens outside the memorized sequence. To test this
hypothesis we create a diagnostic dataset with a known distribution of
duplicates that gives us some control over the likelihood of memorization of
certain parts of the training data. Our analysis of two GPT-Neo models
fine-tuned on this dataset interestingly shows that (i) an increase of the
nucleus size reduces memorization only modestly, and (ii) even when models do
not engage in ""hard"" memorization -- a verbatim reproduction of training
samples -- they may still display ""soft"" memorization whereby they generate
outputs that echo the training data but without a complete one-by-one
resemblance.",2024-08-29
Spatio-Temporal Context Prompting for Zero-Shot Action Detection,2024-08-28 17:59:05+00:00,http://arxiv.org/abs/2408.15996v2,"Wei-Jhe Huang, Min-Hung Chen, Shang-Hong Lai","cs.CV, cs.AI",table2text,"Spatio-temporal action detection encompasses the tasks of localizing and
classifying individual actions within a video. Recent works aim to enhance this
process by incorporating interaction modeling, which captures the relationship
between people and their surrounding context. However, these approaches have
primarily focused on fully-supervised learning, and the current limitation lies
in the lack of generalization capability to recognize unseen action categories.
In this paper, we aim to adapt the pretrained image-language models to detect
unseen actions. To this end, we propose a method which can effectively leverage
the rich knowledge of visual-language models to perform Person-Context
Interaction. Meanwhile, our Context Prompting module will utilize contextual
information to prompt labels, thereby enhancing the generation of more
representative text features. Moreover, to address the challenge of recognizing
distinct actions by multiple people at the same timestamp, we design the
Interest Token Spotting mechanism which employs pretrained visual knowledge to
find each person's interest context tokens, and then these tokens will be used
for prompting to generate text features tailored to each individual. To
evaluate the ability to detect unseen actions, we propose a comprehensive
benchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our
method achieves superior results compared to previous approaches and can be
further extended to multi-action videos, bringing it closer to real-world
applications. The code and data can be found in
https://webber2933.github.io/ST-CLIP-project-page.",2024-08-28
DIAGen: Diverse Image Augmentation with Generative Models,2024-08-26 19:09:13+00:00,http://arxiv.org/abs/2408.14584v1,"Tobias Lingenberg, Markus Reuter, Gopika Sudhakaran, Dominik Gojny, Stefan Roth, Simone Schaub-Meyer","cs.CV, cs.AI",table2text,"Simple data augmentation techniques, such as rotations and flips, are widely
used to enhance the generalization power of computer vision models. However,
these techniques often fail to modify high-level semantic attributes of a
class. To address this limitation, researchers have explored generative
augmentation methods like the recently proposed DA-Fusion. Despite some
progress, the variations are still largely limited to textural changes, thus
falling short on aspects like varied viewpoints, environment, weather
conditions, or even class-level semantic attributes (eg, variations in a dog's
breed). To overcome this challenge, we propose DIAGen, building upon DA-Fusion.
First, we apply Gaussian noise to the embeddings of an object learned with
Textual Inversion to diversify generations using a pre-trained diffusion
model's knowledge. Second, we exploit the general knowledge of a text-to-text
generative model to guide the image generation of the diffusion model with
varied class-specific prompts. Finally, we introduce a weighting mechanism to
mitigate the impact of poorly generated samples. Experimental results across
various datasets show that DIAGen not only enhances semantic diversity but also
improves the performance of subsequent classifiers. The advantages of DIAGen
over standard augmentations and the DA-Fusion baseline are particularly
pronounced with out-of-distribution samples.",2024-08-26
"Predictability and Causality in Spanish and English Natural Language
  Generation",2024-08-26 14:09:28+00:00,http://arxiv.org/abs/2408.14283v1,"Andrea Busto-Castiñeira, Francisco J. González-Castaño, Silvia García-Méndez, Francisco de Arriba-Pérez",cs.CL,table2text,"In recent years, the field of Natural Language Generation (NLG) has been
boosted by the recent advances in deep learning technologies. Nonetheless,
these new data-intensive methods introduce language-dependent disparities in
NLG as the main training data sets are in English. Also, most neural NLG
systems use decoder-only (causal) transformer language models, which work well
for English, but were not designed with other languages in mind. In this work
we depart from the hypothesis that they may introduce generation bias in target
languages with less rigid word ordering, subject omission, or different
attachment preferences for relative clauses, so that for these target languages
other language generation strategies may be more desirable. This paper first
compares causal and non-causal language modeling for English and Spanish, two
languages with different grammatical structures and over 1.5 billion and 0.5
billion speakers, respectively. For this purpose, we define a novel metric of
average causal and non-causal context-conditioned entropy of the grammatical
category distribution for both languages as an information-theoretic a priori
approach. The evaluation of natural text sources (such as training data) in
both languages reveals lower average non-causal conditional entropy in Spanish
and lower causal conditional entropy in English. According to this experiment,
Spanish is more predictable than English given a non-causal context. Then, by
applying a conditional relative entropy metric to text generation experiments,
we obtain as insights that the best performance is respectively achieved with
causal NLG in English, and with non-causal NLG in Spanish. These insights
support further research in NLG in Spanish using bidirectional transformer
language models.",2024-08-26
"Biomedical Large Languages Models Seem not to be Superior to Generalist
  Models on Unseen Medical Data",2024-08-25 13:36:22+00:00,http://arxiv.org/abs/2408.13833v1,"Felix J. Dorfner, Amin Dada, Felix Busch, Marcus R. Makowski, Tianyu Han, Daniel Truhn, Jens Kleesiek, Madhumita Sushil, Jacqueline Lammert, Lisa C. Adams, Keno K. Bressem",cs.CL,table2text,"Large language models (LLMs) have shown potential in biomedical applications,
leading to efforts to fine-tune them on domain-specific data. However, the
effectiveness of this approach remains unclear. This study evaluates the
performance of biomedically fine-tuned LLMs against their general-purpose
counterparts on a variety of clinical tasks. We evaluated their performance on
clinical case challenges from the New England Journal of Medicine (NEJM) and
the Journal of the American Medical Association (JAMA) and on several clinical
tasks (e.g., information extraction, document summarization, and clinical
coding). Using benchmarks specifically chosen to be likely outside the
fine-tuning datasets of biomedical models, we found that biomedical LLMs mostly
perform inferior to their general-purpose counterparts, especially on tasks not
focused on medical knowledge. While larger models showed similar performance on
case tasks (e.g., OpenBioLLM-70B: 66.4% vs. Llama-3-70B-Instruct: 65% on JAMA
cases), smaller biomedical models showed more pronounced underperformance
(e.g., OpenBioLLM-8B: 30% vs. Llama-3-8B-Instruct: 64.3% on NEJM cases).
Similar trends were observed across the CLUE (Clinical Language Understanding
Evaluation) benchmark tasks, with general-purpose models often performing
better on text generation, question answering, and coding tasks. Our results
suggest that fine-tuning LLMs to biomedical data may not provide the expected
benefits and may potentially lead to reduced performance, challenging
prevailing assumptions about domain-specific adaptation of LLMs and
highlighting the need for more rigorous evaluation frameworks in healthcare AI.
Alternative approaches, such as retrieval-augmented generation, may be more
effective in enhancing the biomedical capabilities of LLMs without compromising
their general knowledge.",2024-08-25
DHP Benchmark: Are LLMs Good NLG Evaluators?,2024-08-25 02:01:38+00:00,http://arxiv.org/abs/2408.13704v1,"Yicheng Wang, Jiayi Yuan, Yu-Neng Chuang, Zhuoer Wang, Yingchi Liu, Mark Cusick, Param Kulkarni, Zhengping Ji, Yasser Ibrahim, Xia Hu","cs.CL, cs.AI",table2text,"Large Language Models (LLMs) are increasingly serving as evaluators in
Natural Language Generation (NLG) tasks. However, the capabilities of LLMs in
scoring NLG quality remain inadequately explored. Current studies depend on
human assessments and simple metrics that fail to capture the discernment of
LLMs across diverse NLG tasks. To address this gap, we propose the Discernment
of Hierarchical Perturbation (DHP) benchmarking framework, which provides
quantitative discernment scores for LLMs utilizing hierarchically perturbed
text data and statistical tests to measure the NLG evaluation capabilities of
LLMs systematically. We have re-established six evaluation datasets for this
benchmark, covering four NLG tasks: Summarization, Story Completion, Question
Answering, and Translation. Our comprehensive benchmarking of five major LLM
series provides critical insight into their strengths and limitations as NLG
evaluators.",2024-08-25
"BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large
  Language Models",2024-08-23 02:21:21+00:00,http://arxiv.org/abs/2408.12798v1,"Yige Li, Hanxun Huang, Yunhan Zhao, Xingjun Ma, Jun Sun",cs.AI,table2text,"Generative Large Language Models (LLMs) have made significant strides across
various tasks, but they remain vulnerable to backdoor attacks, where specific
triggers in the prompt cause the LLM to generate adversary-desired responses.
While most backdoor research has focused on vision or text classification
tasks, backdoor attacks in text generation have been largely overlooked. In
this work, we introduce \textit{BackdoorLLM}, the first comprehensive benchmark
for studying backdoor attacks on LLMs. \textit{BackdoorLLM} features: 1) a
repository of backdoor benchmarks with a standardized training pipeline, 2)
diverse attack strategies, including data poisoning, weight poisoning, hidden
state attacks, and chain-of-thought attacks, 3) extensive evaluations with over
200 experiments on 8 attacks across 7 scenarios and 6 model architectures, and
4) key insights into the effectiveness and limitations of backdoors in LLMs. We
hope \textit{BackdoorLLM} will raise awareness of backdoor threats and
contribute to advancing AI safety. The code is available at
\url{https://github.com/bboylyg/BackdoorLLM}.",2024-08-23
"GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender
  Bias in Large Language Models",2024-08-22 15:35:46+00:00,http://arxiv.org/abs/2408.12494v1,"Kunsheng Tang, Wenbo Zhou, Jie Zhang, Aishan Liu, Gelei Deng, Shuai Li, Peigui Qi, Weiming Zhang, Tianwei Zhang, Nenghai Yu","cs.CL, cs.AI",table2text,"Large language models (LLMs) have exhibited remarkable capabilities in
natural language generation, but they have also been observed to magnify
societal biases, particularly those related to gender. In response to this
issue, several benchmarks have been proposed to assess gender bias in LLMs.
However, these benchmarks often lack practical flexibility or inadvertently
introduce biases. To address these shortcomings, we introduce GenderCARE, a
comprehensive framework that encompasses innovative Criteria, bias Assessment,
Reduction techniques, and Evaluation metrics for quantifying and mitigating
gender bias in LLMs. To begin, we establish pioneering criteria for gender
equality benchmarks, spanning dimensions such as inclusivity, diversity,
explainability, objectivity, robustness, and realisticity. Guided by these
criteria, we construct GenderPair, a novel pair-based benchmark designed to
assess gender bias in LLMs comprehensively. Our benchmark provides standardized
and realistic evaluations, including previously overlooked gender groups such
as transgender and non-binary individuals. Furthermore, we develop effective
debiasing techniques that incorporate counterfactual data augmentation and
specialized fine-tuning strategies to reduce gender bias in LLMs without
compromising their overall performance. Extensive experiments demonstrate a
significant reduction in various gender bias benchmarks, with reductions
peaking at over 90% and averaging above 35% across 17 different LLMs.
Importantly, these reductions come with minimal variability in mainstream
language tasks, remaining below 2%. By offering a realistic assessment and
tailored reduction of gender biases, we hope that our GenderCARE can represent
a significant step towards achieving fairness and equity in LLMs. More details
are available at https://github.com/kstanghere/GenderCARE-ccs24.",2024-08-22
Preference-Guided Reflective Sampling for Aligning Language Models,2024-08-22 07:18:46+00:00,http://arxiv.org/abs/2408.12163v1,"Hai Ye, Hwee Tou Ng",cs.CL,table2text,"Large language models (LLMs) are aligned with human preferences by
reinforcement learning from human feedback (RLHF). Effective data sampling is
crucial for RLHF, as it determines the efficiency of model training, ensuring
that models learn from the informative samples. To achieve better data
generation, we propose a new sampling method called Preference-Guided
Reflective Sampling (PRS). PRS frames the response generation as an
optimization process to the explicitly specified user preference described in
natural language. It employs a tree-based generation framework to enable an
efficient sampling process, which guides the direction of generation through
preference and better explores the sampling space with adaptive
self-refinement. Notably, PRS can align LLMs to diverse preferences. We study
preference-controlled text generation for instruction following and
keyword-focused document summarization. Our findings indicate that PRS, across
different LLM policies, generates training data with much higher rewards than
strong baselines. PRS also excels in post-RL training.",2024-08-22
Xinyu: An Efficient LLM-based System for Commentary Generation,2024-08-21 13:34:29+00:00,http://arxiv.org/abs/2408.11609v2,"Yiquan Wu, Bo Tang, Chenyang Xi, Yu Yu, Pengyu Wang, Yifei Liu, Kun Kuang, Haiying Deng, Zhiyu Li, Feiyu Xiong, Jie Hu, Peng Cheng, Zhonghao Wang, Yi Wang, Yi Luo, Mingchuan Yang","cs.CL, cs.AI, I.2.7",table2text,"Commentary provides readers with a deep understanding of events by presenting
diverse arguments and evidence. However, creating commentary is a
time-consuming task, even for skilled commentators. Large language models
(LLMs) have simplified the process of natural language generation, but their
direct application in commentary creation still faces challenges due to unique
task requirements. These requirements can be categorized into two levels: 1)
fundamental requirements, which include creating well-structured and logically
consistent narratives, and 2) advanced requirements, which involve generating
quality arguments and providing convincing evidence. In this paper, we
introduce Xinyu, an efficient LLM-based system designed to assist commentators
in generating Chinese commentaries. To meet the fundamental requirements, we
deconstruct the generation process into sequential steps, proposing targeted
strategies and supervised fine-tuning (SFT) for each step. To address the
advanced requirements, we present an argument ranking model for arguments and
establish a comprehensive evidence database that includes up-to-date events and
classic books, thereby strengthening the substantiation of the evidence with
retrieval augmented generation (RAG) technology. To evaluate the generated
commentaries more fairly, corresponding to the two-level requirements, we
introduce a comprehensive evaluation metric that considers five distinct
perspectives in commentary generation. Our experiments confirm the
effectiveness of our proposed system. We also observe a significant increase in
the efficiency of commentators in real-world scenarios, with the average time
spent on creating a commentary dropping from 4 hours to 20 minutes.
Importantly, such an increase in efficiency does not compromise the quality of
the commentaries.",2024-08-21
"Tracing Privacy Leakage of Language Models to Training Data via Adjusted
  Influence Functions",2024-08-20 00:40:49+00:00,http://arxiv.org/abs/2408.10468v3,"Jinxin Liu, Zao Yang","cs.LG, cs.CL, cs.CR",table2text,"The responses generated by Large Language Models (LLMs) can include sensitive
information from individuals and organizations, leading to potential privacy
leakage. This work implements Influence Functions (IFs) to trace privacy
leakage back to the training data, thereby mitigating privacy concerns of
Language Models (LMs). However, we notice that current IFs struggle to
accurately estimate the influence of tokens with large gradient norms,
potentially overestimating their influence. When tracing the most influential
samples, this leads to frequently tracing back to samples with large gradient
norm tokens, overshadowing the actual most influential samples even if their
influences are well estimated. To address this issue, we propose Heuristically
Adjusted IF (HAIF), which reduces the weight of tokens with large gradient
norms, thereby significantly improving the accuracy of tracing the most
influential samples. To establish easily obtained groundtruth for tracing
privacy leakage, we construct two datasets, PII-E and PII-CR, representing two
distinct scenarios: one with identical text in the model outputs and
pre-training data, and the other where models leverage their reasoning
abilities to generate text divergent from pre-training data. HAIF significantly
improves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E
dataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA
IFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs
on real-world pretraining data CLUECorpus2020, demonstrating strong robustness
regardless prompt and response lengths.",2024-08-20
"SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From
  Pre-Trained Foundation Models",2024-08-19 17:32:15+00:00,http://arxiv.org/abs/2408.10174v2,"Anke Tang, Li Shen, Yong Luo, Shuai Xie, Han Hu, Lefei Zhang, Bo Du, Dacheng Tao","cs.LG, cs.AI",table2text,"Deep model training on extensive datasets is increasingly becoming
cost-prohibitive, prompting the widespread adoption of deep model fusion
techniques to leverage knowledge from pre-existing models. From simple weight
averaging to more sophisticated methods like AdaMerging, model fusion
effectively improves model performance and accelerates the development of new
models. However, potential interference between parameters of individual models
and the lack of interpretability in the fusion progress remain significant
challenges. Existing methods often try to resolve the parameter interference
issue by evaluating attributes of parameters, such as their magnitude or sign,
or by parameter pruning. In this study, we begin by examining the fine-tuning
of linear layers through the lens of subspace analysis and explicitly define
parameter interference as an optimization problem to shed light on this
subject. Subsequently, we introduce an innovative approach to model fusion
called zero-shot Sparse MIxture of Low-rank Experts (SMILE) construction, which
allows for the upscaling of source models into an MoE model without extra data
or further training. Our approach relies on the observation that fine-tuning
mostly keeps the important parts from the pre-training, but it uses less
significant or unused areas to adapt to new tasks. Also, the issue of parameter
interference, which is intrinsically intractable in the original parameter
space, can be managed by expanding the dimensions. We conduct extensive
experiments across diverse scenarios, such as image classification and text
generation tasks, using full fine-tuning and LoRA fine-tuning, and we apply our
method to large language models (CLIP models, Flan-T5 models, and Mistral-7B
models), highlighting the adaptability and scalability of SMILE. Code is
available at https://github.com/tanganke/fusion_bench",2024-08-19
LLMs' Understanding of Natural Language Revealed,2024-07-29 01:21:11+00:00,http://arxiv.org/abs/2407.19630v1,Walid S. Saba,cs.AI,table2text,"Large language models (LLMs) are the result of a massive experiment in
bottom-up, data-driven reverse engineering of language at scale. Despite their
utility in a number of downstream NLP tasks, ample research has shown that LLMs
are incapable of performing reasoning in tasks that require quantification over
and the manipulation of symbolic variables (e.g., planning and problem
solving); see for example [25][26]. In this document, however, we will focus on
testing LLMs for their language understanding capabilities, their supposed
forte. As we will show here, the language understanding capabilities of LLMs
have been widely exaggerated. While LLMs have proven to generate human-like
coherent language (since that's how they were designed), their language
understanding capabilities have not been properly tested. In particular, we
believe that the language understanding capabilities of LLMs should be tested
by performing an operation that is the opposite of 'text generation' and
specifically by giving the LLM snippets of text as input and then querying what
the LLM ""understood"". As we show here, when doing so it will become apparent
that LLMs do not truly understand language, beyond very superficial inferences
that are essentially the byproduct of the memorization of massive amounts of
ingested text.",2024-07-29
"The Power of Combining Data and Knowledge: GPT-4o is an Effective
  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of
  Lung Cancer",2024-07-25 09:42:24+00:00,http://arxiv.org/abs/2407.17900v2,"Danqing Hu, Bing Liu, Xiaofeng Zhu, Nan Wu","cs.CL, cs.LG",table2text,"Lymph node metastasis (LNM) is a crucial factor in determining the initial
treatment for patients with lung cancer, yet accurate preoperative diagnosis of
LNM remains challenging. Recently, large language models (LLMs) have garnered
significant attention due to their remarkable text generation capabilities.
Leveraging the extensive medical knowledge learned from vast corpora, LLMs can
estimate probabilities for clinical problems, though their performance has
historically been inferior to data-driven machine learning models. In this
paper, we propose a novel ensemble method that combines the medical knowledge
acquired by LLMs with the latent patterns identified by machine learning models
to enhance LNM prediction performance. Initially, we developed machine learning
models using patient data. We then designed a prompt template to integrate the
patient data with the predicted probability from the machine learning model.
Subsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,
to estimate the likelihood of LNM based on patient data and then adjust the
estimate using the machine learning output. Finally, we collected three outputs
from the GPT-4o using the same prompt and ensembled these results as the final
prediction. Using the proposed method, our models achieved an AUC value of
0.765 and an AP value of 0.415 for LNM prediction, significantly improving
predictive performance compared to baseline machine learning models. The
experimental results indicate that GPT-4o can effectively leverage its medical
knowledge and the probabilities predicted by machine learning models to achieve
more accurate LNM predictions. These findings demonstrate that LLMs can perform
well in clinical risk prediction tasks, offering a new paradigm for integrating
medical knowledge and patient data in clinical predictions.",2024-07-25
"Are Large Language Models Possible to Conduct Cognitive Behavioral
  Therapy?",2024-07-25 03:01:47+00:00,http://arxiv.org/abs/2407.17730v1,"Hao Shen, Zihan Li, Minqiang Yang, Minghui Ni, Yongfeng Tao, Zhengyang Yu, Weihao Zheng, Chen Xu, Bin Hu",cs.CL,table2text,"In contemporary society, the issue of psychological health has become
increasingly prominent, characterized by the diversification, complexity, and
universality of mental disorders. Cognitive Behavioral Therapy (CBT), currently
the most influential and clinically effective psychological treatment method
with no side effects, has limited coverage and poor quality in most countries.
In recent years, researches on the recognition and intervention of emotional
disorders using large language models (LLMs) have been validated, providing new
possibilities for psychological assistance therapy. However, are LLMs truly
possible to conduct cognitive behavioral therapy? Many concerns have been
raised by mental health experts regarding the use of LLMs for therapy. Seeking
to answer this question, we collected real CBT corpus from online video
websites, designed and conducted a targeted automatic evaluation framework
involving the evaluation of emotion tendency of generated text, structured
dialogue pattern and proactive inquiry ability. For emotion tendency, we
calculate the emotion tendency score of the CBT dialogue text generated by each
model. For structured dialogue pattern, we use a diverse range of automatic
evaluation metrics to compare speaking style, the ability to maintain
consistency of topic and the use of technology in CBT between different models
. As for inquiring to guide the patient, we utilize PQA (Proactive Questioning
Ability) metric. We also evaluated the CBT ability of the LLM after integrating
a CBT knowledge base to explore the help of introducing additional knowledge to
enhance the model's CBT counseling ability. Four LLM variants with excellent
performance on natural language processing are evaluated, and the experimental
result shows the great potential of LLMs in psychological counseling realm,
especially after combining with other technological means.",2024-07-25
"IgnitionInnovators at ""Discharge Me!"": Chain-of-Thought Instruction
  Finetuning Large Language Models for Discharge Summaries",2024-07-24 21:02:53+00:00,http://arxiv.org/abs/2407.17636v1,"An Quang Tang, Xiuzhen Zhang, Minh Ngoc Dinh",cs.CL,table2text,"This paper presents our proposed approach to the Discharge Me! shared task,
collocated with the 23th Workshop on Biomedical Natural Language Processing
(BioNLP). In this work, we develop an LLM-based framework for solving the
Discharge Summary Documentation (DSD) task, i.e., generating the two critical
target sections `Brief Hospital Course' and `Discharge Instructions' in the
discharge summary. By streamlining the recent instruction-finetuning process on
LLMs, we explore several prompting strategies for optimally adapting LLMs to
specific generation task of DSD. Experimental results show that providing a
clear output structure, complimented by a set of comprehensive
Chain-of-Thoughts (CoT) questions, effectively improves the model's reasoning
capability, and thereby, enhancing the structural correctness and faithfulness
of clinical information in the generated text. Source code is available at:
https://github.com/antangrocket1312/Discharge_LLM",2024-07-24
A Survey of Text Style Transfer: Applications and Ethical Implications,2024-07-23 17:15:23+00:00,http://arxiv.org/abs/2407.16737v1,"Sourabrata Mukherjee, Mateusz Lango, Zdenek Kasner, Ondrej Dušek",cs.CL,table2text,"Text style transfer (TST) is an important task in controllable text
generation, which aims to control selected attributes of language use, such as
politeness, formality, or sentiment, without altering the style-independent
content of the text. The field has received considerable research attention in
recent years and has already been covered in several reviews, but the focus has
mostly been on the development of new algorithms and learning from different
types of data (supervised, unsupervised, out-of-domain, etc.) and not so much
on the application side. However, TST-related technologies are gradually
reaching a production- and deployment-ready level, and therefore, the inclusion
of the application perspective in TST research becomes crucial. Similarly, the
often overlooked ethical considerations of TST technology have become a
pressing issue. This paper presents a comprehensive review of TST applications
that have been researched over the years, using both traditional linguistic
approaches and more recent deep learning methods. We discuss current
challenges, future research directions, and ethical implications of TST
applications in text generation. By providing a holistic overview of the
landscape of TST applications, we hope to stimulate further research and
contribute to a better understanding of the potential as well as ethical
considerations associated with TST.",2024-07-23
"FairFlow: An Automated Approach to Model-based Counterfactual Data
  Augmentation For NLP",2024-07-23 12:29:37+00:00,http://arxiv.org/abs/2407.16431v1,"Ewoenam Kwaku Tokpo, Toon Calders",cs.CL,table2text,"Despite the evolution of language models, they continue to portray harmful
societal biases and stereotypes inadvertently learned from training data. These
inherent biases often result in detrimental effects in various applications.
Counterfactual Data Augmentation (CDA), which seeks to balance demographic
attributes in training data, has been a widely adopted approach to mitigate
bias in natural language processing. However, many existing CDA approaches rely
on word substitution techniques using manually compiled word-pair dictionaries.
These techniques often lead to out-of-context substitutions, resulting in
potential quality issues. The advancement of model-based techniques, on the
other hand, has been challenged by the need for parallel training data. Works
in this area resort to manually generated parallel data that are expensive to
collect and are consequently limited in scale. This paper proposes FairFlow, an
automated approach to generating parallel data for training counterfactual text
generator models that limits the need for human intervention. Furthermore, we
show that FairFlow significantly overcomes the limitations of dictionary-based
word-substitution approaches whilst maintaining good performance.",2024-07-23
"Robust Privacy Amidst Innovation with Large Language Models Through a
  Critical Assessment of the Risks",2024-07-23 04:20:14+00:00,http://arxiv.org/abs/2407.16166v1,"Yao-Shun Chuang, Atiquer Rahman Sarkar, Noman Mohammed, Xiaoqian Jiang",cs.CL,table2text,"This study examines integrating EHRs and NLP with large language models
(LLMs) to improve healthcare data management and patient care. It focuses on
using advanced models to create secure, HIPAA-compliant synthetic patient notes
for biomedical research. The study used de-identified and re-identified MIMIC
III datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.
Text generation employed templates and keyword extraction for contextually
relevant notes, with one-shot generation for comparison. Privacy assessment
checked PHI occurrence, while text utility was tested using an ICD-9 coding
task. Text quality was evaluated with ROUGE and cosine similarity metrics to
measure semantic similarity with source notes. Analysis of PHI occurrence and
text utility via the ICD-9 coding task showed that the keyword-based method had
low risk and good performance. One-shot generation showed the highest PHI
exposure and PHI co-occurrence, especially in geographic location and date
categories. The Normalized One-shot method achieved the highest classification
accuracy. Privacy analysis revealed a critical balance between data utility and
privacy protection, influencing future data use and sharing. Re-identified data
consistently outperformed de-identified data. This study demonstrates the
effectiveness of keyword-based methods in generating privacy-protecting
synthetic clinical notes that retain data usability, potentially transforming
clinical data-sharing practices. The superior performance of re-identified over
de-identified data suggests a shift towards methods that enhance utility and
privacy by using dummy PHIs to perplex privacy attacks.",2024-07-23
"Finetuning Generative Large Language Models with Discrimination
  Instructions for Knowledge Graph Completion",2024-07-23 02:25:01+00:00,http://arxiv.org/abs/2407.16127v1,"Yang Liu, Xiaobin Tian, Zequn Sun, Wei Hu","cs.CL, cs.AI",table2text,"Traditional knowledge graph (KG) completion models learn embeddings to
predict missing facts. Recent works attempt to complete KGs in a
text-generation manner with large language models (LLMs). However, they need to
ground the output of LLMs to KG entities, which inevitably brings errors. In
this paper, we present a finetuning framework, DIFT, aiming to unleash the KG
completion ability of LLMs and avoid grounding errors. Given an incomplete
fact, DIFT employs a lightweight model to obtain candidate entities and
finetunes an LLM with discrimination instructions to select the correct one
from the given candidates. To improve performance while reducing instruction
data, DIFT uses a truncated sampling method to select useful facts for
finetuning and injects KG embeddings into the LLM. Extensive experiments on
benchmark datasets demonstrate the effectiveness of our proposed framework.",2024-07-23
"When Do Universal Image Jailbreaks Transfer Between Vision-Language
  Models?",2024-07-21 16:27:24+00:00,http://arxiv.org/abs/2407.15211v1,"Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Cristóbal Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, John Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, Ethan Perez","cs.CL, cs.AI, cs.CR, cs.CV, cs.LG",table2text,"The integration of new modalities into frontier AI systems offers exciting
capabilities, but also increases the possibility such systems can be
adversarially manipulated in undesirable ways. In this work, we focus on a
popular class of vision-language models (VLMs) that generate text outputs
conditioned on visual and textual inputs. We conducted a large-scale empirical
study to assess the transferability of gradient-based universal image
""jailbreaks"" using a diverse set of over 40 open-parameter VLMs, including 18
new VLMs that we publicly release. Overall, we find that transferable
gradient-based image jailbreaks are extremely difficult to obtain. When an
image jailbreak is optimized against a single VLM or against an ensemble of
VLMs, the jailbreak successfully jailbreaks the attacked VLM(s), but exhibits
little-to-no transfer to any other VLMs; transfer is not affected by whether
the attacked and target VLMs possess matching vision backbones or language
models, whether the language model underwent instruction-following and/or
safety-alignment training, or many other factors. Only two settings display
partially successful transfer: between identically-pretrained and
identically-initialized VLMs with slightly different VLM training data, and
between different training checkpoints of a single VLM. Leveraging these
results, we then demonstrate that transfer can be significantly improved
against a specific target VLM by attacking larger ensembles of ""highly-similar""
VLMs. These results stand in stark contrast to existing evidence of universal
and transferable text jailbreaks against language models and transferable
adversarial attacks against image classifiers, suggesting that VLMs may be more
robust to gradient-based transfer attacks.",2024-07-21
"Is Behavior Cloning All You Need? Understanding Horizon in Imitation
  Learning",2024-07-20 23:31:56+00:00,http://arxiv.org/abs/2407.15007v1,"Dylan J. Foster, Adam Block, Dipendra Misra","cs.LG, cs.AI, math.ST, stat.ML, stat.TH",table2text,"Imitation learning (IL) aims to mimic the behavior of an expert in a
sequential decision making task by learning from demonstrations, and has been
widely applied to robotics, autonomous driving, and autoregressive text
generation. The simplest approach to IL, behavior cloning (BC), is thought to
incur sample complexity with unfavorable quadratic dependence on the problem
horizon, motivating a variety of different online algorithms that attain
improved linear horizon dependence under stronger assumptions on the data and
the learner's access to the expert.
  We revisit the apparent gap between offline and online IL from a
learning-theoretic perspective, with a focus on general policy classes up to
and including deep neural networks. Through a new analysis of behavior cloning
with the logarithmic loss, we show that it is possible to achieve
horizon-independent sample complexity in offline IL whenever (i) the range of
the cumulative payoffs is controlled, and (ii) an appropriate notion of
supervised learning complexity for the policy class is controlled. Specializing
our results to deterministic, stationary policies, we show that the gap between
offline and online IL is not fundamental: (i) it is possible to achieve linear
dependence on horizon in offline IL under dense rewards (matching what was
previously only known to be achievable in online IL); and (ii) without further
assumptions on the policy class, online IL cannot improve over offline IL with
the logarithmic loss, even in benign MDPs. We complement our theoretical
results with experiments on standard RL tasks and autoregressive language
generation to validate the practical relevance of our findings.",2024-07-20
Check-Eval: A Checklist-based Approach for Evaluating Text Quality,2024-07-19 17:14:16+00:00,http://arxiv.org/abs/2407.14467v1,"Jayr Pereira, Roberto Lotufo","cs.CL, cs.AI",table2text,"Evaluating the quality of text generated by large language models (LLMs)
remains a significant challenge. Traditional metrics often fail to align well
with human judgments, particularly in tasks requiring creativity and nuance. In
this paper, we propose Check-Eval, a novel evaluation framework leveraging LLMs
to assess the quality of generated text through a checklist-based approach.
Check-Eval can be employed as both a reference-free and reference-dependent
evaluation method, providing a structured and interpretable assessment of text
quality. The framework consists of two main stages: checklist generation and
checklist evaluation. We validate Check-Eval on two benchmark datasets:
Portuguese Legal Semantic Textual Similarity and SummEval. Our results
demonstrate that Check-Eval achieves higher correlations with human judgments
compared to existing metrics, such as G-Eval and GPTScore, underscoring its
potential as a more reliable and effective evaluation framework for natural
language generation tasks. The code for our experiments is available at
https://anonymous.4open.science/r/check-eval-0DB4.",2024-07-19
"From Instruction to Insight: Exploring the Functional and Semantic Roles
  of Text in Interactive Dashboards",2024-07-19 16:48:00+00:00,http://arxiv.org/abs/2407.14451v1,"Nicole Sultanum, Vidya Setlur",cs.HC,table2text,"There is increased interest in the interplay between text and visuals in the
field of data visualization. However, this attention has predominantly been on
the use of text in standalone visualizations or augmenting text stories
supported by a series of independent views. In this paper, we shift from the
traditional focus on single-chart annotations to characterize the nuanced but
crucial communication role of text in the complex environment of interactive
dashboards. Through a survey and analysis of 190 dashboards in the wild, plus
13 expert interview sessions with experienced dashboard authors, we highlight
the distinctive nature of text as an integral component of the dashboard
experience, while delving into the categories, semantic levels, and functional
roles of text, and exploring how these text elements are coalesced by dashboard
authors to guide and inform dashboard users.
  Our contributions are: 1) we distill qualitative and quantitative findings
from our studies to characterize current practices of text use in dashboards,
including a categorization of text-based components and design patterns; 2) we
leverage current practices and existing literature to propose, discuss, and
validate recommended practices for text in dashboards, embodied as 12
heuristics that underscore the semantic and functional role of text in offering
navigational cues, contextualizing data insights, supporting reading order,
etc; 3) we reflect on our findings to identify gaps and propose opportunities
for data visualization researchers to push the boundaries on text usage for
dashboards, from authoring support and interactivity to text generation and
content personalization.
  Our research underscores the significance of elevating text as a first-class
citizen in data visualization, and the need to support the inclusion of textual
components and their interactive affordances in dashboard design.",2024-07-19
"Clinical Reading Comprehension with Encoder-Decoder Models Enhanced by
  Direct Preference Optimization",2024-07-19 03:12:10+00:00,http://arxiv.org/abs/2407.14000v1,"Md Sultan Al Nahian, Ramakanth Kavuluru","cs.IR, cs.CL, cs.LG",table2text,"Extractive question answering over clinical text is a crucial need to help
deal with the deluge of clinical text generated in hospitals. While encoder
models (e.g., BERT) have been popular for this reading comprehension task,
recently encoder-decoder models (e.g., T5) are on the rise. There is also the
emergence of preference optimization techniques to align decoder-only LLMs with
human preferences. In this paper, we combine encoder-decoder models with the
direct preference optimization (DPO) method to improve over prior state of the
art for the RadQA radiology question answering task by 12-15 F1 points. To the
best of our knowledge, this effort is the first to show that DPO method also
works for reading comprehension via novel heuristics to generate preference
data without human inputs.",2024-07-19
Do LLMs have Consistent Values?,2024-07-16 08:58:00+00:00,http://arxiv.org/abs/2407.12878v2,"Naama Rozen, Gal Elidan, Amir Globerson, Ella Daniel","cs.CL, cs.AI",table2text,"Values are a basic driving force underlying human behavior. Large Language
Models (LLM) technology is constantly improving towards human-like dialogue.
However, little research has been done to study the values exhibited in text
generated by LLMs. Here we study this question by turning to the rich
literature on value structure in psychology. We ask whether LLMs exhibit the
same value structure that has been demonstrated in humans, including the
ranking of values, and correlation between values. We show that the results of
this analysis strongly depend on how the LLM is prompted, and that under a
particular prompting strategy (referred to as 'Value Anchoring') the agreement
with human data is quite compelling. Our results serve both to improve our
understanding of values in LLMs, as well as introduce novel methods for
assessing consistency in LLM responses.",2024-07-16
Are you still on track!? Catching LLM Task Drift with Activations,2024-06-02 16:53:21+00:00,http://arxiv.org/abs/2406.00799v4,"Sahar Abdelnabi, Aideen Fay, Giovanni Cherubin, Ahmed Salem, Mario Fritz, Andrew Paverd","cs.CR, cs.CL, cs.CY",table2text,"Large Language Models (LLMs) are routinely used in retrieval-augmented
applications to orchestrate tasks and process inputs from users and other
sources. These inputs, even in a single LLM interaction, can come from a
variety of sources, of varying trustworthiness and provenance. This opens the
door to prompt injection attacks, where the LLM receives and acts upon
instructions from supposedly data-only sources, thus deviating from the user's
original instructions. We define this as task drift, and we propose to catch it
by scanning and analyzing the LLM's activations. We compare the LLM's
activations before and after processing the external input in order to detect
whether this input caused instruction drift. We develop two probing methods and
find that simply using a linear classifier can detect drift with near perfect
ROC AUC on an out-of-distribution test set. We show that this approach
generalizes surprisingly well to unseen task domains, such as prompt
injections, jailbreaks, and malicious instructions, without being trained on
any of these attacks. Our setup does not require any modification of the LLM
(e.g., fine-tuning) or any text generation, thus maximizing deployability and
cost efficiency and avoiding reliance on unreliable model output. To foster
future research on activation-based task inspection, decoding, and
interpretability, we will release our large-scale TaskTracker toolkit,
comprising a dataset of over 500K instances, representations from 5 SoTA
language models, and inspection tools.",2024-06-02
Evaluating Large Language Model Biases in Persona-Steered Generation,2024-05-30 17:06:03+00:00,http://arxiv.org/abs/2405.20253v1,"Andy Liu, Mona Diab, Daniel Fried",cs.CL,table2text,"The task of persona-steered text generation requires large language models
(LLMs) to generate text that reflects the distribution of views that an
individual fitting a persona could have. People have multifaceted personas, but
prior work on bias in LLM-generated opinions has only explored multiple-choice
settings or one-dimensional personas. We define an incongruous persona as a
persona with multiple traits where one trait makes its other traits less likely
in human survey data, e.g. political liberals who support increased military
spending. We find that LLMs are 9.7% less steerable towards incongruous
personas than congruous ones, sometimes generating the stereotypical stance
associated with its demographic rather than the target stance. Models that we
evaluate that are fine-tuned with Reinforcement Learning from Human Feedback
(RLHF) are more steerable, especially towards stances associated with political
liberals and women, but present significantly less diverse views of personas.
We also find variance in LLM steerability that cannot be predicted from
multiple-choice opinion evaluation. Our results show the importance of
evaluating models in open-ended text generation, as it can surface new LLM
opinion biases. Moreover, such a setup can shed light on our ability to steer
models toward a richer and more diverse range of viewpoints.",2024-05-30
Context Injection Attacks on Large Language Models,2024-05-30 16:36:47+00:00,http://arxiv.org/abs/2405.20234v1,"Cheng'an Wei, Kai Chen, Yue Zhao, Yujia Gong, Lu Xiang, Shenchen Zhu",cs.AI,table2text,"Large Language Models (LLMs) such as ChatGPT and Llama-2 have become
prevalent in real-world applications, exhibiting impressive text generation
performance. LLMs are fundamentally developed from a scenario where the input
data remains static and lacks a clear structure. To behave interactively over
time, LLM-based chat systems must integrate additional contextual information
(i.e., chat history) into their inputs, following a pre-defined structure. This
paper identifies how such integration can expose LLMs to misleading context
from untrusted sources and fail to differentiate between system and user
inputs, allowing users to inject context. We present a systematic methodology
for conducting context injection attacks aimed at eliciting disallowed
responses by introducing fabricated context. This could lead to illegal
actions, inappropriate content, or technology misuse. Our context fabrication
strategies, acceptance elicitation and word anonymization, effectively create
misleading contexts that can be structured with attacker-customized prompt
templates, achieving injection through malicious user messages. Comprehensive
evaluations on real-world LLMs such as ChatGPT and Llama-2 confirm the efficacy
of the proposed attack with success rates reaching 97%. We also discuss
potential countermeasures that can be adopted for attack detection and
developing more secure models. Our findings provide insights into the
challenges associated with the real-world deployment of LLMs for interactive
and structured data scenarios.",2024-05-30
"Multi-Aspect Controllable Text Generation with Disentangled
  Counterfactual Augmentation",2024-05-30 11:25:42+00:00,http://arxiv.org/abs/2405.19958v1,"Yi Liu, Xiangyu Liu, Xiangrong Zhu, Wei Hu","cs.CL, cs.AI",table2text,"Multi-aspect controllable text generation aims to control the generated texts
in attributes from multiple aspects (e.g., ""positive"" from sentiment and
""sport"" from topic). For ease of obtaining training samples, existing works
neglect attribute correlations formed by the intertwining of different
attributes. Particularly, the stereotype formed by imbalanced attribute
correlations significantly affects multi-aspect control. In this paper, we
propose MAGIC, a new multi-aspect controllable text generation method with
disentangled counterfactual augmentation. We alleviate the issue of imbalanced
attribute correlations during training using counterfactual feature vectors in
the attribute latent space by disentanglement. During inference, we enhance
attribute correlations by target-guided counterfactual augmentation to further
improve multi-aspect control. Experiments show that MAGIC outperforms
state-of-the-art baselines in both imbalanced and balanced attribute
correlation scenarios. Our source code and data are available at
https://github.com/nju-websoft/MAGIC.",2024-05-30
"WRDScore: New Metric for Evaluation of Natural Language Generation
  Models",2024-05-29 16:00:46+00:00,http://arxiv.org/abs/2405.19220v1,Ravil Mussabayev,"cs.CL, cs.AI",table2text,"The problem of natural language generation, and, more specifically, method
name prediction, faces significant difficulties when proposed models need to be
evaluated on test data. Such a metric would need to consider the versatility
with which a single method can be named, with respect to both semantics and
syntax. Measuring the direct overlap between the predicted and reference (true)
sequences will not be able to capture these subtleties. Other existing
embedding based metrics either do not measure precision and recall or impose
strict unrealistic assumptions on both sequences. To address these issues, we
propose a new metric that, on the one hand, is very simple and lightweight,
and, on the other hand, is able to calculate precision and recall without
resorting to any assumptions while obtaining good performance with respect to
the human judgement.",2024-05-29
"MetaToken: Detecting Hallucination in Image Descriptions by Meta
  Classification",2024-05-29 15:28:42+00:00,http://arxiv.org/abs/2405.19186v1,"Laura Fieback, Jakob Spiegelberg, Hanno Gottschalk","cs.CV, cs.CL, cs.LG, I.4",table2text,"Large Vision Language Models (LVLMs) have shown remarkable capabilities in
multimodal tasks like visual question answering or image captioning. However,
inconsistencies between the visual information and the generated text, a
phenomenon referred to as hallucinations, remain an unsolved problem with
regard to the trustworthiness of LVLMs. To address this problem, recent works
proposed to incorporate computationally costly Large (Vision) Language Models
in order to detect hallucinations on a sentence- or subsentence-level. In this
work, we introduce MetaToken, a lightweight binary classifier to detect
hallucinations on the token-level at negligible cost. Based on a statistical
analysis, we reveal key factors of hallucinations in LVLMs which have been
overseen in previous works. MetaToken can be applied to any open-source LVLM
without any knowledge about ground truth data providing a reliable detection of
hallucinations. We evaluate our method on four state-of-the-art LVLMs
demonstrating the effectiveness of our approach.",2024-05-29
Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities,2024-05-29 00:23:55+00:00,http://arxiv.org/abs/2405.18669v1,"Vicky Zayats, Peter Chen, Melissa Merrari, Dirk Padfield","cs.LG, cs.AI, cs.CL, eess.AS",table2text,"Integrating multiple generative foundation models, especially those trained
on different modalities, into something greater than the sum of its parts poses
significant challenges. Two key hurdles are the availability of aligned data
(concepts that contain similar meaning but is expressed differently in
different modalities), and effectively leveraging unimodal representations in
cross-domain generative tasks, without compromising their original unimodal
capabilities.
  We propose Zipper, a multi-tower decoder architecture that addresses these
concerns by using cross-attention to flexibly compose multimodal generative
models from independently pre-trained unimodal decoders. In our experiments
fusing speech and text modalities, we show the proposed architecture performs
very competitively in scenarios with limited aligned text-speech data. We also
showcase the flexibility of our model to selectively maintain unimodal (e.g.,
text-to-text generation) generation performance by freezing the corresponding
modal tower (e.g. text). In cross-modal tasks such as automatic speech
recognition (ASR) where the output modality is text, we show that freezing the
text backbone results in negligible performance degradation. In cross-modal
tasks such as text-to-speech generation (TTS) where the output modality is
speech, we show that using a pre-trained speech backbone results in superior
performance to the baseline.",2024-05-29
Augmenting Textual Generation via Topology Aware Retrieval,2024-05-27 19:02:18+00:00,http://arxiv.org/abs/2405.17602v1,"Yu Wang, Nedim Lipka, Ruiyi Zhang, Alexa Siu, Yuying Zhao, Bo Ni, Xin Wang, Ryan Rossi, Tyler Derr",cs.IR,table2text,"Despite the impressive advancements of Large Language Models (LLMs) in
generating text, they are often limited by the knowledge contained in the input
and prone to producing inaccurate or hallucinated content. To tackle these
issues, Retrieval-augmented Generation (RAG) is employed as an effective
strategy to enhance the available knowledge base and anchor the responses in
reality by pulling additional texts from external databases. In real-world
applications, texts are often linked through entities within a graph, such as
citations in academic papers or comments in social networks. This paper
exploits these topological relationships to guide the retrieval process in RAG.
Specifically, we explore two kinds of topological connections: proximity-based,
focusing on closely connected nodes, and role-based, which looks at nodes
sharing similar subgraph structures. Our empirical research confirms their
relevance to text relationships, leading us to develop a Topology-aware
Retrieval-augmented Generation framework. This framework includes a retrieval
module that selects texts based on their topological relationships and an
aggregation module that integrates these texts into prompts to stimulate LLMs
for text generation. We have curated established text-attributed networks and
conducted comprehensive experiments to validate the effectiveness of this
framework, demonstrating its potential to enhance RAG with topological
awareness.",2024-05-27
"Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via
  Code Rewriting",2024-05-25 08:57:28+00:00,http://arxiv.org/abs/2405.16133v2,"Tong Ye, Yangkai Du, Tengfei Ma, Lingfei Wu, Xuhong Zhang, Shouling Ji, Wenhai Wang","cs.SE, cs.AI",table2text,"Large Language Models (LLMs) have exhibited remarkable proficiency in
generating code. However, the misuse of LLM-generated (Synthetic) code has
prompted concerns within both educational and industrial domains, highlighting
the imperative need for the development of synthetic code detectors. Existing
methods for detecting LLM-generated content are primarily tailored for general
text and often struggle with code content due to the distinct grammatical
structure of programming languages and massive ""low-entropy"" tokens. Building
upon this, our work proposes a novel zero-shot synthetic code detector based on
the similarity between the code and its rewritten variants. Our method relies
on the intuition that the differences between the LLM-rewritten and original
codes tend to be smaller when the original code is synthetic. We utilize
self-supervised contrastive learning to train a code similarity model and
assess our approach on two synthetic code detection benchmarks. Our results
demonstrate a notable enhancement over existing synthetic content detectors
designed for general texts, with an improvement of 20.5% in the APPS benchmark
and 29.1% in the MBPP benchmark.",2024-05-25
"Effective Unsupervised Constrained Text Generation based on Perturbed
  Masking",2024-04-24 13:42:26+00:00,http://arxiv.org/abs/2404.15877v1,"Yingwen Fu, Wenjie Ou, Zhou Yu, Yue Lin",cs.CL,table2text,"Unsupervised constrained text generation aims to generate text under a given
set of constraints without any supervised data. Current state-of-the-art
methods stochastically sample edit positions and actions, which may cause
unnecessary search steps. In this paper, we propose PMCTG to improve
effectiveness by searching for the best edit position and action in each step.
Specifically, PMCTG extends perturbed masking technique to effectively search
for the most incongruent token to edit. Then it introduces four multi-aspect
scoring functions to select edit action to further reduce search difficulty.
Since PMCTG does not require supervised data, it could be applied to different
generation tasks. We show that under the unsupervised setting, PMCTG achieves
new state-of-the-art results in two representative tasks, namely
keywords-to-sentence generation and paraphrasing.",2024-04-24
"Semantic Routing for Enhanced Performance of LLM-Assisted Intent-Based
  5G Core Network Management and Orchestration",2024-04-24 13:34:20+00:00,http://arxiv.org/abs/2404.15869v1,"Dimitrios Michael Manias, Ali Chouman, Abdallah Shami","cs.NI, cs.AI",table2text,"Large language models (LLMs) are rapidly emerging in Artificial Intelligence
(AI) applications, especially in the fields of natural language processing and
generative AI. Not limited to text generation applications, these models
inherently possess the opportunity to leverage prompt engineering, where the
inputs of such models can be appropriately structured to articulate a model's
purpose explicitly. A prominent example of this is intent-based networking, an
emerging approach for automating and maintaining network operations and
management. This paper presents semantic routing to achieve enhanced
performance in LLM-assisted intent-based management and orchestration of 5G
core networks. This work establishes an end-to-end intent extraction framework
and presents a diverse dataset of sample user intents accompanied by a thorough
analysis of the effects of encoders and quantization on overall system
performance. The results show that using a semantic router improves the
accuracy and efficiency of the LLM deployment compared to stand-alone LLMs with
prompting architectures.",2024-04-24
"Visual Delta Generator with Large Multi-modal Models for Semi-supervised
  Composed Image Retrieval",2024-04-23 21:00:22+00:00,http://arxiv.org/abs/2404.15516v1,"Young Kyun Jang, Donghyun Kim, Zihang Meng, Dat Huynh, Ser-Nam Lim","cs.CV, cs.AI",table2text,"Composed Image Retrieval (CIR) is a task that retrieves images similar to a
query, based on a provided textual modification. Current techniques rely on
supervised learning for CIR models using labeled triplets of the reference
image, text, target image. These specific triplets are not as commonly
available as simple image-text pairs, limiting the widespread use of CIR and
its scalability. On the other hand, zero-shot CIR can be relatively easily
trained with image-caption pairs without considering the image-to-image
relation, but this approach tends to yield lower accuracy. We propose a new
semi-supervised CIR approach where we search for a reference and its related
target images in auxiliary data and learn our large language model-based Visual
Delta Generator (VDG) to generate text describing the visual difference (i.e.,
visual delta) between the two. VDG, equipped with fluent language knowledge and
being model agnostic, can generate pseudo triplets to boost the performance of
CIR models. Our approach significantly improves the existing supervised
learning approaches and achieves state-of-the-art results on the CIR
benchmarks.",2024-04-23
Identifying Fairness Issues in Automatically Generated Testing Content,2024-04-23 14:56:15+00:00,http://arxiv.org/abs/2404.15104v1,"Kevin Stowe, Benny Longwill, Alyssa Francis, Tatsuya Aoyama, Debanjan Ghosh, Swapna Somasundaran","cs.CL, I.2.7",table2text,"Natural language generation tools are powerful and effective for generating
content. However, language models are known to display bias and fairness
issues, making them impractical to deploy for many use cases. We here focus on
how fairness issues impact automatically generated test content, which can have
stringent requirements to ensure the test measures only what it was intended to
measure. Specifically, we identify test content that is focused on particular
domains and experiences that only reflect a certain demographic or that are
potentially emotionally upsetting; both of which could inadvertently impact a
test-taker's score. This kind of content doesn't reflect typical biases out of
context, making it challenging even for modern models that contain safeguards.
We build a dataset of 621 generated texts annotated for fairness and explore a
variety of methods for classification: fine-tuning, topic-based classification,
and prompting, including few-shot and self-correcting prompts. We find that
combining prompt self-correction and few-shot learning performs best, yielding
an F1 score of .791 on our held-out test set, while much smaller BERT- and
topic-based models have competitive performance on out-of-domain data.",2024-04-23
"Navigating the Path of Writing: Outline-guided Text Generation with
  Large Language Models",2024-04-22 06:57:43+00:00,http://arxiv.org/abs/2404.13919v1,"Yukyung Lee, Soonwon Ka, Bokyung Son, Pilsung Kang, Jaewook Kang","cs.CL, cs.AI, cs.HC",table2text,"Large Language Models (LLMs) have significantly impacted the writing process,
enabling collaborative content creation and enhancing productivity. However,
generating high-quality, user-aligned text remains challenging. In this paper,
we propose Writing Path, a framework that uses explicit outlines to guide LLMs
in generating goal-oriented, high-quality pieces of writing. Our approach draws
inspiration from structured writing planning and reasoning paths, focusing on
capturing and reflecting user intentions throughout the writing process. We
construct a diverse dataset from unstructured blog posts to benchmark writing
performance and introduce a comprehensive evaluation framework assessing the
quality of outlines and generated texts. Our evaluations with GPT-3.5-turbo,
GPT-4, and HyperCLOVA X demonstrate that the Writing Path approach
significantly enhances text quality according to both LLMs and human
evaluations. This study highlights the potential of integrating
writing-specific techniques into LLMs to enhance their ability to meet the
diverse writing needs of users.",2024-04-22
"Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level
  Knowledge Distillation",2024-04-19 02:59:09+00:00,http://arxiv.org/abs/2404.12596v1,"Lasal Jayawardena, Prasan Yapa","cs.CL, cs.AI, cs.LG",table2text,"Over the past year, the field of Natural Language Generation (NLG) has
experienced an exponential surge, largely due to the introduction of Large
Language Models (LLMs). These models have exhibited the most effective
performance in a range of domains within the Natural Language Processing and
Generation domains. However, their application in domain-specific tasks, such
as paraphrasing, presents significant challenges. The extensive number of
parameters makes them difficult to operate on commercial hardware, and they
require substantial time for inference, leading to high costs in a production
setting. In this study, we tackle these obstacles by employing LLMs to develop
three distinct models for the paraphrasing field, applying a method referred to
as sequence-level knowledge distillation. These distilled models are capable of
maintaining the quality of paraphrases generated by the LLM. They demonstrate
faster inference times and the ability to generate diverse paraphrases of
comparable quality. A notable characteristic of these models is their ability
to exhibit syntactic diversity while also preserving lexical diversity,
features previously uncommon due to existing data quality issues in datasets
and not typically observed in neural-based approaches. Human evaluation of our
models shows that there is only a 4% drop in performance compared to the LLM
teacher model used in the distillation process, despite being 1000 times
smaller. This research provides a significant contribution to the NLG field,
offering a more efficient and cost-effective solution for paraphrasing tasks.",2024-04-19
Sampling-based Pseudo-Likelihood for Membership Inference Attacks,2024-04-17 11:12:59+00:00,http://arxiv.org/abs/2404.11262v1,"Masahiro Kaneko, Youmi Ma, Yuki Wata, Naoaki Okazaki",cs.CL,table2text,"Large Language Models (LLMs) are trained on large-scale web data, which makes
it difficult to grasp the contribution of each text. This poses the risk of
leaking inappropriate data such as benchmarks, personal information, and
copyrighted texts in the training data. Membership Inference Attacks (MIA),
which determine whether a given text is included in the model's training data,
have been attracting attention. Previous studies of MIAs revealed that
likelihood-based classification is effective for detecting leaks in LLMs.
However, the existing methods cannot be applied to some proprietary models like
ChatGPT or Claude 3 because the likelihood is unavailable to the user. In this
study, we propose a Sampling-based Pseudo-Likelihood (\textbf{SPL}) method for
MIA (\textbf{SaMIA}) that calculates SPL using only the text generated by an
LLM to detect leaks. The SaMIA treats the target text as the reference text and
multiple outputs from the LLM as text samples, calculates the degree of
$n$-gram match as SPL, and determines the membership of the text in the
training data. Even without likelihoods, SaMIA performed on par with existing
likelihood-based methods.",2024-04-17
"Incubating Text Classifiers Following User Instruction with Nothing but
  LLM",2024-04-16 19:53:35+00:00,http://arxiv.org/abs/2404.10877v1,"Letian Peng, Jingbo Shang",cs.CL,table2text,"In this paper, we aim to generate text classification data given arbitrary
class definitions (i.e., user instruction), so one can train a small text
classifier without any human annotation or raw corpus. Compared with pioneer
attempts, our proposed Incubator is the first framework that can handle
complicated and even mutually dependent classes (e.g., ""TED Talk given by
Educator"" and ""Other""). Specifically, Incubator is an LLM firstly tuned on the
instruction-to-data mappings that we obtained from classification datasets and
descriptions on HuggingFace together with in-context augmentation by GPT-4. We
then refine Incubator by learning on the cluster centers of semantic textual
embeddings to emphasize the uniformity and semantic diversity in generations.
We compare Incubator on various classification tasks with strong baselines such
as direct LLM-based inference and training data generation by prompt
engineering. Experiments show Incubator is able to (1) perform well on
traditional benchmarks, (2) take label dependency and user preference into
consideration, and (3) enable logical text mining by incubating multiple
classifiers.",2024-04-16
"LaDiC: Are Diffusion Models Really Inferior to Autoregressive
  Counterparts for Image-to-Text Generation?",2024-04-16 17:47:16+00:00,http://arxiv.org/abs/2404.10763v1,"Yuchi Wang, Shuhuai Ren, Rundong Gao, Linli Yao, Qingyan Guo, Kaikai An, Jianhong Bai, Xu Sun","cs.AI, cs.CL, cs.CV",table2text,"Diffusion models have exhibited remarkable capabilities in text-to-image
generation. However, their performance in image-to-text generation,
specifically image captioning, has lagged behind Auto-Regressive (AR) models,
casting doubt on their applicability for such tasks. In this work, we revisit
diffusion models, highlighting their capacity for holistic context modeling and
parallel decoding. With these benefits, diffusion models can alleviate the
inherent limitations of AR methods, including their slow inference speed, error
propagation, and unidirectional constraints. Furthermore, we identify the prior
underperformance of diffusion models stemming from the absence of an effective
latent space for image-text alignment, and the discrepancy between continuous
diffusion processes and discrete textual data. In response, we introduce a
novel architecture, LaDiC, which utilizes a split BERT to create a dedicated
latent space for captions and integrates a regularization module to manage
varying text lengths. Our framework also includes a diffuser for semantic
image-to-text conversion and a Back&Refine technique to enhance token
interactivity during inference. LaDiC achieves state-of-the-art performance for
diffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2
CIDEr, demonstrating exceptional performance without pre-training or ancillary
modules. This indicates strong competitiveness with AR models, revealing the
previously untapped potential of diffusion models in image-to-text generation.",2024-04-16
"Gaining More Insight into Neural Semantic Parsing with Challenging
  Benchmarks",2024-04-12 09:48:58+00:00,http://arxiv.org/abs/2404.08354v2,"Xiao Zhang, Chunliu Wang, Rik van Noord, Johan Bos",cs.CL,table2text,"The Parallel Meaning Bank (PMB) serves as a corpus for semantic processing
with a focus on semantic parsing and text generation. Currently, we witness an
excellent performance of neural parsers and generators on the PMB. This might
suggest that such semantic processing tasks have by and large been solved. We
argue that this is not the case and that performance scores from the past on
the PMB are inflated by non-optimal data splits and test sets that are too
easy. In response, we introduce several changes. First, instead of the prior
random split, we propose a more systematic splitting approach to improve the
reliability of the standard test data. Second, except for the standard test
set, we also propose two challenge sets: one with longer texts including
discourse structure, and one that addresses compositional generalization. We
evaluate five neural models for semantic parsing and meaning-to-text
generation. Our results show that model performance declines (in some cases
dramatically) on the challenge sets, revealing the limitations of neural models
when confronting such challenges.",2024-04-12
Language Models for Text Classification: Is In-Context Learning Enough?,2024-03-26 12:47:39+00:00,http://arxiv.org/abs/2403.17661v1,"Aleksandra Edwards, Jose Camacho-Collados","cs.CL, cs.AI",table2text,"Recent foundational language models have shown state-of-the-art performance
in many NLP tasks in zero- and few-shot settings. An advantage of these models
over more standard approaches based on fine-tuning is the ability to understand
instructions written in natural language (prompts), which helps them generalise
better to different tasks and domains without the need for specific training
data. This makes them suitable for addressing text classification problems for
domains with limited amounts of annotated instances. However, existing research
is limited in scale and lacks understanding of how text generation models
combined with prompting techniques compare to more established methods for text
classification such as fine-tuning masked language models. In this paper, we
address this research gap by performing a large-scale evaluation study for 16
text classification datasets covering binary, multiclass, and multilabel
problems. In particular, we compare zero- and few-shot approaches of large
language models to fine-tuning smaller language models. We also analyse the
results by prompt, classification type, domain, and number of labels. In
general, the results show how fine-tuning smaller and more efficient language
models can still outperform few-shot approaches of larger language models,
which have room for improvement when it comes to text classification.",2024-03-26
Outcome-Constrained Large Language Models for Countering Hate Speech,2024-03-25 19:44:06+00:00,http://arxiv.org/abs/2403.17146v1,"Lingzi Hong, Pengcheng Luo, Eduardo Blanco, Xiaoying Song",cs.CL,table2text,"Counterspeech that challenges or responds to hate speech has been seen as an
alternative to mitigate the negative impact of hate speech and foster
productive online communications. Research endeavors have been directed to
using language models for the automatic generation of counterspeech to assist
efforts in combating online hate. Existing research focuses on the generation
of counterspeech with certain linguistic attributes, such as being polite,
informative, and intent-driven. However, it remains unclear what impact the
counterspeech might have in an online environment. We first explore methods
that utilize large language models (LLM) to generate counterspeech constrained
by potential conversation outcomes. We build two conversation outcome
classifiers that predict the incivility level and the hater reentry behavior
following replies to hate with Reddit data, then propose four methods to
incorporate the desired outcomes, i.e., low conversation incivility and
non-hateful hater reentry, into the text generation process, including Prompt
with Instructions, Prompt and Select, LLM finetune, and LLM transformer
reinforcement learning (TRL). Evaluation results show effective strategies to
generate outcome-constrained counterspeech and the linguistic characteristics
of texts generated by different methods.",2024-03-25
"UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation
  Model for Urban Indicator Prediction",2024-03-25 14:57:18+00:00,http://arxiv.org/abs/2403.16831v1,"Xixuan Hao, Wei Chen, Yibo Yan, Siru Zhong, Kun Wang, Qingsong Wen, Yuxuan Liang","cs.CV, cs.AI",table2text,"Urban indicator prediction aims to infer socio-economic metrics in diverse
urban landscapes using data-driven methods. However, prevalent pre-trained
models, particularly those reliant on satellite imagery, face dual challenges.
Firstly, concentrating solely on macro-level patterns from satellite data may
introduce bias, lacking nuanced details at micro levels, such as architectural
details at a place. Secondly, the lack of interpretability in pre-trained
models limits their utility in providing transparent evidence for urban
planning. In response to these issues, we devise a novel Vision-Language
Pre-Trained Model (UrbanVLP) in this paper. Our UrbanVLP seamlessly integrates
multi-granularity information from both macro (satellite) and micro
(street-view) levels, overcoming the limitations of prior pre-trained models.
Moreover, it introduces automatic text generation and calibration, elevating
interpretability in downstream applications by producing high-quality text
descriptions of urban imagery. Rigorous experiments conducted across six
socio-economic tasks underscore UrbanVLP's superior performance. We also deploy
a web platform to verify its practicality.",2024-03-25
"Visually Guided Generative Text-Layout Pre-training for Document
  Intelligence",2024-03-25 08:00:43+00:00,http://arxiv.org/abs/2403.16516v2,"Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu, Kam-Fai Wong","cs.CL, cs.CV",table2text,"Prior study shows that pre-training techniques can boost the performance of
visual document understanding (VDU), which typically requires models to gain
abilities to perceive and reason both document texts and layouts (e.g.,
locations of texts and table-cells). To this end, we propose visually guided
generative text-layout pre-training, named ViTLP. Given a document image, the
model optimizes hierarchical language and layout modeling objectives to
generate the interleaved text and layout sequence. In addition, to address the
limitation of processing long documents by Transformers, we introduce a
straightforward yet effective multi-segment generative pre-training scheme,
facilitating ViTLP to process word-intensive documents of any length. ViTLP can
function as a native OCR model to localize and recognize texts of document
images. Besides, ViTLP can be effectively applied to various downstream VDU
tasks. Extensive experiments show that ViTLP achieves competitive performance
over existing baselines on benchmark VDU tasks, including information
extraction, document classification, and document question answering.",2024-03-25
"The Frontier of Data Erasure: Machine Unlearning for Large Language
  Models",2024-03-23 09:26:15+00:00,http://arxiv.org/abs/2403.15779v1,"Youyang Qu, Ming Ding, Nan Sun, Kanchana Thilakarathna, Tianqing Zhu, Dusit Niyato",cs.AI,table2text,"Large Language Models (LLMs) are foundational to AI advancements,
facilitating applications like predictive text generation. Nonetheless, they
pose risks by potentially memorizing and disseminating sensitive, biased, or
copyrighted information from their vast datasets. Machine unlearning emerges as
a cutting-edge solution to mitigate these concerns, offering techniques for
LLMs to selectively discard certain data. This paper reviews the latest in
machine unlearning for LLMs, introducing methods for the targeted forgetting of
information to address privacy, ethical, and legal challenges without
necessitating full model retraining. It divides existing research into
unlearning from unstructured/textual data and structured/classification data,
showcasing the effectiveness of these approaches in removing specific data
while maintaining model efficacy. Highlighting the practicality of machine
unlearning, this analysis also points out the hurdles in preserving model
integrity, avoiding excessive or insufficient data removal, and ensuring
consistent outputs, underlining the role of machine unlearning in advancing
responsible, ethical AI.",2024-03-23
EAGLE: A Domain Generalization Framework for AI-generated Text Detection,2024-03-23 02:44:20+00:00,http://arxiv.org/abs/2403.15690v1,"Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu","cs.CL, cs.AI, cs.LG",table2text,"With the advancement in capabilities of Large Language Models (LLMs), one
major step in the responsible and safe use of such LLMs is to be able to detect
text generated by these models. While supervised AI-generated text detectors
perform well on text generated by older LLMs, with the frequent release of new
LLMs, building supervised detectors for identifying text from such new models
would require new labeled training data, which is infeasible in practice. In
this work, we tackle this problem and propose a domain generalization framework
for the detection of AI-generated text from unseen target generators. Our
proposed framework, EAGLE, leverages the labeled data that is available so far
from older language models and learns features invariant across these
generators, in order to detect text generated by an unknown target generator.
EAGLE learns such domain-invariant features by combining the representational
power of self-supervised contrastive learning with domain adversarial training.
Through our experiments we demonstrate how EAGLE effectively achieves
impressive performance in detecting text generated by unseen target generators,
including recent state-of-the-art ones such as GPT-4 and Claude, reaching
detection scores of within 4.7% of a fully supervised detector.",2024-03-23
"InstaSynth: Opportunities and Challenges in Generating Synthetic
  Instagram Data with ChatGPT for Sponsored Content Detection",2024-03-22 13:58:42+00:00,http://arxiv.org/abs/2403.15214v1,"Thales Bertaglia, Lily Heisig, Rishabh Kaushal, Adriana Iamnitchi","cs.CY, cs.CL, cs.SI",table2text,"Large Language Models (LLMs) raise concerns about lowering the cost of
generating texts that could be used for unethical or illegal purposes,
especially on social media. This paper investigates the promise of such models
to help enforce legal requirements related to the disclosure of sponsored
content online. We investigate the use of LLMs for generating synthetic
Instagram captions with two objectives: The first objective (fidelity) is to
produce realistic synthetic datasets. For this, we implement content-level and
network-level metrics to assess whether synthetic captions are realistic. The
second objective (utility) is to create synthetic data that is useful for
sponsored content detection. For this, we evaluate the effectiveness of the
generated synthetic data for training classifiers to identify undisclosed
advertisements on Instagram. Our investigations show that the objectives of
fidelity and utility may conflict and that prompt engineering is a useful but
insufficient strategy. Additionally, we find that while individual synthetic
posts may appear realistic, collectively they lack diversity, topic
connectivity, and realistic user interaction patterns.",2024-03-22
"ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate
  Professional and Non-Professional Styled Text",2024-03-14 06:49:16+00:00,http://arxiv.org/abs/2403.09131v2,"Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yueting Zhuang","cs.CL, cs.AI, 68T50, I.2.7",table2text,"Large Language Models (LLMs) have demonstrated efficacy in various linguistic
applications, including text summarization and controlled text generation.
However, studies into their capacity of switching between styles via
fine-tuning remain underexplored. This study concentrates on textual
professionalism and introduces a novel methodology, named ProSwitch, which
equips a language model with the ability to produce both professional and
non-professional responses through knowledge-guided instruction tuning.
ProSwitch unfolds across three phases: data preparation for gathering domain
knowledge and training corpus; instruction tuning for optimizing language
models with multiple levels of instruction formats; and comprehensive
evaluation for assessing the professionalism discrimination and reference-based
quality of generated text. Comparative analysis of ProSwitch against both
general and specialized language models reveals that our approach outperforms
baselines in switching between professional and non-professional text
generation.",2024-03-14
"SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and
  Related Observable Overgeneration Mistakes",2024-03-12 15:06:22+00:00,http://arxiv.org/abs/2403.07726v1,"Timothee Mickus, Elaine Zosa, Raúl Vázquez, Teemu Vahtola, Jörg Tiedemann, Vincent Segonne, Alessandro Raganato, Marianna Apidianaki",cs.CL,table2text,"This paper presents the results of the SHROOM, a shared task focused on
detecting hallucinations: outputs from natural language generation (NLG)
systems that are fluent, yet inaccurate. Such cases of overgeneration put in
jeopardy many NLG applications, where correctness is often mission-critical.
The shared task was conducted with a newly constructed dataset of 4000 model
outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine
translation, paraphrase generation and definition modeling.
  The shared task was tackled by a total of 58 different users grouped in 42
teams, out of which 27 elected to write a system description paper;
collectively, they submitted over 300 prediction sets on both tracks of the
shared task. We observe a number of key trends in how this approach was tackled
-- many participants rely on a handful of model, and often rely either on
synthetic data for fine-tuning or zero-shot prompting strategies. While a
majority of the teams did outperform our proposed baseline system, the
performances of top-scoring systems are still consistent with a random handling
of the more challenging items.",2024-03-12
"generAItor: Tree-in-the-Loop Text Generation for Language Model
  Explainability and Adaptation",2024-03-12 13:09:15+00:00,http://arxiv.org/abs/2403.07627v1,"Thilo Spinner, Rebecca Kehlbeck, Rita Sevastjanova, Tobias Stähle, Daniel A. Keim, Oliver Deussen, Mennatallah El-Assady","cs.HC, cs.LG, I.2.7; H.5.2",table2text,"Large language models (LLMs) are widely deployed in various downstream tasks,
e.g., auto-completion, aided writing, or chat-based text generation. However,
the considered output candidates of the underlying search algorithm are
under-explored and under-explained. We tackle this shortcoming by proposing a
tree-in-the-loop approach, where a visual representation of the beam search
tree is the central component for analyzing, explaining, and adapting the
generated outputs. To support these tasks, we present generAItor, a visual
analytics technique, augmenting the central beam search tree with various
task-specific widgets, providing targeted visualizations and interaction
possibilities. Our approach allows interactions on multiple levels and offers
an iterative pipeline that encompasses generating, exploring, and comparing
output candidates, as well as fine-tuning the model based on adapted data. Our
case study shows that our tool generates new insights in gender bias analysis
beyond state-of-the-art template-based methods. Additionally, we demonstrate
the applicability of our approach in a qualitative user study. Finally, we
quantitatively evaluate the adaptability of the model to few samples, as
occurring in text-generation use cases.",2024-03-12
"Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource
  Agglutinative Data-to-Text Generation",2024-03-12 11:53:27+00:00,http://arxiv.org/abs/2403.07567v1,"Francois Meyer, Jan Buys",cs.CL,table2text,"Most data-to-text datasets are for English, so the difficulties of modelling
data-to-text for low-resource languages are largely unexplored. In this paper
we tackle data-to-text for isiXhosa, which is low-resource and agglutinative.
We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of
WebNLG, which presents a new linguistic context that shifts modelling demands
to subword-driven techniques. We also develop an evaluation framework for T2X
that measures how accurately generated text describes the data. This enables
future users of T2X to go beyond surface-level metrics in evaluation. On the
modelling side we explore two classes of methods - dedicated data-to-text
models trained from scratch and pretrained language models (PLMs). We propose a
new dedicated architecture aimed at agglutinative data-to-text, the Subword
Segmental Pointer Generator (SSPG). It jointly learns to segment words and copy
entities, and outperforms existing dedicated models for 2 agglutinative
languages (isiXhosa and Finnish). We investigate pretrained solutions for T2X,
which reveals that standard PLMs come up short. Fine-tuning machine translation
models emerges as the best method overall. These findings underscore the
distinct challenge presented by T2X: neither well-established data-to-text
architectures nor customary pretrained methodologies prove optimal. We conclude
with a qualitative analysis of generation errors and an ablation study.",2024-03-12
"Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A
  Brain-Inspired Method for Parameter-Efficient Fine-Tuning",2024-03-12 09:32:25+00:00,http://arxiv.org/abs/2403.07440v1,"Yao Liang, Yuwei Wang, Yi Zeng","cs.CL, cs.AI",table2text,"Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have
been proven to significantly enhance model performance on a variety of
downstream tasks and effectively control the output behaviors of LPLMs. Recent
studies have proposed numerous methods for fine-tuning a small number of
parameters based on open-source LPLMs, reducing the demand for computational
and storage resources. Among these, reparameterization fine-tuning methods
represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that
although these methods perform well in many aspects, there is still
considerable room for improvement in terms of complex task adaptability,
performance, stability, and algorithm complexity. In response to this, inspired
by the idea that the functions of the brain are shaped by its geometric
structure, this paper integrates this idea into LoRA technology and proposes a
new matrix transformation-based reparameterization method for efficient
fine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).
MTLoRA aims to dynamically alter its spatial geometric structure by applying a
transformation-matrix T to perform linear transformations, such as rotation,
scaling, and translation, on the task-specific parameter matrix, generating new
matrix feature patterns (eigenvectors) to mimic the fundamental influence of
complex geometric structure feature patterns in the brain on functions, thereby
enhancing the model's performance in downstream tasks. In Natural Language
Understanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and
the results reveal that MTLoRA achieves an overall performance increase of
about 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,
MTLoRA improves performance by an average of 0.95% and 0.31% in the DART and
WebNLG tasks, respectively.",2024-03-12
Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs,2024-03-12 08:13:52+00:00,http://arxiv.org/abs/2403.07398v1,"Tianqing Fang, Zeming Chen, Yangqiu Song, Antoine Bosselut","cs.CL, cs.AI",table2text,"Event commonsense reasoning requires the ability to reason about the
relationship between events, as well as infer implicit context underlying that
relationship. However, data scarcity makes it challenging for language models
to learn to generate commonsense inferences for contexts and questions
involving interactions between complex events. To address this demand, we
present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop
logical queries (e.g., the joint effect or cause of both event A and B, or the
effect of the effect of event C) from an existing commonsense knowledge graph
(CSKG), and verbalizing them using handcrafted rules and large language models
into multiple-choice and text generation questions. Our experiments show that
language models trained on COM2 exhibit significant improvements in complex
reasoning ability, resulting in enhanced zero-shot performance in both
in-domain and out-of-domain tasks for question answering and generative
commonsense reasoning, without expensive human annotations.",2024-03-12
"Premonition: Using Generative Models to Preempt Future Data Changes in
  Continual Learning",2024-03-12 06:29:54+00:00,http://arxiv.org/abs/2403.07356v1,"Mark D. McDonnell, Dong Gong, Ehsan Abbasnejad, Anton van den Hengel","cs.CV, cs.LG",table2text,"Continual learning requires a model to adapt to ongoing changes in the data
distribution, and often to the set of tasks to be performed. It is rare,
however, that the data and task changes are completely unpredictable. Given a
description of an overarching goal or data theme, which we call a realm, humans
can often guess what concepts are associated with it. We show here that the
combination of a large language model and an image generation model can
similarly provide useful premonitions as to how a continual learning challenge
might develop over time. We use the large language model to generate text
descriptions of semantically related classes that might potentially appear in
the data stream in future. These descriptions are then rendered using Stable
Diffusion to generate new labelled image samples. The resulting synthetic
dataset is employed for supervised pre-training, but is discarded prior to
commencing continual learning, along with the pre-training classification head.
We find that the backbone of our pre-trained networks can learn representations
useful for the downstream continual learning problem, thus becoming a valuable
input to any existing continual learning method. Although there are
complexities arising from the domain gap between real and synthetic images, we
show that pre-training models in this manner improves multiple Class Incremenal
Learning (CIL) methods on fine-grained image classification benchmarks.
Supporting code can be found at https://github.com/cl-premonition/premonition.",2024-03-12
One Category One Prompt: Dataset Distillation using Diffusion Models,2024-03-11 20:23:59+00:00,http://arxiv.org/abs/2403.07142v1,"Ali Abbasi, Ashkan Shahbazi, Hamed Pirsiavash, Soheil Kolouri","cs.CV, cs.CL, cs.LG",table2text,"The extensive amounts of data required for training deep neural networks pose
significant challenges on storage and transmission fronts. Dataset distillation
has emerged as a promising technique to condense the information of massive
datasets into a much smaller yet representative set of synthetic samples.
However, traditional dataset distillation approaches often struggle to scale
effectively with high-resolution images and more complex architectures due to
the limitations in bi-level optimization. Recently, several works have proposed
exploiting knowledge distillation with decoupled optimization schemes to scale
up dataset distillation. Although these methods effectively address the
scalability issue, they rely on extensive image augmentations requiring the
storage of soft labels for augmented images. In this paper, we introduce
Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for
dataset distillation, leveraging recent advancements in generative
text-to-image foundation models. Our approach utilizes textual inversion, a
technique for fine-tuning text-to-image generative models, to create concise
and informative representations for large datasets. By employing these learned
text prompts, we can efficiently store and infer new samples for introducing
data variability within a fixed memory budget. We show the effectiveness of our
method through extensive experiments across various computer vision benchmark
datasets with different memory budgets.",2024-03-11
Narrating Causal Graphs with Large Language Models,2024-03-11 19:19:59+00:00,http://arxiv.org/abs/2403.07118v1,"Atharva Phatak, Vijay K. Mago, Ameeta Agrawal, Aravind Inbasekaran, Philippe J. Giabbanelli",cs.CL,table2text,"The use of generative AI to create text descriptions from graphs has mostly
focused on knowledge graphs, which connect concepts using facts. In this work
we explore the capability of large pretrained language models to generate text
from causal graphs, where salient concepts are represented as nodes and
causality is represented via directed, typed edges. The causal reasoning
encoded in these graphs can support applications as diverse as healthcare or
marketing. Using two publicly available causal graph datasets, we empirically
investigate the performance of four GPT-3 models under various settings. Our
results indicate that while causal text descriptions improve with training
data, compared to fact-based graphs, they are harder to generate under
zero-shot settings. Results further suggest that users of generative AI can
deploy future applications faster since similar performances are obtained when
training a model with only a few examples as compared to fine-tuning via a
large curated dataset.",2024-03-11
LSTM-Based Text Generation: A Study on Historical Datasets,2024-03-11 18:25:01+00:00,http://arxiv.org/abs/2403.07087v1,"Mustafa Abbas Hussein Hussein, Serkan Savaş","cs.CL, cs.AI",table2text,"This paper presents an exploration of Long Short-Term Memory (LSTM) networks
in the realm of text generation, focusing on the utilization of historical
datasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in
handling sequential data, are applied here to model complex language patterns
and structures inherent in historical texts. The study demonstrates that
LSTM-based models, when trained on historical datasets, can not only generate
text that is linguistically rich and contextually relevant but also provide
insights into the evolution of language patterns over time. The finding
presents models that are highly accurate and efficient in predicting text from
works of Nietzsche, with low loss values and a training time of 100 iterations.
The accuracy of the model is 0.9521, indicating high accuracy. The loss of the
model is 0.2518, indicating its effectiveness. The accuracy of the model in
predicting text from the work of Shakespeare is 0.9125, indicating a low error
rate. The training time of the model is 100, mirroring the efficiency of the
Nietzsche dataset. This efficiency demonstrates the effectiveness of the model
design and training methodology, especially when handling complex literary
texts. This research contributes to the field of natural language processing by
showcasing the versatility of LSTM networks in text generation and offering a
pathway for future explorations in historical linguistics and beyond.",2024-03-11
"Evolving Knowledge Distillation with Large Language Models and Active
  Learning",2024-03-11 03:55:24+00:00,http://arxiv.org/abs/2403.06414v1,"Chengyuan Liu, Yangyang Kang, Fubang Zhao, Kun Kuang, Zhuoren Jiang, Changlong Sun, Fei Wu",cs.CL,table2text,"Large language models (LLMs) have demonstrated remarkable capabilities across
various NLP tasks. However, their computational costs are prohibitively high.
To address this issue, previous research has attempted to distill the knowledge
of LLMs into smaller models by generating annotated data. Nonetheless, these
works have mainly focused on the direct use of LLMs for text generation and
labeling, without fully exploring their potential to comprehend the target task
and acquire valuable knowledge. In this paper, we propose EvoKD: Evolving
Knowledge Distillation, which leverages the concept of active learning to
interactively enhance the process of data generation using large language
models, simultaneously improving the task capabilities of small domain model
(student model). Different from previous work, we actively analyze the student
model's weaknesses, and then synthesize labeled samples based on the analysis.
In addition, we provide iterative feedback to the LLMs regarding the student
model's performance to continuously construct diversified and challenging
samples. Experiments and analysis on different NLP tasks, namely, text
classification and named entity recognition show the effectiveness of EvoKD.",2024-03-11
"Defending Against Unforeseen Failure Modes with Latent Adversarial
  Training",2024-03-08 04:22:48+00:00,http://arxiv.org/abs/2403.05030v1,"Stephen Casper, Lennart Schulze, Oam Patel, Dylan Hadfield-Menell","cs.CR, cs.AI, cs.LG",table2text,"AI systems sometimes exhibit harmful unintended behaviors post-deployment.
This is often despite extensive diagnostics and debugging by developers.
Minimizing risks from models is challenging because the attack surface is so
large. It is not tractable to exhaustively search for inputs that may cause a
model to fail. Red-teaming and adversarial training (AT) are commonly used to
make AI systems more robust. However, they have not been sufficient to avoid
many real-world failure modes that differ from the ones adversarially trained
on. In this work, we utilize latent adversarial training (LAT) to defend
against vulnerabilities without generating inputs that elicit them. LAT
leverages the compressed, abstract, and structured latent representations of
concepts that the network actually uses for prediction. We use LAT to remove
trojans and defend against held-out classes of adversarial attacks. We show in
image classification, text classification, and text generation tasks that LAT
usually improves both robustness and performance on clean data relative to AT.
This suggests that LAT can be a promising tool for defending against failure
modes that are not explicitly identified by developers.",2024-03-08
"Direct Alignment of Draft Model for Speculative Decoding with
  Chat-Fine-Tuned LLMs",2024-02-29 19:55:06+00:00,http://arxiv.org/abs/2403.00858v3,"Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott","cs.LG, cs.AI, cs.CL",table2text,"Text generation with Large Language Models (LLMs) is known to be memory bound
due to the combination of their auto-regressive nature, huge parameter counts,
and limited memory bandwidths, often resulting in low token rates. Speculative
decoding has been proposed as a solution for LLM inference acceleration.
However, since draft models are often unavailable in the modern open-source LLM
families, e.g., for Llama 2 7B, training a high-quality draft model is required
to enable inference acceleration via speculative decoding. In this paper, we
propose a simple draft model training framework for direct alignment to
chat-capable target models. With the proposed framework, we train Llama 2 Chat
Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of
the original size. Our training framework only consists of pretraining,
distillation dataset generation, and finetuning with knowledge distillation,
with no additional alignment procedure. For the finetuning step, we use
instruction-response pairs generated by target model for distillation in
plausible data distribution, and propose a new Total Variation Distance++
(TVD++) loss that incorporates variance reduction techniques inspired from the
policy gradient method in reinforcement learning. Our empirical results show
that Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3
block efficiency and 2.4$\times$ speed-up relative to autoregressive decoding
on various tasks with no further task-specific fine-tuning.",2024-02-29
"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic
  Compositional Reasoning via Counterfactual Examples",2024-02-20 18:59:55+00:00,http://arxiv.org/abs/2402.13254v2,"Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee","cs.CV, cs.AI, cs.CL, cs.LG",table2text,"We propose CounterCurate, a framework to comprehensively improve the
visio-linguistic compositional reasoning capability for both contrastive and
generative multimodal models. In particular, we identify two critical
under-explored problems: the neglect of the physically grounded reasoning
(counting and position understanding) and the potential of using highly capable
text and image generation models for semantic counterfactual fine-tuning. Our
work pioneers an approach that addresses these gaps. We first spotlight the
near-chance performance of multimodal models like CLIP and LLaVA in physically
grounded compositional reasoning. We then apply simple data augmentation using
grounded image generation model GLIGEN to generate fine-tuning data, resulting
in significant performance improvements: +33% and +37% for CLIP and LLaVA,
respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we
exploit the capabilities of high-performing text generation and image
generation models, specifically GPT-4V and DALLE-3, to curate challenging
semantic counterfactuals, thereby further enhancing compositional reasoning
capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms
GPT-4V.",2024-02-20
"RomanSetu: Efficiently unlocking multilingual capabilities of Large
  Language Models models via Romanization",2024-01-25 16:11:41+00:00,http://arxiv.org/abs/2401.14280v2,"Jaavid Aktar Husain, Raj Dabre, Aswanth Kumar, Jay Gala, Thanmay Jayakumar, Ratish Puduppully, Anoop Kunchukuttan","cs.CL, cs.AI",table2text,"This study addresses the challenge of extending Large Language Models (LLMs)
to non-English languages using non-Roman scripts. We propose an approach that
utilizes the romanized form of text as an interface for LLMs, hypothesizing
that its frequent informal use and shared tokens with English enhance
cross-lingual alignment. Our approach involves the continual pretraining of an
English LLM like Llama 2 on romanized text of non-English, non-Roman script
languages, followed by instruction tuning on romanized data. The results
indicate that romanized text not only reduces token fertility by 2x-4x but also
matches or outperforms native script representation across various NLU, NLG,
and MT tasks. Moreover, the embeddings computed on romanized text exhibit
closer alignment with their English translations than those from the native
script. Our approach presents a promising direction for leveraging the power of
English LLMs in languages traditionally underrepresented in NLP.",2024-01-25
"Consistency Guided Knowledge Retrieval and Denoising in LLMs for
  Zero-shot Document-level Relation Triplet Extraction",2024-01-24 17:04:28+00:00,http://arxiv.org/abs/2401.13598v1,"Qi Sun, Kun Huang, Xiaocui Yang, Rong Tong, Kun Zhang, Soujanya Poria",cs.CL,table2text,"Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in
information systems that aims to simultaneously extract entities with semantic
relations from a document. Existing methods heavily rely on a substantial
amount of fully labeled data. However, collecting and annotating data for newly
emerging relations is time-consuming and labor-intensive. Recent advanced Large
Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text
generation capabilities, inspiring us to explore an alternative approach for
obtaining auto-labeled documents with new relations. In this paper, we propose
a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework,
which generates labeled data by retrieval and denoising knowledge from LLMs,
called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide
ChatGPT to generate labeled long-text data step by step. To improve the quality
of synthetic data, we propose a denoising strategy based on the consistency of
cross-document knowledge. Leveraging our denoised synthetic data, we proceed to
fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets.
We perform experiments for both zero-shot document-level relation and triplet
extraction on two public datasets. The experimental results illustrate that our
GenRDK framework outperforms strong baselines.",2024-01-24
IndiText Boost: Text Augmentation for Low Resource India Languages,2024-01-23 20:54:40+00:00,http://arxiv.org/abs/2401.13085v1,"Onkar Litake, Niraj Yagnik, Shreyas Labhsetwar","cs.CL, cs.AI, cs.LG",table2text,"Text Augmentation is an important task for low-resource languages. It helps
deal with the problem of data scarcity. A data augmentation strategy is used to
deal with the problem of data scarcity. Through the years, much work has been
done on data augmentation for the English language. In contrast, very less work
has been done on Indian languages. This is contrary to the fact that data
augmentation is used to deal with data scarcity. In this work, we focus on
implementing techniques like Easy Data Augmentation, Back Translation,
Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for
text classification on different languages. We focus on 6 Indian languages
namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to
our knowledge, no such work exists for text augmentation on Indian languages.
We carry out binary as well as multi-class text classification to make our
results more comparable. We get surprising results as basic data augmentation
techniques surpass LLMs.",2024-01-23
Unsupervised Learning of Graph from Recipes,2024-01-22 16:25:47+00:00,http://arxiv.org/abs/2401.12088v1,"Aissatou Diallo, Antonis Bikakis, Luke Dickens, Anthony Hunter, Rob Miller",cs.CL,table2text,"Cooking recipes are one of the most readily available kinds of procedural
text. They consist of natural language instructions that can be challenging to
interpret. In this paper, we propose a model to identify relevant information
from recipes and generate a graph to represent the sequence of actions in the
recipe. In contrast with other approaches, we use an unsupervised approach. We
iteratively learn the graph structure and the parameters of a $\mathsf{GNN}$
encoding the texts (text-to-graph) one sequence at a time while providing the
supervision by decoding the graph into text (graph-to-text) and comparing the
generated text to the input. We evaluate the approach by comparing the
identified entities with annotated datasets, comparing the difference between
the input and output texts, and comparing our generated graphs with those
generated by state of the art methods.",2024-01-22
"Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated
  Text",2024-01-22 16:09:47+00:00,http://arxiv.org/abs/2401.12070v1,"Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein","cs.CL, cs.AI, cs.LG",table2text,"Detecting text generated by modern large language models is thought to be
hard, as both LLMs and humans can exhibit a wide range of complex behaviors.
However, we find that a score based on contrasting two closely related language
models is highly accurate at separating human-generated and machine-generated
text. Based on this mechanism, we propose a novel LLM detector that only
requires simple calculations using a pair of pre-trained LLMs. The method,
called Binoculars, achieves state-of-the-art accuracy without any training
data. It is capable of spotting machine text from a range of modern LLMs
without any model-specific modifications. We comprehensively evaluate
Binoculars on a number of text sources and in varied situations. Over a wide
range of document types, Binoculars detects over 90% of generated samples from
ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being
trained on any ChatGPT data.",2024-01-22
Self-training from Self-memory in Data-to-text Generation,2024-01-19 09:13:28+00:00,http://arxiv.org/abs/2401.10567v1,Hoang-Thang Ta,cs.CL,table2text,"This paper introduces a novel training model, self-training from self-memory
(STSM) in data-to-text generation (DTG), allowing the model to self-train on
subsets, including self-memory as outputs inferred directly from the trained
models and/or the new data. The quality of self-memory is validated by two
models, data-to-text (D2T) and text-to-data (T2D), by two pre-defined
conditions: (1) the appearance of all source values in the outputs of the D2T
model and (2) the ability to convert back to source data in the outputs in the
T2D model. We utilize a greedy algorithm to generate shorter D2T outputs if
they contain all source values. Subsequently, we use the T2D model to confirm
that these outputs can capture input relationships by demonstrating their
capacity to convert text back into data. With 30% of the dataset, we can train
the D2T model with a competitive performance compared to full training in the
same setup. We experiment with our model on two datasets, E2E NLG and DART.
STSM offers the D2T model a generalization capability from its subset memory
while reducing training data volume. Ultimately, we anticipate that this paper
will contribute to continual learning solutions that adapt to new training
data, incorporating it as a form of self-memory in DTG tasks. The curated
dataset is publicly available at: https://github.com/hoangthangta/STSM.",2024-01-19
"Large Language Models for Scientific Information Extraction: An
  Empirical Study for Virology",2024-01-18 15:04:55+00:00,http://arxiv.org/abs/2401.10040v1,"Mahsa Shamsabadi, Jennifer D'Souza, Sören Auer","cs.CL, cs.AI, cs.DL, cs.IT, math.IT",table2text,"In this paper, we champion the use of structured and semantic content
representation of discourse-based scholarly communication, inspired by tools
like Wikipedia infoboxes or structured Amazon product descriptions. These
representations provide users with a concise overview, aiding scientists in
navigating the dense academic landscape. Our novel automated approach leverages
the robust text generation capabilities of LLMs to produce structured scholarly
contribution summaries, offering both a practical solution and insights into
LLMs' emergent abilities.
  For LLMs, the prime focus is on improving their general intelligence as
conversational agents. We argue that these models can also be applied
effectively in information extraction (IE), specifically in complex IE tasks
within terse domains like Science. This paradigm shift replaces the traditional
modular, pipelined machine learning approach with a simpler objective expressed
through instructions. Our results show that finetuned FLAN-T5 with 1000x fewer
parameters than the state-of-the-art GPT-davinci is competitive for the task.",2024-01-18
"Evolutionary Computation in the Era of Large Language Model: Survey and
  Roadmap",2024-01-18 14:58:17+00:00,http://arxiv.org/abs/2401.10034v1,"Xingyu Wu, Sheng-hao Wu, Jibin Wu, Liang Feng, Kay Chen Tan","cs.NE, cs.AI, cs.CL",table2text,"Large Language Models (LLMs), built upon Transformer-based architectures with
massive pretraining on diverse data, have not only revolutionized natural
language processing but also extended their prowess to various domains, marking
a significant stride towards artificial general intelligence. The interplay
between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives
and methodologies, reveals intriguing parallels, especially in their shared
optimization nature, black-box characteristics, and proficiency in handling
complex problems. Meanwhile, EA can not only provide an optimization framework
for LLM's further enhancement under black-box settings but also empower LLM
with flexible global search and iterative mechanism in applications. On the
other hand, LLM's abundant domain knowledge enables EA to perform smarter
searches, while its text processing capability assist in deploying EA across
various tasks. Based on their complementary advantages, this paper presents a
comprehensive review and forward-looking roadmap, categorizing their mutual
inspiration into LLM-enhanced evolutionary optimization and EA-enhanced LLM.
Some integrated synergy methods are further introduced to exemplify the
amalgamation of LLMs and EAs in various application scenarios, including neural
architecture search, code generation, software engineering, and text
generation. As the first comprehensive review specifically focused on the EA
research in the era of LLMs, this paper provides a foundational stepping stone
for understanding and harnessing the collaborative potential of LLMs and EAs.
By presenting a comprehensive review, categorization, and critical analysis, we
contribute to the ongoing discourse on the cross-disciplinary study of these
two powerful paradigms. The identified challenges and future directions offer
guidance to unlock the full potential of this innovative collaboration.",2024-01-18
Aligning Large Language Models with Counterfactual DPO,2024-01-17 19:43:43+00:00,http://arxiv.org/abs/2401.09566v2,Bradley Butcher,"cs.CL, cs.AI",table2text,"Advancements in large language models (LLMs) have demonstrated remarkable
capabilities across a diverse range of applications. These models excel in
generating text completions that are contextually coherent and cover an
extensive array of subjects. However, the vast datasets required for their
training make aligning response styles during the pretraining and instruction
tuning phases challenging. Consequently, an additional alignment phase is
typically employed, wherein the model is further trained with human preference
data to better align its outputs with human expectations. While this process
doesn't introduce new capabilities per se, it does accentuate generation styles
innate to the model. This paper explores the utilization of counterfactual
prompting within the framework of Direct Preference Optimization (DPO) to align
the model's style without relying on human intervention. We demonstrate that
this method effectively instils desirable behaviour, mitigates undesirable
ones, and encourages the model to disregard inappropriate instructions. Our
findings suggest that counterfactual prompting with DPO presents a low-resource
way to fine-tune LLMs to meet the demands for responsible and ethically aligned
AI systems.",2024-01-17
"Contrastive Perplexity for Controlled Generation: An Application in
  Detoxifying Large Language Models",2024-01-16 16:49:39+00:00,http://arxiv.org/abs/2401.08491v2,"Tassilo Klein, Moin Nabi","cs.CL, cs.LG",table2text,"The generation of undesirable and factually incorrect content of large
language models poses a significant challenge and remains largely an unsolved
issue. This paper studies the integration of a contrastive learning objective
for fine-tuning LLMs for implicit knowledge editing and controlled text
generation. Optimizing the training objective entails aligning text
perplexities in a contrastive fashion. To facilitate training the model in a
self-supervised fashion, we leverage an off-the-shelf LLM for training data
generation. We showcase applicability in the domain of detoxification. Herein,
the proposed approach leads to a significant decrease in the generation of
toxic content while preserving general utility for downstream tasks such as
commonsense reasoning and reading comprehension. The proposed approach is
conceptually simple but empirically powerful.",2024-01-16
"Ask the experts: sourcing high-quality datasets for nutritional
  counselling through Human-AI collaboration",2024-01-16 15:07:09+00:00,http://arxiv.org/abs/2401.08420v1,"Simone Balloccu, Ehud Reiter, Vivek Kumar, Diego Reforgiato Recupero, Daniele Riboni",cs.CL,table2text,"Large Language Models (LLMs), with their flexible generation abilities, can
be powerful data sources in domains with few or no available corpora. However,
problems like hallucinations and biases limit such applications. In this case
study, we pick nutrition counselling, a domain lacking any public resource, and
show that high-quality datasets can be gathered by combining LLMs,
crowd-workers and nutrition experts. We first crowd-source and cluster a novel
dataset of diet-related issues, then work with experts to prompt ChatGPT into
producing related supportive text. Finally, we let the experts evaluate the
safety of the generated text. We release HAI-coaching, the first
expert-annotated nutrition counselling dataset containing ~2.4K dietary
struggles from crowd workers, and ~97K related supportive texts generated by
ChatGPT. Extensive analysis shows that ChatGPT while producing highly fluent
and human-like text, also manifests harmful behaviours, especially in sensitive
topics like mental health, making it unsuitable for unsupervised use.",2024-01-16
Fine-grained Hallucination Detection and Editing for Language Models,2024-01-12 19:02:48+00:00,http://arxiv.org/abs/2401.06855v2,"Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, Hannaneh Hajishirzi",cs.CL,table2text,"Large language models (LMs) are prone to generate diverse factually incorrect
statements, which are widely called hallucinations. Current approaches
predominantly focus on coarse-grained automatic hallucination detection or
editing, overlooking nuanced error levels. In this paper, we propose a novel
task -- automatic fine-grained hallucination detection -- and present a
comprehensive taxonomy encompassing six hierarchically defined types of
hallucination. To facilitate evaluation, we introduce a new benchmark that
includes fine-grained human judgments on two LM outputs across various domains.
Our analysis reveals that ChatGPT and Llama 2-Chat exhibit hallucinations in
60% and 75% of their outputs, respectively, and a majority of these
hallucinations fall into categories that have been underexplored. As an initial
step to address this, we train FAVA, a retrieval-augmented LM by carefully
designing synthetic data generations to detect and correct fine-grained
hallucinations. On our benchmark, our automatic and human evaluations show that
FAVA significantly outperforms ChatGPT on fine-grained hallucination detection
by a large margin though a large room for future improvement still exists.
FAVA's suggested edits also improve the factuality of LM-generated text,
resulting in 5-10% FActScore improvements.",2024-01-12
"Between Lines of Code: Unraveling the Distinct Patterns of Machine and
  Human Programmers",2024-01-12 09:15:20+00:00,http://arxiv.org/abs/2401.06461v2,"Yuling Shi, Hongyu Zhang, Chengcheng Wan, Xiaodong Gu","cs.SE, cs.AI, cs.CL",table2text,"Large language models have catalyzed an unprecedented wave in code
generation. While achieving significant advances, they blur the distinctions
between machine-and human-authored source code, causing integrity and
authenticity issues of software artifacts. Previous methods such as DetectGPT
have proven effective in discerning machine-generated texts, but they do not
identify and harness the unique patterns of machine-generated code. Thus, its
applicability falters when applied to code. In this paper, we carefully study
the specific patterns that characterize machine and human-authored code.
Through a rigorous analysis of code attributes such as length, lexical
diversity, and naturalness, we expose unique pat-terns inherent to each source.
We particularly notice that the structural segmentation of code is a critical
factor in identifying its provenance. Based on our findings, we propose a novel
machine-generated code detection method called DetectCodeGPT, which improves
DetectGPT by capturing the distinct structural patterns of code. Diverging from
conventional techniques that depend on external LLMs for perturbations,
DetectCodeGPT perturbs the code corpus by strategically inserting spaces and
newlines, ensuring both efficacy and efficiency. Experiment results show that
our approach significantly outperforms state-of-the-art techniques in detecting
machine-generated code.",2024-01-12
"Automating Knowledge Acquisition for Content-Centric Cognitive Agents
  Using LLMs",2023-12-27 02:31:51+00:00,http://arxiv.org/abs/2312.16378v1,"Sanjay Oruganti, Sergei Nirenburg, Jesse English, Marjorie McShane","cs.CL, cs.AI",table2text,"The paper describes a system that uses large language model (LLM) technology
to support the automatic learning of new entries in an intelligent agent's
semantic lexicon. The process is bootstrapped by an existing non-toy lexicon
and a natural language generator that converts formal, ontologically-grounded
representations of meaning into natural language sentences. The learning method
involves a sequence of LLM requests and includes an automatic quality control
step. To date, this learning method has been applied to learning multiword
expressions whose meanings are equivalent to those of transitive verbs in the
agent's lexicon. The experiment demonstrates the benefits of a hybrid learning
architecture that integrates knowledge-based methods and resources with both
traditional data analytics and LLMs.",2023-12-27
"Zur Darstellung eines mehrstufigen Prototypbegriffs in der
  multilingualen automatischen Sprachgenerierung: vom Korpus über word
  embeddings bis hin zum automatischen Wörterbuch",2023-12-26 19:39:25+00:00,http://arxiv.org/abs/2312.16311v1,María José Domínguez Vázquez,cs.CL,table2text,"The multilingual dictionary of noun valency Portlex is considered to be the
trigger for the creation of the automatic language generators Xera and
Combinatoria, whose development and use is presented in this paper. Both
prototypes are used for the automatic generation of nominal phrases with their
mono- and bi-argumental valence slots, which could be used, among others, as
dictionary examples or as integrated components of future autonomous
E-Learning-Tools. As samples for new types of automatic valency dictionaries
including user interaction, we consider the language generators as we know them
today. In the specific methodological procedure for the development of the
language generators, the syntactic-semantic description of the noun slots turns
out to be the main focus from a syntagmatic and paradigmatic point of view.
Along with factors such as representativeness, grammatical correctness,
semantic coherence, frequency and the variety of lexical candidates, as well as
semantic classes and argument structures, which are fixed components of both
resources, a concept of a multi-sided prototype stands out. The combined
application of this prototype concept as well as of word embeddings together
with techniques from the field of automatic natural language processing and
generation (NLP and NLG) opens up a new way for the future development of
automatically generated plurilingual valency dictionaries. All things
considered, the paper depicts the language generators both from the point of
view of their development as well as from that of the users. The focus lies on
the role of the prototype concept within the development of the resources.",2023-12-26
PersianLLaMA: Towards Building First Persian Large Language Model,2023-12-25 12:48:55+00:00,http://arxiv.org/abs/2312.15713v1,"Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi, Hassan Naderi, Behrouz Minaei Bidgoli","cs.CL, cs.AI",table2text,"Despite the widespread use of the Persian language by millions globally,
limited efforts have been made in natural language processing for this
language. The use of large language models as effective tools in various
natural language processing tasks typically requires extensive textual data and
robust hardware resources. Consequently, the scarcity of Persian textual data
and the unavailability of powerful hardware resources have hindered the
development of large language models for Persian. This paper introduces the
first large Persian language model, named PersianLLaMA, trained on a collection
of Persian texts and datasets. This foundational model comes in two versions,
with 7 and 13 billion parameters, trained on formal and colloquial Persian
texts using two different approaches. PersianLLaMA has been evaluated for
natural language generation tasks based on the latest evaluation methods,
namely using larger language models, and for natural language understanding
tasks based on automated machine metrics. The results indicate that
PersianLLaMA significantly outperforms its competitors in both understanding
and generating Persian text. PersianLLaMA marks an important step in the
development of Persian natural language processing and can be a valuable
resource for the Persian-speaking community. This large language model can be
used for various natural language processing tasks, especially text generation
like chatbots, question-answering, machine translation, and text summarization",2023-12-25
"Balancing the Style-Content Trade-Off in Sentiment Transfer Using
  Polarity-Aware Denoising",2023-12-22 14:06:54+00:00,http://arxiv.org/abs/2312.14708v1,"Sourabrata Mukherjee, Zdeněk Kasner, Ondřej Dušek",cs.CL,table2text,"Text sentiment transfer aims to flip the sentiment polarity of a sentence
(positive to negative or vice versa) while preserving its sentiment-independent
content. Although current models show good results at changing the sentiment,
content preservation in transferred sentences is insufficient. In this paper,
we present a sentiment transfer model based on polarity-aware denoising, which
accurately controls the sentiment attributes in generated text, preserving the
content to a great extent and helping to balance the style-content trade-off.
Our proposed model is structured around two key stages in the sentiment
transfer process: better representation learning using a shared encoder and
sentiment-controlled generation using separate sentiment-specific decoders.
Empirical results show that our methods outperforms state-of-the-art baselines
in terms of content preservation while staying competitive in terms of style
transfer accuracy and fluency.",2023-12-22
Assaying on the Robustness of Zero-Shot Machine-Generated Text Detectors,2023-12-20 10:53:53+00:00,http://arxiv.org/abs/2312.12918v2,"Yi-Fan Zhang, Zhang Zhang, Liang Wang, Tieniu Tan, Rong Jin",cs.CL,table2text,"To combat the potential misuse of Natural Language Generation (NLG)
technology, a variety of algorithms have been developed for the detection of
AI-generated texts. Traditionally, this task is treated as a binary
classification problem. Although supervised learning has demonstrated promising
results, acquiring labeled data for detection purposes poses real-world
challenges and the risk of overfitting. In an effort to address these issues,
we delve into the realm of zero-shot machine-generated text detection. Existing
zero-shot detectors, typically designed for specific tasks or topics, often
assume uniform testing scenarios, limiting their practicality. In our research,
we explore various advanced Large Language Models (LLMs) and their specialized
variants, contributing to this field in several ways. In empirical studies, we
uncover a significant correlation between topics and detection performance.
Secondly, we delve into the influence of topic shifts on zero-shot detectors.
These investigations shed light on the adaptability and robustness of these
detection methods across diverse topics. The code is available at
\url{https://github.com/yfzhang114/robustness-detection}.",2023-12-20
"Instruct-SCTG: Guiding Sequential Controlled Text Generation through
  Instructions",2023-12-19 16:20:49+00:00,http://arxiv.org/abs/2312.12299v1,"Yinhong Liu, Yixuan Su, Ehsan Shareghi, Nigel Collier","cs.CL, cs.AI",table2text,"Instruction-tuned large language models have shown remarkable performance in
aligning generated text with user intentions across various tasks. However,
maintaining human-like discourse structure in the generated text remains a
challenging research question. In this paper, we propose Instruct-SCTG, a
flexible and effective sequential framework that harnesses instruction-tuned
language models to generate structurally coherent text in both fine-tuned and
zero-shot setups. Our framework generates articles in a section-by-section
manner, aligned with the desired human structure using natural language
instructions. Furthermore, we introduce a new automatic metric that measures
discourse divergence in a fuzzy manner. Extensive experiments on three datasets
from representative domains of news and recipes demonstrate the
state-of-the-art performance of our framework in imposing discourse structure
during text generation, as verified by both automatic and human evaluation. Our
code will be available on Github.",2023-12-19
"External Knowledge Augmented Polyphone Disambiguation Using Large
  Language Model",2023-12-19 08:00:10+00:00,http://arxiv.org/abs/2312.11920v1,Chen Li,cs.CL,table2text,"One of the key issues in Mandarin Chinese text-to-speech (TTS) systems is
polyphone disambiguation when doing grapheme-to-phoneme (G2P) conversion. In
this paper, we introduce a novel method to solve the problem as a generation
task. Following the trending research of large language models (LLM) and prompt
learning, the proposed method consists of three modules. Retrieval module
incorporates external knowledge which is a multi-level semantic dictionary of
Chinese polyphonic characters to format the sentence into a prompt. Generation
module adopts the decoder-only Transformer architecture to induce the target
text. Postprocess module corrects the generated text into a valid result if
needed. Experimental results show that our method outperforms the existing
methods on a public dataset called CPP. We also empirically study the impacts
of different templates of the prompt, different sizes of training data, and
whether to incorporate external knowledge.",2023-12-19
"Compositional Generalization for Multi-label Text Classification: A
  Data-Augmentation Approach",2023-12-18 15:18:57+00:00,http://arxiv.org/abs/2312.11276v2,"Yuyang Chai, Zhuang Li, Jiahui Liu, Lei Chen, Fei Li, Donghong Ji, Chong Teng",cs.CL,table2text,"Despite significant advancements in multi-label text classification, the
ability of existing models to generalize to novel and seldom-encountered
complex concepts, which are compositions of elementary ones, remains
underexplored. This research addresses this gap. By creating unique data splits
across three benchmarks, we assess the compositional generalization ability of
existing multi-label text classification models. Our results show that these
models often fail to generalize to compositional concepts encountered
infrequently during training, leading to inferior performance on tests with
these new combinations. To address this, we introduce a data augmentation
method that leverages two innovative text generation models designed to enhance
the classification models' capacity for compositional generalization. Our
experiments show that this data augmentation approach significantly improves
the compositional generalization capabilities of classification models on our
benchmarks, with both generation models surpassing other text generation
baselines.",2023-12-18
Deep dive into language traits of AI-generated Abstracts,2023-12-17 06:03:33+00:00,http://arxiv.org/abs/2312.10617v1,"Vikas Kumar, Amisha Bharti, Devanshu Verma, Vasudha Bhatnagar","cs.CL, cs.LG",table2text,"Generative language models, such as ChatGPT, have garnered attention for
their ability to generate human-like writing in various fields, including
academic research. The rapid proliferation of generated texts has bolstered the
need for automatic identification to uphold transparency and trust in the
information. However, these generated texts closely resemble human writing and
often have subtle differences in the grammatical structure, tones, and
patterns, which makes systematic scrutinization challenging. In this work, we
attempt to detect the Abstracts generated by ChatGPT, which are much shorter in
length and bounded. We extract the texts semantic and lexical properties and
observe that traditional machine learning models can confidently detect these
Abstracts.",2023-12-17
"A Soft Contrastive Learning-based Prompt Model for Few-shot Sentiment
  Analysis",2023-12-16 15:17:28+00:00,http://arxiv.org/abs/2312.10479v1,"Jingyi Zhou, Jie Zhou, Jiabao Zhao, Siyin Wang, Haijun Shan, Gui Tao, Qi Zhang, Xuanjing Huang",cs.CL,table2text,"Few-shot text classification has attracted great interest in both academia
and industry due to the lack of labeled data in many fields. Different from
general text classification (e.g., topic classification), few-shot sentiment
classification is more challenging because the semantic distances among the
classes are more subtle. For instance, the semantic distances between the
sentiment labels in a positive or negative polarity (e.g., ``love"" and ``joy"",
``remorse"" and ``sadness"") are close, while the distances are large for the
sentiment labels in two opposite polarities (e.g., ``love"" and ``sadness""). To
address this problem, we propose a Soft Contrastive learning-based Prompt
(\texttt{SCP}) model for few-shot sentiment analysis. First, we design a
sentiment-aware chain of thought prompt module to guide the model to predict
the sentiment from coarse grain to fine grain via a series of intermediate
reasoning steps. Then, we propose a soft contrastive learning algorithm to take
the correlation of the labels into account. A series of experiments on several
sentiment analysis datasets show the great advantages of \texttt{SCP} by
comparing it with SOTA baselines (e.g., ChatGPT).",2023-12-16
Continuous Diffusion for Mixed-Type Tabular Data,2023-12-16 12:21:03+00:00,http://arxiv.org/abs/2312.10431v1,"Markus Mueller, Kathrin Gruber, Dennis Fok","cs.LG, stat.ML",table2text,"Score-based generative models (or diffusion models for short) have proven
successful across many domains in generating text and image data. However, the
consideration of mixed-type tabular data with this model family has fallen
short so far. Existing research mainly combines different diffusion processes
without explicitly accounting for the feature heterogeneity inherent to tabular
data. In this paper, we combine score matching and score interpolation to
ensure a common type of continuous noise distribution that affects both
continuous and categorical features alike. Further, we investigate the impact
of distinct noise schedules per feature or per data type. We allow for
adaptive, learnable noise schedules to ensure optimally allocated model
capacity and balanced generative capability. Results show that our model
consistently outperforms state-of-the-art benchmark models and that accounting
for heterogeneity within the noise schedule design boosts the sample quality.",2023-12-16
GSQA: An End-to-End Model for Generative Spoken Question Answering,2023-12-15 13:33:18+00:00,http://arxiv.org/abs/2312.09781v1,"Min-Han Shih, Ho-Lam Chung, Yu-Chi Pai, Ming-Hao Hsu, Guan-Ting Lin, Shang-Wen Li, Hung-yi Lee","cs.CL, cs.AI",table2text,"In recent advancements in spoken question answering (QA), end-to-end models
have made significant strides. However, previous research has primarily focused
on extractive span selection. While this extractive-based approach is effective
when answers are present directly within the input, it falls short in
addressing abstractive questions, where answers are not directly extracted but
inferred from the given information. To bridge this gap, we introduce the first
end-to-end Generative Spoken Question Answering (GSQA) model that empowers the
system to engage in abstractive reasoning. The challenge in training our GSQA
model lies in the absence of a spoken abstractive QA dataset. We propose using
text models for initialization and leveraging the extractive QA dataset to
transfer knowledge from the text generative model to the spoken generative
model. Experimental results indicate that our model surpasses the previous
extractive model by 3% on extractive QA datasets. Furthermore, the GSQA model
has only been fine-tuned on the spoken extractive QA dataset. Despite not
having seen any spoken abstractive QA data, it can still closely match the
performance of the cascade model. In conclusion, our GSQA model shows the
potential to generalize to a broad spectrum of questions, thus further
expanding spoken question answering capabilities of abstractive QA. Our code is
available at
\href{https://voidful.github.io/GSQA}{https://voidful.github.io/GSQA}",2023-12-15
Fast Sampling via De-randomization for Discrete Diffusion Models,2023-12-14 18:14:11+00:00,http://arxiv.org/abs/2312.09193v1,"Zixiang Chen, Huizhuo Yuan, Yongqian Li, Yiwen Kou, Junkai Zhang, Quanquan Gu","cs.LG, cs.AI, stat.ML",table2text,"Diffusion models have emerged as powerful tools for high-quality data
generation, such as image generation. Despite its success in continuous spaces,
discrete diffusion models, which apply to domains such as texts and natural
languages, remain under-studied and often suffer from slow generation speed. In
this paper, we propose a novel de-randomized diffusion process, which leads to
an accelerated algorithm for discrete diffusion models. Our technique
significantly reduces the number of function evaluations (i.e., calls to the
neural network), making the sampling process much faster. Furthermore, we
introduce a continuous-time (i.e., infinite-step) sampling algorithm that can
provide even better sample qualities than its discrete-time (finite-step)
counterpart. Extensive experiments on natural language generation and machine
translation tasks demonstrate the superior performance of our method in terms
of both generation speed and sample quality over existing methods for discrete
diffusion models.",2023-12-14
ToViLaG: Your Visual-Language Generative Model is Also An Evildoer,2023-12-13 08:25:07+00:00,http://arxiv.org/abs/2312.11523v1,"Xinpeng Wang, Xiaoyuan Yi, Han Jiang, Shanlin Zhou, Zhihua Wei, Xing Xie","cs.CL, cs.AI",table2text,"Warning: this paper includes model outputs showing offensive content. Recent
large-scale Visual-Language Generative Models (VLGMs) have achieved
unprecedented improvement in multimodal image/text generation. However, these
models might also generate toxic content, e.g., offensive text and pornography
images, raising significant ethical risks. Despite exhaustive studies on toxic
degeneration of language models, this problem remains largely unexplored within
the context of visual-language generation. This work delves into the propensity
for toxicity generation and susceptibility to toxic data across various VLGMs.
For this purpose, we built ToViLaG, a dataset comprising 32K
co-toxic/mono-toxic text-image pairs and 1K innocuous but evocative text that
tends to stimulate toxicity. Furthermore, we propose WInToRe, a novel toxicity
metric tailored to visual-language generation, which theoretically reflects
different aspects of toxicity considering both input and output. On such a
basis, we benchmarked the toxicity of a diverse spectrum of VLGMs and
discovered that some models do more evil than expected while some are more
vulnerable to infection, underscoring the necessity of VLGMs detoxification.
Therefore, we develop an innovative bottleneck-based detoxification method. Our
method could reduce toxicity while maintaining comparable generation quality,
providing a promising initial solution to this line of research.",2023-12-13
A Survey of Text Watermarking in the Era of Large Language Models,2023-12-13 06:11:42+00:00,http://arxiv.org/abs/2312.07913v2,"Aiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming Hu, Lijie Wen, Irwin King, Philip S. Yu","cs.CL, 68T50, I.2.7",table2text,"In recent years, significant advancements have been made in the text
generation capabilities of Large Language Models (LLMs), demonstrating
exceptional performance in downstream tasks such as abstract summarization,
dialogue generation, and data-to-text conversion. However, their generative
abilities also pose risks such as the rapid spread of fake news, infringement
of datasets/LLM copyrights, and challenges to academic integrity. Text
watermarking technology emerges as a potential solution. By embedding invisible
yet detectable patterns in generated texts, it helps in tracking and verifying
text origins, thus preventing misuse and piracy.
  This survey aims to comprehensively summarize current text watermarking
technologies, covering three main aspects: (1) an overview and comparison of
different text watermarking techniques; (2) evaluation methods for text
watermarking algorithms, including their success rate, impact on text quality,
robustness, and unforgeability; (3) potential applications of text watermarking
technologies. This survey aims to help researchers thoroughly understanding the
text watermarking technologies, thereby fostering further development.",2023-12-13
On Diverse Preferences for Large Language Model Alignment,2023-12-12 16:17:15+00:00,http://arxiv.org/abs/2312.07401v1,"Dun Zeng, Yong Dai, Pengyu Cheng, Tianhao Hu, Wanshun Chen, Nan Du, Zenglin Xu",cs.AI,table2text,"The alignment of large language models (LLMs) with human values is crucial
for the development of artificial general intelligence (AGI). One promising
approach to achieve this alignment is reinforcement learning from human
feedback, which employs a reward model (RM) learned from human preference
datasets to guide LLMs in generating text that aligns with human preferences.
Through intensive experiments and analysis of reward distribution, this paper
finds that preference datasets are diverse from each other, even though they
are all proposed to align human preference. Hence, mixing diverse human
preference datasets to increase data size for enhancing reward modeling could
fail. To address the issue and capture the shared human values from diverse
preferences, a new training policy called MORE is introduced, which minimizes
preference bias by adaptively adjusting the preference objective across diverse
preferences. Experiments with the Pythia-1.4B model and five mixed preference
datasets show that MORE achieves superior reward accuracy and lower calibration
error, highlighting its ability to leverage diverse human preference data.",2023-12-12
"Multilingual large language models leak human stereotypes across
  language boundaries",2023-12-12 10:24:17+00:00,http://arxiv.org/abs/2312.07141v1,"Yang Trista Cao, Anna Sotnikova, Jieyu Zhao, Linda X. Zou, Rachel Rudinger, Hal Daume III",cs.CL,table2text,"Multilingual large language models have been increasingly popular for their
proficiency in comprehending and generating text across various languages.
Previous research has shown that the presence of stereotypes and biases in
monolingual large language models can be attributed to the nature of their
training data, which is collected from humans and reflects societal biases.
Multilingual language models undergo the same training procedure as monolingual
ones, albeit with training data sourced from various languages. This raises the
question: do stereotypes present in one social context leak across languages
within the model? In our work, we first define the term ``stereotype leakage''
and propose a framework for its measurement. With this framework, we
investigate how stereotypical associations leak across four languages: English,
Russian, Chinese, and Hindi. To quantify the stereotype leakage, we employ an
approach from social psychology, measuring stereotypes via group-trait
associations. We evaluate human stereotypes and stereotypical associations
manifested in multilingual large language models such as mBERT, mT5, and
ChatGPT. Our findings show a noticeable leakage of positive, negative, and
non-polar associations across all languages. Notably, Hindi within multilingual
models appears to be the most susceptible to influence from other languages,
while Chinese is the least. Additionally, ChatGPT exhibits a better alignment
with human scores than other models.",2023-12-12
"Astrocyte-Enabled Advancements in Spiking Neural Networks for Large
  Language Modeling",2023-12-12 06:56:31+00:00,http://arxiv.org/abs/2312.07625v1,"Guobin Shen, Dongcheng Zhao, Yiting Dong, Yang Li, Jindong Li, Yi Zeng","cs.NE, cs.AI",table2text,"Within the complex neuroarchitecture of the brain, astrocytes play crucial
roles in development, structure, and metabolism. These cells regulate neural
activity through tripartite synapses, directly impacting cognitive processes
such as learning and memory. Despite the growing recognition of astrocytes'
significance, traditional Spiking Neural Network (SNN) models remain
predominantly neuron-centric, overlooking the profound influence of astrocytes
on neural dynamics. Inspired by these biological insights, we have developed an
Astrocyte-Modulated Spiking Unit (AM-SU), an innovative framework that
integrates neuron-astrocyte interactions into the computational paradigm,
demonstrating wide applicability across various hardware platforms. Our
Astrocyte-Modulated Spiking Neural Network (AM-SNet) exhibits exceptional
performance in tasks involving memory retention and natural language
generation, particularly in handling long-term dependencies and complex
linguistic structures. The design of AM-SNet not only enhances its biological
authenticity but also introduces novel computational dynamics, enabling more
effective processing of complex temporal dependencies. Furthermore, AM-SNet
shows low latency, high throughput, and reduced memory usage in practical
applications, making it highly suitable for resource-constrained environments.
By successfully integrating astrocytic dynamics into intelligent neural
networks, our work narrows the gap between biological plausibility and neural
modeling, laying the groundwork for future biologically-inspired neural
computing research that includes both neurons and astrocytes.",2023-12-12
Generative AI for Hate Speech Detection: Evaluation and Findings,2023-11-16 16:09:43+00:00,http://arxiv.org/abs/2311.09993v1,"Sagi Pendzel, Tomer Wullach, Amir Adler, Einat Minkov","cs.CL, cs.AI",table2text,"Automatic hate speech detection using deep neural models is hampered by the
scarcity of labeled datasets, leading to poor generalization. To mitigate this
problem, generative AI has been utilized to generate large amounts of synthetic
hate speech sequences from available labeled examples, leveraging the generated
data in finetuning large pre-trained language models (LLMs). In this chapter,
we provide a review of relevant methods, experimental setups and evaluation of
this approach. In addition to general LLMs, such as BERT, RoBERTa and ALBERT,
we apply and evaluate the impact of train set augmentation with generated data
using LLMs that have been already adapted for hate detection, including
RoBERTa-Toxicity, HateBERT, HateXplain, ToxDect, and ToxiGen. An empirical
study corroborates our previous findings, showing that this approach improves
hate speech generalization, boosting recall performance across data
distributions. In addition, we explore and compare the performance of the
finetuned LLMs with zero-shot hate detection using a GPT-3.5 model. Our results
demonstrate that while better generalization is achieved using the GPT-3.5
model, it achieves mediocre recall and low precision on most datasets. It is an
open question whether the sensitivity of models such as GPT-3.5, and onward,
can be improved using similar techniques of text generation.",2023-11-16
"The Curious Decline of Linguistic Diversity: Training Language Models on
  Synthetic Text",2023-11-16 11:31:50+00:00,http://arxiv.org/abs/2311.09807v1,"Yanzhu Guo, Guokan Shang, Michalis Vazirgiannis, Chloé Clavel",cs.CL,table2text,"This study investigates the consequences of training large language models
(LLMs) on synthetic data generated by their predecessors, an increasingly
prevalent practice aimed at addressing the limited supply of human-generated
training data. Diverging from the usual emphasis on performance metrics, we
focus on the impact of this training methodology on linguistic diversity,
especially when conducted recursively over time. To assess this, we developed a
set of novel metrics targeting lexical, syntactic, and semantic diversity,
applying them in recursive fine-tuning experiments across various natural
language generation tasks. Our findings reveal a marked decrease in the
diversity of the models' outputs through successive iterations. This trend
underscores the potential risks of training LLMs on predecessor-generated text,
particularly concerning the preservation of linguistic richness. Our study
highlights the need for careful consideration of the long-term effects of such
training approaches on the linguistic capabilities of LLMs.",2023-11-16
"Aligning Neural Machine Translation Models: Human Feedback in Training
  and Inference",2023-11-15 17:21:58+00:00,http://arxiv.org/abs/2311.09132v1,"Miguel Moura Ramos, Patrick Fernandes, António Farinhas, André F. T. Martins",cs.CL,table2text,"Reinforcement learning from human feedback (RLHF) is a recent technique to
improve the quality of the text generated by a language model, making it closer
to what humans would generate. A core ingredient in RLHF's success in aligning
and improving large language models (LLMs) is its reward model, trained using
human feedback on model outputs. In machine translation (MT), where metrics
trained from human annotations can readily be used as reward models, recent
methods using minimum Bayes risk decoding and reranking have succeeded in
improving the final quality of translation. In this study, we comprehensively
explore and compare techniques for integrating quality metrics as reward models
into the MT pipeline. This includes using the reward model for data filtering,
during the training phase through RL, and at inference time by employing
reranking techniques, and we assess the effects of combining these in a unified
approach. Our experimental results, conducted across multiple translation
tasks, underscore the crucial role of effective data filtering, based on
estimated quality, in harnessing the full potential of RL in enhancing MT
quality. Furthermore, our findings demonstrate the effectiveness of combining
RL training with reranking techniques, showcasing substantial improvements in
translation quality.",2023-11-15
"HELLaMA: LLaMA-based Table to Text Generation by Highlighting the
  Important Evidence",2023-11-15 12:02:52+00:00,http://arxiv.org/abs/2311.08896v1,"Junyi Bian, Xiaolei Qin, Wuhe Zou, Mengzuo Huang, Weidong Zhang",cs.CL,table2text,"Large models have demonstrated significant progress across various domains,
particularly in tasks related to text generation. In the domain of Table to
Text, many Large Language Model (LLM)-based methods currently resort to
modifying prompts to invoke public APIs, incurring potential costs and
information leaks. With the advent of open-source large models, fine-tuning
LLMs has become feasible. In this study, we conducted parameter-efficient
fine-tuning on the LLaMA2 model. Distinguishing itself from previous
fine-tuning-based table-to-text methods, our approach involves injecting
reasoning information into the input by emphasizing table-specific row data.
Our model consists of two modules: 1) a table reasoner that identifies relevant
row evidence, and 2) a table summarizer that generates sentences based on the
highlighted table. To facilitate this, we propose a search strategy to
construct reasoning labels for training the table reasoner. On both the FetaQA
and QTSumm datasets, our approach achieved state-of-the-art results.
Additionally, we observed that highlighting input tables significantly enhances
the model's performance and provides valuable interpretability.",2023-11-15
"MAP's not dead yet: Uncovering true language model modes by conditioning
  away degeneracy",2023-11-15 09:38:53+00:00,http://arxiv.org/abs/2311.08817v1,"Davis Yoshida, Kartik Goyal, Kevin Gimpel","cs.CL, cs.AI, cs.LG",table2text,"It has been widely observed that exact or approximate MAP (mode-seeking)
decoding from natural language generation (NLG) models consistently leads to
degenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has
generally been attributed to either a fundamental inadequacy of modes in models
or weaknesses in language modeling. Contrastingly in this work, we emphasize
that degenerate modes can even occur in the absence of any model error, due to
contamination of the training data. Specifically, we show that mixing even a
tiny amount of low-entropy noise with a population text distribution can cause
the data distribution's mode to become degenerate, implying that any models
trained on it will be as well. As the unconditional mode of NLG models will
often be degenerate, we therefore propose to apply MAP decoding to the model's
distribution conditional on avoiding specific degeneracies. Using exact-search,
we empirically verify that the length-conditional modes of machine translation
models and language models are indeed more fluent and topical than their
unconditional modes. For the first time, we also share many examples of exact
modal sequences from these models, and from several variants of the LLaMA-7B
model. Notably, the modes of the LLaMA models are still degenerate, showing
that improvements in modeling have not fixed this issue. Because of the cost of
exact mode finding algorithms, we develop an approximate mode finding approach,
ACBS, which finds sequences that are both high-likelihood and high-quality. We
apply this approach to LLaMA-7B, a model which was not trained for instruction
following, and find that we are able to elicit reasonable outputs without any
finetuning.",2023-11-15
"X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented
  Instruction Tuning with Auxiliary Evaluation Aspects",2023-11-15 09:01:55+00:00,http://arxiv.org/abs/2311.08788v1,"Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, Lifu Huang","cs.CL, cs.AI, cs.LG",table2text,"Natural Language Generation (NLG) typically involves evaluating the generated
text in various aspects (e.g., consistency and naturalness) to obtain a
comprehensive assessment. However, multi-aspect evaluation remains challenging
as it may require the evaluator to generalize to any given evaluation aspect
even if it's absent during training. In this paper, we introduce X-Eval, a
two-stage instruction tuning framework to evaluate the text in both seen and
unseen aspects customized by end users. X-Eval consists of two learning stages:
the vanilla instruction tuning stage that improves the model's ability to
follow evaluation instructions, and an enhanced instruction tuning stage that
exploits the connections between fine-grained evaluation aspects to better
assess text quality. To support the training of X-Eval, we collect
AspectInstruct, the first instruction tuning dataset tailored for multi-aspect
NLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance
task diversity, we devise an augmentation strategy that converts human rating
annotations into diverse forms of NLG evaluation tasks, including scoring,
comparison, ranking, and Boolean question answering. Extensive experiments
across three essential categories of NLG tasks: dialogue generation,
summarization, and data-to-text coupled with 21 aspects in meta-evaluation,
demonstrate that our X-Eval enables even a lightweight language model to
achieve a comparable if not higher correlation with human judgments compared to
the state-of-the-art NLG evaluators, such as GPT-4.",2023-11-15
TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer,2023-11-14 18:50:51+00:00,http://arxiv.org/abs/2311.08389v1,"Huashan Sun, Yixiao Wu, Yinghao Li, Jiawei Li, Yizhe Yang, Yang Gao","cs.CL, cs.AI, I.2.7",table2text,"Text style is highly abstract, as it encompasses various aspects of a
speaker's characteristics, habits, logical thinking, and the content they
express. However, previous text-style transfer tasks have primarily focused on
data-driven approaches, lacking in-depth analysis and research from the
perspectives of linguistics and cognitive science. In this paper, we introduce
a novel task called Text Speech-Style Transfer (TSST). The main objective is to
further explore topics related to human cognition, such as personality and
emotion, based on the capabilities of existing LLMs. Considering the objective
of our task and the distinctive characteristics of oral speech in real-life
scenarios, we trained multi-dimension (i.e. filler words, vividness,
interactivity, emotionality) evaluation models for the TSST and validated their
correlation with human assessments. We thoroughly analyze the performance of
several large language models (LLMs) and identify areas where further
improvement is needed. Moreover, driven by our evaluation models, we have
released a new corpus that improves the capabilities of LLMs in generating text
with speech-style characteristics. In summary, we present the TSST task, a new
benchmark for style transfer and emphasizing human-oriented evaluation,
exploring and advancing the performance of current LLMs.",2023-11-14
"Artificial Text Boundary Detection with Topological Data Analysis and
  Sliding Window Techniques",2023-11-14 17:48:19+00:00,http://arxiv.org/abs/2311.08349v1,"Laida Kushnareva, Tatiana Gaintseva, German Magai, Serguei Barannikov, Dmitry Abulkhanov, Kristian Kuznetsov, Irina Piontkovskaya, Sergey Nikolenko",cs.CL,table2text,"Due to the rapid development of text generation models, people increasingly
often encounter texts that may start out as written by a human but then
continue as machine-generated results of large language models. Detecting the
boundary between human-written and machine-generated parts of such texts is a
very challenging problem that has not received much attention in literature. In
this work, we consider and compare a number of different approaches for this
artificial text boundary detection problem, comparing several predictors over
features of different nature. We show that supervised fine-tuning of the
RoBERTa model works well for this task in general but fails to generalize in
important cross-domain and cross-generator settings, demonstrating a tendency
to overfit to spurious properties of the data. Then, we propose novel
approaches based on features extracted from a frozen language model's
embeddings that are able to outperform both the human accuracy level and
previously considered baselines on the Real or Fake Text benchmark. Moreover,
we adapt perplexity-based approaches for the boundary detection task and
analyze their behaviour. We analyze the robustness of all proposed classifiers
in cross-domain and cross-model settings, discovering important properties of
the data that can negatively influence the performance of artificial text
boundary detection algorithms.",2023-11-14
Learning Globally Optimized Language Structure via Adversarial Training,2023-11-12 08:21:43+00:00,http://arxiv.org/abs/2311.06771v1,Xuwang Yin,"cs.CL, cs.AI",table2text,"Recent work has explored integrating autoregressive language models with
energy-based models (EBMs) to enhance text generation capabilities. However,
learning effective EBMs for text is challenged by the discrete nature of
language. This work proposes an adversarial training strategy to address
limitations in prior efforts. Specifically, an iterative adversarial attack
algorithm is presented to generate negative samples for training the EBM by
perturbing text from the autoregressive model. This aims to enable the EBM to
suppress spurious modes outside the support of the data distribution.
Experiments on an arithmetic sequence generation task demonstrate that the
proposed adversarial training approach can substantially enhance the quality of
generated sequences compared to prior methods. The results highlight the
promise of adversarial techniques to improve discrete EBM training. Key
contributions include: (1) an adversarial attack strategy tailored to text to
generate negative samples, circumventing MCMC limitations; (2) an adversarial
training algorithm for EBMs leveraging these attacks; (3) empirical validation
of performance improvements on a sequence generation task.",2023-11-12
Synthetic Speaking Children -- Why We Need Them and How to Make Them,2023-11-08 22:58:22+00:00,http://arxiv.org/abs/2311.06307v1,"Muhammad Ali Farooq, Dan Bigioi, Rishabh Jain, Wang Yao, Mariam Yiwere, Peter Corcoran","cs.HC, cs.AI, cs.SD, eess.AS",table2text,"Contemporary Human Computer Interaction (HCI) research relies primarily on
neural network models for machine vision and speech understanding of a system
user. Such models require extensively annotated training datasets for optimal
performance and when building interfaces for users from a vulnerable population
such as young children, GDPR introduces significant complexities in data
collection, management, and processing. Motivated by the training needs of an
Edge AI smart toy platform this research explores the latest advances in
generative neural technologies and provides a working proof of concept of a
controllable data generation pipeline for speech driven facial training data at
scale. In this context, we demonstrate how StyleGAN2 can be finetuned to create
a gender balanced dataset of children's faces. This dataset includes a variety
of controllable factors such as facial expressions, age variations, facial
poses, and even speech-driven animations with realistic lip synchronization. By
combining generative text to speech models for child voice synthesis and a 3D
landmark based talking heads pipeline, we can generate highly realistic,
entirely synthetic, talking child video clips. These video clips can provide
valuable, and controllable, synthetic training data for neural network models,
bridging the gap when real data is scarce or restricted due to privacy
regulations.",2023-11-08
Aspects of human memory and Large Language Models,2023-11-07 09:39:12+00:00,http://arxiv.org/abs/2311.03839v2,Romuald A. Janik,"cs.CL, cs.AI, cs.LG, q-bio.NC",table2text,"Large Language Models (LLMs) are huge artificial neural networks which
primarily serve to generate text, but also provide a very sophisticated
probabilistic model of language use. Since generating a semantically consistent
text requires a form of effective memory, we investigate the memory properties
of LLMs and find surprising similarities with key characteristics of human
memory. We argue that the human-like memory properties of the Large Language
Model do not follow automatically from the LLM architecture but are rather
learned from the statistics of the training textual data. These results
strongly suggest that the biological features of human memory leave an imprint
on the way that we structure our textual narratives.",2023-11-07
A Simple yet Efficient Ensemble Approach for AI-generated Text Detection,2023-11-06 13:11:02+00:00,http://arxiv.org/abs/2311.03084v2,"Harika Abburi, Kalyani Roy, Michael Suesserman, Nirmala Pudota, Balaji Veeramani, Edward Bowen, Sanmitra Bhattacharya","cs.CL, cs.AI",table2text,"Recent Large Language Models (LLMs) have demonstrated remarkable capabilities
in generating text that closely resembles human writing across wide range of
styles and genres. However, such capabilities are prone to potential abuse,
such as fake news generation, spam email creation, and misuse in academic
assignments. Hence, it is essential to build automated approaches capable of
distinguishing between artificially generated text and human-authored text. In
this paper, we propose a simple yet efficient solution to this problem by
ensembling predictions from multiple constituent LLMs. Compared to previous
state-of-the-art approaches, which are perplexity-based or uses ensembles with
a number of LLMs, our condensed ensembling approach uses only two constituent
LLMs to achieve comparable performance. Experiments conducted on four benchmark
datasets for generative text classification show performance improvements in
the range of 0.5 to 100\% compared to previous state-of-the-art approaches. We
also study the influence that the training data from individual LLMs have on
model performance. We found that substituting commercially-restrictive
Generative Pre-trained Transformer (GPT) data with data generated from other
open language models such as Falcon, Large Language Model Meta AI (LLaMA2), and
Mosaic Pretrained Transformers (MPT) is a feasible alternative when developing
generative text detectors. Furthermore, to demonstrate zero-shot
generalization, we experimented with an English essays dataset, and results
suggest that our ensembling approach can handle new data effectively.",2023-11-06
"An Ensemble Method Based on the Combination of Transformers with
  Convolutional Neural Networks to Detect Artificially Generated Text",2023-10-26 11:17:03+00:00,http://arxiv.org/abs/2310.17312v1,"Vijini Liyanage, Davide Buscaldi",cs.CL,table2text,"Thanks to the state-of-the-art Large Language Models (LLMs), language
generation has reached outstanding levels. These models are capable of
generating high quality content, thus making it a challenging task to detect
generated text from human-written content. Despite the advantages provided by
Natural Language Generation, the inability to distinguish automatically
generated text can raise ethical concerns in terms of authenticity.
Consequently, it is important to design and develop methodologies to detect
artificial content. In our work, we present some classification models
constructed by ensembling transformer models such as Sci-BERT, DeBERTa and
XLNet, with Convolutional Neural Networks (CNNs). Our experiments demonstrate
that the considered ensemble architectures surpass the performance of the
individual transformer models for classification. Furthermore, the proposed
SciBERT-CNN ensemble model produced an F1-score of 98.36% on the ALTA shared
task 2023 data.",2023-10-26
Automatic Logical Forms improve fidelity in Table-to-Text generation,2023-10-26 10:00:24+00:00,http://arxiv.org/abs/2310.17279v1,"Iñigo Alonso, Eneko Agirre",cs.CL,table2text,"Table-to-text systems generate natural language statements from structured
data like tables. While end-to-end techniques suffer from low factual
correctness (fidelity), a previous study reported gains when using manual
logical forms (LF) that represent the selected content and the semantics of the
target text. Given the manual step, it was not clear whether automatic LFs
would be effective, or whether the improvement came from content selection
alone. We present TlT which, given a table and a selection of the content,
first produces LFs and then the textual statement. We show for the first time
that automatic LFs improve quality, with an increase in fidelity of 30 points
over a comparable system not using LFs. Our experiments allow to quantify the
remaining challenges for high factual correctness, with automatic selection of
content coming first, followed by better Logic-to-Text generation and, to a
lesser extent, better Table-to-Logic parsing.",2023-10-26
Beyond MLE: Convex Learning for Text Generation,2023-10-26 08:08:43+00:00,http://arxiv.org/abs/2310.17217v1,"Chenze Shao, Zhengrui Ma, Min Zhang, Yang Feng","cs.CL, cs.AI, cs.LG",table2text,"Maximum likelihood estimation (MLE) is a statistical method used to estimate
the parameters of a probability distribution that best explain the observed
data. In the context of text generation, MLE is often used to train generative
language models, which can then be used to generate new text. However, we argue
that MLE is not always necessary and optimal, especially for closed-ended text
generation tasks like machine translation. In these tasks, the goal of model is
to generate the most appropriate response, which does not necessarily require
it to estimate the entire data distribution with MLE. To this end, we propose a
novel class of training objectives based on convex functions, which enables
text generation models to focus on highly probable outputs without having to
estimate the entire data distribution. We investigate the theoretical
properties of the optimal predicted distribution when applying convex functions
to the loss, demonstrating that convex functions can sharpen the optimal
distribution, thereby enabling the model to better capture outputs with high
probabilities. Experiments on various text generation tasks and models show the
effectiveness of our approach. It enables autoregressive models to bridge the
gap between greedy and beam search, and facilitates the learning of
non-autoregressive models with a maximum improvement of 9+ BLEU points.
Moreover, our approach also exhibits significant impact on large language
models (LLMs), substantially enhancing their generative capability on various
tasks. Source code is available at
\url{https://github.com/ictnlp/Convex-Learning}.",2023-10-26
"Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text
  Generation",2023-10-25 20:05:07+00:00,http://arxiv.org/abs/2310.16964v1,"Mateusz Lango, Ondřej Dušek","cs.CL, I.2.7",table2text,"Hallucination of text ungrounded in the input is a well-known problem in
neural data-to-text generation. Many methods have been proposed to mitigate it,
but they typically require altering model architecture or collecting additional
data, and thus cannot be easily applied to an existing model. In this paper, we
explore a new way to mitigate hallucinations by combining the probabilistic
output of a generator language model (LM) with the output of a special ""text
critic"" classifier, which guides the generation by assessing the match between
the input data and the text generated so far. Our method does not need any
changes to the underlying LM's architecture or training procedure and can thus
be combined with any model and decoding operating on word probabilities. The
critic does not need any additional training data, using the base LM's training
data and synthetic negative examples. Our experimental results show that our
method improves over the baseline on the WebNLG and OpenDialKG benchmarks.",2023-10-25
"A Comprehensive Evaluation of Constrained Text Generation for Large
  Language Models",2023-10-25 03:58:49+00:00,http://arxiv.org/abs/2310.16343v1,"Xiang Chen, Xiaojun Wan",cs.CL,table2text,"Advancements in natural language generation (NLG) and large language models
(LLMs) have led to proficient text generation in various tasks. However,
integrating intricate constraints into neural text generation, due to LLMs'
opacity, remains challenging. This study investigates constrained text
generation for LLMs, where predefined constraints are applied during LLM's
generation process. Our research examines multiple LLMs, including ChatGPT and
GPT-4, categorizing constraints into lexical, structural, and relation-based
types. We also present various benchmarks to facilitate fair evaluation. The
study addresses some key research questions, including the extent of LLMs'
compliance with constraints. Results illuminate LLMs' capacity and deficiency
to incorporate constraints and provide insights for future developments in
constrained text generation. Codes and datasets will be released upon
acceptance.",2023-10-25
"Octopus: A Multitask Model and Toolkit for Arabic Natural Language
  Generation",2023-10-24 19:06:55+00:00,http://arxiv.org/abs/2310.16127v1,"AbdelRahim Elmadany, El Moatez Billah Nagoudi, Muhammad Abdul-Mageed",cs.CL,table2text,"Understanding Arabic text and generating human-like responses is a
challenging endeavor. While many researchers have proposed models and solutions
for individual problems, there is an acute shortage of a comprehensive Arabic
natural language generation toolkit that is capable of handling a wide range of
tasks. In this work, we present a novel Arabic text-to-text Transformer model,
namely AraT5v2. Our new model is methodically trained on extensive and diverse
data, utilizing an extended sequence length of 2,048 tokens. We explore various
pretraining strategies including unsupervised, supervised, and joint
pertaining, under both single and multitask settings. Our models outperform
competitive baselines with large margins. We take our work one step further by
developing and publicly releasing Octopus, a Python-based package and
command-line toolkit tailored for eight Arabic generation tasks all exploiting
a single model. We release the models and the toolkit on our public repository.",2023-10-24
"Woodpecker: Hallucination Correction for Multimodal Large Language
  Models",2023-10-24 17:58:07+00:00,http://arxiv.org/abs/2310.16045v1,"Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, Enhong Chen","cs.CV, cs.AI, cs.CL, cs.LG",table2text,"Hallucination is a big shadow hanging over the rapidly evolving Multimodal
Large Language Models (MLLMs), referring to the phenomenon that the generated
text is inconsistent with the image content. In order to mitigate
hallucinations, existing studies mainly resort to an instruction-tuning manner
that requires retraining the models with specific data. In this paper, we pave
a different way, introducing a training-free method named Woodpecker. Like a
woodpecker heals trees, it picks out and corrects hallucinations from the
generated text. Concretely, Woodpecker consists of five stages: key concept
extraction, question formulation, visual knowledge validation, visual claim
generation, and hallucination correction. Implemented in a post-remedy manner,
Woodpecker can easily serve different MLLMs, while being interpretable by
accessing intermediate outputs of the five stages. We evaluate Woodpecker both
quantitatively and qualitatively and show the huge potential of this new
paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement
in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released
at https://github.com/BradyFU/Woodpecker.",2023-10-24
"Let the Pretrained Language Models ""Imagine"" for Short Texts Topic
  Modeling",2023-10-24 00:23:30+00:00,http://arxiv.org/abs/2310.15420v1,"Pritom Saha Akash, Jie Huang, Kevin Chen-Chuan Chang",cs.CL,table2text,"Topic models are one of the compelling methods for discovering latent
semantics in a document collection. However, it assumes that a document has
sufficient co-occurrence information to be effective. However, in short texts,
co-occurrence information is minimal, which results in feature sparsity in
document representation. Therefore, existing topic models (probabilistic or
neural) mostly fail to mine patterns from them to generate coherent topics. In
this paper, we take a new approach to short-text topic modeling to address the
data-sparsity issue by extending short text into longer sequences using
existing pre-trained language models (PLMs). Besides, we provide a simple
solution extending a neural topic model to reduce the effect of noisy
out-of-topics text generation from PLMs. We observe that our model can
substantially improve the performance of short-text topic modeling. Extensive
experiments on multiple real-world datasets under extreme data sparsity
scenarios show that our models can generate high-quality topics outperforming
state-of-the-art models.",2023-10-24
"Data Augmentation Techniques for Machine Translation of Code-Switched
  Texts: A Comparative Study",2023-10-23 18:09:41+00:00,http://arxiv.org/abs/2310.15262v1,"Injy Hamed, Nizar Habash, Ngoc Thang Vu",cs.CL,table2text,"Code-switching (CSW) text generation has been receiving increasing attention
as a solution to address data scarcity. In light of this growing interest, we
need more comprehensive studies comparing different augmentation approaches. In
this work, we compare three popular approaches: lexical replacements,
linguistic theories, and back-translation (BT), in the context of Egyptian
Arabic-English CSW. We assess the effectiveness of the approaches on machine
translation and the quality of augmentations through human evaluation. We show
that BT and CSW predictive-based lexical replacement, being trained on CSW
parallel data, perform best on both tasks. Linguistic theories and random
lexical replacement prove to be effective in the lack of CSW parallel data,
where both approaches achieve similar results.",2023-10-23
"Statistical Depth for Ranking and Characterizing Transformer-Based Text
  Embeddings",2023-10-23 15:02:44+00:00,http://arxiv.org/abs/2310.15010v1,"Parker Seegmiller, Sarah Masud Preum",cs.CL,table2text,"The popularity of transformer-based text embeddings calls for better
statistical tools for measuring distributions of such embeddings. One such tool
would be a method for ranking texts within a corpus by centrality, i.e.
assigning each text a number signifying how representative that text is of the
corpus as a whole. However, an intrinsic center-outward ordering of
high-dimensional text representations is not trivial. A statistical depth is a
function for ranking k-dimensional objects by measuring centrality with respect
to some observed k-dimensional distribution. We adopt a statistical depth to
measure distributions of transformer-based text embeddings, transformer-based
text embedding (TTE) depth, and introduce the practical use of this depth for
both modeling and distributional inference in NLP pipelines. We first define
TTE depth and an associated rank sum test for determining whether two corpora
differ significantly in embedding space. We then use TTE depth for the task of
in-context learning prompt selection, showing that this approach reliably
improves performance over statistical baseline approaches across six text
classification tasks. Finally, we use TTE depth and the associated rank sum
test to characterize the distributions of synthesized and human-generated
corpora, showing that five recent synthetic data augmentation processes cause a
measurable distributional shift away from associated human-generated text.",2023-10-23
"Did the Neurons Read your Book? Document-level Membership Inference for
  Large Language Models",2023-10-23 15:00:46+00:00,http://arxiv.org/abs/2310.15007v1,"Matthieu Meeus, Shubham Jain, Marek Rei, Yves-Alexandre de Montjoye","cs.CL, cs.CR, cs.LG",table2text,"With large language models (LLMs) poised to become embedded in our daily
lives, questions are starting to be raised about the dataset(s) they learned
from. These questions range from potential bias or misinformation LLMs could
retain from their training data to questions of copyright and fair use of
human-generated text. However, while these questions emerge, developers of the
recent state-of-the-art LLMs become increasingly reluctant to disclose details
on their training corpus. We here introduce the task of document-level
membership inference for real-world LLMs, i.e. inferring whether the LLM has
seen a given document during training or not. First, we propose a procedure for
the development and evaluation of document-level membership inference for LLMs
by leveraging commonly used data sources for training and the model release
date. We then propose a practical, black-box method to predict document-level
membership and instantiate it on OpenLLaMA-7B with both books and academic
papers. We show our methodology to perform very well, reaching an impressive
AUC of 0.856 for books and 0.678 for papers. We then show our approach to
outperform the sentence-level membership inference attacks used in the privacy
literature for the document-level membership task. We finally evaluate whether
smaller models might be less sensitive to document-level inference and show
OpenLLaMA-3B to be approximately as sensitive as OpenLLaMA-7B to our approach.
Taken together, our results show that accurate document-level membership can be
inferred for LLMs, increasing the transparency of technology poised to change
our lives.",2023-10-23
"A Survey on LLM-generated Text Detection: Necessity, Methods, and Future
  Directions",2023-10-23 09:01:13+00:00,http://arxiv.org/abs/2310.14724v2,"Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F. Wong, Lidia S. Chao","cs.CL, cs.AI",table2text,"The powerful ability to understand, follow, and generate complex language
emerging from large language models (LLMs) makes LLM-generated text flood many
areas of our daily lives at an incredible speed and is widely accepted by
humans. As LLMs continue to expand, there is an imperative need to develop
detectors that can detect LLM-generated text. This is crucial to mitigate
potential misuse of LLMs and safeguard realms like artistic expression and
social networks from harmful influence of LLM-generated content. The
LLM-generated text detection aims to discern if a piece of text was produced by
an LLM, which is essentially a binary classification task. The detector
techniques have witnessed notable advancements recently, propelled by
innovations in watermarking techniques, zero-shot methods, fine-turning LMs
methods, adversarial learning methods, LLMs as detectors, and human-assisted
methods. In this survey, we collate recent research breakthroughs in this area
and underscore the pressing need to bolster detector research. We also delve
into prevalent datasets, elucidating their limitations and developmental
requirements. Furthermore, we analyze various LLM-generated text detection
paradigms, shedding light on challenges like out-of-distribution problems,
potential attacks, and data ambiguity. Conclusively, we highlight interesting
directions for future research in LLM-generated text detection to advance the
implementation of responsible artificial intelligence (AI). Our aim with this
survey is to provide a clear and comprehensive introduction for newcomers while
also offering seasoned researchers a valuable update in the field of
LLM-generated text detection. The useful resources are publicly available at:
https://github.com/NLP2CT/LLM-generated-Text-Detection.",2023-10-23
"Text generation for dataset augmentation in security classification
  tasks",2023-10-22 22:25:14+00:00,http://arxiv.org/abs/2310.14429v1,"Alexander P. Welsh, Matthew Edwards","cs.CR, cs.CL",table2text,"Security classifiers, designed to detect malicious content in computer
systems and communications, can underperform when provided with insufficient
training data. In the security domain, it is often easy to find samples of the
negative (benign) class, and challenging to find enough samples of the positive
(malicious) class to train an effective classifier. This study evaluates the
application of natural language text generators to fill this data gap in
multiple security-related text classification tasks. We describe a variety of
previously-unexamined language-model fine-tuning approaches for this purpose
and consider in particular the impact of disproportionate class-imbalances in
the training set. Across our evaluation using three state-of-the-art
classifiers designed for offensive language detection, review fraud detection,
and SMS spam detection, we find that models trained with GPT-3 data
augmentation strategies outperform both models trained without augmentation and
models trained using basic data augmentation strategies already in common
usage. In particular, we find substantial benefits for GPT-3 data augmentation
strategies in situations with severe limitations on known positive-class
samples.",2023-10-22
Towards Understanding Sycophancy in Language Models,2023-10-20 14:46:48+00:00,http://arxiv.org/abs/2310.13548v2,"Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, Ethan Perez","cs.CL, cs.AI, cs.LG, stat.ML, I.2.6",table2text,"Reinforcement learning from human feedback (RLHF) is a popular technique for
training high-quality AI assistants. However, RLHF may also encourage model
responses that match user beliefs over truthful responses, a behavior known as
sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models
and whether human preference judgements are responsible. We first demonstrate
that five state-of-the-art AI assistants consistently exhibit sycophantic
behavior across four varied free-form text-generation tasks. To understand if
human preferences drive this broadly observed behavior of RLHF models, we
analyze existing human preference data. We find that when a response matches a
user's views, it is more likely to be preferred. Moreover, both humans and
preference models (PMs) prefer convincingly-written sycophantic responses over
correct ones a non-negligible fraction of the time. Optimizing model outputs
against PMs also sometimes sacrifices truthfulness in favor of sycophancy.
Overall, our results indicate that sycophancy is a general behavior of RLHF
models, likely driven in part by human preference judgements favoring
sycophantic responses.",2023-10-20
"GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language
  Models",2023-10-12 16:46:58+00:00,http://arxiv.org/abs/2310.08487v1,"Yuanchun Shen, Ruotong Liao, Zhen Han, Yunpu Ma, Volker Tresp",cs.CL,table2text,"While multi-modal models have successfully integrated information from image,
video, and audio modalities, integrating graph modality into large language
models (LLMs) remains unexplored. This discrepancy largely stems from the
inherent divergence between structured graph data and unstructured text data.
Incorporating graph knowledge provides a reliable source of information,
enabling potential solutions to address issues in text generation, e.g.,
hallucination, and lack of domain knowledge. To evaluate the integration of
graph knowledge into language models, a dedicated dataset is needed. However,
there is currently no benchmark dataset specifically designed for multimodal
graph-language models. To address this gap, we propose GraphextQA, a question
answering dataset with paired subgraphs, retrieved from Wikidata, to facilitate
the evaluation and future development of graph-language models. Additionally,
we introduce a baseline model called CrossGNN, which conditions answer
generation on the paired graphs by cross-attending question-aware graph
features at decoding. The proposed dataset is designed to evaluate
graph-language models' ability to understand graphs and make use of it for
answer generation. We perform experiments with language-only models and the
proposed graph-language model to validate the usefulness of the paired graphs
and to demonstrate the difficulty of the task.",2023-10-12
DistillSpec: Improving Speculative Decoding via Knowledge Distillation,2023-10-12 16:21:04+00:00,http://arxiv.org/abs/2310.08461v1,"Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, Rishabh Agarwal","cs.CL, cs.AI, cs.LG",table2text,"Speculative decoding (SD) accelerates large language model inference by
employing a faster draft model for generating multiple tokens, which are then
verified in parallel by the larger target model, resulting in the text
generated according to the target model distribution. However, identifying a
compact draft model that is well-aligned with the target model is challenging.
To tackle this issue, we propose DistillSpec that uses knowledge distillation
to better align the draft model with the target model, before applying SD.
DistillSpec makes two key design choices, which we demonstrate via systematic
study to be crucial to improving the draft and target alignment: utilizing
on-policy data generation from the draft model, and tailoring the divergence
function to the task and decoding strategy. Notably, DistillSpec yields
impressive 10 - 45% speedups over standard SD on a range of standard
benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine
DistillSpec with lossy SD to achieve fine-grained control over the latency vs.
task performance trade-off. Finally, in practical scenarios with models of
varying sizes, first using distillation to boost the performance of the target
model and then applying DistillSpec to train a well-aligned draft model can
reduce decoding latency by 6-10x with minimal performance drop, compared to
standard decoding without distillation.",2023-10-12
"CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large
  Language Models",2023-10-12 12:31:23+00:00,http://arxiv.org/abs/2310.08279v1,"Rui Yang, Li Fang, Yi Zhou","cs.CL, cs.AI",table2text,"Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce
and infer missing connections within knowledge graphs. Text-based approaches,
like SimKGC, have outperformed graph embedding methods, showcasing the promise
of inductive KGC. However, the efficacy of text-based methods hinges on the
quality of entity textual descriptions. In this paper, we identify the key
issue of whether large language models (LLMs) can generate effective text. To
mitigate hallucination in LLM-generated text in this paper, we introduce a
constraint-based prompt that utilizes the entity and its textual description as
contextual constraints to enhance data quality. Our Constrained-Prompt
Knowledge Graph Completion (CP-KGC) method demonstrates effective inference
under low resource computing conditions and surpasses prior results on the
WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC
tasks and provides new directions for future research.",2023-10-12
"Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented
  Models",2023-10-11 15:30:35+00:00,http://arxiv.org/abs/2310.07589v1,"Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker",cs.AI,table2text,"Considerable effort has been dedicated to mitigating toxicity, but existing
methods often require drastic modifications to model parameters or the use of
computationally intensive auxiliary models. Furthermore, previous approaches
have often neglected the crucial factor of language's evolving nature over
time. In this work, we present a comprehensive perspective on toxicity
mitigation that takes into account its changing nature. We introduce
Goodtriever, a flexible methodology that matches the current state-of-the-art
toxicity mitigation while achieving 43% relative latency reduction during
inference and being more computationally efficient. By incorporating a
retrieval-based approach at decoding time, Goodtriever enables
toxicity-controlled text generation. Our research advocates for an increased
focus on adaptable mitigation techniques, which better reflect the data drift
models face when deployed in the wild. Code and data are available at
https://github.com/for-ai/goodtriever.",2023-10-11
Multimodal Graph Learning for Generative Tasks,2023-10-11 13:25:03+00:00,http://arxiv.org/abs/2310.07478v2,"Minji Yoon, Jing Yu Koh, Bryan Hooi, Ruslan Salakhutdinov",cs.AI,table2text,"Multimodal learning combines multiple data modalities, broadening the types
and complexity of data our models can utilize: for example, from plain text to
image-caption pairs. Most multimodal learning algorithms focus on modeling
simple one-to-one pairs of data from two modalities, such as image-caption
pairs, or audio-text pairs. However, in most real-world settings, entities of
different modalities interact with each other in more complex and multifaceted
ways, going beyond one-to-one mappings. We propose to represent these complex
relationships as graphs, allowing us to capture data with any number of
modalities, and with complex relationships between modalities that can flexibly
vary from one sample to another. Toward this goal, we propose Multimodal Graph
Learning (MMGL), a general and systematic framework for capturing information
from multiple multimodal neighbors with relational structures among them. In
particular, we focus on MMGL for generative tasks, building upon pretrained
Language Models (LMs), aiming to augment their text generation with multimodal
neighbor contexts. We study three research questions raised by MMGL: (1) how
can we infuse multiple neighbor information into the pretrained LMs, while
avoiding scalability issues? (2) how can we infuse the graph structure
information among multimodal neighbors into the LMs? and (3) how can we
finetune the pretrained LMs to learn from the neighbor context in a
parameter-efficient manner? We conduct extensive experiments to answer these
three questions on MMGL and analyze the empirical results to pave the way for
future MMGL research.",2023-10-11
"MatChat: A Large Language Model and Application Service Platform for
  Materials Science",2023-10-11 05:11:46+00:00,http://arxiv.org/abs/2310.07197v1,"Ziyi Chen, Fankai Xie, Meng Wan, Yang Yuan, Miao Liu, Zongguo Wang, Sheng Meng, Yangang Wang","cond-mat.mtrl-sci, cs.AI",table2text,"The prediction of chemical synthesis pathways plays a pivotal role in
materials science research. Challenges, such as the complexity of synthesis
pathways and the lack of comprehensive datasets, currently hinder our ability
to predict these chemical processes accurately. However, recent advancements in
generative artificial intelligence (GAI), including automated text generation
and question-answering systems, coupled with fine-tuning techniques, have
facilitated the deployment of large-scale AI models tailored to specific
domains. In this study, we harness the power of the LLaMA2-7B model and enhance
it through a learning process that incorporates 13,878 pieces of structured
material knowledge data. This specialized AI model, named MatChat, focuses on
predicting inorganic material synthesis pathways. MatChat exhibits remarkable
proficiency in generating and reasoning with knowledge in materials science.
Although MatChat requires further refinement to meet the diverse material
design needs, this research undeniably highlights its impressive reasoning
capabilities and innovative potential in the field of materials science.
MatChat is now accessible online and open for use, with both the model and its
application framework available as open source. This study establishes a robust
foundation for collaborative innovation in the integration of generative AI in
materials science.",2023-10-11
"The Temporal Structure of Language Processing in the Human Brain
  Corresponds to The Layered Hierarchy of Deep Language Models",2023-10-11 01:03:42+00:00,http://arxiv.org/abs/2310.07106v1,"Ariel Goldstein, Eric Ham, Mariano Schain, Samuel Nastase, Zaid Zada, Avigail Dabush, Bobbi Aubrey, Harshvardhan Gazula, Amir Feder, Werner K Doyle, Sasha Devore, Patricia Dugan, Daniel Friedman, Roi Reichart, Michael Brenner, Avinatan Hassidim, Orrin Devinsky, Adeen Flinker, Omer Levy, Uri Hasson","cs.CL, cs.AI, cs.LG, q-bio.NC",table2text,"Deep Language Models (DLMs) provide a novel computational paradigm for
understanding the mechanisms of natural language processing in the human brain.
Unlike traditional psycholinguistic models, DLMs use layered sequences of
continuous numerical vectors to represent words and context, allowing a
plethora of emerging applications such as human-like text generation. In this
paper we show evidence that the layered hierarchy of DLMs may be used to model
the temporal dynamics of language comprehension in the brain by demonstrating a
strong correlation between DLM layer depth and the time at which layers are
most predictive of the human brain. Our ability to temporally resolve
individual layers benefits from our use of electrocorticography (ECoG) data,
which has a much higher temporal resolution than noninvasive methods like fMRI.
Using ECoG, we record neural activity from participants listening to a
30-minute narrative while also feeding the same narrative to a high-performing
DLM (GPT2-XL). We then extract contextual embeddings from the different layers
of the DLM and use linear encoding models to predict neural activity. We first
focus on the Inferior Frontal Gyrus (IFG, or Broca's area) and then extend our
model to track the increasing temporal receptive window along the linguistic
processing hierarchy from auditory to syntactic and semantic areas. Our results
reveal a connection between human language processing and DLMs, with the DLM's
layer-by-layer accumulation of contextual information mirroring the timing of
neural activity in high-order language areas.",2023-10-11
A Semantic Invariant Robust Watermark for Large Language Models,2023-10-10 06:49:43+00:00,http://arxiv.org/abs/2310.06356v1,"Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, Lijie Wen","cs.CR, cs.CL, 68T50, I.2.7",table2text,"Watermark algorithms for large language models (LLMs) have achieved extremely
high accuracy in detecting text generated by LLMs. Such algorithms typically
involve adding extra watermark logits to the LLM's logits at each generation
step. However, prior algorithms face a trade-off between attack robustness and
security robustness. This is because the watermark logits for a token are
determined by a certain number of preceding tokens; a small number leads to low
security robustness, while a large number results in insufficient attack
robustness. In this work, we propose a semantic invariant watermarking method
for LLMs that provides both attack robustness and security robustness. The
watermark logits in our work are determined by the semantics of all preceding
tokens. Specifically, we utilize another embedding LLM to generate semantic
embeddings for all preceding tokens, and then these semantic embeddings are
transformed into the watermark logits through our trained watermark model.
Subsequent analyses and experiments demonstrated the attack robustness of our
method in semantically invariant settings: synonym substitution and text
paraphrasing settings. Finally, we also show that our watermark possesses
adequate security robustness. Our code and data are available at
https://github.com/THU-BPM/Robust_Watermark.",2023-10-10
"Generative quantum machine learning via denoising diffusion
  probabilistic models",2023-10-09 17:03:08+00:00,http://arxiv.org/abs/2310.05866v1,"Bingzhi Zhang, Peng Xu, Xiaohui Chen, Quntao Zhuang","quant-ph, cs.AI, cs.LG",table2text,"Deep generative models are key-enabling technology to computer vision, text
generation and large language models. Denoising diffusion probabilistic models
(DDPMs) have recently gained much attention due to their ability to generate
diverse and high-quality samples in many computer vision tasks, as well as to
incorporate flexible model architectures and relatively simple training scheme.
Quantum generative models, empowered by entanglement and superposition, have
brought new insight to learning classical and quantum data. Inspired by the
classical counterpart, we propose the quantum denoising diffusion probabilistic
models (QuDDPM) to enable efficiently trainable generative learning of quantum
data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity,
while introduces multiple intermediate training tasks as interpolation between
the target distribution and noise to avoid barren plateau and guarantee
efficient training. We demonstrate QuDDPM's capability in learning correlated
quantum noise model and learning topological structure of nontrivial
distribution of quantum data.",2023-10-09
"RAUCG: Retrieval-Augmented Unsupervised Counter Narrative Generation for
  Hate Speech",2023-10-09 12:01:26+00:00,http://arxiv.org/abs/2310.05650v1,"Shuyu Jiang, Wenyi Tang, Xingshu Chen, Rui Tanga, Haizhou Wang, Wenxian Wang",cs.CL,table2text,"The Counter Narrative (CN) is a promising approach to combat online hate
speech (HS) without infringing on freedom of speech. In recent years, there has
been a growing interest in automatically generating CNs using natural language
generation techniques. However, current automatic CN generation methods mainly
rely on expert-authored datasets for training, which are time-consuming and
labor-intensive to acquire. Furthermore, these methods cannot directly obtain
and extend counter-knowledge from external statistics, facts, or examples. To
address these limitations, we propose Retrieval-Augmented Unsupervised Counter
Narrative Generation (RAUCG) to automatically expand external counter-knowledge
and map it into CNs in an unsupervised paradigm. Specifically, we first
introduce an SSF retrieval method to retrieve counter-knowledge from the
multiple perspectives of stance consistency, semantic overlap rate, and fitness
for HS. Then we design an energy-based decoding mechanism by quantizing
knowledge injection, countering and fluency constraints into differentiable
functions, to enable the model to build mappings from counter-knowledge to CNs
without expert-authored CN data. Lastly, we comprehensively evaluate model
performance in terms of language quality, toxicity, persuasiveness, relevance,
and success rate of countering HS, etc. Experimental results show that RAUCG
outperforms strong baselines on all metrics and exhibits stronger
generalization capabilities, achieving significant improvements of +2.0% in
relevance and +4.5% in success rate of countering metrics. Moreover, RAUCG
enabled GPT2 to outperform T0 in all metrics, despite the latter being
approximately eight times larger than the former. Warning: This paper may
contain offensive or upsetting content!",2023-10-09
On the Zero-Shot Generalization of Machine-Generated Text Detectors,2023-10-08 13:49:51+00:00,http://arxiv.org/abs/2310.05165v1,"Xiao Pu, Jingyu Zhang, Xiaochuang Han, Yulia Tsvetkov, Tianxing He",cs.CL,table2text,"The rampant proliferation of large language models, fluent enough to generate
text indistinguishable from human-written language, gives unprecedented
importance to the detection of machine-generated text. This work is motivated
by an important research question: How will the detectors of machine-generated
text perform on outputs of a new generator, that the detectors were not trained
on? We begin by collecting generation data from a wide range of LLMs, and train
neural detectors on data from each generator and test its performance on
held-out generators. While none of the detectors can generalize to all
generators, we observe a consistent and interesting pattern that the detectors
trained on data from a medium-size LLM can zero-shot generalize to the larger
version. As a concrete application, we demonstrate that robust detectors can be
built on an ensemble of training data from medium-sized models.",2023-10-08
Learning Personalized Story Evaluation,2023-10-05 04:15:48+00:00,http://arxiv.org/abs/2310.03304v3,"Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng Yang, Andrew Cohen, Lei Li, Yuandong Tian",cs.CL,table2text,"While large language models (LLMs) have shown impressive results for more
objective tasks such as QA and retrieval, it remains nontrivial to evaluate
their performance on open-ended text generation for reasons including (1) data
contamination; (2) multi-dimensional evaluation criteria; and (3)
subjectiveness stemming from reviewers' personal preferences. To address such
issues, we propose to model personalization in an uncontaminated open-ended
generation assessment. We create two new datasets Per-MPST and Per-DOC for
personalized story evaluation, by re-purposing existing datasets with proper
anonymization and new personalized labels. We further develop a personalized
story evaluation model PERSE to infer reviewer preferences and provide a
personalized evaluation. Specifically, given a few exemplary reviews from a
particular reviewer, PERSE predicts either a detailed review or fine-grained
comparison in several aspects (such as interestingness and surprise) for that
reviewer on a new text input. Experimental results show that PERSE outperforms
GPT-4 by 15.8% on Kendall correlation of story ratings, and by 13.7% on
pairwise preference prediction accuracy. Both datasets and code will be
released.",2023-10-05
LPML: LLM-Prompting Markup Language for Mathematical Reasoning,2023-09-21 02:46:20+00:00,http://arxiv.org/abs/2309.13078v2,"Ryutaro Yamauchi, Sho Sonoda, Akiyoshi Sannai, Wataru Kumagai","cs.AI, cs.LG, cs.PL",table2text,"In utilizing large language models (LLMs) for mathematical reasoning,
addressing the errors in the reasoning and calculation present in the generated
text by LLMs is a crucial challenge. In this paper, we propose a novel
framework that integrates the Chain-of-Thought (CoT) method with an external
tool (Python REPL). We discovered that by prompting LLMs to generate structured
text in XML-like markup language, we could seamlessly integrate CoT and the
external tool and control the undesired behaviors of LLMs. With our approach,
LLMs can utilize Python computation to rectify errors within CoT. We applied
our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and
demonstrated that combining CoT and Python REPL through the markup language
enhances the reasoning capability of LLMs. Our approach enables LLMs to write
the markup language and perform advanced mathematical reasoning using only
zero-shot prompting.",2023-09-21
"MBR and QE Finetuning: Training-time Distillation of the Best and Most
  Expensive Decoding Methods",2023-09-19 23:39:07+00:00,http://arxiv.org/abs/2309.10966v5,"Mara Finkelstein, Subhajit Naskar, Mehdi Mirzazadeh, Apurva Shah, Markus Freitag",cs.CL,table2text,"Recent research in decoding methods for Natural Language Generation (NLG)
tasks has shown that MAP decoding is not optimal, because model probabilities
do not always align with human preferences. Stronger decoding methods,
including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR)
decoding, have since been proposed to mitigate the model-perplexity-vs-quality
mismatch. While these decoding methods achieve state-of-the-art performance,
they are prohibitively expensive to compute. In this work, we propose MBR
finetuning and QE finetuning which distill the quality gains from these
decoding methods at training time, while using an efficient decoding algorithm
at inference time. Using the canonical NLG task of Neural Machine Translation
(NMT), we show that even with self-training, these finetuning methods
significantly outperform the base model. Moreover, when using an external LLM
as a teacher model, these finetuning methods outperform finetuning on
human-generated references. These findings suggest new ways to leverage
monolingual data to achieve improvements in model quality that are on par with,
or even exceed, improvements from human-curated data, while maintaining maximum
efficiency during decoding.",2023-09-19
"CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain
  Performance and Calibration",2023-09-14 16:16:40+00:00,http://arxiv.org/abs/2309.07822v1,"Rachneet Sachdeva, Martin Tutek, Iryna Gurevych",cs.CL,table2text,"In recent years, large language models (LLMs) have shown remarkable
capabilities at scale, particularly at generating text conditioned on a prompt.
In our work, we investigate the use of LLMs to augment training data of small
language models~(SLMs) with automatically generated counterfactual~(CF)
instances -- i.e. minimally altered inputs -- in order to improve
out-of-domain~(OOD) performance of SLMs in the extractive question
answering~(QA) setup. We show that, across various LLM generators, such data
augmentation consistently enhances OOD performance and improves model
calibration for both confidence-based and rationale-augmented calibrator
models. Furthermore, these performance improvements correlate with higher
diversity of CF instances in terms of their surface form and semantic content.
Finally, we show that CF augmented models which are easier to calibrate also
exhibit much lower entropy when assigning importance, indicating that
rationale-augmented calibrators prefer concise explanations.",2023-09-14
Semantic reconstruction of continuous language from MEG signals,2023-09-14 13:19:53+00:00,http://arxiv.org/abs/2309.07701v1,"Bo Wang, Xiran Xu, Longxiang Zhang, Boda Xiao, Xihong Wu, Jing Chen","cs.HC, eess.SP, q-bio.NC",table2text,"Decoding language from neural signals holds considerable theoretical and
practical importance. Previous research has indicated the feasibility of
decoding text or speech from invasive neural signals. However, when using
non-invasive neural signals, significant challenges are encountered due to
their low quality. In this study, we proposed a data-driven approach for
decoding semantic of language from Magnetoencephalography (MEG) signals
recorded while subjects were listening to continuous speech. First, a
multi-subject decoding model was trained using contrastive learning to
reconstruct continuous word embeddings from MEG data. Subsequently, a beam
search algorithm was adopted to generate text sequences based on the
reconstructed word embeddings. Given a candidate sentence in the beam, a
language model was used to predict the subsequent words. The word embeddings of
the subsequent words were correlated with the reconstructed word embedding.
These correlations were then used as a measure of the probability for the next
word. The results showed that the proposed continuous word embedding model can
effectively leverage both subject-specific and subject-shared information.
Additionally, the decoded text exhibited significant similarity to the target
text, with an average BERTScore of 0.816, a score comparable to that in the
previous fMRI study.",2023-09-14
Auto-Regressive Next-Token Predictors are Universal Learners,2023-09-13 14:15:03+00:00,http://arxiv.org/abs/2309.06979v1,Eran Malach,"cs.LG, cs.CL",table2text,"Large language models display remarkable capabilities in logical and
mathematical reasoning, allowing them to solve complex tasks. Interestingly,
these abilities emerge in networks trained on the simple task of next-token
prediction. In this work, we present a theoretical framework for studying
auto-regressive next-token predictors. We demonstrate that even simple models
such as linear next-token predictors, trained on Chain-of-Thought (CoT) data,
can approximate any function efficiently computed by a Turing machine. We
introduce a new complexity measure -- length complexity -- which measures the
number of intermediate tokens in a CoT sequence required to approximate some
target function, and analyze the interplay between length complexity and other
notions of complexity. Finally, we show experimentally that simple next-token
predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs),
display non-trivial performance on text generation and arithmetic tasks. Our
results demonstrate that the power of language models can be attributed, to a
great extent, to the auto-regressive next-token training scheme, and not
necessarily to a particular choice of architecture.",2023-09-13
Scaled Prompt-Tuning for Few-Shot Natural Language Generation,2023-09-13 07:12:31+00:00,http://arxiv.org/abs/2309.06759v1,"Ting Hu, Christoph Meinel, Haojin Yang",cs.CL,table2text,"The increasingly Large Language Models (LLMs) demonstrate stronger language
understanding and generation capabilities, while the memory demand and
computation cost of fine-tuning LLMs on downstream tasks are non-negligible.
Besides, fine-tuning generally requires a certain amount of data from
individual tasks whilst data collection cost is another issue to consider in
real-world applications. In this work, we focus on Parameter-Efficient
Fine-Tuning (PEFT) methods for few-shot Natural Language Generation (NLG),
which freeze most parameters in LLMs and tune a small subset of parameters in
few-shot cases so that memory footprint, training cost, and labeling cost are
reduced while maintaining or even improving the performance. We propose a
Scaled Prompt-Tuning (SPT) method which surpasses conventional PT with better
performance and generalization ability but without an obvious increase in
training cost. Further study on intermediate SPT suggests the superior
transferability of SPT in few-shot scenarios, providing a recipe for
data-deficient and computation-limited circumstances. Moreover, a comprehensive
comparison of existing PEFT methods reveals that certain approaches exhibiting
decent performance with modest training cost such as Prefix-Tuning in prior
study could struggle in few-shot NLG tasks, especially on challenging datasets.",2023-09-13
"Text Encoders Lack Knowledge: Leveraging Generative LLMs for
  Domain-Specific Semantic Textual Similarity",2023-09-12 19:32:45+00:00,http://arxiv.org/abs/2309.06541v1,"Joseph Gatto, Omar Sharif, Parker Seegmiller, Philip Bohlman, Sarah Masud Preum",cs.CL,table2text,"Amidst the sharp rise in the evaluation of large language models (LLMs) on
various tasks, we find that semantic textual similarity (STS) has been
under-explored. In this study, we show that STS can be cast as a text
generation problem while maintaining strong performance on multiple STS
benchmarks. Additionally, we show generative LLMs significantly outperform
existing encoder-based STS models when characterizing the semantic similarity
between two texts with complex semantic relationships dependent on world
knowledge. We validate this claim by evaluating both generative LLMs and
existing encoder-based STS models on three newly collected STS challenge sets
which require world knowledge in the domains of Health, Politics, and Sports.
All newly collected data is sourced from social media content posted after May
2023 to ensure the performance of closed-source models like ChatGPT cannot be
credited to memorization. Our results show that, on average, generative LLMs
outperform the best encoder-only baselines by an average of 22.3% on STS tasks
requiring world knowledge. Our results suggest generative language models with
STS-specific prompting strategies achieve state-of-the-art performance in
complex, domain-specific STS tasks.",2023-09-12
"Neural Latent Geometry Search: Product Manifold Inference via
  Gromov-Hausdorff-Informed Bayesian Optimization",2023-09-09 14:29:22+00:00,http://arxiv.org/abs/2309.04810v1,"Haitz Saez de Ocariz Borde, Alvaro Arroyo, Ismael Morales, Ingmar Posner, Xiaowen Dong","cs.LG, stat.ML",table2text,"Recent research indicates that the performance of machine learning models can
be improved by aligning the geometry of the latent space with the underlying
data structure. Rather than relying solely on Euclidean space, researchers have
proposed using hyperbolic and spherical spaces with constant curvature, or
combinations thereof, to better model the latent space and enhance model
performance. However, little attention has been given to the problem of
automatically identifying the optimal latent geometry for the downstream task.
We mathematically define this novel formulation and coin it as neural latent
geometry search (NLGS). More specifically, we introduce a principled method
that searches for a latent geometry composed of a product of constant curvature
model spaces with minimal query evaluations. To accomplish this, we propose a
novel notion of distance between candidate latent geometries based on the
Gromov-Hausdorff distance from metric geometry. In order to compute the
Gromov-Hausdorff distance, we introduce a mapping function that enables the
comparison of different manifolds by embedding them in a common
high-dimensional ambient space. Finally, we design a graph search space based
on the calculated distances between candidate manifolds and use Bayesian
optimization to search for the optimal latent geometry in a query-efficient
manner. This is a general method which can be applied to search for the optimal
latent geometry for a variety of models and downstream tasks. Extensive
experiments on synthetic and real-world datasets confirm the efficacy of our
method in identifying the optimal latent geometry for multiple machine learning
problems.",2023-09-09
"EPA: Easy Prompt Augmentation on Large Language Models via Multiple
  Sources and Multiple Targets",2023-09-09 09:03:50+00:00,http://arxiv.org/abs/2309.04725v1,"Hongyuan Lu, Wai Lam",cs.CL,table2text,"Large language models (LLMs) have shown promising performance on various NLP
tasks via task prompting. And their performance can be further improved by
appending task demonstrations to the head of the prompt. And usually, a better
performance can be achieved with more demonstrations. However, asking the users
to write the demonstrations can be cumbersome. As a simple yet cost-effective
workaround, this paper proposes a novel method called EPA (\textbf{E}asy
\textbf{P}rompt \textbf{A}ugmentation)\footnote{While this paper considers
augmenting prompts via demonstrations, we name it EPA as the name EDA is
already taken by a well-known NLP method \citep{wei-zou-2019-eda}.} that
effectively minimizes user efforts in writing demonstrations while improving
the model performance at the same time. EPA achieves these goals by
automatically augmenting the demonstrations with multiple sources/targets,
where each of them paraphrases each other. This is well motivated as augmenting
data via paraphrasing effectively improves neural language models. EPA thus
employs paraphrasing as an augmentation method for in-context learning.
Extensive experiments indicate that EPA effectively improves both NLU and NLG
tasks, covering from natural language inference to machine translation in
translating tens of languages.\footnote{Code and data will be released upon
publication.}",2023-09-09
ConDA: Contrastive Domain Adaptation for AI-generated Text Detection,2023-09-07 19:51:30+00:00,http://arxiv.org/abs/2309.03992v1,"Amrita Bhattacharjee, Tharindu Kumarage, Raha Moraffah, Huan Liu","cs.CL, cs.AI, cs.LG",table2text,"Large language models (LLMs) are increasingly being used for generating text
in a variety of use cases, including journalistic news articles. Given the
potential malicious nature in which these LLMs can be used to generate
disinformation at scale, it is important to build effective detectors for such
AI-generated text. Given the surge in development of new LLMs, acquiring
labeled training data for supervised detectors is a bottleneck. However, there
might be plenty of unlabeled text data available, without information on which
generator it came from. In this work we tackle this data problem, in detecting
AI-generated news text, and frame the problem as an unsupervised domain
adaptation task. Here the domains are the different text generators, i.e. LLMs,
and we assume we have access to only the labeled source data and unlabeled
target data. We develop a Contrastive Domain Adaptation framework, called
ConDA, that blends standard domain adaptation techniques with the
representation power of contrastive learning to learn domain invariant
representations that are effective for the final unsupervised detection task.
Our experiments demonstrate the effectiveness of our framework, resulting in
average performance gains of 31.7% from the best performing baselines, and
within 0.8% margin of a fully supervised detector. All our code and data is
available at https://github.com/AmritaBh/ConDA-gen-text-detection.",2023-09-07
"Parameter Efficient Audio Captioning With Faithful Guidance Using
  Audio-text Shared Latent Representation",2023-09-06 19:42:52+00:00,http://arxiv.org/abs/2309.03340v1,"Arvind Krishna Sridhar, Yinyi Guo, Erik Visser, Rehana Mahfuz","cs.CL, cs.MM, cs.SD",table2text,"There has been significant research on developing pretrained transformer
architectures for multimodal-to-text generation tasks. Albeit performance
improvements, such models are frequently overparameterized, hence suffer from
hallucination and large memory footprint making them challenging to deploy on
edge devices. In this paper, we address both these issues for the application
of automated audio captioning. First, we propose a data augmentation technique
for generating hallucinated audio captions and show that similarity based on an
audio-text shared latent space is suitable for detecting hallucination. Then,
we propose a parameter efficient inference time faithful decoding algorithm
that enables smaller audio captioning models with performance equivalent to
larger models trained with more data. During the beam decoding step, the
smaller model utilizes an audio-text shared latent representation to
semantically align the generated text with corresponding input audio. Faithful
guidance is introduced into the beam probability by incorporating the cosine
similarity between latent representation projections of greedy rolled out
intermediate beams and audio clip. We show the efficacy of our algorithm on
benchmark datasets and evaluate the proposed scheme against baselines using
conventional audio captioning and semantic similarity metrics while
illustrating tradeoffs between performance and complexity.",2023-09-06
Persona-aware Generative Model for Code-mixed Language,2023-09-06 11:20:41+00:00,http://arxiv.org/abs/2309.02915v1,"Ayan Sengupta, Md Shad Akhtar, Tanmoy Chakraborty","cs.CL, cs.LG",table2text,"Code-mixing and script-mixing are prevalent across online social networks and
multilingual societies. However, a user's preference toward code-mixing depends
on the socioeconomic status, demographics of the user, and the local context,
which existing generative models mostly ignore while generating code-mixed
texts. In this work, we make a pioneering attempt to develop a persona-aware
generative model to generate texts resembling real-life code-mixed texts of
individuals. We propose a Persona-aware Generative Model for Code-mixed
Generation, PARADOX, a novel Transformer-based encoder-decoder model that
encodes an utterance conditioned on a user's persona and generates code-mixed
texts without monolingual reference data. We propose an alignment module that
re-calibrates the generated sequence to resemble real-life code-mixed texts.
PARADOX generates code-mixed texts that are semantically more meaningful and
linguistically more valid. To evaluate the personification capabilities of
PARADOX, we propose four new metrics -- CM BLEU, CM Rouge-1, CM Rouge-L and CM
KS. On average, PARADOX achieves 1.6 points better CM BLEU, 47% better
perplexity and 32% better semantic coherence than the non-persona-based
counterparts.",2023-09-06
"Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction
  Tuning",2023-09-05 21:27:27+00:00,http://arxiv.org/abs/2309.02591v1,"Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, Armen Aghajanyan","cs.LG, cs.CL, cs.CV",table2text,"We present CM3Leon (pronounced ""Chameleon""), a retrieval-augmented,
token-based, decoder-only multi-modal language model capable of generating and
infilling both text and images. CM3Leon uses the CM3 multi-modal architecture
but additionally shows the extreme benefits of scaling up and tuning on more
diverse instruction-style data. It is the first multi-modal model trained with
a recipe adapted from text-only language models, including a large-scale
retrieval-augmented pre-training stage and a second multi-task supervised
fine-tuning (SFT) stage. It is also a general-purpose model that can do both
text-to-image and image-to-text generation, allowing us to introduce
self-contained contrastive decoding methods that produce high-quality outputs.
Extensive experiments demonstrate that this recipe is highly effective for
multi-modal models. CM3Leon achieves state-of-the-art performance in
text-to-image generation with 5x less training compute than comparable methods
(zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon can also demonstrate
unprecedented levels of controllability in tasks ranging from language-guided
image editing to image-controlled generation and segmentation.",2023-09-05
PromptTTS 2: Describing and Generating Voices with Text Prompt,2023-09-05 14:45:27+00:00,http://arxiv.org/abs/2309.02285v1,"Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao, Tao Qin, Jiang Bian","eess.AS, cs.CL, cs.LG, cs.SD",table2text,"Speech conveys more information than just text, as the same word can be
uttered in various voices to convey diverse information. Compared to
traditional text-to-speech (TTS) methods relying on speech prompts (reference
speech) for voice variability, using text prompts (descriptions) is more
user-friendly since speech prompts can be hard to find or may not exist at all.
TTS approaches based on the text prompt face two challenges: 1) the one-to-many
problem, where not all details about voice variability can be described in the
text prompt, and 2) the limited availability of text prompt datasets, where
vendors and large cost of data labeling are required to write text prompt for
speech. In this work, we introduce PromptTTS 2 to address these challenges with
a variation network to provide variability information of voice not captured by
text prompts, and a prompt generation pipeline to utilize the large language
models (LLM) to compose high quality text prompts. Specifically, the variation
network predicts the representation extracted from the reference speech (which
contains full information about voice) based on the text prompt representation.
For the prompt generation pipeline, it generates text prompts for speech with a
speech understanding model to recognize voice attributes (e.g., gender, speed)
from speech and a large language model to formulate text prompt based on the
recognition results. Experiments on a large-scale (44K hours) speech dataset
demonstrate that compared to the previous works, PromptTTS 2 generates voices
more consistent with text prompts and supports the sampling of diverse voice
variability, thereby offering users more choices on voice generation.
Additionally, the prompt generation pipeline produces high-quality prompts,
eliminating the large labeling cost. The demo page of PromptTTS 2 is available
online\footnote{https://speechresearch.github.io/prompttts2}.",2023-09-05
"Studying the impacts of pre-training using ChatGPT-generated text on
  downstream tasks",2023-09-02 12:56:15+00:00,http://arxiv.org/abs/2309.05668v1,Sarthak Anand,"cs.CL, cs.AI",table2text,"In recent times, significant advancements have been witnessed in the field of
language models, particularly with the emergence of Large Language Models
(LLMs) that are trained on vast amounts of data extracted from internet
archives. These LLMs, such as ChatGPT, have become widely accessible, allowing
users to generate text for various purposes including articles, essays, jokes,
and poetry. Given that LLMs are trained on a diverse range of text sources,
encompassing platforms like Reddit and Twitter, it is foreseeable that future
training datasets will also incorporate text generated by previous iterations
of the models themselves. In light of this development, our research aims to
investigate the influence of artificial text in the pre-training phase of
language models. Specifically, we conducted a comparative analysis between a
language model, RoBERTa, pre-trained using CNN/DailyMail news articles, and
ChatGPT, which employed the same articles for its training and evaluated their
performance on three downstream tasks as well as their potential gender bias,
using sentiment analysis as a metric. Through a series of experiments, we
demonstrate that the utilization of artificial text during pre-training does
not have a significant impact on either the performance of the models in
downstream tasks or their gender bias. In conclusion, our findings suggest that
the inclusion of text generated by LLMs in their own pre-training process does
not yield substantial effects on the subsequent performance of the models in
downstream tasks or their potential gender bias.",2023-09-02
"BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment
  of Continuation Writing",2023-09-02 11:46:05+00:00,http://arxiv.org/abs/2309.00916v1,"Chen Wang, Minpeng Liao, Zhongqiang Huang, Jinliang Lu, Junhong Wu, Yuchen Liu, Chengqing Zong, Jiajun Zhang","cs.CL, cs.SD, eess.AS",table2text,"The emergence of large language models (LLMs) has sparked significant
interest in extending their remarkable language capabilities to speech.
However, modality alignment between speech and text still remains an open
problem. Current solutions can be categorized into two strategies. One is a
cascaded approach where outputs (tokens or states) of a separately trained
speech recognition system are used as inputs for LLMs, which limits their
potential in modeling alignment between speech and text. The other is an
end-to-end approach that relies on speech instruction data, which is very
difficult to collect in large quantities. In this paper, we address these
issues and propose the BLSP approach that Bootstraps Language-Speech
Pre-training via behavior alignment of continuation writing. We achieve this by
learning a lightweight modality adapter between a frozen speech encoder and an
LLM, ensuring that the LLM exhibits the same generation behavior regardless of
the modality of input: a speech segment or its transcript. The training process
can be divided into two steps. The first step prompts an LLM to generate texts
with speech transcripts as prefixes, obtaining text continuations. In the
second step, these continuations are used as supervised signals to train the
modality adapter in an end-to-end manner. We demonstrate that this
straightforward process can extend the capabilities of LLMs to speech, enabling
speech recognition, speech translation, spoken language understanding, and
speech conversation, even in zero-shot cross-lingual scenarios.",2023-09-02
Bias and Fairness in Large Language Models: A Survey,2023-09-02 00:32:55+00:00,http://arxiv.org/abs/2309.00770v1,"Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed","cs.CL, cs.AI, cs.CY, cs.LG",table2text,"Rapid advancements of large language models (LLMs) have enabled the
processing, understanding, and generation of human-like text, with increasing
integration into systems that touch our social sphere. Despite this success,
these models can learn, perpetuate, and amplify harmful social biases. In this
paper, we present a comprehensive survey of bias evaluation and mitigation
techniques for LLMs. We first consolidate, formalize, and expand notions of
social bias and fairness in natural language processing, defining distinct
facets of harm and introducing several desiderata to operationalize fairness
for LLMs. We then unify the literature by proposing three intuitive taxonomies,
two for bias evaluation, namely metrics and datasets, and one for mitigation.
Our first taxonomy of metrics for bias evaluation disambiguates the
relationship between metrics and evaluation datasets, and organizes metrics by
the different levels at which they operate in a model: embeddings,
probabilities, and generated text. Our second taxonomy of datasets for bias
evaluation categorizes datasets by their structure as counterfactual inputs or
prompts, and identifies the targeted harms and social groups; we also release a
consolidation of publicly-available datasets for improved access. Our third
taxonomy of techniques for bias mitigation classifies methods by their
intervention during pre-processing, in-training, intra-processing, and
post-processing, with granular subcategories that elucidate research trends.
Finally, we identify open problems and challenges for future work. Synthesizing
a wide range of recent research, we aim to provide a clear guide of the
existing literature that empowers researchers and practitioners to better
understand and prevent the propagation of bias in LLMs.",2023-09-02
Reinforcement Learning for Generative AI: A Survey,2023-08-28 06:15:14+00:00,http://arxiv.org/abs/2308.14328v2,"Yuanjiang Cao, Quan Z. Sheng, Julian McAuley, Lina Yao","cs.LG, cs.AI",table2text,"Deep Generative AI has been a long-standing essential topic in the machine
learning community, which can impact a number of application areas like text
generation and computer vision. The major paradigm to train a generative model
is maximum likelihood estimation, which pushes the learner to capture and
approximate the target data distribution by decreasing the divergence between
the model distribution and the target distribution. This formulation
successfully establishes the objective of generative tasks, while it is
incapable of satisfying all the requirements that a user might expect from a
generative model. Reinforcement learning, serving as a competitive option to
inject new training signals by creating new objectives that exploit novel
signals, has demonstrated its power and flexibility to incorporate human
inductive bias from multiple angles, such as adversarial learning,
hand-designed rules and learned reward model to build a performant model.
Thereby, reinforcement learning has become a trending research field and has
stretched the limits of generative AI in both model design and application. It
is reasonable to summarize and conclude advances in recent years with a
comprehensive review. Although there are surveys in different application areas
recently, this survey aims to shed light on a high-level review that spans a
range of application areas. We provide a rigorous taxonomy in this area and
make sufficient coverage on various models and applications. Notably, we also
surveyed the fast-developing large language model area. We conclude this survey
by showing the potential directions that might tackle the limit of current
models and expand the frontiers for generative AI.",2023-08-28
"MedAlign: A Clinician-Generated Dataset for Instruction Following with
  Electronic Medical Records",2023-08-27 12:24:39+00:00,http://arxiv.org/abs/2308.14089v1,"Scott L. Fleming, Alejandro Lozano, William J. Haberkorn, Jenelle A. Jindal, Eduardo P. Reis, Rahul Thapa, Louis Blankemeier, Julian Z. Genkins, Ethan Steinberg, Ashwin Nayak, Birju S. Patel, Chia-Chun Chiang, Alison Callahan, Zepeng Huo, Sergios Gatidis, Scott J. Adams, Oluseyi Fayanju, Shreya J. Shah, Thomas Savage, Ethan Goh, Akshay S. Chaudhari, Nima Aghaeepour, Christopher Sharp, Michael A. Pfeffer, Percy Liang, Jonathan H. Chen, Keith E. Morse, Emma P. Brunskill, Jason A. Fries, Nigam H. Shah","cs.CL, cs.AI, cs.LG",table2text,"The ability of large language models (LLMs) to follow natural language
instructions with human-level fluency suggests many opportunities in healthcare
to reduce administrative burden and improve quality of care. However,
evaluating LLMs on realistic text generation tasks for healthcare remains
challenging. Existing question answering datasets for electronic health record
(EHR) data fail to capture the complexity of information needs and
documentation burdens experienced by clinicians. To address these challenges,
we introduce MedAlign, a benchmark dataset of 983 natural language instructions
for EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes
clinician-written reference responses for 303 instructions, and provides 276
longitudinal EHRs for grounding instruction-response pairs. We used MedAlign to
evaluate 6 general domain LLMs, having clinicians rank the accuracy and quality
of each LLM response. We found high error rates, ranging from 35% (GPT-4) to
68% (MPT-7B-Instruct), and an 8.3% drop in accuracy moving from 32k to 2k
context lengths for GPT-4. Finally, we report correlations between clinician
rankings and automated natural language generation metrics as a way to rank
LLMs without human review. We make MedAlign available under a research data use
agreement to enable LLM evaluations on tasks aligned with clinician needs and
preferences.",2023-08-27
"Planning with Logical Graph-based Language Model for Instruction
  Generation",2023-08-26 06:28:14+00:00,http://arxiv.org/abs/2308.13782v1,"Fan Zhang, Kebing Jin, Hankz Hankui Zhuo","cs.CL, cs.AI",table2text,"Despite the superior performance of large language models to generate natural
language texts, it is hard to generate texts with correct logic according to a
given task, due to the difficulties for neural models to capture implied rules
from free-form texts. In this paper, we propose a novel graph-based language
model, Logical-GLM, to infuse logic into language models for more valid text
generation and interpretability. Specifically, we first capture information
from natural language instructions and construct logical bayes graphs that
generally describe domains. Next, we generate logical skeletons to guide
language model training, infusing domain knowledge into language models.
Finally, we alternately optimize the searching policy of graphs and language
models until convergence. The experimental results show that Logical-GLM is
both effective and efficient compared with traditional language models, despite
using smaller-scale training data and fewer parameters. Our approach can
generate instructional texts with more correct logic owing to the internalized
domain knowledge. Moreover, the usage of logical graphs reflects the inner
mechanism of the language models, which improves the interpretability of
black-box models.",2023-08-26
1.5 million materials narratives generated by chatbots,2023-08-25 22:00:53+00:00,http://arxiv.org/abs/2308.13687v1,"Yang Jeong Park, Sung Eun Jerng, Jin-Sung Park, Choah Kwon, Chia-Wei Hsu, Zhichu Ren, Sungroh Yoon, Ju Li","cond-mat.mtrl-sci, cs.CL",table2text,"The advent of artificial intelligence (AI) has enabled a comprehensive
exploration of materials for various applications. However, AI models often
prioritize frequently encountered materials in the scientific literature,
limiting the selection of suitable candidates based on inherent physical and
chemical properties. To address this imbalance, we have generated a dataset of
1,494,017 natural language-material paragraphs based on combined OQMD,
Materials Project, JARVIS, COD and AFLOW2 databases, which are dominated by ab
initio calculations and tend to be much more evenly distributed on the periodic
table. The generated text narratives were then polled and scored by both human
experts and ChatGPT-4, based on three rubrics: technical accuracy, language and
structure, and relevance and depth of content, showing similar scores but with
human-scored depth of content being the most lagging. The merger of
multi-modality data sources and large language model (LLM) holds immense
potential for AI frameworks to help the exploration and discovery of
solid-state materials for specific applications.",2023-08-25
"ChatGPT as Data Augmentation for Compositional Generalization: A Case
  Study in Open Intent Detection",2023-08-25 17:51:23+00:00,http://arxiv.org/abs/2308.13517v1,"Yihao Fang, Xianzhi Li, Stephen W. Thomas, Xiaodan Zhu","cs.CL, cs.AI",table2text,"Open intent detection, a crucial aspect of natural language understanding,
involves the identification of previously unseen intents in user-generated
text. Despite the progress made in this field, challenges persist in handling
new combinations of language components, which is essential for compositional
generalization. In this paper, we present a case study exploring the use of
ChatGPT as a data augmentation technique to enhance compositional
generalization in open intent detection tasks. We begin by discussing the
limitations of existing benchmarks in evaluating this problem, highlighting the
need for constructing datasets for addressing compositional generalization in
open intent detection tasks. By incorporating synthetic data generated by
ChatGPT into the training process, we demonstrate that our approach can
effectively improve model performance. Rigorous evaluation of multiple
benchmarks reveals that our method outperforms existing techniques and
significantly enhances open intent detection capabilities. Our findings
underscore the potential of large language models like ChatGPT for data
augmentation in natural language understanding tasks.",2023-08-25
"GeoExplainer: A Visual Analytics Framework for Spatial Modeling
  Contextualization and Report Generation",2023-08-25 16:55:33+00:00,http://arxiv.org/abs/2308.13588v1,"Fan Lei, Yuxin Ma, Stewart Fotheringham, Elizabeth Mack, Ziqi Li, Mehak Sachdeva, Sarah Bardin, Ross Maciejewski","cs.HC, cs.LG",table2text,"Geographic regression models of various descriptions are often applied to
identify patterns and anomalies in the determinants of spatially distributed
observations. These types of analyses focus on answering why questions about
underlying spatial phenomena, e.g., why is crime higher in this locale, why do
children in one school district outperform those in another, etc.? Answers to
these questions require explanations of the model structure, the choice of
parameters, and contextualization of the findings with respect to their
geographic context. This is particularly true for local forms of regression
models which are focused on the role of locational context in determining human
behavior. In this paper, we present GeoExplainer, a visual analytics framework
designed to support analysts in creating explanative documentation that
summarizes and contextualizes their spatial analyses. As analysts create their
spatial models, our framework flags potential issues with model parameter
selections, utilizes template-based text generation to summarize model outputs,
and links with external knowledge repositories to provide annotations that help
to explain the model results. As analysts explore the model results, all
visualizations and annotations can be captured in an interactive report
generation widget. We demonstrate our framework using a case study modeling the
determinants of voting in the 2016 US Presidential Election.",2023-08-25
Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection,2023-08-22 01:55:03+00:00,http://arxiv.org/abs/2308.11119v1,Masato Tamura,"cs.CV, cs.LG",table2text,"This paper presents a novel method that leverages a visual-language model,
CLIP, as a data source for zero-shot anomaly detection. Tremendous efforts have
been put towards developing anomaly detectors due to their potential industrial
applications. Considering the difficulty in acquiring various anomalous samples
for training, most existing methods train models with only normal samples and
measure discrepancies from the distribution of normal samples during inference,
which requires training a model for each object category. The problem of this
inefficient training requirement has been tackled by designing a CLIP-based
anomaly detector that applies prompt-guided classification to each part of an
image in a sliding window manner. However, the method still suffers from the
labor of careful prompt ensembling with known object categories. To overcome
the issues above, we propose leveraging CLIP as a data source for training. Our
method generates text embeddings with the text encoder in CLIP with typical
prompts that include words of normal and anomaly. In addition to these words,
we insert several randomly generated words into prompts, which enables the
encoder to generate a diverse set of normal and anomalous samples. Using the
generated embeddings as training data, a feed-forward neural network learns to
extract features of normal and anomaly from CLIP's embeddings, and as a result,
a category-agnostic anomaly detector can be obtained without any training
images. Experimental results demonstrate that our method achieves
state-of-the-art performance without laborious prompt ensembling in zero-shot
setups.",2023-08-22
"Data-to-text Generation for Severely Under-Resourced Languages with
  GPT-3.5: A Bit of Help Needed from Google Translate",2023-08-19 09:19:34+00:00,http://arxiv.org/abs/2308.09957v1,"Michela Lorandi, Anya Belz","cs.CL, cs.AI",table2text,"LLMs like GPT are great at tasks involving English which dominates in their
training data. In this paper, we look at how they cope with tasks involving
languages that are severely under-represented in their training data, in the
context of data-to-text generation for Irish, Maltese, Welsh and Breton. During
the prompt-engineering phase we tested a range of prompt types and formats on
GPT-3.5 and~4 with a small sample of example input/output pairs. We then fully
evaluated the two most promising prompts in two scenarios: (i) direct
generation into the under-resourced language, and (ii) generation into English
followed by translation into the under-resourced language. We find that
few-shot prompting works better for direct generation into under-resourced
languages, but that the difference disappears when pivoting via English. The
few-shot + translation system variants were submitted to the WebNLG 2023 shared
task where they outperformed competitor systems by substantial margins in all
languages on all metrics. We conclude that good performance on under-resourced
languages can be achieved out-of-the box with state-of-the-art LLMs. However,
our best results (for Welsh) remain well below the lowest ranked English system
at WebNLG'20.",2023-08-19
Mirror Diffusion Models,2023-08-11 18:31:54+00:00,http://arxiv.org/abs/2308.06342v2,Jaesung Tae,cs.LG,table2text,"Diffusion models have successfully been applied to generative tasks in
various continuous domains. However, applying diffusion to discrete categorical
data remains a non-trivial task. Moreover, generation in continuous domains
often requires clipping in practice, which motivates the need for a theoretical
framework for adapting diffusion to constrained domains. Inspired by the mirror
Langevin algorithm for the constrained sampling problem, in this theoretical
report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the
context of simplex diffusion and propose natural extensions to popular domains
such as image and text generation.",2023-08-11
"Few-Shot Data-to-Text Generation via Unified Representation and
  Multi-Source Learning",2023-08-10 03:09:12+00:00,http://arxiv.org/abs/2308.05317v1,"Alexander Hanbo Li, Mingyue Shang, Evangelia Spiliopoulou, Jie Ma, Patrick Ng, Zhiguo Wang, Bonan Min, William Wang, Kathleen McKeown, Vittorio Castelli, Dan Roth, Bing Xiang",cs.CL,table2text,"We present a novel approach for structured data-to-text generation that
addresses the limitations of existing methods that primarily focus on specific
types of structured data. Our proposed method aims to improve performance in
multi-task training, zero-shot and few-shot scenarios by providing a unified
representation that can handle various forms of structured data such as tables,
knowledge graph triples, and meaning representations. We demonstrate that our
proposed approach can effectively adapt to new structured forms, and can
improve performance in comparison to current methods. For example, our method
resulted in a 66% improvement in zero-shot BLEU scores when transferring models
trained on table inputs to a knowledge graph dataset. Our proposed method is an
important step towards a more general data-to-text generation framework.",2023-08-10
"Emotion-Conditioned Text Generation through Automatic Prompt
  Optimization",2023-08-09 10:42:38+00:00,http://arxiv.org/abs/2308.04857v1,"Yarik Menchaca Resendiz, Roman Klinger",cs.CL,table2text,"Conditional natural language generation methods often require either
expensive fine-tuning or training a large language model from scratch. Both are
unlikely to lead to good results without a substantial amount of data and
computational resources. Prompt learning without changing the parameters of a
large language model presents a promising alternative. It is a cost-effective
approach, while still achieving competitive results. While this procedure is
now established for zero- and few-shot text classification and structured
prediction, it has received limited attention in conditional text generation.
We present the first automatic prompt optimization approach for
emotion-conditioned text generation with instruction-fine-tuned models. Our
method uses an iterative optimization procedure that changes the prompt by
adding, removing, or replacing tokens. As objective function, we only require a
text classifier that measures the realization of the conditional variable in
the generated text. We evaluate the method on emotion-conditioned text
generation with a focus on event reports and compare it to manually designed
prompts that also act as the seed for the optimization procedure. The optimized
prompts achieve 0.75 macro-average F1 to fulfill the emotion condition in
contrast to manually designed seed prompts with only 0.22 macro-average F1.",2023-08-09
"DataTales: Investigating the use of Large Language Models for Authoring
  Data-Driven Articles",2023-08-08 06:21:58+00:00,http://arxiv.org/abs/2308.04076v1,"Nicole Sultanum, Arjun Srinivasan","cs.HC, cs.CL",table2text,"Authoring data-driven articles is a complex process requiring authors to not
only analyze data for insights but also craft a cohesive narrative that
effectively communicates the insights. Text generation capabilities of
contemporary large language models (LLMs) present an opportunity to assist the
authoring of data-driven articles and expedite the writing process. In this
work, we investigate the feasibility and perceived value of leveraging LLMs to
support authors of data-driven articles. We designed a prototype system,
DataTales, that leverages a LLM to generate textual narratives accompanying a
given chart. Using DataTales as a design probe, we conducted a qualitative
study with 11 professionals to evaluate the concept, from which we distilled
affordances and opportunities to further integrate LLMs as valuable data-driven
article authoring assistants.",2023-08-08
Generative Forests,2023-08-07 14:58:53+00:00,http://arxiv.org/abs/2308.03648v1,"Richard Nock, Mathieu Guillame-Bert","cs.LG, I.2.6",table2text,"Tabular data represents one of the most prevalent form of data. When it comes
to data generation, many approaches would learn a density for the data
generation process, but would not necessarily end up with a sampler, even less
so being exact with respect to the underlying density. A second issue is on
models: while complex modeling based on neural nets thrives in image or text
generation (etc.), less is known for powerful generative models on tabular
data. A third problem is the visible chasm on tabular data between training
algorithms for supervised learning with remarkable properties (e.g. boosting),
and a comparative lack of guarantees when it comes to data generation. In this
paper, we tackle the three problems, introducing new tree-based generative
models convenient for density modeling and tabular data generation that improve
on modeling capabilities of recent proposals, and a training algorithm which
simplifies the training setting of previous approaches and displays
boosting-compliant convergence. This algorithm has the convenient property to
rely on a supervised training scheme that can be implemented by a few tweaks to
the most popular induction scheme for decision tree induction with two classes.
Experiments are provided on missing data imputation and comparing generated
data to real data, displaying the quality of the results obtained by our
approach, in particular against state of the art.",2023-08-07
"Boosting Chinese ASR Error Correction with Dynamic Error Scaling
  Mechanism",2023-08-07 09:19:59+00:00,http://arxiv.org/abs/2308.03423v1,"Jiaxin Fan, Yong Zhang, Hanzhang Li, Jianzong Wang, Zhitao Li, Sheng Ouyang, Ning Cheng, Jing Xiao","cs.CL, cs.AI",table2text,"Chinese Automatic Speech Recognition (ASR) error correction presents
significant challenges due to the Chinese language's unique features, including
a large character set and borderless, morpheme-based structure. Current
mainstream models often struggle with effectively utilizing word-level features
and phonetic information. This paper introduces a novel approach that
incorporates a dynamic error scaling mechanism to detect and correct
phonetically erroneous text generated by ASR output. This mechanism operates by
dynamically fusing word-level features and phonetic information, thereby
enriching the model with additional semantic data. Furthermore, our method
implements unique error reduction and amplification strategies to address the
issues of matching wrong words caused by incorrect characters. Experimental
results indicate substantial improvements in ASR error correction,
demonstrating the effectiveness of our proposed method and yielding promising
results on established datasets.",2023-08-07
"Towards Multiple References Era -- Addressing Data Leakage and Limited
  Reference Diversity in NLG Evaluation",2023-08-06 14:49:26+00:00,http://arxiv.org/abs/2308.03131v4,"Xianfeng Zeng, Yijin Liu, Fandong Meng, Jie Zhou",cs.CL,table2text,"N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely
utilized across a range of natural language generation (NLG) tasks. However,
recent studies have revealed a weak correlation between these matching-based
metrics and human evaluations, especially when compared with neural-based
metrics like BLEURT. In this paper, we conjecture that the performance
bottleneck in matching-based metrics may be caused by the limited diversity of
references. To address this issue, we propose to utilize \textit{multiple
references} to enhance the consistency between these metrics and human
evaluations. Within the WMT Metrics benchmarks, we observe that the
multi-references F200spBLEU surpasses the conventional single-reference one by
an accuracy improvement of 7.2\%. Remarkably, it also exceeds the neural-based
BERTscore by an accuracy enhancement of 3.9\%. Moreover, we observe that the
data leakage issue in large language models (LLMs) can be mitigated to a large
extent by our multi-reference metric. We release the code and data at
\url{https://github.com/SefaZeng/LLM-Ref}",2023-08-06
Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?,2023-08-02 17:11:37+00:00,http://arxiv.org/abs/2308.01284v1,"Amrita Bhattacharjee, Huan Liu","cs.CL, cs.AI",table2text,"Large language models (LLMs) such as ChatGPT are increasingly being used for
various use cases, including text content generation at scale. Although
detection methods for such AI-generated text exist already, we investigate
ChatGPT's performance as a detector on such AI-generated text, inspired by
works that use ChatGPT as a data labeler or annotator. We evaluate the
zero-shot performance of ChatGPT in the task of human-written vs. AI-generated
text detection, and perform experiments on publicly available datasets. We
empirically investigate if ChatGPT is symmetrically effective in detecting
AI-generated or human-written text. Our findings provide insight on how ChatGPT
and similar LLMs may be leveraged in automated detection pipelines by simply
focusing on solving a specific aspect of the problem and deriving the rest from
that solution. All code and data is available at
\url{https://github.com/AmritaBh/ChatGPT-as-Detector}.",2023-08-02
Feature-aware conditional GAN for category text generation,2023-08-02 04:43:54+00:00,http://arxiv.org/abs/2308.00939v1,"Xinze Li, Kezhi Mao, Fanfan Lin, Zijian Feng","cs.CL, cs.AI",table2text,"Category text generation receives considerable attentions since it is
beneficial for various natural language processing tasks. Recently, the
generative adversarial network (GAN) has attained promising performance in text
generation, attributed to its adversarial training process. However, there are
several issues in text GANs, including discreteness, training instability, mode
collapse, lack of diversity and controllability etc. To address these issues,
this paper proposes a novel GAN framework, the feature-aware conditional GAN
(FA-GAN), for controllable category text generation. In FA-GAN, the generator
has a sequence-to-sequence structure for improving sentence diversity, which
consists of three encoders including a special feature-aware encoder and a
category-aware encoder, and one relational-memory-core-based decoder with the
Gumbel SoftMax activation function. The discriminator has an additional
category classification head. To generate sentences with specified categories,
the multi-class classification loss is supplemented in the adversarial
training. Comprehensive experiments have been conducted, and the results show
that FA-GAN consistently outperforms 10 state-of-the-art text generation
approaches on 6 text classification datasets. The case study demonstrates that
the synthetic sentences generated by FA-GAN can match the required categories
and are aware of the features of conditioned sentences, with good readability,
fluency, and text authenticity.",2023-08-02
"CoSMo: A constructor specification language for Abstract Wikipedia's
  content selection process",2023-08-01 13:57:23+00:00,http://arxiv.org/abs/2308.02539v1,"Kutz Arrieta, Pablo R. Fillottrani, C. Maria Keet","cs.CL, I.2.4; H.2.3",table2text,"Representing snippets of information abstractly is a task that needs to be
performed for various purposes, such as database view specification and the
first stage in the natural language generation pipeline for generative AI from
structured input, i.e., the content selection stage to determine what needs to
be verbalised. For the Abstract Wikipedia project, requirements analysis
revealed that such an abstract representation requires multilingual modelling,
content selection covering declarative content and functions, and both classes
and instances. There is no modelling language that meets either of the three
features, let alone a combination. Following a rigorous language design process
inclusive of broad stakeholder consultation, we created CoSMo, a novel {\sc
Co}ntent {\sc S}election {\sc Mo}deling language that meets these and other
requirements so that it may be useful both in Abstract Wikipedia as well as
other contexts. We describe the design process, rationale and choices, the
specification, and preliminary evaluation of the language.",2023-08-01
Tackling Hallucinations in Neural Chart Summarization,2023-08-01 09:26:40+00:00,http://arxiv.org/abs/2308.00399v1,"Saad Obaid ul Islam, Iza Škrjanec, Ondřej Dušek, Vera Demberg","cs.CL, cs.LG",table2text,"Hallucinations in text generation occur when the system produces text that is
not grounded in the input. In this work, we tackle the problem of
hallucinations in neural chart summarization. Our analysis shows that the
target side of chart summarization training datasets often contains additional
information, leading to hallucinations. We propose a natural language inference
(NLI) based method to preprocess the training data and show through human
evaluation that our method significantly reduces hallucinations. We also found
that shortening long-distance dependencies in the input sequence and adding
chart-related information like title and legends improves the overall
performance.",2023-08-01
"Learning Multi-modal Representations by Watching Hundreds of Surgical
  Video Lectures",2023-07-27 22:38:12+00:00,http://arxiv.org/abs/2307.15220v1,"Kun Yuan, Vinkle Srivastav, Tong Yu, Joel Lavanchy, Pietro Mascagni, Nassir Navab, Nicolas Padoy","cs.CV, cs.AI",table2text,"Recent advancements in surgical computer vision applications have been driven
by fully-supervised methods, primarily using only visual data. These methods
rely on manually annotated surgical videos to predict a fixed set of object
categories, limiting their generalizability to unseen surgical procedures and
downstream tasks. In this work, we put forward the idea that the surgical video
lectures available through open surgical e-learning platforms can provide
effective supervisory signals for multi-modal representation learning without
relying on manual annotations. We address the surgery-specific linguistic
challenges present in surgical video lectures by employing multiple
complementary automatic speech recognition systems to generate text
transcriptions. We then present a novel method, SurgVLP - Surgical Vision
Language Pre-training, for multi-modal representation learning. SurgVLP
constructs a new contrastive learning objective to align video clip embeddings
with the corresponding multiple text embeddings by bringing them together
within a joint latent space. To effectively show the representation capability
of the learned joint latent space, we introduce several vision-and-language
tasks for surgery, such as text-based video retrieval, temporal activity
grounding, and video captioning, as benchmarks for evaluation. We further
demonstrate that without using any labeled ground truth, our approach can be
employed for traditional vision-only surgical downstream tasks, such as
surgical tool, phase, and triplet recognition. The code will be made available
at https://github.com/CAMMA-public/SurgVLP",2023-07-27
Evaluating Generative Models for Graph-to-Text Generation,2023-07-27 09:03:05+00:00,http://arxiv.org/abs/2307.14712v1,"Shuzhou Yuan, Michael Färber","cs.CL, cs.AI",table2text,"Large language models (LLMs) have been widely employed for graph-to-text
generation tasks. However, the process of finetuning LLMs requires significant
training resources and annotation work. In this paper, we explore the
capability of generative models to generate descriptive text from graph data in
a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two
graph-to-text datasets and compare their performance with that of finetuned LLM
models such as T5 and BART. Our results demonstrate that generative models are
capable of generating fluent and coherent text, achieving BLEU scores of 10.57
and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error
analysis reveals that generative models still struggle with understanding the
semantic relations between entities, and they also tend to generate text with
hallucinations or irrelevant information. As a part of error analysis, we
utilize BERT to detect machine-generated text and achieve high macro-F1 scores.
We have made the text generated by generative models publicly available.",2023-07-27
Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts,2023-07-21 15:49:59+00:00,http://arxiv.org/abs/2307.11661v1,"Mayug Maniparambil, Chris Vorster, Derek Molloy, Noel Murphy, Kevin McGuinness, Noel E. O'Connor","cs.CV, cs.AI, cs.CL, cs.LG",table2text,"Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have
revolutionized visual representation learning by providing good performance on
downstream datasets. VLMs are 0-shot adapted to a downstream dataset by
designing prompts that are relevant to the dataset. Such prompt engineering
makes use of domain expertise and a validation dataset. Meanwhile, recent
developments in generative pretrained models like GPT-4 mean they can be used
as advanced internet search tools. They can also be manipulated to provide
visual information in any structure. In this work, we show that GPT-4 can be
used to generate text that is visually descriptive and how this can be used to
adapt CLIP to downstream tasks. We show considerable improvements in 0-shot
transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD
(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.
We also design a simple few-shot adapter that learns to choose the best
possible sentences to construct generalizable classifiers that outperform the
recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized
fine-grained datasets. We will release the code, prompts, and auxiliary text
dataset upon acceptance.",2023-07-21
OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?,2023-07-21 14:58:44+00:00,http://arxiv.org/abs/2307.11636v1,"Runjia Li, Shuyang Sun, Mohamed Elhoseiny, Philip Torr","cs.CV, cs.CL",table2text,"This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scale
dataset for humour generation and understanding. Humour is an abstract,
subjective, and context-dependent cognitive construct involving several
cognitive factors, making it a challenging task to generate and interpret.
Hence, humour generation and understanding can serve as a new task for
evaluating the ability of deep-learning methods to process abstract and
subjective information. Due to the scarcity of data, humour-related generation
tasks such as captioning remain under-explored. To address this gap,
OxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores to
train a generalizable humour captioning model. Contrary to existing captioning
datasets, OxfordTVG-HIC features a wide range of emotional and semantic
diversity resulting in out-of-context examples that are particularly conducive
to generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive
content. We also show how OxfordTVG-HIC can be leveraged for evaluating the
humour of a generated text. Through explainability analysis of the trained
models, we identify the visual and linguistic cues influential for evoking
humour prediction (and generation). We observe qualitatively that these cues
are aligned with the benign violation theory of humour in cognitive psychology.",2023-07-21
"Jina Embeddings: A Novel Set of High-Performance Sentence Embedding
  Models",2023-07-20 20:37:24+00:00,http://arxiv.org/abs/2307.11224v1,"Michael Günther, Louis Milliken, Jonathan Geuter, Georgios Mastrapas, Bo Wang, Han Xiao","cs.CL, cs.AI, cs.IR, cs.LG, 68T50, H.3.1; H.3.3; I.2.7; I.5.4",table2text,"Jina Embeddings constitutes a set of high-performance sentence embedding
models adept at translating various textual inputs into numerical
representations, thereby capturing the semantic essence of the text. While
these models are not exclusively designed for text generation, they excel in
applications such as dense retrieval and semantic textual similarity. This
paper details the development of Jina Embeddings, starting with the creation of
a high-quality pairwise and triplet dataset. It underlines the crucial role of
data cleaning in dataset preparation, gives in-depth insights into the model
training process, and concludes with a comprehensive performance evaluation
using the Massive Textual Embedding Benchmark (MTEB).",2023-07-20
"Visual Flow-based Programming Plugin for Brain Computer Interface in
  Computer-Aided Design",2023-07-20 16:50:39+00:00,http://arxiv.org/abs/2307.11023v1,"Tong Bill Xu, Saleh Kalantari","cs.HC, cs.SE",table2text,"Over the last half century, the main application of Brain Computer
Interfaces, BCIs has been controlling wheelchairs and neural prostheses or
generating text or commands for people with restricted mobility. There has been
very limited attention in the field to applications for computer aided design,
despite the potential of BCIs to provide a new form of environmental
interaction. In this paper we introduce the development and application of
Neuron, a novel BCI tool that enables designers with little experience in
neuroscience or computer programming to gain access to neurological data, along
with established metrics relevant to design, create BCI interaction prototypes,
both with digital onscreen objects and physical devices, and evaluate designs
based on neurological information and record measurements for further analysis.
After discussing the BCI tool development, the article presents its
capabilities through two case studies, along with a brief evaluation of the
tool performance and a discussion of implications, limitations, and future
improvement.",2023-07-20
Generative Language Models on Nucleotide Sequences of Human Genes,2023-07-20 06:59:02+00:00,http://arxiv.org/abs/2307.10634v1,"Musa Nuri Ihtiyar, Arzucan Ozgur","q-bio.GN, cs.CL, cs.LG",table2text,"Language models, primarily transformer-based ones, obtained colossal success
in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3
for NLG are very crucial. DNA sequences are very close to natural language in
terms of structure, so if the DNA-related bioinformatics domain is concerned,
discriminative models, like DNABert, exist. Yet, the generative side of the
coin is mainly unexplored to the best of our knowledge. Consequently, we
focused on developing an autoregressive generative language model like GPT-3
for DNA sequences. Because working with whole DNA sequences is challenging
without substantial computational resources, we decided to carry out our study
on a smaller scale, focusing on nucleotide sequences of human genes, unique
parts in DNA with specific functionalities, instead of the whole DNA. This
decision did not change the problem structure a lot due to the fact that both
DNA and genes can be seen as 1D sequences consisting of four different
nucleotides without losing much information and making too much simplification.
First of all, we systematically examined an almost entirely unexplored problem
and observed that RNNs performed the best while simple techniques like N-grams
were also promising. Another beneficial point was learning how to work with
generative models on languages we do not understand, unlike natural language.
How essential using real-life tasks beyond the classical metrics such as
perplexity is observed. Furthermore, checking whether the data-hungry nature of
these models can be changed through selecting a language with minimal
vocabulary size, four owing to four different types of nucleotides, is
examined. The reason for reviewing this was that choosing such a language might
make the problem easier. However, what we observed in this study was it did not
provide that much of a change in the amount of data needed.",2023-07-20
"FinGPT: Democratizing Internet-scale Data for Financial Large Language
  Models",2023-07-19 22:43:57+00:00,http://arxiv.org/abs/2307.10485v1,"Xiao-Yang Liu, Guoxuan Wang, Daochen Zha","cs.CL, cs.LG, q-fin.GN",table2text,"Large language models (LLMs) have demonstrated remarkable proficiency in
understanding and generating human-like texts, which may potentially
revolutionize the finance industry. However, existing LLMs often fall short in
the financial field, which is mainly attributed to the disparities between
general text data and financial text data. Unfortunately, there is only a
limited number of financial text datasets available (quite small size), and
BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the
training logs were released). In light of this, we aim to democratize
Internet-scale financial data for LLMs, which is an open challenge due to
diverse data sources, low signal-to-noise ratio, and high time-validity. To
address the challenges, we introduce an open-sourced and data-centric
framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that
automates the collection and curation of real-time financial data from >34
diverse sources on the Internet, providing researchers and practitioners with
accessible and transparent resources to develop their FinLLMs. Additionally, we
propose a simple yet effective strategy for fine-tuning FinLLM using the
inherent feedback from the market, dubbed Reinforcement Learning with Stock
Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that
enables users to customize their own FinLLMs from open-source general-purpose
LLMs at a low cost. Finally, we showcase several FinGPT applications, including
robo-advisor, sentiment analysis for algorithmic trading, and low-code
development. FinGPT aims to democratize FinLLMs, stimulate innovation, and
unlock new opportunities in open finance. The codes are available at
https://github.com/AI4Finance-Foundation/FinGPT and
https://github.com/AI4Finance-Foundation/FinNLP",2023-07-19
"Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and
  Addressing Sociological Implications",2023-07-18 11:38:45+00:00,http://arxiv.org/abs/2307.09162v1,Vishesh Thakur,cs.CL,table2text,"Gender bias in artificial intelligence (AI) and natural language processing
has garnered significant attention due to its potential impact on societal
perceptions and biases. This research paper aims to analyze gender bias in
Large Language Models (LLMs) with a focus on multiple comparisons between GPT-2
and GPT-3.5, some prominent language models, to better understand its
implications. Through a comprehensive literature review, the study examines
existing research on gender bias in AI language models and identifies gaps in
the current knowledge. The methodology involves collecting and preprocessing
data from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis
techniques to evaluate gender bias in the generated text. The findings shed
light on gendered word associations, language usage, and biased narratives
present in the outputs of these Large Language Models. The discussion explores
the ethical implications of gender bias and its potential consequences on
social perceptions and marginalized communities. Additionally, the paper
presents strategies for reducing gender bias in LLMs, including algorithmic
approaches and data augmentation techniques. The research highlights the
importance of interdisciplinary collaborations and the role of sociological
studies in mitigating gender bias in AI models. By addressing these issues, we
can pave the way for more inclusive and unbiased AI systems that have a
positive impact on society.",2023-07-18
COLLIE: Systematic Construction of Constrained Text Generation Tasks,2023-07-17 17:48:51+00:00,http://arxiv.org/abs/2307.08689v1,"Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, Karthik Narasimhan","cs.CL, cs.AI, cs.LG",table2text,"Text generation under constraints have seen increasing interests in natural
language processing, especially with the rapidly improving capabilities of
large language models. However, existing benchmarks for constrained generation
usually focus on fixed constraint types (e.g.,generate a sentence containing
certain words) that have proved to be easy for state-of-the-art models like
GPT-4. We present COLLIE, a grammar-based framework that allows the
specification of rich, compositional constraints with diverse generation levels
(word, sentence, paragraph, passage) and modeling challenges (e.g.,language
understanding, logical reasoning, counting, semantic planning). We also develop
tools for automatic extraction of task instances given a constraint structure
and a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 2080
instances comprising 13 constraint structures. We perform systematic
experiments across five state-of-the-art instruction-tuned language models and
analyze their performances to reveal shortcomings. COLLIE is designed to be
extensible and lightweight, and we hope the community finds it useful to
develop more complex constraints and evaluations in the future.",2023-07-17
Fast Quantum Algorithm for Attention Computation,2023-07-16 14:00:42+00:00,http://arxiv.org/abs/2307.08045v1,"Yeqi Gao, Zhao Song, Xin Yang, Ruizhe Zhang","quant-ph, cs.LG",table2text,"Large language models (LLMs) have demonstrated exceptional performance across
a wide range of tasks. These models, powered by advanced deep learning
techniques, have revolutionized the field of natural language processing (NLP)
and have achieved remarkable results in various language-related tasks.
  LLMs have excelled in tasks such as machine translation, sentiment analysis,
question answering, text generation, text classification, language modeling,
and more. They have proven to be highly effective in capturing complex
linguistic patterns, understanding context, and generating coherent and
contextually relevant text. The attention scheme plays a crucial role in the
architecture of large language models (LLMs). It is a fundamental component
that enables the model to capture and utilize contextual information during
language processing tasks effectively. Making the attention scheme computation
faster is one of the central questions to speed up the LLMs computation. It is
well-known that quantum machine has certain computational advantages compared
to the classical machine. However, it is currently unknown whether quantum
computing can aid in LLM.
  In this work, we focus on utilizing Grover's Search algorithm to compute a
sparse attention computation matrix efficiently. We achieve a polynomial
quantum speed-up over the classical method. Moreover, the attention matrix
outputted by our quantum algorithm exhibits an extra low-rank structure that
will be useful in obtaining a faster training algorithm for LLMs. Additionally,
we present a detailed analysis of the algorithm's error analysis and time
complexity within the context of computing the attention matrix.",2023-07-16
"Using Large Language Models for Zero-Shot Natural Language Generation
  from Knowledge Graphs",2023-07-14 12:45:03+00:00,http://arxiv.org/abs/2307.07312v1,"Agnes Axelsson, Gabriel Skantze","cs.CL, 68T50, I.2.7; I.2.4",table2text,"In any system that uses structured knowledge graph (KG) data as its
underlying knowledge representation, KG-to-text generation is a useful tool for
turning parts of the graph data into text that can be understood by humans.
Recent work has shown that models that make use of pretraining on large amounts
of text data can perform well on the KG-to-text task even with relatively small
sets of training data on the specific graph-to-text task. In this paper, we
build on this concept by using large language models to perform zero-shot
generation based on nothing but the model's understanding of the triple
structure from what it can read. We show that ChatGPT achieves near
state-of-the-art performance on some measures of the WebNLG 2020 challenge, but
falls behind on others. Additionally, we compare factual, counter-factual and
fictional statements, and show that there is a significant connection between
what the LLM already knows about the data it is parsing and the quality of the
output text.",2023-07-14
Generating Efficient Training Data via LLM-based Attribute Manipulation,2023-07-14 00:10:03+00:00,http://arxiv.org/abs/2307.07099v1,"Letian Peng, Yuwei Zhang, Jingbo Shang",cs.CL,table2text,"In this paper, we propose a novel method, Chain-of-Thoughts Attribute
Manipulation (CoTAM), to guide few-shot learning by carefully crafted data from
Large Language Models (LLMs). The main idea is to create data with changes only
in the attribute targeted by the task. Inspired by facial attribute
manipulation, our approach generates label-switched data by leveraging LLMs to
manipulate task-specific attributes and reconstruct new sentences in a
controlled manner. Instead of conventional latent representation controlling,
we implement chain-of-thoughts decomposition and reconstruction to adapt the
procedure to LLMs. Extensive results on text classification and other tasks
verify the advantage of CoTAM over other LLM-based text generation methods with
the same number of training examples. Analysis visualizes the attribute
manipulation effectiveness of CoTAM and presents the potential of LLM-guided
learning with even less supervision.",2023-07-14
"DIALGEN: Collaborative Human-LM Generated Dialogues for Improved
  Understanding of Human-Human Conversations",2023-07-13 20:02:50+00:00,http://arxiv.org/abs/2307.07047v1,"Bo-Ru Lu, Nikita Haduong, Chia-Hsuan Lee, Zeqiu Wu, Hao Cheng, Paul Koester, Jean Utke, Tao Yu, Noah A. Smith, Mari Ostendorf",cs.CL,table2text,"Applications that could benefit from automatic understanding of human-human
conversations often come with challenges associated with private information in
real-world data such as call center or clinical conversations. Working with
protected data also increases costs of annotation, which limits technology
development. To address these challenges, we propose DIALGEN, a
human-in-the-loop semi-automated dialogue generation framework. DIALGEN uses a
language model (ChatGPT) that can follow schema and style specifications to
produce fluent conversational text, generating a complex conversation through
iteratively generating subdialogues and using human feedback to correct
inconsistencies or redirect the flow. In experiments on structured
summarization of agent-client information gathering calls, framed as dialogue
state tracking, we show that DIALGEN data enables significant improvement in
model performance.",2023-07-13
Reading Radiology Imaging Like The Radiologist,2023-07-12 05:36:47+00:00,http://arxiv.org/abs/2307.05921v3,Yuhao Wang,"cs.CV, cs.AI",table2text,"Automated radiology report generation aims to generate radiology reports that
contain rich, fine-grained descriptions of radiology imaging. Compared with
image captioning in the natural image domain, medical images are very similar
to each other, with only minor differences in the occurrence of diseases. Given
the importance of these minor differences in the radiology report, it is
crucial to encourage the model to focus more on the subtle regions of disease
occurrence. Secondly, the problem of visual and textual data biases is serious.
Not only do normal cases make up the majority of the dataset, but sentences
describing areas with pathological changes also constitute only a small part of
the paragraph. Lastly, generating medical image reports involves the challenge
of long text generation, which requires more expertise and empirical training
in medical knowledge. As a result, the difficulty of generating such reports is
increased. To address these challenges, we propose a disease-oriented retrieval
framework that utilizes similar reports as prior knowledge references. We
design a factual consistency captioning generator to generate more accurate and
factually consistent disease descriptions. Our framework can find most similar
reports for a given disease from the CXR database by retrieving a
disease-oriented mask consisting of the position and morphological
characteristics. By referencing the disease-oriented similar report and the
visual features, the factual consistency model can generate a more accurate
radiology report.",2023-07-12
"PatternGPT :A Pattern-Driven Framework for Large Language Model Text
  Generation",2023-07-02 04:32:41+00:00,http://arxiv.org/abs/2307.00470v4,"Le Xiao, Xin Shan","cs.CL, cs.AI",table2text,"Large language models(LLMS)have shown excellent text generation capabilities,
capable of generating fluent human-like responses for many downstream tasks.
However, applying large language models to real-world critical tasks remains
challenging due to their susceptibility to hallucinations and inability to
directly use external knowledge. To cope with the above challenges, this paper
proposes PatternGPT, a pattern-driven text generation framework for Large
Language Models. Firstly, the framework utilizes the extraction capability of
Large Language Models to generate rich and diversified structured and
formalized patterns, which facilitates the introduction of external knowledge
to do the computation, and then draws on the idea of federated learning to use
multiple agents to achieve the sharing in order to obtain more diversified
patterns, and finally uses judgment criteria and optimization algorithm to
search for high-quality patterns to guide the generation of models. Finally,
external knowledge such as judgment criteria and optimization algorithms are
used to search for high-quality patterns, and the searched patterns are used to
guide model generation. This framework has the advantages of generating
diversified patterns, protecting data privacy, combining external knowledge,
and improving the quality of generation, which provides an effective method to
optimize the text generation capability of large language models, and make it
better applied to the field of intelligent dialogue and content generation.",2023-07-02
"Benchmarking Large Language Model Capabilities for Conditional
  Generation",2023-06-29 08:59:40+00:00,http://arxiv.org/abs/2306.16793v1,"Joshua Maynez, Priyanka Agrawal, Sebastian Gehrmann",cs.CL,table2text,"Pre-trained large language models (PLMs) underlie most new developments in
natural language processing. They have shifted the field from
application-specific model pipelines to a single model that is adapted to a
wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside
techniques like few-shot learning, have additionally shifted the output
modality to generation instead of classification or regression. Despite their
ubiquitous use, the generation quality of language models is rarely evaluated
when these models are introduced. Additionally, it is unclear how existing
generation tasks--while they can be used to compare systems at a high
level--relate to the real world use cases for which people have been adopting
them. In this work, we discuss how to adapt existing application-specific
generation benchmarks to PLMs and provide an in-depth, empirical study of the
limitations and capabilities of PLMs in natural language generation tasks along
dimensions such as scale, architecture, input and output language. Our results
show that PLMs differ in their applicability to different data regimes and
their generalization to multiple languages and inform which PLMs to use for a
given generation task setup. We share best practices to be taken into
consideration when benchmarking generation capabilities during the development
of upcoming PLMs.",2023-06-29
Joint Level Generation and Translation Using Gameplay Videos,2023-06-29 03:46:44+00:00,http://arxiv.org/abs/2306.16662v1,"Negar Mirgati, Matthew Guzdial","cs.CV, cs.LG",table2text,"Procedural Content Generation via Machine Learning (PCGML) faces a
significant hurdle that sets it apart from other fields, such as image or text
generation, which is limited annotated data. Many existing methods for
procedural level generation via machine learning require a secondary
representation besides level images. However, the current methods for obtaining
such representations are laborious and time-consuming, which contributes to
this problem. In this work, we aim to address this problem by utilizing
gameplay videos of two human-annotated games to develop a novel multi-tail
framework that learns to perform simultaneous level translation and generation.
The translation tail of our framework can convert gameplay video frames to an
equivalent secondary representation, while its generation tail can produce
novel level segments. Evaluation results and comparisons between our framework
and baselines suggest that combining the level generation and translation tasks
can lead to an overall improved performance regarding both tasks. This
represents a possible solution to limited annotated level data, and we
demonstrate the potential for future versions to generalize to unseen games.",2023-06-29
"You Can Generate It Again: Data-to-text Generation with Verification and
  Correction Prompting",2023-06-28 05:34:25+00:00,http://arxiv.org/abs/2306.15933v1,"Xuan Ren, Lingqiao Liu","cs.CL, cs.AI, cs.LG",table2text,"Despite significant advancements in existing models, generating text
descriptions from structured data input, known as data-to-text generation,
remains a challenging task. In this paper, we propose a novel approach that
goes beyond traditional one-shot generation methods by introducing a multi-step
process consisting of generation, verification, and correction stages. Our
approach, VCP(Verification and Correction Prompting), begins with the model
generating an initial output. We then proceed to verify the correctness of
different aspects of the generated text. The observations from the verification
step are converted into a specialized error-indication prompt, which instructs
the model to regenerate the output while considering the identified errors. To
enhance the model's correction ability, we have developed a carefully designed
training procedure. This procedure enables the model to incorporate feedback
from the error-indication prompt, resulting in improved output generation.
Through experimental results, we demonstrate that our approach effectively
reduces slot error rates while maintaining the overall quality of the generated
text.",2023-06-28
Knowledge Graph-Augmented Korean Generative Commonsense Reasoning,2023-06-26 07:23:47+00:00,http://arxiv.org/abs/2306.14470v1,"Dahyun Jung, Jaehyung Seo, Jaewook Lee, Chanjun Park, Heuiseok Lim","cs.CL, cs.AI",table2text,"Generative commonsense reasoning refers to the task of generating acceptable
and logical assumptions about everyday situations based on commonsense
understanding. By utilizing an existing dataset such as Korean CommonGen,
language generation models can learn commonsense reasoning specific to the
Korean language. However, language models often fail to consider the
relationships between concepts and the deep knowledge inherent to concepts. To
address these limitations, we propose a method to utilize the Korean knowledge
graph data for text generation. Our experimental result shows that the proposed
method can enhance the efficiency of Korean commonsense inference, thereby
underlining the significance of employing supplementary data.",2023-06-26
AudioPaLM: A Large Language Model That Can Speak and Listen,2023-06-22 14:37:54+00:00,http://arxiv.org/abs/2306.12925v1,"Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo Velimirović, Damien Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka, Christian Frank","cs.CL, cs.AI, cs.SD, eess.AS, stat.ML",table2text,"We introduce AudioPaLM, a large language model for speech understanding and
generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2
[Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified
multimodal architecture that can process and generate text and speech with
applications including speech recognition and speech-to-speech translation.
AudioPaLM inherits the capability to preserve paralinguistic information such
as speaker identity and intonation from AudioLM and the linguistic knowledge
present only in text large language models such as PaLM-2. We demonstrate that
initializing AudioPaLM with the weights of a text-only large language model
improves speech processing, successfully leveraging the larger quantity of text
training data used in pretraining to assist with the speech tasks. The
resulting model significantly outperforms existing systems for speech
translation tasks and has the ability to perform zero-shot speech-to-text
translation for many languages for which input/target language combinations
were not seen in training. AudioPaLM also demonstrates features of audio
language models, such as transferring a voice across languages based on a short
spoken prompt. We release examples of our method at
https://google-research.github.io/seanet/audiopalm/examples",2023-06-22
Open-Domain Text Evaluation via Meta Distribution Modeling,2023-06-20 20:37:54+00:00,http://arxiv.org/abs/2306.11879v1,"Sidi Lu, Asli Celikyilmaz, Tianlu Wang, Nanyun Peng",cs.CL,table2text,"Recent advances in open-domain text generation models powered by large
pre-trained language models (LLMs) have achieved remarkable performance.
However, evaluating and controlling these models for desired attributes remains
a challenge, as traditional reference-based metrics such as BLEU, ROUGE, and
METEOR are insufficient for open-ended generation tasks. Similarly, while
trainable discriminator-based evaluation metrics show promise, obtaining
high-quality training data is a non-trivial task. In this paper, we introduce a
novel approach to evaluate open-domain generation - the Meta-Distribution
Methods (MDM). Drawing on the correlation between the rising parameter counts
and the improving performance of LLMs, MDM creates a mapping from the contrast
of two probabilistic distributions -- one known to be superior to the other --
to quality measures, which can be viewed as a distribution of distributions
i.e. Meta-Distribution. We investigate MDM for open-domain text generation
evaluation under two paradigms: 1) \emph{Generative} MDM, which leverages the
Meta-Distribution Methods to generate in-domain negative samples for training
discriminator-based metrics; 2) \emph{Discriminative} MDM, which directly uses
distribution discrepancies between two language models for evaluation. Our
experiments on multi-turn dialogue and factuality in abstractive summarization
demonstrate that MDMs correlate better with human judgment than existing
automatic evaluation metrics on both tasks, highlighting the strong performance
and generalizability of such methods.",2023-06-20
"ChatGPT is not Enough: Enhancing Large Language Models with Knowledge
  Graphs for Fact-aware Language Modeling",2023-06-20 12:21:06+00:00,http://arxiv.org/abs/2306.11489v1,"Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, Xindong Wu","cs.CL, cs.AI",table2text,"Recently, ChatGPT, a representative large language model (LLM), has gained
considerable attention due to its powerful emergent abilities. Some researchers
suggest that LLMs could potentially replace structured knowledge bases like
knowledge graphs (KGs) and function as parameterized knowledge bases. However,
while LLMs are proficient at learning probabilistic language patterns based on
large corpus and engaging in conversations with humans, they, like previous
smaller pre-trained language models (PLMs), still have difficulty in recalling
facts while generating knowledge-grounded contents. To overcome these
limitations, researchers have proposed enhancing data-driven PLMs with
knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus
improving their performance to generate texts requiring factual knowledge and
providing more informed responses to user queries. This paper reviews the
studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced
pre-trained language models (KGPLMs) as well as their applications. Inspired by
existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by
developing knowledge graph-enhanced large language models (KGLLMs). KGLLM
provides a solution to enhance LLMs' factual reasoning ability, opening up new
avenues for LLM research.",2023-06-20
Explicit Syntactic Guidance for Neural Text Generation,2023-06-20 12:16:31+00:00,http://arxiv.org/abs/2306.11485v2,"Yafu Li, Leyang Cui, Jianhao Yan, Yongjing Yin, Wei Bi, Shuming Shi, Yue Zhang",cs.CL,table2text,"Most existing text generation models follow the sequence-to-sequence
paradigm. Generative Grammar suggests that humans generate natural language
texts by learning language grammar. We propose a syntax-guided generation
schema, which generates the sequence guided by a constituency parse tree in a
top-down direction. The decoding process can be decomposed into two parts: (1)
predicting the infilling texts for each constituent in the lexicalized syntax
context given the source sentence; (2) mapping and expanding each constituent
to construct the next-level syntax context. Accordingly, we propose a
structural beam search method to find possible syntax structures
hierarchically. Experiments on paraphrase generation and machine translation
show that the proposed method outperforms autoregressive baselines, while also
demonstrating effectiveness in terms of interpretability, controllability, and
diversity.",2023-06-20
"Semi-supervised Relation Extraction via Data Augmentation and
  Consistency-training",2023-06-16 19:45:42+00:00,http://arxiv.org/abs/2306.10153v1,Komal K. Teru,"cs.CL, cs.IR",table2text,"Due to the semantic complexity of the Relation extraction (RE) task,
obtaining high-quality human labelled data is an expensive and noisy process.
To improve the sample efficiency of the models, semi-supervised learning (SSL)
methods aim to leverage unlabelled data in addition to learning from limited
labelled data points. Recently, strong data augmentation combined with
consistency-based semi-supervised learning methods have advanced the state of
the art in several SSL tasks. However, adapting these methods to the RE task
has been challenging due to the difficulty of data augmentation for RE. In this
work, we leverage the recent advances in controlled text generation to perform
high quality data augmentation for the RE task. We further introduce small but
significant changes to model architecture that allows for generation of more
training data by interpolating different data points in their latent space.
These data augmentations along with consistency training result in very
competitive results for semi-supervised relation extraction on four benchmark
datasets.",2023-06-16
"Building blocks for complex tasks: Robust generative event extraction
  for radiology reports under domain shifts",2023-06-15 23:16:58+00:00,http://arxiv.org/abs/2306.09544v1,"Sitong Zhou, Meliha Yetisgen, Mari Ostendorf",cs.CL,table2text,"This paper explores methods for extracting information from radiology reports
that generalize across exam modalities to reduce requirements for annotated
data. We demonstrate that multi-pass T5-based text-to-text generative models
exhibit better generalization across exam modalities compared to approaches
that employ BERT-based task-specific classification layers. We then develop
methods that reduce the inference cost of the model, making large-scale corpus
processing more feasible for clinical applications. Specifically, we introduce
a generative technique that decomposes complex tasks into smaller subtask
blocks, which improves a single-pass model when combined with multitask
training. In addition, we leverage target-domain contexts during inference to
enhance domain adaptation, enabling use of smaller models. Analyses offer
insights into the benefits of different cost reduction strategies.",2023-06-15
"Opportunities and Challenges for ChatGPT and Large Language Models in
  Biomedicine and Health",2023-06-15 20:19:08+00:00,http://arxiv.org/abs/2306.10070v1,"Shubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai, Qingqing Zhu, Xiuying Chen, Yifan Yang, Qingyu Chen, Won Kim, Donald C. Comeau, Rezarta Islamaj, Aadit Kapoor, Xin Gao, Zhiyong Lu","cs.CY, cs.AI, cs.CL, q-bio.QM",table2text,"ChatGPT has drawn considerable attention from both the general public and
domain experts with its remarkable text generation capabilities. This has
subsequently led to the emergence of diverse applications in the field of
biomedicine and health. In this work, we examine the diverse applications of
large language models (LLMs), such as ChatGPT, in biomedicine and health.
Specifically we explore the areas of biomedical information retrieval, question
answering, medical text summarization, information extraction, and medical
education, and investigate whether LLMs possess the transformative power to
revolutionize these tasks or whether the distinct complexities of biomedical
domain presents unique challenges. Following an extensive literature survey, we
find that significant advances have been made in the field of text generation
tasks, surpassing the previous state-of-the-art methods. For other
applications, the advances have been modest. Overall, LLMs have not yet
revolutionized the biomedicine, but recent rapid progress indicates that such
methods hold great potential to provide valuable means for accelerating
discovery and improving health. We also find that the use of LLMs, like
ChatGPT, in the fields of biomedicine and health entails various risks and
challenges, including fabricated information in its generated responses, as
well as legal and privacy concerns associated with sensitive patient data. We
believe this first-of-its-kind survey can provide a comprehensive overview to
biomedical researchers and healthcare practitioners on the opportunities and
challenges associated with using ChatGPT and other LLMs for transforming
biomedicine and health.",2023-06-15
DiffuDetox: A Mixed Diffusion Model for Text Detoxification,2023-06-14 13:41:23+00:00,http://arxiv.org/abs/2306.08505v1,"Griffin Floto, Mohammad Mahdi Abdollah Pour, Parsa Farinneya, Zhenwei Tang, Ali Pesaranghader, Manasa Bharadwaj, Scott Sanner","cs.CL, cs.LG",table2text,"Text detoxification is a conditional text generation task aiming to remove
offensive content from toxic text. It is highly useful for online forums and
social media, where offensive content is frequently encountered. Intuitively,
there are diverse ways to detoxify sentences while preserving their meanings,
and we can select from detoxified sentences before displaying text to users.
Conditional diffusion models are particularly suitable for this task given
their demonstrated higher generative diversity than existing conditional text
generation models based on language models. Nonetheless, text fluency declines
when they are trained with insufficient data, which is the case for this task.
In this work, we propose DiffuDetox, a mixed conditional and unconditional
diffusion model for text detoxification. The conditional model takes toxic text
as the condition and reduces its toxicity, yielding a diverse set of detoxified
sentences. The unconditional model is trained to recover the input text, which
allows the introduction of additional fluent text for training and thus ensures
text fluency. Extensive experimental results and in-depth analysis demonstrate
the effectiveness of our proposed DiffuDetox.",2023-06-14
Unifying Large Language Models and Knowledge Graphs: A Roadmap,2023-06-14 07:15:26+00:00,http://arxiv.org/abs/2306.08302v2,"Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu","cs.CL, cs.AI",table2text,"Large language models (LLMs), such as ChatGPT and GPT4, are making new waves
in the field of natural language processing and artificial intelligence, due to
their emergent ability and generalizability. However, LLMs are black-box
models, which often fall short of capturing and accessing factual knowledge. In
contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are
structured knowledge models that explicitly store rich factual knowledge. KGs
can enhance LLMs by providing external knowledge for inference and
interpretability. Meanwhile, KGs are difficult to construct and evolving by
nature, which challenges the existing methods in KGs to generate new facts and
represent unseen knowledge. Therefore, it is complementary to unify LLMs and
KGs together and simultaneously leverage their advantages. In this article, we
present a forward-looking roadmap for the unification of LLMs and KGs. Our
roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,
which incorporate KGs during the pre-training and inference phases of LLMs, or
for the purpose of enhancing understanding of the knowledge learned by LLMs; 2)
LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,
completion, construction, graph-to-text generation, and question answering; and
3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a
mutually beneficial way to enhance both LLMs and KGs for bidirectional
reasoning driven by both data and knowledge. We review and summarize existing
efforts within these three frameworks in our roadmap and pinpoint their future
research directions.",2023-06-14
"Large Language Models Sometimes Generate Purely Negatively-Reinforced
  Text",2023-06-13 06:40:37+00:00,http://arxiv.org/abs/2306.07567v2,Fabien Roger,"cs.LG, cs.CL",table2text,"When using adversarial training, it is common practice to train against the
most egregious failures. However, this might imply using examples with
sensitive information (such as leaked passwords or security vulnerabilities) as
training data. One might assume that language models trained with gradient
descent never generate text snippets which were only present in examples
associated with the lowest possible reward. In this paper, we show that this
assumption is wrong: in some situations, large language models do learn from
such negatively-reinforced examples. We present a specific training setup that
enables Pythia-160M to guess passwords 13% more often than it would by guessing
randomly, despite only showing it these passwords on examples where the model
is incentivized to not output these passwords. Our code is available at
www.github.com/FabienRoger/Learning-From-Negative-Examples",2023-06-13
"SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling
  with Backtracking",2023-06-08 17:59:58+00:00,http://arxiv.org/abs/2306.05426v1,"Chris Cundy, Stefano Ermon","cs.LG, cs.AI",table2text,"In many domains, autoregressive models can achieve low log-likelihood on the
task of predicting the next observation. However, this maximum-likelihood (MLE)
objective does not necessarily match a downstream use-case of autoregressively
generating high-quality sequences. The MLE objective weights sequences
proportionally to their frequency under the data distribution, with no guidance
for the model's behaviour out of distribution (OOD): leading to compounding
error during autoregressive generation. In order to address this compounding
error problem, we formulate sequence generation as an imitation learning (IL)
problem. This allows us to minimize a variety of divergences between the
distribution of sequences generated by an autoregressive model and sequences
from a dataset, including divergences with weight on OOD generated sequences.
The IL framework also allows us to incorporate backtracking by introducing a
backspace action into the generation process. This further mitigates the
compounding error problem by allowing the model to revert a sampled token if it
takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented
without adversarial training or major architectural changes. We identify the
SequenceMatch-$\chi^2$ divergence as a more suitable training objective for
autoregressive models which are used for generation. We show that empirically,
SequenceMatch training leads to improvements over MLE on text generation with
language models.",2023-06-08
"Increasing Diversity While Maintaining Accuracy: Text Data Generation
  with Large Language Models and Human Interventions",2023-06-07 04:27:09+00:00,http://arxiv.org/abs/2306.04140v1,"John Joon Young Chung, Ece Kamar, Saleema Amershi",cs.CL,table2text,"Large language models (LLMs) can be used to generate text data for training
and evaluating other models. However, creating high-quality datasets with LLMs
can be challenging. In this work, we explore human-AI partnerships to
facilitate high diversity and accuracy in LLM-based text data generation. We
first examine two approaches to diversify text generation: 1) logit
suppression, which minimizes the generation of languages that have already been
frequently generated, and 2) temperature sampling, which flattens the token
sampling probability. We found that diversification approaches can increase
data diversity but often at the cost of data accuracy (i.e., text and labels
being appropriate for the target domain). To address this issue, we examined
two human interventions, 1) label replacement (LR), correcting misaligned
labels, and 2) out-of-scope filtering (OOSF), removing instances that are out
of the user's domain of interest or to which no considered label applies. With
oracle studies, we found that LR increases the absolute accuracy of models
trained with diversified datasets by 14.4%. Moreover, we found that some models
trained with data generated with LR interventions outperformed LLM-based
few-shot classification. In contrast, OOSF was not effective in increasing
model accuracy, implying the need for future work in human-in-the-loop text
data generation.",2023-06-07
Structured Voronoi Sampling,2023-06-05 17:32:35+00:00,http://arxiv.org/abs/2306.03061v1,"Afra Amini, Li Du, Ryan Cotterell","cs.CL, cs.AI",table2text,"Recently, there has been a growing interest in the development of
gradient-based sampling algorithms for text generation, especially in the
context of controlled generation. However, there exists a lack of theoretically
grounded and principled approaches for this task. In this paper, we take an
important step toward building a principled approach for sampling from language
models with gradient-based methods. We use discrete distributions given by
language models to define densities and develop an algorithm based on
Hamiltonian Monte Carlo to sample from them. We name our gradient-based
technique Structured Voronoi Sampling (SVS). In an experimental setup where the
reference distribution is known, we show that the empirical distribution of SVS
samples is closer to the reference distribution compared to alternative
sampling schemes. Furthermore, in a controlled generation task, SVS is able to
generate fluent and diverse samples while following the control targets
significantly better than other methods.",2023-06-05
"Adaptive and Personalized Exercise Generation for Online Language
  Learning",2023-06-04 20:18:40+00:00,http://arxiv.org/abs/2306.02457v1,"Peng Cui, Mrinmaya Sachan","cs.CL, cs.AI",table2text,"Adaptive learning aims to provide customized educational activities (e.g.,
exercises) to address individual learning needs. However, manual construction
and delivery of such activities is a laborious process. Thus, in this paper, we
study a novel task of adaptive and personalized exercise generation for online
language learning. To this end, we combine a knowledge tracing model that
estimates each student's evolving knowledge states from their learning history
and a controlled text generation model that generates exercise sentences based
on the student's current estimated knowledge state and instructor requirements
of desired properties (e.g., domain knowledge and difficulty). We train and
evaluate our model on real-world learner interaction data from Duolingo and
demonstrate that LMs guided by student states can generate superior exercises.
Then, we discuss the potential use of our model in educational applications
using various simulations. These simulations show that our model can adapt to
students' individual abilities and can facilitate their learning efficiency by
personalizing learning sequences.",2023-06-04
Exposing Bias in Online Communities through Large-Scale Language Models,2023-06-04 08:09:26+00:00,http://arxiv.org/abs/2306.02294v1,"Celine Wald, Lukas Pfahler","cs.CL, cs.CY, cs.LG",table2text,"Progress in natural language generation research has been shaped by the
ever-growing size of language models. While large language models pre-trained
on web data can generate human-sounding text, they also reproduce social biases
and contribute to the propagation of harmful stereotypes. This work utilises
the flaw of bias in language models to explore the biases of six different
online communities. In order to get an insight into the communities'
viewpoints, we fine-tune GPT-Neo 1.3B with six social media datasets. The bias
of the resulting models is evaluated by prompting the models with different
demographics and comparing the sentiment and toxicity values of these
generations. Together, these methods reveal that bias differs in type and
intensity for the various models. This work not only affirms how easily bias is
absorbed from training data but also presents a scalable method to identify and
compare the bias of different datasets or communities. Additionally, the
examples generated for this work demonstrate the limitations of using automated
sentiment and toxicity classifiers in bias research.",2023-06-04
"Exploring semantic information in disease: Simple Data Augmentation
  Techniques for Chinese Disease Normalization",2023-06-02 22:12:05+00:00,http://arxiv.org/abs/2306.01931v1,"Wenqian Cui, Shaohui Liu, Xiangling Fu, Xien Liu, Ji Wu","cs.CL, cs.AI",table2text,"The disease is a core concept in the medical field, and the task of
normalizing disease names is the basis of all disease-related tasks. However,
due to the multi-axis and multi-grain nature of disease names, incorrect
information is often injected and harms the performance when using general text
data augmentation techniques. To address the above problem, we propose a set of
data augmentation techniques that work together as an augmented training task
for disease normalization. Our data augmentation methods are based on both the
clinical disease corpus and standard disease corpus derived from ICD-10 coding.
Extensive experiments are conducted to show the effectiveness of our proposed
methods. The results demonstrate that our methods can have up to 3\%
performance gain compared to non-augmented counterparts, and they can work even
better on smaller datasets.",2023-06-02
"Fine-Grained Human Feedback Gives Better Rewards for Language Model
  Training",2023-06-02 17:11:37+00:00,http://arxiv.org/abs/2306.01693v1,"Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, Hannaneh Hajishirzi",cs.CL,table2text,"Language models (LMs) often exhibit undesirable text generation behaviors,
including generating false, toxic, or irrelevant outputs. Reinforcement
learning from human feedback (RLHF) - where human preference judgments on LM
outputs are transformed into a learning signal - has recently shown promise in
addressing these issues. However, such holistic feedback conveys limited
information on long text outputs; it does not indicate which aspects of the
outputs influenced user preference; e.g., which parts contain what type(s) of
errors. In this paper, we use fine-grained human feedback (e.g., which sentence
is false, which sub-sentence is irrelevant) as an explicit training signal. We
introduce Fine-Grained RLHF, a framework that enables training and learning
from reward functions that are fine-grained in two respects: (1) density,
providing a reward after every segment (e.g., a sentence) is generated; and (2)
incorporating multiple reward models associated with different feedback types
(e.g., factual incorrectness, irrelevance, and information incompleteness). We
conduct experiments on detoxification and long-form question answering to
illustrate how learning with such reward functions leads to improved
performance, supported by both automatic and human evaluation. Additionally, we
show that LM behaviors can be customized using different combinations of
fine-grained reward models. We release all data, collected human feedback, and
codes at https://FineGrainedRLHF.github.io.",2023-06-02
Preference-grounded Token-level Guidance for Language Model Fine-tuning,2023-06-01 07:00:07+00:00,http://arxiv.org/abs/2306.00398v1,"Shentao Yang, Shujian Zhang, Congying Xia, Yihao Feng, Caiming Xiong, Mingyuan Zhou",cs.CL,table2text,"Aligning language models (LMs) with preferences is an important problem in
natural language generation. A key challenge is that preferences are typically
provided at the sequence level while LM training and generation both occur at
the token level. There is, therefore, a granularity mismatch between the
preference and the LM training losses, which may complicate the learning
problem. In this paper, we address this issue by developing an alternate
training process, where we iterate between grounding the sequence-level
preference into token-level training guidance, and improving the LM with the
learned guidance. For guidance learning, we design a framework that extends the
pairwise-preference learning in imitation learning to both variable-length LM
generation and utilizing the preference among multiple generations. For LM
training, based on the amount of supervised data, we present two minimalist
learning objectives that utilize the learned guidance. In experiments, our
method performs competitively on two distinct representative LM tasks --
discrete-prompt generation and text summarization.",2023-06-01
Learning to Imagine: Visually-Augmented Natural Language Generation,2023-05-26 13:59:45+00:00,http://arxiv.org/abs/2305.16944v2,"Tianyi Tang, Yushuo Chen, Yifan Du, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen",cs.CL,table2text,"People often imagine relevant scenes to aid in the writing process. In this
work, we aim to utilize visual information for composition in the same manner
as humans. We propose a method, LIVE, that makes pre-trained language models
(PLMs) Learn to Imagine for Visuallyaugmented natural language gEneration.
First, we imagine the scene based on the text: we use a diffusion model to
synthesize high-quality images conditioned on the input texts. Second, we use
CLIP to determine whether the text can evoke the imagination in a posterior
way. Finally, our imagination is dynamic, and we conduct synthesis for each
sentence rather than generate only one image for an entire paragraph.
Technically, we propose a novel plug-and-play fusion layer to obtain
visually-augmented representations for each text. Our vision-text fusion layer
is compatible with Transformerbased architecture. We have conducted extensive
experiments on four generation tasks using BART and T5, and the automatic
results and human evaluation demonstrate the effectiveness of our proposed
method. We will release the code, model, and data at the link:
https://github.com/RUCAIBox/LIVE.",2023-05-26
RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting,2023-05-25 03:26:26+00:00,http://arxiv.org/abs/2305.15685v1,"Lei Shu, Liangchen Luo, Jayakumar Hoskere, Yun Zhu, Canoee Liu, Simon Tong, Jindong Chen, Lei Meng","cs.CL, cs.AI",table2text,"Large Language Models (LLMs) have demonstrated impressive zero-shot
capabilities in long-form text generation tasks expressed through natural
language instructions. However, user expectations for long-form text rewriting
is high, and unintended rewrites (''hallucinations'') produced by the model can
negatively impact its overall performance. Existing evaluation benchmarks
primarily focus on limited rewriting styles and sentence-level rewriting rather
than long-form open-ended rewriting.We introduce OpenRewriteEval, a novel
benchmark that covers a wide variety of rewriting types expressed through
natural language instructions. It is specifically designed to facilitate the
evaluation of open-ended rewriting of long-form texts. In addition, we propose
a strong baseline model, RewriteLM, an instruction-tuned large language model
for long-form text rewriting. We develop new strategies that facilitate the
generation of diverse instructions and preference data with minimal human
intervention. We conduct empirical experiments and demonstrate that our model
outperforms the current state-of-the-art LLMs in text rewriting. Specifically,
it excels in preserving the essential content and meaning of the source text,
minimizing the generation of ''hallucinated'' content, while showcasing the
ability to generate rewrites with diverse wording and structures.",2023-05-25
"Balancing Effect of Training Dataset Distribution of Multiple Styles for
  Multi-Style Text Transfer",2023-05-24 21:36:15+00:00,http://arxiv.org/abs/2305.15582v1,"Debarati Das, David Ma, Dongyeop Kang",cs.CL,table2text,"Text style transfer is an exciting task within the field of natural language
generation that is often plagued by the need for high-quality paired datasets.
Furthermore, training a model for multi-attribute text style transfer requires
datasets with sufficient support across all combinations of the considered
stylistic attributes, adding to the challenges of training a style transfer
model. This paper explores the impact of training data input diversity on the
quality of the generated text from the multi-style transfer model. We construct
a pseudo-parallel dataset by devising heuristics to adjust the style
distribution in the training samples. We balance our training dataset using
marginal and joint distributions to train our style transfer models. We observe
that a balanced dataset produces more effective control effects over multiple
styles than an imbalanced or skewed one. Through quantitative analysis, we
explore the impact of multiple style distributions in training data on
style-transferred output. These findings will better inform the design of
style-transfer datasets.",2023-05-24
"Peek Across: Improving Multi-Document Modeling via Cross-Document
  Question-Answering",2023-05-24 17:48:40+00:00,http://arxiv.org/abs/2305.15387v1,"Avi Caciularu, Matthew E. Peters, Jacob Goldberger, Ido Dagan, Arman Cohan","cs.CL, cs.AI",table2text,"The integration of multi-document pre-training objectives into language
models has resulted in remarkable improvements in multi-document downstream
tasks. In this work, we propose extending this idea by pre-training a generic
multi-document model from a novel cross-document question answering
pre-training objective. To that end, given a set (or cluster) of
topically-related documents, we systematically generate semantically-oriented
questions from a salient sentence in one document and challenge the model,
during pre-training, to answer these questions while ""peeking"" into other
topically-related documents. In a similar manner, the model is also challenged
to recover the sentence from which the question was generated, again while
leveraging cross-document information. This novel multi-document QA formulation
directs the model to better recover cross-text informational relations, and
introduces a natural augmentation that artificially increases the pre-training
data. Further, unlike prior multi-document models that focus on either
classification or summarization tasks, our pre-training objective formulation
enables the model to perform tasks that involve both short text generation
(e.g., QA) and long text generation (e.g., summarization). Following this
scheme, we pre-train our model -- termed QAmden -- and evaluate its performance
across several multi-document tasks, including multi-document QA,
summarization, and query-focused summarization, yielding improvements of up to
7%, and significantly outperforms zero-shot GPT-3.5 and GPT-4.",2023-05-24
"Not All Metrics Are Guilty: Improving NLG Evaluation with LLM
  Paraphrasing",2023-05-24 11:53:29+00:00,http://arxiv.org/abs/2305.15067v1,"Tianyi Tang, Hongyuan Lu, Yuchen Eleanor Jiang, Haoyang Huang, Dongdong Zhang, Wayne Xin Zhao, Furu Wei",cs.CL,table2text,"Most research about natural language generation (NLG) relies on evaluation
benchmarks with limited references for a sample, which may result in poor
correlations with human judgements. The underlying reason is that one semantic
meaning can actually be expressed in different forms, and the evaluation with a
single or few references may not accurately reflect the quality of the model's
hypotheses. To address this issue, this paper presents a novel method, named
Para-Ref, to enhance existing evaluation benchmarks by enriching the number of
references. We leverage large language models (LLMs) to paraphrase a single
reference into multiple high-quality ones in diverse expressions. Experimental
results on representative NLG tasks of machine translation, text summarization,
and image caption demonstrate that our method can effectively improve the
correlation with human evaluation for sixteen automatic evaluation metrics by
+7.82% in ratio. We release the code and data at
https://github.com/RUCAIBox/Para-Ref.",2023-05-24
Ghostbuster: Detecting Text Ghostwritten by Large Language Models,2023-05-24 11:37:10+00:00,http://arxiv.org/abs/2305.15047v1,"Vivek Verma, Eve Fleisig, Nicholas Tomlin, Dan Klein","cs.CL, cs.AI",table2text,"We introduce Ghostbuster, a state-of-the-art system for detecting
AI-generated text. Our method works by passing documents through a series of
weaker language models and running a structured search over possible
combinations of their features, then training a classifier on the selected
features to determine if the target document was AI-generated. Crucially,
Ghostbuster does not require access to token probabilities from the target
model, making it useful for detecting text generated by black-box models or
unknown model versions. In conjunction with our model, we release three new
datasets of human and AI-generated text as detection benchmarks that cover
multiple domains (student essays, creative fiction, and news) and task setups:
document-level detection, author identification, and a challenge task of
paragraph-level detection. Ghostbuster averages 99.1 F1 across all three
datasets on document-level detection, outperforming previous approaches such as
GPTZero and DetectGPT by up to 32.7 F1.",2023-05-24
Active Learning for Natural Language Generation,2023-05-24 11:27:53+00:00,http://arxiv.org/abs/2305.15040v1,"Yotam Perlitz, Ariel Gera, Michal Shmueli-Scheuer, Dafna Sheinwald, Noam Slonim, Liat Ein-Dor",cs.CL,table2text,"The field of text generation suffers from a severe shortage of labeled data
due to the extremely expensive and time consuming process involved in manual
annotation. A natural approach for coping with this problem is active learning
(AL), a well-known machine learning technique for improving annotation
efficiency by selectively choosing the most informative examples to label.
However, while AL has been well-researched in the context of text
classification, its application to text generation remained largely unexplored.
In this paper, we present a first systematic study of active learning for text
generation, considering a diverse set of tasks and multiple leading AL
strategies. Our results indicate that existing AL strategies, despite their
success in classification, are largely ineffective for the text generation
scenario, and fail to consistently surpass the baseline of random example
selection. We highlight some notable differences between the classification and
generation scenarios, and analyze the selection behaviors of existing AL
strategies. Our findings motivate exploring novel approaches for applying AL to
NLG tasks.",2023-05-24
LLMDet: A Large Language Models Detection Tool,2023-05-24 10:45:16+00:00,http://arxiv.org/abs/2305.15004v1,"Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, Tat-Seng Chua",cs.CL,table2text,"With the advancement of generative language models, the generated text has
come remarkably close to high-quality human-authored text in terms of fluency
and diversity. This calls for a highly practical detection tool that can
identify the source of text, preferably pinpointing the language model it
originates from. However, existing detection tools typically require access to
language models and can only differentiate between machine-generated and
human-authored text, failing to meet the requirements of rapid detection and
text tracing. Therefore, in this paper, we propose an efficient, secure, and
scalable detection tool called LLMDet, which calculates the proxy perplexity of
text by utilizing the prior information of the model's next-token
probabilities, obtained through pre-training. Subsequently, we use the
self-watermarking information of the model, as measured by proxy perplexity, to
detect the source of the text. We found that our method demonstrates impressive
detection performance while ensuring speed and security, particularly achieving
a recognition accuracy of 97.97\% for human-authored text. Furthermore, our
detection tool also shows promising results in identifying the large language
model (e.g., GPT-2, OPT, LLaMA, Vicuna...) responsible for the text. We release
the code and processed data at \url{https://github.com/TrustedLLM/LLMDet}.",2023-05-24
The ACL OCL Corpus: advancing Open science in Computational Linguistics,2023-05-24 10:35:56+00:00,http://arxiv.org/abs/2305.14996v1,"Shaurya Rohatgi, Yanxia Qin, Benjamin Aw, Niranjana Unnithan, Min-Yen Kan","cs.CL, cs.DL",table2text,"We present a scholarly corpus from the ACL Anthology to assist Open
scientific research in the Computational Linguistics domain, named as ACL OCL.
Compared with previous ARC and AAN versions, ACL OCL includes structured
full-texts with logical sections, references to figures, and links to a large
knowledge resource (semantic scholar). ACL OCL contains 74k scientific papers,
together with 210k figures extracted up to September 2022. To observe the
development in the computational linguistics domain, we detect the topics of
all OCL papers with a supervised neural model. We observe ''Syntax: Tagging,
Chunking and Parsing'' topic is significantly shrinking and ''Natural Language
Generation'' is resurging. Our dataset is open and available to download from
HuggingFace in https://huggingface.co/datasets/ACL-OCL/ACL-OCL-Corpus.",2023-05-24
"Large Language Models are Effective Table-to-Text Generators,
  Evaluators, and Feedback Providers",2023-05-24 10:22:30+00:00,http://arxiv.org/abs/2305.14987v1,"Yilun Zhao, Haowei Zhang, Shengyun Si, Linyong Nan, Xiangru Tang, Arman Cohan",cs.CL,table2text,"Large language models (LLMs) have shown remarkable ability on controllable
text generation. However, the potential of LLMs in generating text from
structured tables remains largely under-explored. In this paper, we study the
capabilities of LLMs for table-to-text generation tasks, particularly aiming to
investigate their performance in generating natural language statements that
can be logically entailed by a provided table. First, we investigate how LLMs
compare to state-of-the-art table-to-text fine-tuned models, and demonstrate
that LLMs can generate statements with higher faithfulness compared with
previous state-of-the-art fine-tuned models. Given this finding, we next
explore whether LLMs can serve as faithfulness-level automated evaluation
metrics. Through human evaluation, we show that evaluation metrics adopted from
LLMs correlates better with human judgments compared with existing
faithfulness-level metrics. Finally, we demonstrate that LLMs using
chain-of-thought prompting can generate high-fidelity natural language feedback
for other table-to-text models' generations, provide insights for future work
regarding the distillation of text generation capabilities from LLMs to smaller
models.",2023-05-24
Universal Self-adaptive Prompting,2023-05-24 09:09:48+00:00,http://arxiv.org/abs/2305.14926v1,"Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Hanjun Dai, Julian Martin Eisenschlos, Sercan O. Arik, Tomas Pfister","cs.CL, cs.AI, cs.LG",table2text,"A hallmark of modern large language models (LLMs) is their impressive general
zero-shot and few-shot abilities, often elicited through prompt-based and/or
in-context learning. However, while highly coveted and being the most general,
zero-shot performances in LLMs are still typically weaker due to the lack of
guidance and the difficulty of applying existing automatic prompt design
methods in general tasks when ground-truth labels are unavailable. In this
study, we address this by presenting Universal Self-adaptive Prompting (USP),
an automatic prompt design approach specifically tailored for zero-shot
learning (while compatible with few-shot). Requiring only a small amount of
unlabeled data & an inference-only LLM, USP is highly versatile: to achieve
universal prompting, USP categorizes a possible NLP task into one of the three
possible task types, and then uses a corresponding selector to select the most
suitable queries & zero-shot model-generated responses as
pseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a
fully automated way. We evaluate zero-shot USP with two PaLM models, and
demonstrate performances that are considerably stronger than standard zero-shot
baselines and are comparable to or even superior than few-shot baselines across
more than 20 natural language understanding (NLU) and natural language
generation (NLG) tasks.",2023-05-24
Faithful Low-Resource Data-to-Text Generation through Cycle Training,2023-05-24 06:44:42+00:00,http://arxiv.org/abs/2305.14793v1,"Zhuoer Wang, Marcus Collins, Nikhita Vedula, Simone Filice, Shervin Malmasi, Oleg Rokhlenko",cs.CL,table2text,"Methods to generate text from structured data have advanced significantly in
recent years, primarily due to fine-tuning of pre-trained language models on
large datasets. However, such models can fail to produce output faithful to the
input data, particularly on out-of-domain data. Sufficient annotated data is
often not available for specific domains, leading us to seek an unsupervised
approach to improve the faithfulness of output text. Since the problem is
fundamentally one of consistency between the representations of the structured
data and text, we evaluate the effectiveness of cycle training in this work.
Cycle training uses two models which are inverses of each other: one that
generates text from structured data, and one which generates the structured
data from natural language text. We show that cycle training, when initialized
with a small amount of supervised data (100 samples in our case), achieves
nearly the same performance as fully supervised approaches for the data-to-text
generation task on the WebNLG, E2E, WTQ, and WSQL datasets. We perform
extensive empirical analysis with automated evaluation metrics and a newly
designed human evaluation schema to reveal different cycle training strategies'
effectiveness of reducing various types of generation errors. Our code is
publicly available at https://github.com/Edillower/CycleNLG.",2023-05-24
In-Context Demonstration Selection with Cross Entropy Difference,2023-05-24 05:04:00+00:00,http://arxiv.org/abs/2305.14726v1,"Dan Iter, Reid Pryzant, Ruochen Xu, Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu","cs.CL, cs.AI",table2text,"Large language models (LLMs) can use in-context demonstrations to improve
performance on zero-shot tasks. However, selecting the best in-context examples
is challenging because model performance can vary widely depending on the
selected examples. We present a cross-entropy difference (CED) method for
selecting in-context demonstrations. Our method is based on the observation
that the effectiveness of in-context demonstrations negatively correlates with
the perplexity of the test example by a language model that was finetuned on
that demonstration. We utilize parameter efficient finetuning to train small
models on training data that are used for computing the cross-entropy
difference between a test example and every candidate in-context demonstration.
This metric is used to rank and select in-context demonstrations independently
for each test input. We evaluate our method on a mix-domain dataset that
combines 8 benchmarks, representing 4 text generation tasks, showing that CED
for in-context demonstration selection can improve performance for a variety of
LLMs.",2023-05-24
Diffusion Models in NLP: A Survey,2023-05-24 03:25:32+00:00,http://arxiv.org/abs/2305.14671v1,"Hao Zou, Zae Myung Kim, Dongyeop Kang",cs.CL,table2text,"This survey paper provides a comprehensive review of the use of diffusion
models in natural language processing (NLP). Diffusion models are a class of
mathematical models that aim to capture the diffusion of information or signals
across a network or manifold. In NLP, diffusion models have been used in a
variety of applications, such as natural language generation, sentiment
analysis, topic modeling, and machine translation. This paper discusses the
different formulations of diffusion models used in NLP, their strengths and
limitations, and their applications. We also perform a thorough comparison
between diffusion models and alternative generative models, specifically
highlighting the autoregressive (AR) models, while also examining how diverse
architectures incorporate the Transformer in conjunction with diffusion models.
Compared to AR models, diffusion models have significant advantages for
parallel generation, text interpolation, token-level controls such as syntactic
structures and semantic contents, and robustness. Exploring further
permutations of integrating Transformers into diffusion models would be a
valuable pursuit. Also, the development of multimodal diffusion models and
large-scale diffusion language models with notable capabilities for few-shot
learning would be important directions for the future advance of diffusion
models in NLP.",2023-05-24
QTSumm: A New Benchmark for Query-Focused Table Summarization,2023-05-23 17:43:51+00:00,http://arxiv.org/abs/2305.14303v1,"Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin Liu, Weijin Zou, Simeng Han, Xiangru Tang, Yumo Xu, Arman Cohan, Dragomir Radev",cs.CL,table2text,"People primarily consult tables to conduct data analysis or answer specific
questions. Text generation systems that can provide accurate table summaries
tailored to users' information needs can facilitate more efficient access to
relevant data insights. However, existing table-to-text generation studies
primarily focus on converting tabular data into coherent statements, rather
than addressing information-seeking purposes. In this paper, we define a new
query-focused table summarization task, where text generation models have to
perform human-like reasoning and analysis over the given table to generate a
tailored summary, and we introduce a new benchmark named QTSumm for this task.
QTSumm consists of 5,625 human-annotated query-summary pairs over 2,437 tables
on diverse topics. Moreover, we investigate state-of-the-art models (i.e., text
generation, table-to-text generation, and large language models) on the QTSumm
dataset. Experimental results and manual analysis reveal that our benchmark
presents significant challenges in table-to-text generation for future
research.",2023-05-23
"INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with
  Automatic Feedback",2023-05-23 17:27:22+00:00,http://arxiv.org/abs/2305.14282v1,"Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, Lei Li","cs.CL, cs.AI",table2text,"The field of automatic evaluation of text generation made tremendous progress
in the last few years. In particular, since the advent of neural metrics, like
COMET, BLEURT, and SEScore2, the newest generation of metrics show a high
correlation with human judgment. Unfortunately, quality scores generated with
neural metrics are not interpretable, and it is unclear which part of the
generation output is criticized by the metrics. To address this limitation, we
present INSTRUCTSCORE, an open-source, explainable evaluation metric for text
generation. By harnessing both explicit human instruction and the implicit
knowledge of GPT4, we fine-tune a LLAMA model to create an evaluative metric
that can produce a diagnostic report aligned with human judgment. We evaluate
INSTRUCTSCORE on the WMT22 Zh-En translation task, where our 7B model surpasses
other LLM-based baselines, including those based on 175B GPT3. Impressively,
our INSTRUCTSCORE, even without direct supervision from human-rated data,
achieves performance levels on par with state-of-the-art metrics like COMET22,
which was fine-tuned on human ratings.",2023-05-23
"Process-To-Text: A Framework for the Quantitative Description of
  Processes in Natural Language",2023-05-23 13:14:34+00:00,http://arxiv.org/abs/2305.14044v1,"Yago Fontenla-Seco, Alberto Bugarín-Diz, Manuel Lama",cs.CL,table2text,"In this paper we present the Process-To-Text (P2T) framework for the
automatic generation of textual descriptive explanations of processes. P2T
integrates three AI paradigms: process mining for extracting temporal and
structural information from a process, fuzzy linguistic protoforms for
modelling uncertain terms, and natural language generation for building the
explanations. A real use-case in the cardiology domain is presented, showing
the potential of P2T for providing natural language explanations addressed to
specialists.",2023-05-23
STOAT: Structured Data to Analytical Text With Controls,2023-05-19 17:03:09+00:00,http://arxiv.org/abs/2305.11826v1,"Deepanway Ghosal, Preksha Nema, Aravindan Raghuveer","cs.CL, cs.AI",table2text,"Recent language models have made tremendous progress in the structured data
to text generation task. However, these models still give sub-optimal
performance where logical inference is required to generate the descriptions.
In this work, we specifically focus on analytical text generation from
structured data such as tables. Building on the taxonomy proposed in (Gupta et
al., 2020) we focus on controllable table to text generation for the following
reasoning categories: numerical reasoning, commonsense reasoning, temporal
reasoning, table knowledge, and entity knowledge. We propose STOAT model, which
is table and reasoning aware, with vector-quantization to infuse the given
reasoning categories in the output. We observe that our model provides 10.19%,
1.13% improvement on the PARENT metric in iToTTo and Infotabs for the
analytical sentence task. We also found that our model generates 15.3% more
faithful and analytical descriptions as compared to the baseline models in
human evaluation. We curate and release two reasoning category annotated
table-to-interesting text generation datasets based on the ToTTo (Parikh et
al., 2020) and InfoTabs datasets (Gupta et al.,2020).",2023-05-19
"Generating Visual Spatial Description via Holistic 3D Scene
  Understanding",2023-05-19 15:53:56+00:00,http://arxiv.org/abs/2305.11768v1,"Yu Zhao, Hao Fei, Wei Ji, Jianguo Wei, Meishan Zhang, Min Zhang, Tat-Seng Chua","cs.CV, cs.CL",table2text,"Visual spatial description (VSD) aims to generate texts that describe the
spatial relations of the given objects within images. Existing VSD work merely
models the 2D geometrical vision features, thus inevitably falling prey to the
problem of skewed spatial understanding of target objects. In this work, we
investigate the incorporation of 3D scene features for VSD. With an external 3D
scene extractor, we obtain the 3D objects and scene features for input images,
based on which we construct a target object-centered 3D spatial scene graph
(Go3D-S2G), such that we model the spatial semantics of target objects within
the holistic 3D scenes. Besides, we propose a scene subgraph selecting
mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the
diverse local structure features are navigated to yield spatially-diversified
text generation. Experimental results on two VSD datasets demonstrate that our
framework outperforms the baselines significantly, especially improving on the
cases with complex visual spatial relations. Meanwhile, our method can produce
more spatially-diversified generation. Code is available at
https://github.com/zhaoyucs/VSD.",2023-05-19
"What Comes Next? Evaluating Uncertainty in Neural Text Generators
  Against Human Production Variability",2023-05-19 14:41:55+00:00,http://arxiv.org/abs/2305.11707v1,"Mario Giulianelli, Joris Baan, Wilker Aziz, Raquel Fernández, Barbara Plank","cs.CL, cs.AI, cs.LG",table2text,"In Natural Language Generation (NLG) tasks, for any input, multiple
communicative goals are plausible, and any goal can be put into words, or
produced, in multiple ways. We characterise the extent to which human
production varies lexically, syntactically, and semantically across four NLG
tasks, connecting human production variability to aleatoric or data
uncertainty. We then inspect the space of output strings shaped by a generation
system's predicted probability distribution and decoding algorithm to probe its
uncertainty. For each test input, we measure the generator's calibration to
human production variability. Following this instance-level approach, we
analyse NLG models and decoding strategies, demonstrating that probing a
generator with multiple samples and, when possible, multiple references,
provides the level of detail necessary to gain understanding of a model's
representation of uncertainty.",2023-05-19
Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model,2023-05-18 17:35:28+00:00,http://arxiv.org/abs/2305.11140v1,"Chantal Amrhein, Florian Schottmann, Rico Sennrich, Samuel Läubli","cs.CL, I.2.7",table2text,"Natural language generation models reproduce and often amplify the biases
present in their training data. Previous research explored using
sequence-to-sequence rewriting models to transform biased model outputs (or
original texts) into more gender-fair language by creating pseudo training data
through linguistic rules. However, this approach is not practical for languages
with more complex morphology than English. We hypothesise that creating
training data in the reverse direction, i.e. starting from gender-fair text, is
easier for morphologically complex languages and show that it matches the
performance of state-of-the-art rewriting models for English. To eliminate the
rule-based nature of data creation, we instead propose using machine
translation models to create gender-biased text from real gender-fair text via
round-trip translation. Our approach allows us to train a rewriting model for
German without the need for elaborate handcrafted rules. The outputs of this
model increased gender-fairness as shown in a human evaluation study.",2023-05-18
"Cross-modality Data Augmentation for End-to-End Sign Language
  Translation",2023-05-18 16:34:18+00:00,http://arxiv.org/abs/2305.11096v1,"Jinhui Ye, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Hui Xiong",cs.CL,table2text,"End-to-end sign language translation (SLT) aims to convert sign language
videos into spoken language texts directly without intermediate
representations. It has been a challenging task due to the modality gap between
sign videos and texts and the data scarcity of labeled data. To tackle these
challenges, we propose a novel Cross-modality Data Augmentation (XmDA)
framework to transfer the powerful gloss-to-text translation capabilities to
end-to-end sign language translation (i.e. video-to-text) by exploiting pseudo
gloss-text pairs from the sign gloss translation model. Specifically, XmDA
consists of two key components, namely, cross-modality mix-up and
cross-modality knowledge distillation. The former explicitly encourages the
alignment between sign video features and gloss embeddings to bridge the
modality gap. The latter utilizes the generation knowledge from gloss-to-text
teacher models to guide the spoken language text generation. Experimental
results on two widely used SLT datasets, i.e., PHOENIX-2014T and CSL-Daily,
demonstrate that the proposed XmDA framework significantly and consistently
outperforms the baseline models. Extensive analyses confirm our claim that XmDA
enhances spoken language text generation by reducing the representation
distance between videos and texts, as well as improving the processing of
low-frequency words and long sentences.",2023-05-18
"ReGen: Zero-Shot Text Classification via Training Data Generation with
  Progressive Dense Retrieval",2023-05-18 04:30:09+00:00,http://arxiv.org/abs/2305.10703v1,"Yue Yu, Yuchen Zhuang, Rongzhi Zhang, Yu Meng, Jiaming Shen, Chao Zhang","cs.CL, cs.IR, cs.LG",table2text,"With the development of large language models (LLMs), zero-shot learning has
attracted much attention for various NLP tasks. Different from prior works that
generate training data with billion-scale natural language generation (NLG)
models, we propose a retrieval-enhanced framework to create training data from
a general-domain unlabeled corpus. To realize this, we first conduct
contrastive pretraining to learn an unsupervised dense retriever for extracting
the most relevant documents using class-descriptive verbalizers. We then
further propose two simple strategies, namely Verbalizer Augmentation with
Demonstrations and Self-consistency Guided Filtering to improve the topic
coverage of the dataset while removing noisy examples. Experiments on nine
datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines
and saves around 70% of the time compared to baselines using large NLG models.
Besides, REGEN can be naturally integrated with recently proposed large
language models to boost performance.",2023-05-18
Equivariant Few-Shot Learning from Pretrained Models,2023-05-17 02:20:34+00:00,http://arxiv.org/abs/2305.09900v1,"Sourya Basu, Pulkit Katdare, Prasanna Sattigeri, Vijil Chenthamarakshan, Katherine Driggs-Campbell, Payel Das, Lav R. Varshney","cs.LG, cs.AI, cs.CL, cs.CV",table2text,"Efficient transfer learning algorithms are key to the success of foundation
models on diverse downstream tasks even with limited data. Recent works of
\cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging
(\textit{equitune}) and optimization-based methods, respectively, over features
from group-transformed inputs to obtain equivariant outputs from
non-equivariant neural networks. While \cite{kaba2022equivariance} are only
concerned with training from scratch, we find that equitune performs poorly on
equivariant zero-shot tasks despite good finetuning results. We hypothesize
that this is because pretrained models provide better quality features for
certain transformations than others and simply averaging them is deleterious.
Hence, we propose $\lambda$-\textit{equitune} that averages the features using
\textit{importance weights}, $\lambda$s. These weights are learned directly
from the data using a small neural network, leading to excellent zero-shot and
finetuned results that outperform equitune. Further, we prove that
$\lambda$-equitune is equivariant and a universal approximator of equivariant
functions. Additionally, we show that the method of \cite{kaba2022equivariance}
used with appropriate loss functions, which we call \textit{equizero}, also
gives excellent zero-shot and finetuned performance. Both equitune and equizero
are special cases of $\lambda$-equitune. To show the simplicity and generality
of our method, we validate on a wide range of diverse applications and models
such as 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in
natural language generation (NLG), 4) compositional generalization in
languages, and 5) image classification using pretrained CNNs such as Resnet and
Alexnet.",2023-05-17
"Smaller Language Models are Better Black-box Machine-Generated Text
  Detectors",2023-05-17 00:09:08+00:00,http://arxiv.org/abs/2305.09859v1,"Fatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, Taylor Berg-Kirkpatrick","cs.CL, cs.LG",table2text,"With the advent of fluent generative language models that can produce
convincing utterances very similar to those written by humans, distinguishing
whether a piece of text is machine-generated or human-written becomes more
challenging and more important, as such models could be used to spread
misinformation, fake news, fake reviews and to mimic certain authors and
figures. To this end, there have been a slew of methods proposed to detect
machine-generated text. Most of these methods need access to the logits of the
target model or need the ability to sample from the target. One such black-box
detection method relies on the observation that generated text is locally
optimal under the likelihood function of the generator, while human-written
text is not. We find that overall, smaller and partially-trained models are
better universal text detectors: they can more precisely detect text generated
from both small and larger models. Interestingly, we find that whether the
detector and generator were trained on the same data is not critically
important to the detection success. For instance the OPT-125M model has an AUC
of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT
family, GPTJ-6B, has AUC of 0.45.",2023-05-17
Boosting Event Extraction with Denoised Structure-to-Text Augmentation,2023-05-16 16:52:07+00:00,http://arxiv.org/abs/2305.09598v1,"bo wang, Heyan Huang, Xiaochi Wei, Ge Shi, Xiao Liu, Chong Feng, Tong Zhou, Shuaiqiang Wang, Dawei Yin",cs.CL,table2text,"Event extraction aims to recognize pre-defined event triggers and arguments
from texts, which suffer from the lack of high-quality annotations. In most NLP
applications, involving a large scale of synthetic training data is a practical
and effective approach to alleviate the problem of data scarcity. However, when
applying to the task of event extraction, recent data augmentation methods
often neglect the problem of grammatical incorrectness, structure misalignment,
and semantic drifting, leading to unsatisfactory performances. In order to
solve these problems, we propose a denoised structure-to-text augmentation
framework for event extraction DAEE, which generates additional training data
through the knowledge-based structure-to-text generation model and selects the
effective subset from the generated data iteratively with a deep reinforcement
learning agent. Experimental results on several datasets demonstrate that the
proposed method generates more diverse text representations for event
extraction and achieves comparable results with the state-of-the-art.",2023-05-16
"Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice
  and Feedback",2023-05-15 19:48:59+00:00,http://arxiv.org/abs/2305.08982v1,"Shang-Ling Hsu, Raj Sanjay Shah, Prathik Senthil, Zahra Ashktorab, Casey Dugan, Werner Geyer, Diyi Yang","cs.HC, cs.CL",table2text,"Millions of users come to online peer counseling platforms to seek support on
diverse topics ranging from relationship stress to anxiety. However, studies
show that online peer support groups are not always as effective as expected
largely due to users' negative experiences with unhelpful counselors. Peer
counselors are key to the success of online peer counseling platforms, but most
of them often do not have systematic ways to receive guidelines or supervision.
In this work, we introduce CARE: an interactive AI-based tool to empower peer
counselors through automatic suggestion generation. During the practical
training stage, CARE helps diagnose which specific counseling strategies are
most suitable in the given context and provides tailored example responses as
suggestions. Counselors can choose to select, modify, or ignore any suggestion
before replying to the support seeker. Building upon the Motivational
Interviewing framework, CARE utilizes large-scale counseling conversation data
together with advanced natural language generation techniques to achieve these
functionalities. We demonstrate the efficacy of CARE by performing both
quantitative evaluations and qualitative user studies through simulated chats
and semi-structured interviews. We also find that CARE especially helps novice
counselors respond better in challenging situations.",2023-05-15
Creative Data Generation: A Review Focusing on Text and Poetry,2023-05-15 09:50:15+00:00,http://arxiv.org/abs/2305.08493v1,"Mohamad Elzohbi, Richard Zhao",cs.CL,table2text,"The rapid advancement in machine learning has led to a surge in automatic
data generation, making it increasingly challenging to differentiate between
naturally or human-generated data and machine-generated data. Despite these
advancements, the generation of creative data remains a challenge. This paper
aims to investigate and comprehend the essence of creativity, both in general
and within the context of natural language generation. We review various
approaches to creative writing devices and tasks, with a specific focus on the
generation of poetry. We aim to shed light on the challenges and opportunities
in the field of creative data generation.",2023-05-15
"Taxi1500: A Multilingual Dataset for Text Classification in 1500
  Languages",2023-05-15 09:43:32+00:00,http://arxiv.org/abs/2305.08487v1,"Chunlan Ma, Ayyoob ImaniGooghari, Haotian Ye, Ehsaneddin Asgari, Hinrich Schütze",cs.CL,table2text,"While natural language processing tools have been developed extensively for
some of the world's languages, a significant portion of the world's over 7000
languages are still neglected. One reason for this is that evaluation datasets
do not yet cover a wide range of languages, including low-resource and
endangered ones. We aim to address this issue by creating a text classification
dataset encompassing a large number of languages, many of which currently have
little to no annotated data available. We leverage parallel translations of the
Bible to construct such a dataset by first developing applicable topics and
employing a crowdsourcing tool to collect annotated data. By annotating the
English side of the data and projecting the labels onto other languages through
aligned verses, we generate text classification datasets for more than 1500
languages. We extensively benchmark several existing multilingual language
models using our dataset. To facilitate the advancement of research in this
area, we will release our dataset and code.",2023-05-15
"MatSci-NLP: Evaluating Scientific Language Models on Materials Science
  Language Tasks Using Text-to-Schema Modeling",2023-05-14 22:01:24+00:00,http://arxiv.org/abs/2305.08264v1,"Yu Song, Santiago Miret, Bang Liu","cs.CL, cond-mat.mtrl-sci, cs.AI",table2text,"We present MatSci-NLP, a natural language benchmark for evaluating the
performance of natural language processing (NLP) models on materials science
text. We construct the benchmark from publicly available materials science text
data to encompass seven different NLP tasks, including conventional NLP tasks
like named entity recognition and relation classification, as well as NLP tasks
specific to materials science, such as synthesis action retrieval which relates
to creating synthesis procedures for materials. We study various BERT-based
models pretrained on different scientific text corpora on MatSci-NLP to
understand the impact of pretraining strategies on understanding materials
science text. Given the scarcity of high-quality annotated data in the
materials science domain, we perform our fine-tuning experiments with limited
training data to encourage the generalize across MatSci-NLP tasks. Our
experiments in this low-resource training setting show that language models
pretrained on scientific text outperform BERT trained on general text. MatBERT,
a model pretrained specifically on materials science journals, generally
performs best for most tasks. Moreover, we propose a unified text-to-schema for
multitask learning on \benchmark and compare its performance with traditional
fine-tuning methods. In our analysis of different training methods, we find
that our proposed text-to-schema methods inspired by question-answering
consistently outperform single and multitask NLP fine-tuning methods. The code
and datasets are publicly available at
\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23}.",2023-05-14
"Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive
  Text Generation",2023-05-06 13:20:31+00:00,http://arxiv.org/abs/2305.04044v1,"Kun Zhou, Yifan Li, Wayne Xin Zhao, Ji-Rong Wen",cs.CL,table2text,"Recently, continuous diffusion models (CDM) have been introduced into
non-autoregressive (NAR) text-to-text generation. However, the discrete nature
of text increases the difficulty of CDM to generate coherent and fluent texts,
and also causes the incompatibility problem between CDM and advanced NLP
techniques, especially the popular pre-trained language models~(PLMs). To solve
it, we propose Diffusion-NAT, which introduces discrete diffusion models~(DDM)
into NAR text-to-text generation and integrates BART to improve the
performance. By revising the decoding process of BART and the typical settings
of DDM, we unify the inference process of BART and the denoising process of DDM
into the same NAR masked tokens recovering task. In this way, DDM can rely on
BART to perform denoising, which can benefit from both the rich pre-learned
knowledge of BART and the iterative refining paradigm of DDM. Besides, we also
propose the iterative self-prompting strategy to further improve the generation
quality. Experimental results on 7 datasets show that our approach can
outperform competitive NAR methods, and even surpass autoregressive methods.
Our code and data will be publicly released.",2023-05-06
Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain,2023-05-05 03:02:41+00:00,http://arxiv.org/abs/2305.03256v1,"Liqiang Jing, Xuemeng Song, Xuming Lin, Zhongzhou Zhao, Wei Zhou, Liqiang Nie",cs.CL,table2text,"Existing data-to-text generation efforts mainly focus on generating a
coherent text from non-linguistic input data, such as tables and
attribute-value pairs, but overlook that different application scenarios may
require texts of different styles. Inspired by this, we define a new task,
namely stylized data-to-text generation, whose aim is to generate coherent text
for the given non-linguistic data according to a specific style. This task is
non-trivial, due to three challenges: the logic of the generated text,
unstructured style reference, and biased training samples. To address these
challenges, we propose a novel stylized data-to-text generation model, named
StyleD2T, comprising three components: logic planning-enhanced data embedding,
mask-based style embedding, and unbiased stylized text generation. In the first
component, we introduce a graph-guided logic planner for attribute organization
to ensure the logic of generated text. In the second component, we devise
feature-level mask-based style embedding to extract the essential style signal
from the given unstructured style reference. In the last one, pseudo triplet
augmentation is utilized to achieve unbiased text generation, and a
multi-condition based confidence assignment function is designed to ensure the
quality of pseudo samples. Extensive experiments on a newly collected dataset
from Taobao have been conducted, and the results show the superiority of our
model over existing methods.",2023-05-05
VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation,2023-05-04 23:27:21+00:00,http://arxiv.org/abs/2305.03204v1,"Xilun Chen, Lili Yu, Wenhan Xiong, Barlas Oğuz, Yashar Mehdad, Wen-tau Yih","cs.CV, cs.CL",table2text,"We propose a new two-stage pre-training framework for video-to-text
generation tasks such as video captioning and video question answering: A
generative encoder-decoder model is first jointly pre-trained on massive
image-text data to learn fundamental vision-language concepts, and then adapted
to video data in an intermediate video-text pre-training stage to learn
video-specific skills such as spatio-temporal reasoning. As a result, our
VideoOFA model achieves new state-of-the-art performance on four Video
Captioning benchmarks, beating prior art by an average of 9.7 points in CIDEr
score. It also outperforms existing models on two open-ended Video Question
Answering datasets, showcasing its generalization capability as a universal
video-to-text model.",2023-05-04
"Can LLM Already Serve as A Database Interface? A BIg Bench for
  Large-Scale Database Grounded Text-to-SQLs",2023-05-04 19:02:29+00:00,http://arxiv.org/abs/2305.03111v1,"Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Chenhao Ma, Kevin C. C. Chang, Fei Huang, Reynold Cheng, Yongbin Li",cs.CL,table2text,"Text-to-SQL parsing, which aims at converting natural language instructions
into executable SQLs, has gained increasing attention in recent years. In
particular, Codex and ChatGPT have shown impressive results in this task.
However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on
database schema with few rows of database contents leaving the gap between
academic study and real-world applications. To mitigate this gap, we present
Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks,
containing 12,751 pairs of text-to-SQL data and 95 databases with a total size
of 33.4 GB, spanning 37 professional domains. Our emphasis on database values
highlights the new challenges of dirty database contents, external knowledge
between NL questions and database contents, and SQL efficiency, particularly in
the context of massive databases. To solve these problems, text-to-SQL models
must feature database value comprehension in addition to semantic parsing. The
experimental results demonstrate the significance of database values in
generating accurate text-to-SQLs for big databases. Furthermore, even the most
effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution
accuracy, which is still far from the human result of 92.96%, proving that
challenges still stand. Besides, we also provide an efficiency analysis to
offer insights into generating text-to-efficient-SQLs that are beneficial to
industries. We believe that BIRD will contribute to advancing real-world
applications of text-to-SQL research. The leaderboard and source code are
available: https://bird-bench.github.io/.",2023-05-04
"How to Choose Pretrained Handwriting Recognition Models for Single
  Writer Fine-Tuning",2023-05-04 07:00:28+00:00,http://arxiv.org/abs/2305.02593v1,"Vittorio Pippi, Silvia Cascianelli, Christopher Kermorvant, Rita Cucchiara","cs.CV, cs.DL",table2text,"Recent advancements in Deep Learning-based Handwritten Text Recognition (HTR)
have led to models with remarkable performance on both modern and historical
manuscripts in large benchmark datasets. Nonetheless, those models struggle to
obtain the same performance when applied to manuscripts with peculiar
characteristics, such as language, paper support, ink, and author handwriting.
This issue is very relevant for valuable but small collections of documents
preserved in historical archives, for which obtaining sufficient annotated
training data is costly or, in some cases, unfeasible. To overcome this
challenge, a possible solution is to pretrain HTR models on large datasets and
then fine-tune them on small single-author collections. In this paper, we take
into account large, real benchmark datasets and synthetic ones obtained with a
styled Handwritten Text Generation model. Through extensive experimental
analysis, also considering the amount of fine-tuning lines, we give a
quantitative indication of the most relevant characteristics of such data for
obtaining an HTR model able to effectively transcribe manuscripts in small
collections with as little as five real fine-tuning lines.",2023-05-04
"Governance of the AI, by the AI, and for the AI",2023-05-04 03:29:07+00:00,http://arxiv.org/abs/2305.03719v1,"Andrew W. Torrance, Bill Tomlinson","cs.CY, cs.AI",table2text,"Over the past half century, there have been several false dawns during which
the ""arrival"" of world-changing artificial intelligence (AI) has been heralded.
Tempting fate, the authors believe the age of AI has, indeed, finally arrived.
Powerful image generators, such as DALL-E2 and Midjourney have suddenly allowed
anyone with access the ability easily to create rich and complex art. In a
similar vein, text generators, such as GPT3.5 (including ChatGPT) and BLOOM,
allow users to compose detailed written descriptions of many topics of
interest. And, it is even possible now for a person without extensive expertise
in writing software to use AI to generate code capable of myriad applications.
While AI will continue to evolve and improve, probably at a rapid rate, the
current state of AI is already ushering in profound changes to many different
sectors of society. Every new technology challenges the ability of humanity to
govern it wisely. However, governance is usually viewed as both possible and
necessary due to the disruption new technology often poses to social
structures, industries, the environment, and other important human concerns. In
this article, we offer an analysis of a range of interactions between AI and
governance, with the hope that wise decisions may be made that maximize
benefits and minimize costs. The article addresses two main aspects of this
relationship: the governance of AI by humanity, and the governance of humanity
by AI. The approach we have taken is itself informed by AI, as this article was
written collaboratively by the authors and ChatGPT.",2023-05-04
Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System,2023-05-04 00:17:49+00:00,http://arxiv.org/abs/2305.02468v1,"Namo Bang, Jeehyun Lee, Myoung-Wan Koo",cs.CL,table2text,"Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks
by tracking dialogue states and generating appropriate responses to help users
achieve defined goals. Recently, end-to-end dialogue models pre-trained based
on large datasets have shown promising performance in the conversational
system. However, they share the same parameters to train tasks of the dialogue
system (NLU, DST, NLG), so debugging each task is challenging. Also, they
require a lot of effort to fine-tune large parameters to create a task-oriented
chatbot, making it difficult for non-experts to handle. Therefore, we intend to
train relatively lightweight and fast models compared to PLM. In this paper, we
propose an End-to-end TOD system with Task-Optimized Adapters which learn
independently per task, adding only small number of parameters after fixed
layers of pre-trained network. We also enhance the performance of the DST and
NLG modules through reinforcement learning, overcoming the learning curve that
has lacked at the adapter learning and enabling the natural and consistent
response generation that is appropriate for the goal. Our method is a
model-agnostic approach and does not require prompt-tuning as only input data
without a prompt. As results of the experiment, our method shows competitive
performance on the MultiWOZ benchmark compared to the existing end-to-end
models. In particular, we attain state-of-the-art performance on the DST task
of 2.2 dataset.",2023-05-04
"A Systematic Study of Knowledge Distillation for Natural Language
  Generation with Pseudo-Target Training",2023-05-03 10:49:38+00:00,http://arxiv.org/abs/2305.02031v1,"Nitay Calderon, Subhabrata Mukherjee, Roi Reichart, Amir Kantor","cs.CL, cs.AI",table2text,"Modern Natural Language Generation (NLG) models come with massive
computational and storage requirements. In this work, we study the potential of
compressing them, which is crucial for real-world applications serving millions
of users. We focus on Knowledge Distillation (KD) techniques, in which a small
student model learns to imitate a large teacher model, allowing to transfer
knowledge from the teacher to the student. In contrast to much of the previous
work, our goal is to optimize the model for a specific NLG task and a specific
dataset. Typically, in real-world applications, in addition to labeled data
there is abundant unlabeled task-specific data, which is crucial for attaining
high compression rates via KD. In this work, we conduct a systematic study of
task-specific KD techniques for various NLG tasks under realistic assumptions.
We discuss the special characteristics of NLG distillation and particularly the
exposure bias problem. Following, we derive a family of Pseudo-Target (PT)
augmentation methods, substantially extending prior work on sequence-level KD.
We propose the Joint-Teaching method for NLG distillation, which applies
word-level KD to multiple PTs generated by both the teacher and the student.
Our study provides practical model design observations and demonstrates the
effectiveness of PT training for task-specific KD in NLG.",2023-05-03
Towards Summarizing Multiple Documents with Hierarchical Relationships,2023-05-02 15:18:18+00:00,http://arxiv.org/abs/2305.01498v1,"Miao Li, Eduard Hovy, Jey Han Lau","cs.CL, cs.AI",table2text,"Most existing multi-document summarization (MDS) datasets lack
human-generated and genuine (i.e., not synthetic) summaries or source documents
with explicit inter-document relationships that a summary must capture. To
enhance the capabilities of MDS systems we present PeerSum, a novel dataset for
generating meta-reviews of scientific papers, where the meta-reviews are highly
abstractive and genuine summaries of reviews and corresponding discussions.
These source documents have rich inter-document relationships of an explicit
hierarchical structure with cross-references and often feature conflicts. As
there is a scarcity of research that incorporates hierarchical relationships
into MDS systems through attention manipulation on pre-trained language models,
we additionally present Rammer (Relationship-aware Multi-task Meta-review
Generator), a meta-review generation model that uses sparse attention based on
the hierarchical relationships and a multi-task objective that predicts several
metadata features in addition to the standard text generation objective. Our
experimental results show that PeerSum is a challenging dataset, and Rammer
outperforms other strong baseline MDS models under various evaluation metrics.",2023-05-02
"Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural
  Language Generation",2023-05-01 17:36:06+00:00,http://arxiv.org/abs/2305.00955v1,"Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, André F. T. Martins","cs.CL, cs.AI, cs.LG",table2text,"Many recent advances in natural language generation have been fueled by
training large language models on internet-scale data. However, this paradigm
can lead to models that generate toxic, inaccurate, and unhelpful content, and
automatic evaluation metrics often fail to identify these behaviors. As models
become more capable, human feedback is an invaluable signal for evaluating and
improving models. This survey aims to provide an overview of the recent
research that has leveraged human feedback to improve natural language
generation. First, we introduce an encompassing formalization of feedback, and
identify and organize existing research into a taxonomy following this
formalization. Next, we discuss how feedback can be described by its format and
objective, and cover the two approaches proposed to use feedback (either for
training or decoding): directly using the feedback or training feedback models.
We also discuss existing datasets for human-feedback data collection, and
concerns surrounding feedback collection. Finally, we provide an overview of
the nascent field of AI feedback, which exploits large language models to make
judgments based on a set of principles and minimize the need for human
intervention.",2023-05-01
"A Comprehensive AI Policy Education Framework for University Teaching
  and Learning",2023-04-29 15:35:39+00:00,http://arxiv.org/abs/2305.00280v1,Cecilia Ka Yuk Chan,"cs.CY, cs.AI",table2text,"This study aims to develop an AI education policy for higher education by
examining the perceptions and implications of text generative AI technologies.
Data was collected from 457 students and 180 teachers and staff across various
disciplines in Hong Kong universities, using both quantitative and qualitative
research methods. Based on the findings, the study proposes an AI Ecological
Education Policy Framework to address the multifaceted implications of AI
integration in university teaching and learning. This framework is organized
into three dimensions: Pedagogical, Governance, and Operational. The
Pedagogical dimension concentrates on using AI to improve teaching and learning
outcomes, while the Governance dimension tackles issues related to privacy,
security, and accountability. The Operational dimension addresses matters
concerning infrastructure and training. The framework fosters a nuanced
understanding of the implications of AI integration in academic settings,
ensuring that stakeholders are aware of their responsibilities and can take
appropriate actions accordingly.",2023-04-29
"CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to
  Guardrail Models for Virtual Assistants",2023-04-27 17:39:11+00:00,http://arxiv.org/abs/2304.14364v1,"Albert Yu Sun, Varun Nair, Elliot Schumacher, Anitha Kannan","cs.CL, cs.AI, cs.LG",table2text,"A wave of new task-based virtual assistants has been fueled by increasingly
powerful large language models, such as GPT-4. These conversational agents can
be customized to serve customer-specific use cases, but ensuring that
agent-generated text conforms to designer-specified rules included in prompt
instructions alone is challenging. Therefore, chatbot designers often use
another model, called a guardrail model, to verify that the agent output aligns
with their rules and constraints. We explore using a distillation approach to
guardrail models to monitor the output of the first model using training data
from GPT-4. We find two crucial steps to our CONSCENDI process:
scenario-augmented generation and contrastive training examples. When
generating conversational data, we generate a set of rule-breaking scenarios,
which enumerate a diverse set of high-level ways a rule can be violated. This
scenario-guided approach produces a diverse training set of rule-violating
conversations, and it provides chatbot designers greater control over the
classification process. We also prompt GPT-4 to also generate contrastive
examples by altering conversations with violations into acceptable
conversations. This set of borderline, contrastive examples enables the
distilled model to learn finer-grained distinctions between what is acceptable
and what is not. We find that CONSCENDI results in guardrail models that
improve over baselines.",2023-04-27
Controlled Text Generation with Natural Language Instructions,2023-04-27 15:56:34+00:00,http://arxiv.org/abs/2304.14293v1,"Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, Mrinmaya Sachan","cs.CL, cs.AI, cs.LG",table2text,"Large language models generate fluent texts and can follow natural language
instructions to solve a wide range of tasks without task-specific training.
Nevertheless, it is notoriously difficult to control their generation to
satisfy the various constraints required by different applications. In this
work, we present InstructCTG, a controlled text generation framework that
incorporates different constraints by conditioning on natural language
descriptions and demonstrations of the constraints. In particular, we first
extract the underlying constraints of natural texts through a combination of
off-the-shelf NLP tools and simple heuristics. We then verbalize the
constraints into natural language instructions to form weakly supervised
training data. By prepending natural language descriptions of the constraints
and a few demonstrations, we fine-tune a pre-trained language model to
incorporate various types of constraints. Compared to existing search-based or
score-based methods, InstructCTG is more flexible to different constraint types
and has a much smaller impact on the generation quality and speed because it
does not modify the decoding procedure. Additionally, InstructCTG allows the
model to adapt to new constraints without re-training through the use of
few-shot task generalization and in-context learning abilities of
instruction-tuned language models.",2023-04-27
"SweCTRL-Mini: a data-transparent Transformer-based large language model
  for controllable text generation in Swedish",2023-04-27 07:32:37+00:00,http://arxiv.org/abs/2304.13994v1,"Dmytro Kalpakchi, Johan Boye",cs.CL,table2text,"We present SweCTRL-Mini, a large Swedish language model that can be used for
inference and fine-tuning on a single consumer-grade GPU. The model is based on
the CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019),
which means that users of the SweCTRL-Mini model can control the genre of the
generated text by inserting special tokens in the generation prompts.
SweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a
set of Swedish novels. In this article, we provide (1) a detailed account of
the utilized training data and text pre-processing steps, to the extent that it
is possible to check whether a specific phrase/source was a part of the
training data, and (2) an evaluation of the model on both discriminative tasks,
using automatic evaluation methods, and generative tasks, using human referees.
We also compare the generative capabilities of the model with those of GPT-3.
SweCTRL-Mini is fully open and available for download.",2023-04-27
Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,2023-04-26 17:52:30+00:00,http://arxiv.org/abs/2304.13712v2,"Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu","cs.CL, cs.AI, cs.LG",table2text,"This paper presents a comprehensive and practical guide for practitioners and
end-users working with Large Language Models (LLMs) in their downstream natural
language processing (NLP) tasks. We provide discussions and insights into the
usage of LLMs from the perspectives of models, data, and downstream tasks.
Firstly, we offer an introduction and brief summary of current GPT- and
BERT-style LLMs. Then, we discuss the influence of pre-training data, training
data, and test data. Most importantly, we provide a detailed discussion about
the use and non-use cases of large language models for various natural language
processing tasks, such as knowledge-intensive tasks, traditional natural
language understanding tasks, natural language generation tasks, emergent
abilities, and considerations for specific tasks.We present various use cases
and non-use cases to illustrate the practical applications and limitations of
LLMs in real-world scenarios. We also try to understand the importance of data
and the specific challenges associated with each NLP task. Furthermore, we
explore the impact of spurious biases on LLMs and delve into other essential
considerations, such as efficiency, cost, and latency, to ensure a
comprehensive understanding of deploying LLMs in practice. This comprehensive
guide aims to provide researchers and practitioners with valuable insights and
best practices for working with LLMs, thereby enabling the successful
implementation of these models in a wide range of NLP tasks. A curated list of
practical guide resources of LLMs, regularly updated, can be found at
\url{https://github.com/Mooler0410/LLMsPracticalGuide}.",2023-04-26
Evaluating Inter-Bilingual Semantic Parsing for Indian Languages,2023-04-25 17:24:32+00:00,http://arxiv.org/abs/2304.13005v1,"Divyanshu Aggarwal, Vivek Gupta, Anoop Kunchukuttan",cs.CL,table2text,"Despite significant progress in Natural Language Generation for Indian
languages (IndicNLP), there is a lack of datasets around complex structured
tasks such as semantic parsing. One reason for this imminent gap is the
complexity of the logical form, which makes English to multilingual translation
difficult. The process involves alignment of logical forms, intents and slots
with translated unstructured utterance. To address this, we propose an
Inter-bilingual Seq2seq Semantic parsing dataset IE-SEMPARSE for 11 distinct
Indian languages. We highlight the proposed task's practicality, and evaluate
existing multilingual seq2seq models across several train-test strategies. Our
experiment reveals a high correlation across performance of original
multilingual semantic parsing datasets (such as mTOP, multilingual TOP and
multiATIS++) and our proposed IE-SEMPARSE suite.",2023-04-25
"Which Factors Predict the Chat Experience of a Natural Language
  Generation Dialogue Service?",2023-04-21 07:29:07+00:00,http://arxiv.org/abs/2304.10785v1,Eason Chen,"cs.CL, cs.HC",table2text,"In this paper, we proposed a conceptual model to predict the chat experience
in a natural language generation dialog system. We evaluated the model with 120
participants with Partial Least Squares Structural Equation Modeling (PLS-SEM)
and obtained an R-square (R2) with 0.541. The model considers various factors,
including the prompts used for generation; coherence, sentiment, and similarity
in the conversation; and users' perceived dialog agents' favorability. We then
further explore the effectiveness of the subset of our proposed model. The
results showed that users' favorability and coherence, sentiment, and
similarity in the dialogue are positive predictors of users' chat experience.
Moreover, we found users may prefer dialog agents with characteristics of
Extroversion, Openness, Conscientiousness, Agreeableness, and Non-Neuroticism.
Through our research, an adaptive dialog system might use collected data to
infer factors in our model, predict the chat experience for users through these
factors, and optimize it by adjusting prompts.",2023-04-21
"Multi-aspect Repetition Suppression and Content Moderation of Large
  Language Models",2023-04-20 19:17:49+00:00,http://arxiv.org/abs/2304.10611v1,"Minghui Zhang, Alex Sokolov, Weixin Cai, Si-Qing Chen","cs.CL, cs.LG",table2text,"Natural language generation is one of the most impactful fields in NLP, and
recent years have witnessed its evolution brought about by large language
models (LLMs). As the key instrument for writing assistance applications, they
are generally prone to replicating or extending offensive content provided in
the input. In low-resource data regime, they can also lead to repetitive
outputs (Holtzman et al., 2019) [1]. Usually, offensive content and repetitions
are mitigated with post-hoc methods, including n-gram level blocklists, top-k
and nucleus sampling. In this paper, we introduce a combination of exact and
non-exact repetition suppression using token and sequence level unlikelihood
loss, repetition penalty during training, inference, and post-processing
respectively. We further explore multi-level unlikelihood loss to the extent
that it endows the model with abilities to avoid generating offensive words and
phrases from the beginning. Finally, with comprehensive experiments, we
demonstrate that our proposed methods work exceptionally in controlling the
repetition and content quality of LLM outputs.",2023-04-20
GPT-NER: Named Entity Recognition via Large Language Models,2023-04-20 16:17:26+00:00,http://arxiv.org/abs/2304.10428v1,"Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, Guoyin Wang",cs.CL,table2text,"Despite the fact that large-scale Language Models (LLM) have achieved SOTA
performances on a variety of NLP tasks, its performance on NER is still
significantly below supervised baselines. This is due to the gap between the
two tasks the NER and LLMs: the former is a sequence labeling task in nature
while the latter is a text-generation model.
  In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the
gap by transforming the sequence labeling task to a generation task that can be
easily adapted by LLMs e.g., the task of finding location entities in the input
text ""Columbus is a city"" is transformed to generate the text sequence
""@@Columbus## is a city"", where special tokens @@## marks the entity to
extract. To efficiently address the ""hallucination"" issue of LLMs, where LLMs
have a strong inclination to over-confidently label NULL inputs as entities, we
propose a self-verification strategy by prompting LLMs to ask itself whether
the extracted entities belong to a labeled entity tag.
  We conduct experiments on five widely adopted NER datasets, and GPT-NER
achieves comparable performances to fully supervised baselines, which is the
first time as far as we are concerned. More importantly, we find that GPT-NER
exhibits a greater ability in the low-resource and few-shot setups, when the
amount of training data is extremely scarce, GPT-NER performs significantly
better than supervised models. This demonstrates the capabilities of GPT-NER in
real-world NER applications where the number of labeled examples is limited.",2023-04-20
"Towards Zero-Shot Personalized Table-to-Text Generation with Contrastive
  Persona Distillation",2023-04-18 11:32:33+00:00,http://arxiv.org/abs/2304.08911v1,"Haolan Zhan, Xuming Lin, Shaobo Cui, Zhongzhou Zhao, Wei Zhou, Haiqing Chen",cs.CL,table2text,"Existing neural methods have shown great potentials towards generating
informative text from structured tabular data as well as maintaining high
content fidelity. However, few of them shed light on generating personalized
expressions, which often requires well-aligned persona-table-text datasets that
are difficult to obtain. To overcome these obstacles, we explore personalized
table-to-text generation under a zero-shot setting, by assuming no well-aligned
persona-table-text triples are required during training. To this end, we
firstly collect a set of unpaired persona information and then propose a
semi-supervised approach with contrastive persona distillation (S2P-CPD) to
generate personalized context. Specifically, tabular data and persona
information are firstly represented as latent variables separately. Then, we
devise a latent space fusion technique to distill persona information into the
table representation. Besides, a contrastive-based discriminator is employed to
guarantee the style consistency between the generated context and its
corresponding persona. Experimental results on two benchmarks demonstrate
S2P-CPD's ability on keeping both content fidelity and personalized
expressions.",2023-04-18
"LongForm: Optimizing Instruction Tuning for Long Text Generation with
  Corpus Extraction",2023-04-17 17:36:35+00:00,http://arxiv.org/abs/2304.08460v1,"Abdullatif Köksal, Timo Schick, Anna Korhonen, Hinrich Schütze","cs.CL, cs.AI, cs.LG",table2text,"Instruction tuning enables language models to generalize more effectively and
better follow user intent. However, obtaining instruction data can be costly
and challenging. Prior works employ methods such as expensive human annotation,
crowd-sourced datasets with alignment issues, or generating noisy examples via
LLMs. We introduce the LongForm dataset, which is created by leveraging English
corpus examples with augmented instructions. We select a diverse set of
human-written documents from existing corpora such as C4 and Wikipedia and
generate instructions for the given documents via LLMs. This approach provides
a cheaper and cleaner instruction-tuning dataset and one suitable for long text
generation. We finetune T5, OPT, and LLaMA models on our dataset and show that
even smaller LongForm models have good generalization capabilities for text
generation. Our models outperform 10x larger language models without
instruction tuning on various tasks such as story/recipe generation and
long-form question answering. Moreover, LongForm models outperform prior
instruction-tuned models such as FLAN-T5 and Alpaca by a large margin. Finally,
our models can effectively follow and answer multilingual instructions; we
demonstrate this for news generation. We publicly release our data and models:
https://github.com/akoksal/LongForm.",2023-04-17
"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and
  Dataset",2023-04-17 15:08:15+00:00,http://arxiv.org/abs/2304.08345v1,"Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang, Jing Liu","cs.LG, cs.CL, cs.CV, cs.MM, eess.AS",table2text,"In this paper, we propose a Vision-Audio-Language Omni-peRception pretraining
model (VALOR) for multi-modal understanding and generation. Different from
widely-studied vision-language pretraining models, VALOR jointly models
relationships of vision, audio and language in an end-to-end manner. It
contains three separate encoders for single modality representations, and a
decoder for multimodal conditional text generation. We design two pretext tasks
to pretrain VALOR model, including Multimodal Grouping Alignment (MGA) and
Multimodal Grouping Captioning (MGC). MGA projects vision, language and audio
to the same common space, building vision-language, audio-language and
audiovisual-language alignment simultaneously. MGC learns how to generate text
tokens in conditions of vision, audio or their both. To promote
vision-audio-language pretraining research, we construct a large-scale
high-quality tri-modality dataset named VALOR-1M, which contains 1M audiable
videos with human annotated audiovisual captions. Extensive experiments show
that VALOR can learn strong multimodal correlations and be generalized to
various downstream tasks (e.g., retrieval, captioning and question answering),
with different input modalities (e.g., vision-language, audio-language and
audiovisual-language). VALOR achieves new state-of-the-art performances on
series of public cross-modality benchmarks. Code and data are available at
project page https://casia-iva-group.github.io/projects/VALOR.",2023-04-17
"VISAR: A Human-AI Argumentative Writing Assistant with Visual
  Programming and Rapid Draft Prototyping",2023-04-16 15:29:03+00:00,http://arxiv.org/abs/2304.07810v1,"Zheng Zhang, Jie Gao, Ranjodh Singh Dhaliwal, Toby Jia-Jun Li","cs.HC, cs.AI, cs.CL, cs.LG",table2text,"In argumentative writing, writers must brainstorm hierarchical writing goals,
ensure the persuasiveness of their arguments, and revise and organize their
plans through drafting. Recent advances in large language models (LLMs) have
made interactive text generation through a chat interface (e.g., ChatGPT)
possible. However, this approach often neglects implicit writing context and
user intent, lacks support for user control and autonomy, and provides limited
assistance for sensemaking and revising writing plans. To address these
challenges, we introduce VISAR, an AI-enabled writing assistant system designed
to help writers brainstorm and revise hierarchical goals within their writing
context, organize argument structures through synchronized text editing and
visual programming, and enhance persuasiveness with argumentation spark
recommendations. VISAR allows users to explore, experiment with, and validate
their writing plans using automatic draft prototyping. A controlled lab study
confirmed the usability and effectiveness of VISAR in facilitating the
argumentative writing planning process.",2023-04-16
"ArguGPT: evaluating, understanding and identifying argumentative essays
  generated by GPT models",2023-04-16 01:50:26+00:00,http://arxiv.org/abs/2304.07666v1,"Yikang Liu, Ziyin Zhang, Wanyang Zhang, Shisen Yue, Xiaojing Zhao, Xinyuan Cheng, Yiwen Zhang, Hai Hu",cs.CL,table2text,"AI generated content (AIGC) presents considerable challenge to educators
around the world. Instructors need to be able to detect such text generated by
large language models, either with the naked eye or with the help of some
tools. There is also growing need to understand the lexical, syntactic and
stylistic features of AIGC. To address these challenges in English language
teaching, we first present ArguGPT, a balanced corpus of 4,038 argumentative
essays generated by 7 GPT models in response to essay prompts from three
sources: (1) in-class or homework exercises, (2) TOEFL and (3) GRE writing
tasks. Machine-generated texts are paired with roughly equal number of
human-written essays with three score levels matched in essay prompts. We then
hire English instructors to distinguish machine essays from human ones. Results
show that when first exposed to machine-generated essays, the instructors only
have an accuracy of 61% in detecting them. But the number rises to 67% after
one round of minimal self-training. Next, we perform linguistic analyses of
these essays, which show that machines produce sentences with more complex
syntactic structures while human essays tend to be lexically more complex.
Finally, we test existing AIGC detectors and build our own detectors using SVMs
and RoBERTa. Results suggest that a RoBERTa fine-tuned with the training set of
ArguGPT achieves above 90% accuracy in both essay- and sentence-level
classification. To the best of our knowledge, this is the first comprehensive
analysis of argumentative essays produced by generative large language models.
Machine-authored essays in ArguGPT and our models will be made publicly
available at https://github.com/huhailinguist/ArguGPT",2023-04-16
"ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large
  Language Models in Multilingual Learning",2023-04-12 05:08:52+00:00,http://arxiv.org/abs/2304.05613v1,"Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, Thien Huu Nguyen","cs.CL, cs.AI",table2text,"Over the last few years, large language models (LLMs) have emerged as the
most important breakthroughs in natural language processing (NLP) that
fundamentally transform research and developments in the field. ChatGPT
represents one of the most exciting LLM systems developed recently to showcase
impressive skills for language generation and highly attract public attention.
Among various exciting applications discovered for ChatGPT in English, the
model can process and generate texts for multiple languages due to its
multilingual training data. Given the broad adoption of ChatGPT for English in
different problems and areas, a natural question is whether ChatGPT can also be
applied effectively for other languages or it is necessary to develop more
language-specific technologies. The answer to this question requires a thorough
evaluation of ChatGPT over multiple tasks with diverse languages and large
datasets (i.e., beyond reported anecdotes), which is still missing or limited
in current research. Our work aims to fill this gap for the evaluation of
ChatGPT and similar LLMs to provide more comprehensive information for
multilingual NLP applications. While this work will be an ongoing effort to
include additional experiments in the future, our current paper evaluates
ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,
low, and extremely low resources. We also focus on the zero-shot learning
setting for ChatGPT to improve reproducibility and better simulate the
interactions of general users. Compared to the performance of previous models,
our extensive experimental results demonstrate a worse performance of ChatGPT
for different NLP tasks and languages, calling for further research to develop
better models and understanding for multilingual learning.",2023-04-12
Automated Reading Passage Generation with OpenAI's Large Language Model,2023-04-10 14:30:39+00:00,http://arxiv.org/abs/2304.04616v1,"Ummugul Bezirhan, Matthias von Davier","cs.CL, cs.AI",table2text,"The widespread usage of computer-based assessments and individualized
learning platforms has resulted in an increased demand for the rapid production
of high-quality items. Automated item generation (AIG), the process of using
item models to generate new items with the help of computer technology, was
proposed to reduce reliance on human subject experts at each step of the
process. AIG has been used in test development for some time. Still, the use of
machine learning algorithms has introduced the potential to improve the
efficiency and effectiveness of the process greatly. The approach presented in
this paper utilizes OpenAI's latest transformer-based language model, GPT-3, to
generate reading passages. Existing reading passages were used in carefully
engineered prompts to ensure the AI-generated text has similar content and
structure to a fourth-grade reading passage. For each prompt, we generated
multiple passages, the final passage was selected according to the Lexile score
agreement with the original passage. In the final round, the selected passage
went through a simple revision by a human editor to ensure the text was free of
any grammatical and factual errors. All AI-generated passages, along with
original passages were evaluated by human judges according to their coherence,
appropriateness to fourth graders, and readability.",2023-04-10
"Decoder-Only or Encoder-Decoder? Interpreting Language Model as a
  Regularized Encoder-Decoder",2023-04-08 15:44:29+00:00,http://arxiv.org/abs/2304.04052v1,"Zihao Fu, Wai Lam, Qian Yu, Anthony Man-Cho So, Shengding Hu, Zhiyuan Liu, Nigel Collier","cs.CL, cs.AI, cs.LG",table2text,"The sequence-to-sequence (seq2seq) task aims at generating the target
sequence based on the given input source sequence. Traditionally, most of the
seq2seq task is resolved by the Encoder-Decoder framework which requires an
encoder to encode the source sequence and a decoder to generate the target
text. Recently, a bunch of new approaches have emerged that apply decoder-only
language models directly to the seq2seq task. Despite the significant
advancements in applying language models to the seq2seq task, there is still a
lack of thorough analysis on the effectiveness of the decoder-only language
model architecture. This paper aims to address this gap by conducting a
detailed comparison between the encoder-decoder architecture and the
decoder-only language model framework through the analysis of a regularized
encoder-decoder structure. This structure is designed to replicate all
behaviors in the classical decoder-only language model but has an encoder and a
decoder making it easier to be compared with the classical encoder-decoder
structure. Based on the analysis, we unveil the attention degeneration problem
in the language model, namely, as the generation step number grows, less and
less attention is focused on the source sequence. To give a quantitative
understanding of this problem, we conduct a theoretical sensitivity analysis of
the attention output with respect to the source input. Grounded on our
analysis, we propose a novel partial attention language model to solve the
attention degeneration problem. Experimental results on machine translation,
summarization, and data-to-text generation tasks support our analysis and
demonstrate the effectiveness of our proposed model.",2023-04-08
"Beyond Privacy: Navigating the Opportunities and Challenges of Synthetic
  Data",2023-04-07 16:38:40+00:00,http://arxiv.org/abs/2304.03722v1,"Boris van Breugel, Mihaela van der Schaar",cs.LG,table2text,"Generating synthetic data through generative models is gaining interest in
the ML community and beyond. In the past, synthetic data was often regarded as
a means to private data release, but a surge of recent papers explore how its
potential reaches much further than this -- from creating more fair data to
data augmentation, and from simulation to text generated by ChatGPT. In this
perspective we explore whether, and how, synthetic data may become a dominant
force in the machine learning world, promising a future where datasets can be
tailored to individual needs. Just as importantly, we discuss which fundamental
challenges the community needs to overcome for wider relevance and application
of synthetic data -- the most important of which is quantifying how much we can
trust any finding or prediction drawn from synthetic data.",2023-04-07
Measuring and Manipulating Knowledge Representations in Language Models,2023-04-03 06:24:10+00:00,http://arxiv.org/abs/2304.00740v1,"Evan Hernandez, Belinda Z. Li, Jacob Andreas",cs.CL,table2text,"Neural language models (LMs) represent facts about the world described by
text. Sometimes these facts derive from training data (in most LMs, a
representation of the word banana encodes the fact that bananas are fruits).
Sometimes facts derive from input text itself (a representation of the sentence
""I poured out the bottle"" encodes the fact that the bottle became empty). Tools
for inspecting and modifying LM fact representations would be useful almost
everywhere LMs are used: making it possible to update them when the world
changes, to localize and remove sources of bias, and to identify errors in
generated text. We describe REMEDI, an approach for querying and modifying
factual knowledge in LMs. REMEDI learns a map from textual queries to fact
encodings in an LM's internal representation system. These encodings can be
used as knowledge editors: by adding them to LM hidden representations, we can
modify downstream generation to be consistent with new facts. REMEDI encodings
can also be used as model probes: by comparing them to LM representations, we
can ascertain what properties LMs attribute to mentioned entities, and predict
when they will generate outputs that conflict with background knowledge or
input text. REMEDI thus links work on probing, prompting, and model editing,
and offers steps toward general tools for fine-grained inspection and control
of knowledge in LMs.",2023-04-03
Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts,2023-03-31 20:33:03+00:00,http://arxiv.org/abs/2304.00121v1,"Ryan Koo, Anna Martin, Linghe Wang, Dongyeop Kang","cs.CL, cs.HC",table2text,"Scholarly writing presents a complex space that generally follows a
methodical procedure to plan and produce both rationally sound and creative
compositions. Recent works involving large language models (LLM) demonstrate
considerable success in text generation and revision tasks; however, LLMs still
struggle to provide structural and creative feedback on the document level that
is crucial to academic writing. In this paper, we introduce a novel taxonomy
that categorizes scholarly writing behaviors according to intention, writer
actions, and the information types of the written data. We also provide
ManuScript, an original dataset annotated with a simplified version of our
taxonomy to show writer actions and the intentions behind them. Motivated by
cognitive writing theory, our taxonomy for scientific papers includes three
levels of categorization in order to trace the general writing flow and
identify the distinct writer activities embedded within each higher-level
process. ManuScript intends to provide a complete picture of the scholarly
writing process by capturing the linearity and non-linearity of writing
trajectory, such that writing assistants can provide stronger feedback and
suggestions on an end-to-end level. The collected writing trajectories are
viewed at https://minnesotanlp.github.io/REWARD_demo/",2023-03-31
Assessing Language Model Deployment with Risk Cards,2023-03-31 16:45:42+00:00,http://arxiv.org/abs/2303.18190v1,"Leon Derczynski, Hannah Rose Kirk, Vidhisha Balachandran, Sachin Kumar, Yulia Tsvetkov, M. R. Leiser, Saif Mohammad",cs.CL,table2text,"This paper introduces RiskCards, a framework for structured assessment and
documentation of risks associated with an application of language models. As
with all language, text generated by language models can be harmful, or used to
bring about harm. Automating language generation adds both an element of scale
and also more subtle or emergent undesirable tendencies to the generated text.
Prior work establishes a wide variety of language model harms to many different
actors: existing taxonomies identify categories of harms posed by language
models; benchmarks establish automated tests of these harms; and documentation
standards for models, tasks and datasets encourage transparent reporting.
However, there is no risk-centric framework for documenting the complexity of a
landscape in which some risks are shared across models and contexts, while
others are specific, and where certain conditions may be required for risks to
manifest as harms. RiskCards address this methodological gap by providing a
generic framework for assessing the use of a given language model in a given
scenario. Each RiskCard makes clear the routes for the risk to manifest harm,
their placement in harm taxonomies, and example prompt-output pairs. While
RiskCards are designed to be open-source, dynamic and participatory, we present
a ""starter set"" of RiskCards taken from a broad literature survey, each of
which details a concrete risk presentation. Language model RiskCards initiate a
community knowledge base which permits the mapping of risks and harms to a
specific model or its application scenario, ultimately contributing to a
better, safer and shared understanding of the risk landscape.",2023-03-31
Prefix tuning for automated audio captioning,2023-03-30 16:01:28+00:00,http://arxiv.org/abs/2303.17489v2,"Minkyu Kim, Kim Sung-Bin, Tae-Hyun Oh","eess.AS, cs.MM, cs.SD",table2text,"Audio captioning aims to generate text descriptions from environmental
sounds. One challenge of audio captioning is the difficulty of the
generalization due to the lack of audio-text paired training data. In this
work, we propose a simple yet effective method of dealing with small-scaled
datasets by leveraging a pre-trained language model. We keep the language model
frozen to maintain the expressivity for text generation, and we only learn to
extract global and temporal features from the input audio. To bridge a modality
gap between the audio features and the language model, we employ mapping
networks that translate audio features to the continuous vectors the language
model can understand, called prefixes. We evaluate our proposed method on the
Clotho and AudioCaps dataset and show our method outperforms prior arts in
diverse experimental settings.",2023-03-30
"Humans in Humans Out: On GPT Converging Toward Common Sense in both
  Success and Failure",2023-03-30 10:32:18+00:00,http://arxiv.org/abs/2303.17276v1,"Philipp Koralus, Vincent Wang-Maścianica","cs.AI, cs.CL, cs.HC, cs.LG, 00, 68, I.2.0; I.2.6",table2text,"Increase in computational scale and fine-tuning has seen a dramatic
improvement in the quality of outputs of large language models (LLMs) like GPT.
Given that both GPT-3 and GPT-4 were trained on large quantities of
human-generated text, we might ask to what extent their outputs reflect
patterns of human thinking, both for correct and incorrect cases. The Erotetic
Theory of Reason (ETR) provides a symbolic generative model of both human
success and failure in thinking, across propositional, quantified, and
probabilistic reasoning, as well as decision-making. We presented GPT-3,
GPT-3.5, and GPT-4 with 61 central inference and judgment problems from a
recent book-length presentation of ETR, consisting of experimentally verified
data-points on human judgment and extrapolated data-points predicted by ETR,
with correct inference patterns as well as fallacies and framing effects (the
ETR61 benchmark). ETR61 includes classics like Wason's card task, illusory
inferences, the decoy effect, and opportunity-cost neglect, among others. GPT-3
showed evidence of ETR-predicted outputs for 59% of these examples, rising to
77% in GPT-3.5 and 75% in GPT-4. Remarkably, the production of human-like
fallacious judgments increased from 18% in GPT-3 to 33% in GPT-3.5 and 34% in
GPT-4. This suggests that larger and more advanced LLMs may develop a tendency
toward more human-like mistakes, as relevant thought patterns are inherent in
human-produced training data. According to ETR, the same fundamental patterns
are involved both in successful and unsuccessful ordinary reasoning, so that
the ""bad"" cases could paradoxically be learned from the ""good"" cases. We
further present preliminary evidence that ETR-inspired prompt engineering could
reduce instances of these mistakes.",2023-03-30
Foundation Models and Fair Use,2023-03-28 03:58:40+00:00,http://arxiv.org/abs/2303.15715v1,"Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, Percy Liang","cs.CY, cs.AI, cs.LG",table2text,"Existing foundation models are trained on copyrighted material. Deploying
these models can pose both legal and ethical risks when data creators fail to
receive appropriate attribution or compensation. In the United States and
several other countries, copyrighted content may be used to build foundation
models without incurring liability due to the fair use doctrine. However, there
is a caveat: If the model produces output that is similar to copyrighted data,
particularly in scenarios that affect the market of that data, fair use may no
longer apply to the output of the model. In this work, we emphasize that fair
use is not guaranteed, and additional work may be necessary to keep model
development and deployment squarely in the realm of fair use. First, we survey
the potential risks of developing and deploying foundation models based on
copyrighted content. We review relevant U.S. case law, drawing parallels to
existing and potential applications for generating text, source code, and
visual art. Experiments confirm that popular foundation models can generate
content considerably similar to copyrighted material. Second, we discuss
technical mitigations that can help foundation models stay in line with fair
use. We argue that more research is needed to align mitigation strategies with
the current state of the law. Lastly, we suggest that the law and technical
mitigations should co-evolve. For example, coupled with other policy
mechanisms, the law could more explicitly consider safe harbors when strong
technical tools are used to mitigate infringement harms. This co-evolution may
help strike a balance between intellectual property and innovation, which
speaks to the original goal of fair use. But we emphasize that the strategies
we describe here are not a panacea and more work is needed to develop policies
that address the potential harms of foundation models.",2023-03-28
GPT is becoming a Turing machine: Here are some ways to program it,2023-03-25 00:43:41+00:00,http://arxiv.org/abs/2303.14310v1,"Ana Jojic, Zhen Wang, Nebojsa Jojic",cs.CL,table2text,"We demonstrate that, through appropriate prompting, GPT-3 family of models
can be triggered to perform iterative behaviours necessary to execute (rather
than just write or recall) programs that involve loops, including several
popular algorithms found in computer science curricula or software developer
interviews. We trigger execution and description of Iterations by Regimenting
Self-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong
repetitive structure in an example of an execution path of a target program for
one particular input, 2) Prompting with fragments of execution paths, and 3)
Explicitly forbidding (skipping) self-attention to parts of the generated text.
On a dynamic program execution, IRSA leads to larger accuracy gains than
replacing the model with the much more powerful GPT-4. IRSA has promising
applications in education, as the prompts and responses resemble student
assignments in data structures and algorithms classes. Our findings hold
implications for evaluating LLMs, which typically target the in-context
learning: We show that prompts that may not even cover one full task example
can trigger algorithmic behaviour, allowing solving problems previously thought
of as hard for LLMs, such as logical puzzles. Consequently, prompt design plays
an even more critical role in LLM performance than previously recognized.",2023-03-25
CoBIT: A Contrastive Bi-directional Image-Text Generation Model,2023-03-23 17:24:31+00:00,http://arxiv.org/abs/2303.13455v1,"Haoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, Jiahui Yu","cs.CV, cs.CL",table2text,"The field of vision and language has witnessed a proliferation of pre-trained
foundation models. Most existing methods are independently pre-trained with
contrastive objective like CLIP, image-to-text generative objective like PaLI,
or text-to-image generative objective like Parti. However, the three objectives
can be pre-trained on the same data, image-text pairs, and intuitively they
complement each other as contrasting provides global alignment capacity and
generation grants fine-grained understanding. In this work, we present a
Contrastive Bi-directional Image-Text generation model (CoBIT), which attempts
to unify the three pre-training objectives in one framework. Specifically,
CoBIT employs a novel unicoder-decoder structure, consisting of an image
unicoder, a text unicoder and a cross-modal decoder. The image/text unicoders
can switch between encoding and decoding in different tasks, enabling
flexibility and shared knowledge that benefits both image-to-text and
text-to-image generations. CoBIT achieves superior performance in image
understanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)
and text-based content creation, particularly in zero-shot scenarios. For
instance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in
zero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.",2023-03-23
"Paraphrasing evades detectors of AI-generated text, but retrieval is an
  effective defense",2023-03-23 16:29:27+00:00,http://arxiv.org/abs/2303.13408v1,"Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer","cs.CL, cs.CR, cs.LG",table2text,"To detect the deployment of large language models for malicious use cases
(e.g., fake content creation or academic plagiarism), several approaches have
recently been proposed for identifying AI-generated text via watermarks or
statistical irregularities. How robust are these detection algorithms to
paraphrases of AI-generated text? To stress test these detectors, we first
train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase
paragraphs, optionally leveraging surrounding text (e.g., user-written prompts)
as context. DIPPER also uses scalar knobs to control the amount of lexical
diversity and reordering in the paraphrases. Paraphrasing text generated by
three large language models (including GPT3.5-davinci-003) with DIPPER
successfully evades several detectors, including watermarking, GPTZero,
DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the
detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false
positive rate of 1%), without appreciably modifying the input semantics. To
increase the robustness of AI-generated text detection to paraphrase attacks,
we introduce a simple defense that relies on retrieving semantically-similar
generations and must be maintained by a language model API provider. Given a
candidate text, our algorithm searches a database of sequences previously
generated by the API, looking for sequences that match the candidate text
within a certain threshold. We empirically verify our defense using a database
of 15M generations from a fine-tuned T5-XXL model and find that it can detect
80% to 97% of paraphrased generations across different settings, while only
classifying 1% of human-written sequences as AI-generated. We will open source
our code, model and data for future research.",2023-03-23
Compositional Zero-Shot Domain Transfer with Text-to-Text Models,2023-03-23 15:58:41+00:00,http://arxiv.org/abs/2303.13386v1,"Fangyu Liu, Qianchu Liu, Shruthi Bannur, Fernando Pérez-García, Naoto Usuyama, Sheng Zhang, Tristan Naumann, Aditya Nori, Hoifung Poon, Javier Alvarez-Valle, Ozan Oktay, Stephanie L. Hyland","cs.CL, cs.LG",table2text,"Label scarcity is a bottleneck for improving task performance in specialised
domains. We propose a novel compositional transfer learning framework (DoT5 -
domain compositional zero-shot T5) for zero-shot domain transfer. Without
access to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of
unlabelled in-domain free text) and task knowledge (from task training on more
readily available general-domain data) in a multi-task manner. To improve the
transferability of task training, we design a strategy named NLGU: we
simultaneously train NLG for in-domain label-to-data generation which enables
data augmentation for self-finetuning and NLU for label prediction. We evaluate
DoT5 on the biomedical domain and the resource-lean subdomain of radiology,
focusing on NLI, text summarisation and embedding learning. DoT5 demonstrates
the effectiveness of compositional transfer learning through multi-task
learning. In particular, DoT5 outperforms the current SOTA in zero-shot
transfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with
ablations and a case study demonstrating its ability to solve challenging NLI
examples requiring in-domain expertise.",2023-03-23
JaCoText: A Pretrained Model for Java Code-Text Generation,2023-03-22 19:01:25+00:00,http://arxiv.org/abs/2303.12869v1,"Jessica López Espejel, Mahaman Sanoussi Yahaya Alassan, Walid Dahhane, El Hassane Ettifouri",cs.CL,table2text,"Pretrained transformer-based models have shown high performance in natural
language generation task. However, a new wave of interest has surged: automatic
programming language generation. This task consists of translating natural
language instructions to a programming code. Despite the fact that well-known
pretrained models on language generation have achieved good performance in
learning programming languages, effort is still needed in automatic code
generation. In this paper, we introduce JaCoText, a model based on Transformers
neural network. It aims to generate java source code from natural language
text. JaCoText leverages advantages of both natural language and code
generation models. More specifically, we study some findings from the state of
the art and use them to (1) initialize our model from powerful pretrained
models, (2) explore additional pretraining on our java dataset, (3) carry out
experiments combining the unimodal and bimodal data in the training, and (4)
scale the input and output length during the fine-tuning of the model.
Conducted experiments on CONCODE dataset show that JaCoText achieves new
state-of-the-art results.",2023-03-22
"Chinese Intermediate English Learners outdid ChatGPT in deep cohesion:
  Evidence from English narrative writing",2023-03-21 12:55:54+00:00,http://arxiv.org/abs/2303.11812v1,"Tongquan Zhou, Siyi Cao, Siruo Zhou, Yao Zhang, Aijing He",cs.CL,table2text,"ChatGPT is a publicly available chatbot that can quickly generate texts on
given topics, but it is unknown whether the chatbot is really superior to human
writers in all aspects of writing and whether its writing quality can be
prominently improved on the basis of updating commands. Consequently, this
study compared the writing performance on a narrative topic by ChatGPT and
Chinese intermediate English (CIE) learners so as to reveal the chatbot's
advantage and disadvantage in writing. The data were analyzed in terms of five
discourse components using Coh-Metrix (a special instrument for analyzing
language discourses), and the results revealed that ChatGPT performed better
than human writers in narrativity, word concreteness, and referential cohesion,
but worse in syntactic simplicity and deep cohesion in its initial version.
After more revision commands were updated, while the resulting version was
facilitated in syntactic simplicity, yet it is still lagged far behind CIE
learners' writing in deep cohesion. In addition, the correlation analysis of
the discourse components suggests that narrativity was correlated with
referential cohesion in both ChatGPT and human writers, but the correlations
varied within each group.",2023-03-21
Code-Switching Text Generation and Injection in Mandarin-English ASR,2023-03-20 09:13:27+00:00,http://arxiv.org/abs/2303.10949v1,"Haibin Yu, Yuxuan Hu, Yao Qian, Ma Jin, Linquan Liu, Shujie Liu, Yu Shi, Yanmin Qian, Edward Lin, Michael Zeng","eess.AS, cs.CL, cs.SD",table2text,"Code-switching speech refers to a means of expression by mixing two or more
languages within a single utterance. Automatic Speech Recognition (ASR) with
End-to-End (E2E) modeling for such speech can be a challenging task due to the
lack of data. In this study, we investigate text generation and injection for
improving the performance of an industry commonly-used streaming model,
Transformer-Transducer (T-T), in Mandarin-English code-switching speech
recognition. We first propose a strategy to generate code-switching text data
and then investigate injecting generated text into T-T model explicitly by
Text-To-Speech (TTS) conversion or implicitly by tying speech and text latent
spaces. Experimental results on the T-T model trained with a dataset containing
1,800 hours of real Mandarin-English code-switched speech show that our
approaches to inject generated code-switching text significantly boost the
performance of T-T models, i.e., 16% relative Token-based Error Rate (TER)
reduction averaged on three evaluation sets, and the approach of tying speech
and text latent spaces is superior to that of TTS conversion on the evaluation
set which contains more homogeneous data with the training set.",2023-03-20
"SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language
  Models",2023-03-18 17:56:01+00:00,http://arxiv.org/abs/2303.10464v1,"Vithursan Thangarasa, Abhay Gupta, William Marshall, Tianda Li, Kevin Leong, Dennis DeCoste, Sean Lie, Shreyas Saxena","cs.LG, cs.CL",table2text,"The pre-training and fine-tuning paradigm has contributed to a number of
breakthroughs in Natural Language Processing (NLP). Instead of directly
training on a downstream task, language models are first pre-trained on large
datasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then
fine-tuned on task-specific data (e.g., natural language generation, text
summarization, etc.). Scaling the model and dataset size has helped improve the
performance of LLMs, but unfortunately, this also leads to highly prohibitive
computational costs. Pre-training LLMs often require orders of magnitude more
FLOPs than fine-tuning and the model capacity often remains the same between
the two phases. To achieve training efficiency w.r.t training FLOPs, we propose
to decouple the model capacity between the two phases and introduce Sparse
Pre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits
of using unstructured weight sparsity to train only a subset of weights during
pre-training (Sparse Pre-training) and then recover the representational
capacity by allowing the zeroed weights to learn (Dense Fine-tuning). We
demonstrate that we can induce up to 75% sparsity into a 1.3B parameter GPT-3
XL model resulting in a 2.5x reduction in pre-training FLOPs, without a
significant loss in accuracy on the downstream tasks relative to the dense
baseline. By rigorously evaluating multiple downstream tasks, we also establish
a relationship between sparsity, task complexity, and dataset size. Our work
presents a promising direction to train large GPT models at a fraction of the
training FLOPs using weight sparsity while retaining the benefits of
pre-trained textual representations for downstream tasks.",2023-03-18
HIVE: Harnessing Human Feedback for Instructional Visual Editing,2023-03-16 19:47:41+00:00,http://arxiv.org/abs/2303.09618v1,"Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu","cs.CV, cs.AI, cs.CL, cs.HC, cs.LG",table2text,"Incorporating human feedback has been shown to be crucial to align text
generated by large language models to human preferences. We hypothesize that
state-of-the-art instructional image editing models, where outputs are
generated based on an input image and an editing instruction, could similarly
benefit from human feedback, as their outputs may not adhere to the correct
instructions and preferences of users. In this paper, we present a novel
framework to harness human feedback for instructional visual editing (HIVE).
Specifically, we collect human feedback on the edited images and learn a reward
function to capture the underlying user preferences. We then introduce scalable
diffusion model fine-tuning methods that can incorporate human preferences
based on the estimated reward. Besides, to mitigate the bias brought by the
limitation of data, we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset to boost the
performance of instructional image editing. We conduct extensive empirical
experiments quantitatively and qualitatively, showing that HIVE is favored over
previous state-of-the-art instructional image editing approaches by a large
margin.",2023-03-16
Input-length-shortening and text generation via attention values,2023-03-14 02:11:24+00:00,http://arxiv.org/abs/2303.07585v1,"Neşet Özkan Tan, Alex Yuxuan Peng, Joshua Bensemann, Qiming Bao, Tim Hartill, Mark Gahegan, Michael Witbrock",cs.CL,table2text,"Identifying words that impact a task's performance more than others is a
challenge in natural language processing. Transformers models have recently
addressed this issue by incorporating an attention mechanism that assigns
greater attention (i.e., relevance) scores to some words than others. Because
of the attention mechanism's high computational cost, transformer models
usually have an input-length limitation caused by hardware constraints. This
limitation applies to many transformers, including the well-known bidirectional
encoder representations of the transformer (BERT) model. In this paper, we
examined BERT's attention assignment mechanism, focusing on two questions: (1)
How can attention be employed to reduce input length? (2) How can attention be
used as a control mechanism for conditional text generation? We investigated
these questions in the context of a text classification task. We discovered
that BERT's early layers assign more critical attention scores for text
classification tasks compared to later layers. We demonstrated that the first
layer's attention sums could be used to filter tokens in a given sequence,
considerably decreasing the input length while maintaining good test accuracy.
We also applied filtering, which uses a compute-efficient semantic similarities
algorithm, and discovered that retaining approximately 6\% of the original
sequence is sufficient to obtain 86.5\% accuracy. Finally, we showed that we
could generate data in a stable manner and indistinguishable from the original
one by only using a small percentage (10\%) of the tokens with high attention
scores according to BERT's first layer.",2023-03-14
Diffusion Models for Non-autoregressive Text Generation: A Survey,2023-03-12 05:11:09+00:00,http://arxiv.org/abs/2303.06574v1,"Yifan Li, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen",cs.CL,table2text,"Non-autoregressive (NAR) text generation has attracted much attention in the
field of natural language processing, which greatly reduces the inference
latency but has to sacrifice the generation accuracy. Recently, diffusion
models, a class of latent variable generative models, have been introduced into
NAR text generation, showing improved generation quality. In this survey, we
review the recent progress in diffusion models for NAR text generation. As the
background, we first present the general definition of diffusion models and the
text diffusion models, and then discuss their merits for NAR generation. As the
core content, we further introduce two mainstream diffusion models in existing
text diffusion works, and review the key designs of the diffusion process.
Moreover, we discuss the utilization of pre-trained language models (PLMs) for
text diffusion models and introduce optimization techniques for text data.
Finally, we discuss several promising directions and conclude this paper. Our
survey aims to provide researchers with a systematic reference of related
research on text diffusion models for NAR generation.",2023-03-12
"ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and
  Multilingual Natural Language Generation",2023-03-11 17:14:33+00:00,http://arxiv.org/abs/2303.06458v1,"Bang Yang, Fenglin Liu, Yuexian Zou, Xian Wu, Yaowei Wang, David A. Clifton","cs.CL, cs.AI, cs.CV",table2text,"Natural Language Generation (NLG) accepts input data in the form of images,
videos, or text and generates corresponding natural language text as output.
Existing NLG methods mainly adopt a supervised approach and rely heavily on
coupled data-to-text pairs. However, for many targeted scenarios and for
non-English languages, sufficient quantities of labeled data are often not
available. To relax the dependency on labeled data of downstream tasks, we
propose an intuitive and effective zero-shot learning framework, ZeroNLG, which
can deal with multiple NLG tasks, including image-to-text (image captioning),
video-to-text (video captioning), and text-to-text (neural machine
translation), across English, Chinese, German, and French within a unified
framework. ZeroNLG does not require any labeled downstream pairs for training.
During training, ZeroNLG (i) projects different domains (across modalities and
languages) to corresponding coordinates in a shared common latent space; (ii)
bridges different domains by aligning their corresponding coordinates in this
space; and (iii) builds an unsupervised multilingual auto-encoder to learn to
generate text by reconstructing the input text given its coordinate in shared
latent space. Consequently, during inference, based on the data-to-text
pipeline, ZeroNLG can generate target sentences across different languages
given the coordinate of input data in the common space. Within this unified
framework, given visual (imaging or video) data as input, ZeroNLG can perform
zero-shot visual captioning; given textual sentences as input, ZeroNLG can
perform zero-shot machine translation. We present the results of extensive
experiments on twelve NLG tasks, showing that, without using any labeled
downstream pairs for training, ZeroNLG generates high-quality and believable
outputs and significantly outperforms existing zero-shot methods.",2023-03-11
"Reinforcement Learning-based Counter-Misinformation Response Generation:
  A Case Study of COVID-19 Vaccine Misinformation",2023-03-11 15:55:01+00:00,http://arxiv.org/abs/2303.06433v1,"Bing He, Mustaque Ahamad, Srijan Kumar","cs.SI, cs.LG",table2text,"The spread of online misinformation threatens public health, democracy, and
the broader society. While professional fact-checkers form the first line of
defense by fact-checking popular false claims, they do not engage directly in
conversations with misinformation spreaders. On the other hand, non-expert
ordinary users act as eyes-on-the-ground who proactively counter misinformation
-- recent research has shown that 96% counter-misinformation responses are made
by ordinary users. However, research also found that 2/3 times, these responses
are rude and lack evidence. This work seeks to create a counter-misinformation
response generation model to empower users to effectively correct
misinformation. This objective is challenging due to the absence of datasets
containing ground-truth of ideal counter-misinformation responses, and the lack
of models that can generate responses backed by communication theories. In this
work, we create two novel datasets of misinformation and counter-misinformation
response pairs from in-the-wild social media and crowdsourcing from
college-educated students. We annotate the collected data to distinguish poor
from ideal responses that are factual, polite, and refute misinformation. We
propose MisinfoCorrect, a reinforcement learning-based framework that learns to
generate counter-misinformation responses for an input misinformation post. The
model rewards the generator to increase the politeness, factuality, and
refutation attitude while retaining text fluency and relevancy. Quantitative
and qualitative evaluation shows that our model outperforms several baselines
by generating high-quality counter-responses. This work illustrates the promise
of generative text models for social good -- here, to help create a safe and
reliable information ecosystem. The code and data is accessible on
https://github.com/claws-lab/MisinfoCorrect.",2023-03-11
An Overview on Language Models: Recent Developments and Outlook,2023-03-10 07:55:00+00:00,http://arxiv.org/abs/2303.05759v1,"Chengwei Wei, Yun-Cheng Wang, Bin Wang, C. -C. Jay Kuo",cs.CL,table2text,"Language modeling studies the probability distributions over strings of
texts. It is one of the most fundamental tasks in natural language processing
(NLP). It has been widely used in text generation, speech recognition, machine
translation, etc. Conventional language models (CLMs) aim to predict the
probability of linguistic sequences in a causal manner. In contrast,
pre-trained language models (PLMs) cover broader concepts and can be used in
both causal sequential modeling and fine-tuning for downstream applications.
PLMs have their own training paradigms (usually self-supervised) and serve as
foundation models in modern NLP systems. This overview paper provides an
introduction to both CLMs and PLMs from five aspects, i.e., linguistic units,
structures, training methods, evaluation methods, and applications.
Furthermore, we discuss the relationship between CLMs and PLMs and shed light
on the future directions of language modeling in the pre-trained era.",2023-03-10
Is ChatGPT a Good NLG Evaluator? A Preliminary Study,2023-03-07 16:57:20+00:00,http://arxiv.org/abs/2303.04048v1,"Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou","cs.CL, cs.AI",table2text,"Recently, the emergence of ChatGPT has attracted wide attention from the
computational linguistics community. Many prior studies have shown that ChatGPT
achieves remarkable performance on various NLP tasks in terms of automatic
evaluation metrics. However, the ability of ChatGPT to serve as an evaluation
metric is still underexplored. Considering assessing the quality of NLG models
is an arduous task and previous statistical metrics notoriously show their poor
correlation with human judgments, we wonder whether ChatGPT is a good NLG
evaluation metric. In this report, we provide a preliminary meta-evaluation on
ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT
as a human evaluator and give task-specific (e.g., summarization) and
aspect-specific (e.g., relevance) instruction to prompt ChatGPT to score the
generation of NLG models. We conduct experiments on three widely-used NLG
meta-evaluation datasets (including summarization, story generation and
data-to-text tasks). Experimental results show that compared with previous
automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation
with golden human judgments. We hope our preliminary study could prompt the
emergence of a general-purposed reliable NLG metric.",2023-03-07
"Large Language Models as Zero-Shot Human Models for Human-Robot
  Interaction",2023-03-06 23:16:24+00:00,http://arxiv.org/abs/2303.03548v1,"Bowen Zhang, Harold Soh","cs.RO, cs.CL, cs.HC, cs.LG",table2text,"Human models play a crucial role in human-robot interaction (HRI), enabling
robots to consider the impact of their actions on people and plan their
behavior accordingly. However, crafting good human models is challenging;
capturing context-dependent human behavior requires significant prior knowledge
and/or large amounts of interaction data, both of which are difficult to
obtain. In this work, we explore the potential of large-language models (LLMs)
-- which have consumed vast amounts of human-generated text data -- to act as
zero-shot human models for HRI. Our experiments on three social datasets yield
promising results; the LLMs are able to achieve performance comparable to
purpose-built models. That said, we also discuss current limitations, such as
sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our
findings, we demonstrate how LLM-based human models can be integrated into a
social robot's planning process and applied in HRI scenarios. Specifically, we
present one case study on a simulated trust-based table-clearing task and
replicate past results that relied on custom models. Next, we conduct a new
robot utensil-passing experiment (n = 65) where preliminary results show that
planning with a LLM-based human model can achieve gains over a basic myopic
plan. In summary, our results show that LLMs offer a promising (but incomplete)
approach to human modeling for HRI.",2023-03-06
"DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only
  Training",2023-03-06 11:02:47+00:00,http://arxiv.org/abs/2303.03032v1,"Wei Li, Linchao Zhu, Longyin Wen, Yi Yang","cs.CV, cs.AI, cs.CL",table2text,"Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong
zero-shot transfer capability in many discriminative tasks. Their adaptation to
zero-shot image-conditioned text generation tasks has drawn increasing
interest. Prior arts approach to zero-shot captioning by either utilizing the
existing large language models (e.g., GPT-2) or pre-training the
encoder-decoder network in an end-to-end manner. In this work, we propose a
simple framework, named DeCap, for zero-shot captioning. We introduce a
lightweight visual-aware language decoder. This decoder is both data-efficient
and computation-efficient: 1) it only requires the text data for training,
easing the burden on the collection of paired data. 2) it does not require
end-to-end training. When trained with text-only data, the decoder takes the
text embedding extracted from the off-the-shelf CLIP encoder as a prefix
embedding. The challenge is that the decoder is trained on the text corpus but
at the inference stage, it needs to generate captions based on visual inputs.
The modality gap issue is widely observed in multi-modal contrastive models
that prevents us from directly taking the visual embedding as the prefix
embedding. We propose a training-free mechanism to reduce the modality gap. We
project the visual embedding into the CLIP text embedding space, while the
projected embedding retains the information of the visual input. Taking the
projected embedding as the prefix embedding, the decoder generates high-quality
descriptions that match the visual input. The experiments show that DeCap
outperforms other zero-shot captioning methods and unpaired captioning methods
on the typical image captioning benchmarks, i.e., MSCOCO and NoCaps.",2023-03-06
"UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data
  Generation for Cross-Lingual Learning in Tweet Intimacy Prediction",2023-03-02 12:18:53+00:00,http://arxiv.org/abs/2303.01194v1,"Andrianos Michail, Stefanos Konstantinou, Simon Clematide","cs.CL, cs.AI, 68T50",table2text,"This paper describes the submission of UZH_CLyp for the SemEval 2023 Task 9
""Multilingual Tweet Intimacy Analysis"". We achieved second-best results in all
10 languages according to the official Pearson's correlation regression
evaluation measure. Our cross-lingual transfer learning approach explores the
benefits of using a Head-First Fine-Tuning method (HeFiT) that first updates
only the regression head parameters and then also updates the pre-trained
transformer encoder parameters at a reduced learning rate. Additionally, we
study the impact of using a small set of automatically generated examples (in
our case, from ChatGPT) for low-resource settings where no human-labeled data
is available. Our study shows that HeFiT stabilizes training and consistently
improves results for pre-trained models that lack domain adaptation to tweets.
Our study also shows a noticeable performance increase in cross-lingual
learning when synthetic data is used, confirming the usefulness of current text
generation systems to improve zero-shot baseline results. Finally, we examine
how possible inconsistencies in the annotated data contribute to cross-lingual
interference issues.",2023-03-02
A Universal Question-Answering Platform for Knowledge Graphs,2023-03-01 15:35:32+00:00,http://arxiv.org/abs/2303.00595v1,"Reham Omar, Ishika Dhall, Panos Kalnis, Essam Mansour","cs.AI, cs.CL, cs.DB",table2text,"Knowledge from diverse application domains is organized as knowledge graphs
(KGs) that are stored in RDF engines accessible in the web via SPARQL
endpoints. Expressing a well-formed SPARQL query requires information about the
graph structure and the exact URIs of its components, which is impractical for
the average user. Question answering (QA) systems assist by translating natural
language questions to SPARQL. Existing QA systems are typically based on
application-specific human-curated rules, or require prior information,
expensive pre-processing and model adaptation for each targeted KG. Therefore,
they are hard to generalize to a broad set of applications and KGs.
  In this paper, we propose KGQAn, a universal QA system that does not need to
be tailored to each target KG. Instead of curated rules, KGQAn introduces a
novel formalization of question understanding as a text generation problem to
convert a question into an intermediate abstract representation via a neural
sequence-to-sequence model. We also develop a just-in-time linker that maps at
query time the abstract representation to a SPARQL query for a specific KG,
using only the publicly accessible APIs and the existing indices of the RDF
store, without requiring any pre-processing. Our experiments with several real
KGs demonstrate that KGQAn is easily deployed and outperforms by a large margin
the state-of-the-art in terms of quality of answers and processing time,
especially for arbitrary KGs, unseen during the training.",2023-03-01
TabGenie: A Toolkit for Table-to-Text Generation,2023-02-27 22:05:46+00:00,http://arxiv.org/abs/2302.14169v1,"Zdeněk Kasner, Ekaterina Garanina, Ondřej Plátek, Ondřej Dušek",cs.CL,table2text,"Heterogenity of data-to-text generation datasets limits the research on
data-to-text generation systems. We present TabGenie - a toolkit which enables
researchers to explore, preprocess, and analyze a variety of data-to-text
generation datasets through the unified framework of table-to-text generation.
In TabGenie, all the inputs are represented as tables with associated metadata.
The tables can be explored through the web interface, which also provides an
interactive mode for debugging table-to-text generation, facilitates
side-by-side comparison of generated system outputs, and allows easy exports
for manual analysis. Furthermore, TabGenie is equipped with command line
processing tools and Python bindings for unified dataset loading and
processing. We release TabGenie as a PyPI package and provide its open-source
code and a live demo at https://github.com/kasnerz/tabgenie.",2023-02-27
Tailoring Language Generation Models under Total Variation Distance,2023-02-26 16:32:52+00:00,http://arxiv.org/abs/2302.13344v1,"Haozhe Ji, Pei Ke, Zhipeng Hu, Rongsheng Zhang, Minlie Huang",cs.CL,table2text,"The standard paradigm of neural language generation adopts maximum likelihood
estimation (MLE) as the optimizing method. From a distributional view, MLE in
fact minimizes the Kullback-Leibler divergence (KLD) between the distribution
of the real data and that of the model. However, this approach forces the model
to distribute non-zero (sometimes large) probability mass to all training
samples regardless of their quality. Moreover, in the attempt to cover the
low-probability regions in the data distribution, the model systematically
overestimates the probability of corrupted text sequences, which we conjecture
is one of the main reasons for text degeneration during autoregressive
decoding. To remedy this problem, we leverage the total variation distance
(TVD) with its robustness to outliers, and develop practical bounds to apply it
to language generation. Then, we introduce the TaiLr objective that balances
the tradeoff of estimating TVD. Intuitively, TaiLr downweights real data
samples that have low model probabilities with tunable penalization intensity.
Experimental results show that our method alleviates the overestimation of
degenerated sequences without sacrificing diversity and improves generation
quality on a wide range of text generation tasks.",2023-02-26
Few-Shot Table-to-Text Generation with Prompt-based Adapter,2023-02-24 05:48:53+00:00,http://arxiv.org/abs/2302.12468v1,"Zhixin Guo, Minyxuan Yan, Jiexing Qi, Jianping Zhou, Ziwei He, Zhouhan Lin, Guanjie Zheng, Xinbing Wang",cs.CL,table2text,"Pre-trained language models (PLMs) have made remarkable progress in
table-to-text generation tasks. However, the topological gap between tabular
data and text and the lack of domain-specific knowledge make it difficult for
PLMs to produce faithful text, especially in real-world applications with
limited resources. In this paper, we mitigate the above challenges by
introducing a novel augmentation method: Prompt-based Adapter (PA), which
targets table-to-text generation under few-shot conditions. The core insight
design of the PA is to inject prompt templates for augmenting domain-specific
knowledge and table-related representations into the model for bridging the
structural gap between tabular data and descriptions through adapters. Such
prompt-based knowledge augmentation method brings at least two benefits: (1)
enables us to fully use the large amounts of unlabelled domain-specific
knowledge, which can alleviate the PLMs' inherent shortcomings of lacking
domain knowledge; (2) allows us to design different types of tasks supporting
the generative challenge. Extensive experiments and analyses are conducted on
three open-domain few-shot NLG datasets: Humans, Books, and Songs. Compared to
previous state-of-the-art approaches, our model achieves superior performance
in terms of both fluency and accuracy as judged by human and automatic
evaluations.",2023-02-24
Improved Training of Mixture-of-Experts Language GANs,2023-02-23 09:25:46+00:00,http://arxiv.org/abs/2302.11875v1,"Yekun Chai, Qiyue Yin, Junge Zhang",cs.CL,table2text,"Despite the dramatic success in image generation, Generative Adversarial
Networks (GANs) still face great challenges in synthesizing sequences of
discrete elements, in particular human language. The difficulty in generator
training arises from the limited representation capacity and uninformative
learning signals obtained from the discriminator. In this work, we (1) first
empirically show that the mixture-of-experts approach is able to enhance the
representation capacity of the generator for language GANs and (2) harness the
Feature Statistics Alignment (FSA) paradigm to render fine-grained learning
signals to advance the generator training. Specifically, FSA forces the mean
statistics of the distribution of fake data to approach that of real samples as
close as possible in the finite-dimensional feature space. Empirical study on
synthetic and real benchmarks shows the superior performance in quantitative
evaluation and demonstrates the effectiveness of our approach to adversarial
text generation.",2023-02-23
Improving User Controlled Table-To-Text Generation Robustness,2023-02-20 07:51:15+00:00,http://arxiv.org/abs/2302.09820v1,"Hanxu Hu, Yunqing Liu, Zhongyi Yu, Laura Perez-Beltrachini",cs.CL,table2text,"In this work we study user controlled table-to-text generation where users
explore the content in a table by selecting cells and reading a natural
language description thereof automatically produce by a natural language
generator. Such generation models usually learn from carefully selected cell
combinations (clean cell selections); however, in practice users may select
unexpected, redundant, or incoherent cell combinations (noisy cell selections).
In experiments, we find that models perform well on test sets coming from the
same distribution as the train data but their performance drops when evaluated
on realistic noisy user inputs. We propose a fine-tuning regime with additional
user-simulated noisy cell selections. Models fine-tuned with the proposed
regime gain 4.85 BLEU points on user noisy test cases and 1.4 on clean test
cases; and achieve comparable state-of-the-art performance on the ToTTo
dataset.",2023-02-20
Do We Still Need Clinical Language Models?,2023-02-16 05:08:34+00:00,http://arxiv.org/abs/2302.08091v1,"Eric Lehman, Evan Hernandez, Diwakar Mahajan, Jonas Wulff, Micah J. Smith, Zachary Ziegler, Daniel Nadler, Peter Szolovits, Alistair Johnson, Emily Alsentzer",cs.CL,table2text,"Although recent advances in scaling large language models (LLMs) have
resulted in improvements on many NLP tasks, it remains unclear whether these
models trained primarily with general web text are the right tool in highly
specialized, safety critical domains such as clinical text. Recent results have
suggested that LLMs encode a surprising amount of medical knowledge. This
raises an important question regarding the utility of smaller domain-specific
language models. With the success of general-domain LLMs, is there still a need
for specialized clinical models? To investigate this question, we conduct an
extensive empirical analysis of 12 language models, ranging from 220M to 175B
parameters, measuring their performance on 3 different clinical tasks that test
their ability to parse and reason over electronic health records. As part of
our experiments, we train T5-Base and T5-Large models from scratch on clinical
notes from MIMIC III and IV to directly investigate the efficiency of clinical
tokens. We show that relatively small specialized clinical models substantially
outperform all in-context learning approaches, even when finetuned on limited
annotated data. Further, we find that pretraining on clinical tokens allows for
smaller, more parameter-efficient models that either match or outperform much
larger language models trained on general text. We release the code and the
models used under the PhysioNet Credentialed Health Data license and data use
agreement.",2023-02-16
"Tree-Based Representation and Generation of Natural and Mathematical
  Language",2023-02-15 22:38:34+00:00,http://arxiv.org/abs/2302.07974v1,"Alexander Scarlatos, Andrew Lan",cs.CL,table2text,"Mathematical language in scientific communications and educational scenarios
is important yet relatively understudied compared to natural languages. Recent
works on mathematical language focus either on representing stand-alone
mathematical expressions, especially in their natural tree format, or
mathematical reasoning in pre-trained natural language models. Existing works
on jointly modeling and generating natural and mathematical languages simply
treat mathematical expressions as text, without accounting for the rigid
structural properties of mathematical expressions. In this paper, we propose a
series of modifications to existing language models to jointly represent and
generate text and math: representing mathematical expressions as sequences of
node tokens in their operator tree format, using math symbol and tree position
embeddings to preserve the semantic and structural properties of mathematical
expressions, and using a constrained decoding method to generate mathematically
valid expressions. We ground our modifications in GPT-2, resulting in a model
MathGPT, and demonstrate that it outperforms baselines on mathematical
expression generation tasks.",2023-02-15
"AutoBiasTest: Controllable Sentence Generation for Automated and
  Open-Ended Social Bias Testing in Language Models",2023-02-14 22:07:57+00:00,http://arxiv.org/abs/2302.07371v1,"Rafal Kocielnik, Shrimai Prabhumoye, Vivian Zhang, R. Michael Alvarez, Anima Anandkumar","cs.CL, cs.CY, 68T50, I.2.7; J.5; K.4.1",table2text,"Social bias in Pretrained Language Models (PLMs) affects text generation and
other downstream NLP tasks. Existing bias testing methods rely predominantly on
manual templates or on expensive crowd-sourced data. We propose a novel
AutoBiasTest method that automatically generates sentences for testing bias in
PLMs, hence providing a flexible and low-cost alternative. Our approach uses
another PLM for generation and controls the generation of sentences by
conditioning on social group and attribute terms. We show that generated
sentences are natural and similar to human-produced content in terms of word
length and diversity. We illustrate that larger models used for generation
produce estimates of social bias with lower variance. We find that our bias
scores are well correlated with manual templates, but AutoBiasTest highlights
biases not captured by these templates due to more diverse and realistic test
sentences. By automating large-scale test sentence generation, we enable better
estimation of underlying bias distributions",2023-02-14
Large Scale Multi-Lingual Multi-Modal Summarization Dataset,2023-02-13 18:00:23+00:00,http://arxiv.org/abs/2302.06560v1,"Yash Verma, Anubhav Jangra, Raghvendra Kumar, Sriparna Saha","cs.CL, cs.MM",table2text,"Significant developments in techniques such as encoder-decoder models have
enabled us to represent information comprising multiple modalities. This
information can further enhance many downstream tasks in the field of
information retrieval and natural language processing; however, improvements in
multi-modal techniques and their performance evaluation require large-scale
multi-modal data which offers sufficient diversity. Multi-lingual modeling for
a variety of tasks like multi-modal summarization, text generation, and
translation leverages information derived from high-quality multi-lingual
annotated data. In this work, we present the current largest multi-lingual
multi-modal summarization dataset (M3LS), and it consists of over a million
instances of document-image pairs along with a professionally annotated
multi-modal summary for each pair. It is derived from news articles published
by British Broadcasting Corporation(BBC) over a decade and spans 20 languages,
targeting diversity across five language roots, it is also the largest
summarization dataset for 13 languages and consists of cross-lingual
summarization data for 2 languages. We formally define the multi-lingual
multi-modal summarization task utilizing our dataset and report baseline scores
from various state-of-the-art summarization techniques in a multi-lingual
setting. We also compare it with many similar datasets to analyze the
uniqueness and difficulty of M3LS.",2023-02-13
"Combined Location Online Weather Data: Easy-to-use Targeted Weather
  Analysis for Agriculture",2023-02-13 07:03:53+00:00,http://arxiv.org/abs/2302.06142v1,"Darren Yates, Christopher Blanchard, Allister Clarke, Sabih-Ur Rehman, Md Zahidul Islam, Russell Ford, Rob Walsh","cs.SI, J.2",table2text,"The continuing effects of climate change require farmers and growers to have
greater understanding of how these changes affect crop production. However,
while climatic data is generally available to help provide much of that
understanding, it can often be in a form not easy to digest. The proposed
Combined Location Online Weather Data (CLOWD) framework is an easy-to-use
online platform for analysing recent and historical weather data of any
location within Australia at the click of a map. CLOWD requires no programming
skills and operates in any HTML5 web browser on PC and mobile devices. It
enables comparison between current and previous growing seasons over a range of
environmental parameters, and can create a plain-English PDF report for offline
use, using natural language generation (NLG). This paper details the platform,
the design decisions taken and outlines how farmers and growers can use CLOWD
to better understand current growing conditions. Prototypes of CLOWD are now
online for PCs and smartphones.",2023-02-13
"Investigating the Effect of Relative Positional Embeddings on
  AMR-to-Text Generation with Structural Adapters",2023-02-12 12:43:36+00:00,http://arxiv.org/abs/2302.05900v1,"Sebastien Montella, Alexis Nasr, Johannes Heinecke, Frederic Bechet, Lina M. Rojas-Barahona",cs.CL,table2text,"Text generation from Abstract Meaning Representation (AMR) has substantially
benefited from the popularized Pretrained Language Models (PLMs). Myriad
approaches have linearized the input graph as a sequence of tokens to fit the
PLM tokenization requirements. Nevertheless, this transformation jeopardizes
the structural integrity of the graph and is therefore detrimental to its
resulting representation. To overcome this issue, Ribeiro et al. have recently
proposed StructAdapt, a structure-aware adapter which injects the input graph
connectivity within PLMs using Graph Neural Networks (GNNs). In this paper, we
investigate the influence of Relative Position Embeddings (RPE) on AMR-to-Text,
and, in parallel, we examine the robustness of StructAdapt. Through ablation
studies, graph attack and link prediction, we reveal that RPE might be
partially encoding input graphs. We suggest further research regarding the role
of RPE will provide valuable insights for Graph-to-Text generation.",2023-02-12
Plan-then-Seam: Towards Efficient Table-to-Text Generation,2023-02-10 09:43:15+00:00,http://arxiv.org/abs/2302.05138v1,"Liang Li, Ruiying Geng, Chengyang Fang, Bing Li, Can Ma, Binhua Li, Yongbin Li",cs.CL,table2text,"Table-to-text generation aims at automatically generating text to help people
conveniently obtain salient information in tables. Recent works explicitly
decompose the generation process into content planning and surface generation
stages, employing two autoregressive networks for them respectively. However,
they are computationally expensive due to the non-parallelizable nature of
autoregressive decoding and the redundant parameters of two networks. In this
paper, we propose the first totally non-autoregressive table-to-text model
(Plan-then-Seam, PTS) that produces its outputs in parallel with one single
network. PTS firstly writes and calibrates one plan of the content to be
generated with a novel rethinking pointer predictor, and then takes the plan as
the context for seaming to decode the description. These two steps share
parameters and perform iteratively to capture token inter-dependency while
keeping parallel decoding. Experiments on two public benchmarks show that PTS
achieves 3.0~5.6 times speedup for inference time, reducing 50% parameters,
while maintaining as least comparable performance against strong two-stage
table-to-text competitors.",2023-02-10
"Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot
  Image Captioning",2023-02-09 18:57:56+00:00,http://arxiv.org/abs/2302.04858v1,"Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhiding Yu, Shiyi Lan, Bo Li, Ming-Yu Liu, Yuke Zhu, Mohammad Shoeybi, Bryan Catanzaro, Chaowei Xiao, Anima Anandkumar","cs.CV, cs.AI, cs.CL, cs.IR, cs.LG",table2text,"Augmenting pretrained language models (LMs) with a vision encoder (e.g.,
Flamingo) has obtained state-of-the-art results in image-to-text generation.
However, these models store all the knowledge within their parameters, thus
often requiring enormous model parameters to model the abundant visual concepts
and very rich textual descriptions. Additionally, they are inefficient in
incorporating new data, requiring a computational-expensive fine-tuning
process. In this work, we introduce a Retrieval-augmented Visual Language
Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant
knowledge from the external database for zero and in-context few-shot
image-to-text generations. By storing certain knowledge explicitly in the
external database, our approach reduces the number of model parameters and can
easily accommodate new data during evaluation by simply updating the database.
We also construct an interleaved image and text data that facilitates
in-context few-shot learning capabilities. We demonstrate that Re-ViLM
significantly boosts performance for image-to-text generation tasks, especially
for zero-shot and few-shot generation in out-of-domain settings with 4 times
less parameters compared with baseline methods.",2023-02-09
Lightweight Transformers for Clinical Natural Language Processing,2023-02-09 16:07:31+00:00,http://arxiv.org/abs/2302.04725v1,"Omid Rohanian, Mohammadmahdi Nouriborji, Hannah Jauncey, Samaneh Kouchaki, ISARIC Clinical Characterisation Group, Lei Clifton, Laura Merson, David A. Clifton","cs.CL, cs.AI, cs.LG, 68T50, I.2.7",table2text,"Specialised pre-trained language models are becoming more frequent in NLP
since they can potentially outperform models trained on generic texts. BioBERT
and BioClinicalBERT are two examples of such models that have shown promise in
medical NLP tasks. Many of these models are overparametrised and
resource-intensive, but thanks to techniques like Knowledge Distillation (KD),
it is possible to create smaller versions that perform almost as well as their
larger counterparts. In this work, we specifically focus on development of
compact language models for processing clinical texts (i.e. progress notes,
discharge summaries etc). We developed a number of efficient lightweight
clinical transformers using knowledge distillation and continual learning, with
the number of parameters ranging from 15 million to 65 million. These models
performed comparably to larger models such as BioBERT and ClinicalBioBERT and
significantly outperformed other compact models trained on general or
biomedical data. Our extensive evaluation was done across several standard
datasets and covered a wide range of clinical text-mining tasks, including
Natural Language Inference, Relation Extraction, Named Entity Recognition, and
Sequence Classification. To our knowledge, this is the first comprehensive
study specifically focused on creating efficient and compact transformers for
clinical NLP tasks. The models and code used in this study can be found on our
Huggingface profile at https://huggingface.co/nlpie and Github page at
https://github.com/nlpie-research/Lightweight-Clinical-Transformers,
respectively, promoting reproducibility of our results.",2023-02-09
"Auto-Learning: An Adversarial Process of Two Pre-trained Models for
  Natural Language Generation",2023-02-08 06:09:55+00:00,http://arxiv.org/abs/2302.03896v1,"Zhengqing Yuan, Yuelin Lu, Chao Zhang, Huiwen Xue",cs.CL,table2text,"Pre-trained models have been used in many fields in recent years, ranging
from natural language understanding to computer vision and natural language
generation. However, the performance of these natural language generation
models is overly dependent on the scale of the model and the size of the
dataset. While the larger language model is excellent in some respects, it
cannot learn up-to-date knowledge and is relatively difficult to relearn. In
this paper, a new adversarial process learning method called Auto-Learning.
This can improve the performance of any natural language generation model
without the help of additional datasets. Auto-Learning includes two models: $G$
is a text generation model and $D$ can test whether the data generated by G is
legitimate. Firstly, the fine-tuned $D$ model is used as the brain's knowledge
base before the process. Then the text generated by the $G$ model is used as
the input of $D$ to determine whether the text is legitimate or not. Finally,
$G$ is fine-tuned according to the output of $D$. This adversarial process is
like a self-escalation of the brain through some a priori knowledge. When this
adversarial system wants to learn something new, simply fine-tune the $D$
model. Our approach applies to Autoregressive Language Modeling for all
Transformer classes. The results are good in existing experimental tasks,
including more grammatical text generation and better performance on some text
comprehension tasks.",2023-02-08
What Matters In The Structured Pruning of Generative Language Models?,2023-02-07 22:05:55+00:00,http://arxiv.org/abs/2302.03773v1,"Michael Santacroce, Zixin Wen, Yelong Shen, Yuanzhi Li","cs.CL, cs.LG",table2text,"Auto-regressive large language models such as GPT-3 require enormous
computational resources to use. Traditionally, structured pruning methods are
employed to reduce resource usage. However, their application to and efficacy
for generative language models is heavily under-explored. In this paper we
conduct an comprehensive evaluation of common structured pruning methods,
including magnitude, random, and movement pruning on the feed-forward layers in
GPT-type models. Unexpectedly, random pruning results in performance that is
comparable to the best established methods, across multiple natural language
generation tasks. To understand these results, we provide a framework for
measuring neuron-level redundancy of models pruned by different methods, and
discover that established structured pruning methods do not take into account
the distinctiveness of neurons, leaving behind excess redundancies. In view of
this, we introduce Globally Unique Movement (GUM) to improve the uniqueness of
neurons in pruned models. We then discuss the effects of our techniques on
different redundancy metrics to explain the improved performance.",2023-02-07
"Unleashing the True Potential of Sequence-to-Sequence Models for
  Sequence Tagging and Structure Parsing",2023-02-05 01:37:26+00:00,http://arxiv.org/abs/2302.02275v1,"Han He, Jinho D. Choi",cs.CL,table2text,"Sequence-to-Sequence (S2S) models have achieved remarkable success on various
text generation tasks. However, learning complex structures with S2S models
remains challenging as external neural modules and additional lexicons are
often supplemented to predict non-textual outputs. We present a systematic
study of S2S modeling using contained decoding on four core tasks:
part-of-speech tagging, named entity recognition, constituency and dependency
parsing, to develop efficient exploitation methods costing zero extra
parameters. In particular, 3 lexically diverse linearization schemas and
corresponding constrained decoding methods are designed and evaluated.
Experiments show that although more lexicalized schemas yield longer output
sequences that require heavier training, their sequences being closer to
natural language makes them easier to learn. Moreover, S2S models using our
constrained decoding outperform other S2S approaches using external resources.
Our best models perform better than or comparably to the state-of-the-art for
all 4 tasks, lighting a promise for S2S models to generate non-sequential
structures.",2023-02-05
Grounding Language Models to Images for Multimodal Generation,2023-01-31 18:33:44+00:00,http://arxiv.org/abs/2301.13823v1,"Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried","cs.CL, cs.AI, cs.CV, cs.LG",table2text,"We propose an efficient method to ground pretrained text-only language models
to the visual domain, enabling them to process and generate arbitrarily
interleaved image-and-text data. Our method leverages the abilities of language
models learnt from large scale text-only pretraining, such as in-context
learning and free-form text generation. We keep the language model frozen, and
finetune input and output linear layers to enable cross-modality interactions.
This allows our model to process arbitrarily interleaved image-and-text inputs,
and generate free-form text interleaved with retrieved images. We achieve
strong zero-shot performance on grounded tasks such as contextual image
retrieval and multimodal dialogue, and showcase compelling interactive
abilities. Our approach works with any off-the-shelf language model and paves
the way towards an effective, general solution for leveraging pretrained
language models in visually grounded settings.",2023-01-31
Semi-Parametric Video-Grounded Text Generation,2023-01-27 03:00:43+00:00,http://arxiv.org/abs/2301.11507v1,"Sungdong Kim, Jin-Hwa Kim, Jiyoung Lee, Minjoon Seo","cs.CV, cs.CL, cs.LG",table2text,"Efficient video-language modeling should consider the computational cost
because of a large, sometimes intractable, number of video frames. Parametric
approaches such as the attention mechanism may not be ideal since its
computational cost quadratically increases as the video length increases.
Rather, previous studies have relied on offline feature extraction or frame
sampling to represent the video efficiently, focusing on cross-modal modeling
in short video clips. In this paper, we propose a semi-parametric
video-grounded text generation model, SeViT, a novel perspective on scalable
video-language modeling toward long untrimmed videos. Treating a video as an
external data store, SeViT includes a non-parametric frame retriever to select
a few query-relevant frames from the data store for a given query and a
parametric generator to effectively aggregate the frames with the query via
late fusion methods. Experimental results demonstrate our method has a
significant advantage in longer videos and causal video understanding.
Moreover, our model achieves the new state of the art on four video-language
datasets, iVQA (+4.8), Next-QA (+6.9), and Activitynet-QA (+4.8) in accuracy,
and MSRVTT-Caption (+3.6) in CIDEr.",2023-01-27
"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability
  Curvature",2023-01-26 18:44:06+00:00,http://arxiv.org/abs/2301.11305v1,"Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, Chelsea Finn","cs.CL, cs.AI",table2text,"The fluency and factual knowledge of large language models (LLMs) heightens
the need for corresponding systems to detect whether a piece of text is
machine-written. For example, students may use LLMs to complete written
assignments, leaving instructors unable to accurately assess student learning.
In this paper, we first demonstrate that text sampled from an LLM tends to
occupy negative curvature regions of the model's log probability function.
Leveraging this observation, we then define a new curvature-based criterion for
judging if a passage is generated from a given LLM. This approach, which we
call DetectGPT, does not require training a separate classifier, collecting a
dataset of real or generated passages, or explicitly watermarking generated
text. It uses only log probabilities computed by the model of interest and
random perturbations of the passage from another generic pre-trained language
model (e.g, T5). We find DetectGPT is more discriminative than existing
zero-shot methods for model sample detection, notably improving detection of
fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the
strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See
https://ericmitchell.ai/detectgpt for code, data, and other project
information.",2023-01-26
Distilling Text into Circuits,2023-01-25 13:56:34+00:00,http://arxiv.org/abs/2301.10595v1,"Vincent Wang-Mascianica, Jonathon Liu, Bob Coecke","cs.CL, cs.AI, cs.LO, math.CT",table2text,"This paper concerns the structure of meanings within natural language.
Earlier, a framework named DisCoCirc was sketched that (1) is compositional and
distributional (a.k.a. vectorial); (2) applies to general text; (3) captures
linguistic `connections' between meanings (cf. grammar) (4) updates word
meanings as text progresses; (5) structures sentence types; (6) accommodates
ambiguity. Here, we realise DisCoCirc for a substantial fragment of English.
  When passing to DisCoCirc's text circuits, some `grammatical bureaucracy' is
eliminated, that is, DisCoCirc displays a significant degree of (7) inter- and
intra-language independence. That is, e.g., independence from word-order
conventions that differ across languages, and independence from choices like
many short sentences vs. few long sentences. This inter-language independence
means our text circuits should carry over to other languages, unlike the
language-specific typings of categorial grammars. Hence, text circuits are a
lean structure for the `actual substance of text', that is, the inner-workings
of meanings within text across several layers of expressiveness (cf. words,
sentences, text), and may capture that what is truly universal beneath grammar.
The elimination of grammatical bureaucracy also explains why DisCoCirc: (8)
applies beyond language, e.g. to spatial, visual and other cognitive modes.
While humans could not verbally communicate in terms of text circuits, machines
can.
  We first define a `hybrid grammar' for a fragment of English, i.e. a
purpose-built, minimal grammatical formalism needed to obtain text circuits. We
then detail a translation process such that all text generated by this grammar
yields a text circuit. Conversely, for any text circuit obtained by freely
composing the generators, there exists a text (with hybrid grammar) that gives
rise to it. Hence: (9) text circuits are generative for text.",2023-01-25
"One Model for All Domains: Collaborative Domain-Prefix Tuning for
  Cross-Domain NER",2023-01-25 05:16:43+00:00,http://arxiv.org/abs/2301.10410v1,"Xiang Chen, Lei Li, Qiaoshuo Fei, Ningyu Zhang, Chuanqi Tan, Yong Jiang, Fei Huang, Huajun Chen","cs.CL, cs.AI, cs.DB, cs.IR, cs.LG",table2text,"Cross-domain NER is a challenging task to address the low-resource problem in
practical scenarios. Previous typical solutions mainly obtain a NER model by
pre-trained language models (PLMs) with data from a rich-resource domain and
adapt it to the target domain. Owing to the mismatch issue among entity types
in different domains, previous approaches normally tune all parameters of PLMs,
ending up with an entirely new NER model for each domain. Moreover, current
models only focus on leveraging knowledge in one general source domain while
failing to successfully transfer knowledge from multiple sources to the target.
To address these issues, we introduce Collaborative Domain-Prefix Tuning for
cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,
we present text-to-text generation grounding domain-related instructors to
transfer knowledge to new domain NER tasks without structural modifications. We
utilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate
the potential of PLMs to handle NER tasks across various domains. Experimental
results on the Cross-NER benchmark show that the proposed approach has flexible
transfer ability and performs better on both one-source and multiple-source
cross-domain NER tasks. Codes will be available in
https://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.",2023-01-25
Audience-Centric Natural Language Generation via Style Infusion,2023-01-24 19:57:50+00:00,http://arxiv.org/abs/2301.10283v1,"Samraj Moorjani, Adit Krishnan, Hari Sundaram, Ewa Maslowska, Aravind Sankar","cs.CL, cs.LG",table2text,"Adopting contextually appropriate, audience-tailored linguistic styles is
critical to the success of user-centric language generation systems (e.g.,
chatbots, computer-aided writing, dialog systems). While existing approaches
demonstrate textual style transfer with large volumes of parallel or
non-parallel data, we argue that grounding style on audience-independent
external factors is innately limiting for two reasons. First, it is difficult
to collect large volumes of audience-specific stylistic data. Second, some
stylistic objectives (e.g., persuasiveness, memorability, empathy) are hard to
define without audience feedback.
  In this paper, we propose the novel task of style infusion - infusing the
stylistic preferences of audiences in pretrained language generation models.
Since humans are better at pairwise comparisons than direct scoring - i.e., is
Sample-A more persuasive/polite/empathic than Sample-B - we leverage limited
pairwise human judgments to bootstrap a style analysis model and augment our
seed set of judgments. We then infuse the learned textual style in a GPT-2
based text generator while balancing fluency and style adoption. With
quantitative and qualitative assessments, we show that our infusion approach
can generate compelling stylized examples with generic text prompts. The code
and data are accessible at https://github.com/CrowdDynamicsLab/StyleInfusion.",2023-01-24
ExClaim: Explainable Neural Claim Verification Using Rationalization,2023-01-21 08:26:27+00:00,http://arxiv.org/abs/2301.08914v1,"Sai Gurrapu, Lifu Huang, Feras A. Batarseh",cs.CL,table2text,"With the advent of deep learning, text generation language models have
improved dramatically, with text at a similar level as human-written text. This
can lead to rampant misinformation because content can now be created cheaply
and distributed quickly. Automated claim verification methods exist to validate
claims, but they lack foundational data and often use mainstream news as
evidence sources that are strongly biased towards a specific agenda. Current
claim verification methods use deep neural network models and complex
algorithms for a high classification accuracy but it is at the expense of model
explainability. The models are black-boxes and their decision-making process
and the steps it took to arrive at a final prediction are obfuscated from the
user. We introduce a novel claim verification approach, namely: ExClaim, that
attempts to provide an explainable claim verification system with foundational
evidence. Inspired by the legal system, ExClaim leverages rationalization to
provide a verdict for the claim and justifies the verdict through a natural
language explanation (rationale) to describe the model's decision-making
process. ExClaim treats the verdict classification task as a question-answer
problem and achieves a performance of 0.93 F1 score. It provides subtasks
explanations to also justify the intermediate outcomes. Statistical and
Explainable AI (XAI) evaluations are conducted to ensure valid and trustworthy
outcomes. Ensuring claim verification systems are assured, rational, and
explainable is an essential step toward improving Human-AI trust and the
accessibility of black-box systems.",2023-01-21
Regeneration Learning: A Learning Paradigm for Data Generation,2023-01-21 01:33:34+00:00,http://arxiv.org/abs/2301.08846v1,"Xu Tan, Tao Qin, Jiang Bian, Tie-Yan Liu, Yoshua Bengio","cs.LG, cs.AI, cs.CL, cs.CV, eess.AS",table2text,"Machine learning methods for conditional data generation usually build a
mapping from source conditional data X to target data Y. The target Y (e.g.,
text, speech, music, image, video) is usually high-dimensional and complex, and
contains information that does not exist in source data, which hinders
effective and efficient learning on the source-target mapping. In this paper,
we present a learning paradigm called regeneration learning for data
generation, which first generates Y' (an abstraction/representation of Y) from
X and then generates Y from Y'. During training, Y' is obtained from Y through
either handcrafted rules or self-supervised learning and is used to learn
X-->Y' and Y'-->Y. Regeneration learning extends the concept of representation
learning to data generation tasks, and can be regarded as a counterpart of
traditional representation learning, since 1) regeneration learning handles the
abstraction (Y') of the target data Y for data generation while traditional
representation learning handles the abstraction (X') of source data X for data
understanding; 2) both the processes of Y'-->Y in regeneration learning and
X-->X' in representation learning can be learned in a self-supervised way
(e.g., pre-training); 3) both the mappings from X to Y' in regeneration
learning and from X' to Y in representation learning are simpler than the
direct mapping from X to Y. We show that regeneration learning can be a
widely-used paradigm for data generation (e.g., text generation, speech
recognition, speech synthesis, music composition, image generation, and video
generation) and can provide valuable insights into developing data generation
methods.",2023-01-21
"UserSimCRS: A User Simulation Toolkit for Evaluating Conversational
  Recommender Systems",2023-01-13 13:41:20+00:00,http://arxiv.org/abs/2301.05544v2,"Jafar Afzali, Aleksander Mark Drzewiecki, Krisztian Balog, Shuo Zhang",cs.IR,table2text,"We present an extensible user simulation toolkit to facilitate automatic
evaluation of conversational recommender systems. It builds on an established
agenda-based approach and extends it with several novel elements, including
user satisfaction prediction, persona and context modeling, and conditional
natural language generation. We showcase the toolkit with a pre-existing movie
recommender system and demonstrate its ability to simulate dialogues that mimic
real conversations, while requiring only a handful of manually annotated
dialogues as training data.",2023-01-13
Universal Multimodal Representation for Language Understanding,2023-01-09 13:54:11+00:00,http://arxiv.org/abs/2301.03344v1,"Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao Li, Hai Zhao","cs.CL, cs.AI, cs.CV",table2text,"Representation learning is the foundation of natural language processing
(NLP). This work presents new methods to employ visual information as assistant
signals to general NLP tasks. For each sentence, we first retrieve a flexible
number of images either from a light topic-image lookup table extracted over
the existing sentence-image pairs or a shared cross-modal embedding space that
is pre-trained on out-of-shelf text-image pairs. Then, the text and images are
encoded by a Transformer encoder and convolutional neural network,
respectively. The two sequences of representations are further fused by an
attention layer for the interaction of the two modalities. In this study, the
retrieval process is controllable and flexible. The universal visual
representation overcomes the lack of large-scale bilingual sentence-image
pairs. Our method can be easily applied to text-only tasks without manually
annotated multimodal parallel corpora. We apply the proposed method to a wide
range of natural language generation and understanding tasks, including neural
machine translation, natural language inference, and semantic similarity.
Experimental results show that our method is generally effective for different
tasks and languages. Analysis indicates that the visual signals enrich textual
representations of content words, provide fine-grained grounding information
about the relationship between concepts and events, and potentially conduce to
disambiguation.",2023-01-09
Sequentially Controlled Text Generation,2023-01-05 21:23:51+00:00,http://arxiv.org/abs/2301.02299v1,"Alexander Spangher, Xinyu Hua, Yao Ming, Nanyun Peng","cs.CL, cs.AI, cs.LG",table2text,"While GPT-2 generates sentences that are remarkably human-like, longer
documents can ramble and do not follow human-like writing structure. We study
the problem of imposing structure on long-range text. We propose a novel
controlled text generation task, sequentially controlled text generation, and
identify a dataset, NewsDiscourse as a starting point for this task. We develop
a sequential controlled text generation pipeline with generation and editing.
We test different degrees of structural awareness and show that, in general,
more structural awareness results in higher control-accuracy, grammaticality,
coherency and topicality, approaching human-level writing performance.",2023-01-05
"Towards Table-to-Text Generation with Pretrained Language Model: A Table
  Structure Understanding and Text Deliberating Approach",2023-01-05 14:03:26+00:00,http://arxiv.org/abs/2301.02071v1,"Miao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Jingbo Zhou, Dejing Dou, Hui Xiong","cs.CL, cs.AI",table2text,"Although remarkable progress on the neural table-to-text methods has been
made, the generalization issues hinder the applicability of these models due to
the limited source tables. Large-scale pretrained language models sound like a
promising solution to tackle such issues. However, how to effectively bridge
the gap between the structured table and the text input by fully leveraging
table information to fuel the pretrained model is still not well explored.
Besides, another challenge of integrating the deliberation mechanism into the
text-to-text pretrained model for solving the table-to-text task remains seldom
studied. In this paper, to implement the table-to-text generation with
pretrained language model, we propose a table structure understanding and text
deliberating approach, namely TASD. Specifically, we devise a three-layered
multi-head attention network to realize the table-structure-aware text
generation model with the help of the pretrained language model. Furthermore, a
multi-pass decoder framework is adopted to enhance the capability of polishing
generated text for table descriptions. The empirical studies, as well as human
evaluation, on two public datasets, validate that our approach can generate
faithful and fluent descriptive texts for different types of tables.",2023-01-05
eVAE: Evolutionary Variational Autoencoder,2023-01-01 23:54:35+00:00,http://arxiv.org/abs/2301.00011v1,"Zhangkai Wu, Longbing Cao, Lei Qi","cs.NE, cs.LG",table2text,"The surrogate loss of variational autoencoders (VAEs) poses various
challenges to their training, inducing the imbalance between task fitting and
representation inference. To avert this, the existing strategies for VAEs focus
on adjusting the tradeoff by introducing hyperparameters, deriving a tighter
bound under some mild assumptions, or decomposing the loss components per
certain neural settings. VAEs still suffer from uncertain tradeoff learning.We
propose a novel evolutionary variational autoencoder (eVAE) building on the
variational information bottleneck (VIB) theory and integrative evolutionary
neural learning. eVAE integrates a variational genetic algorithm into VAE with
variational evolutionary operators including variational mutation, crossover,
and evolution. Its inner-outer-joint training mechanism synergistically and
dynamically generates and updates the uncertain tradeoff learning in the
evidence lower bound (ELBO) without additional constraints. Apart from learning
a lossy compression and representation of data under the VIB assumption, eVAE
presents an evolutionary paradigm to tune critical factors of VAEs and deep
neural networks and addresses the premature convergence and random search
problem by integrating evolutionary optimization into deep learning.
Experiments show that eVAE addresses the KL-vanishing problem for text
generation with low reconstruction loss, generates all disentangled factors
with sharp images, and improves the image generation quality,respectively. eVAE
achieves better reconstruction loss, disentanglement, and generation-inference
balance than its competitors.",2023-01-01
MAUVE Scores for Generative Models: Theory and Practice,2022-12-30 07:37:40+00:00,http://arxiv.org/abs/2212.14578v1,"Krishna Pillutla, Lang Liu, John Thickstun, Sean Welleck, Swabha Swayamdipta, Rowan Zellers, Sewoong Oh, Yejin Choi, Zaid Harchaoui","cs.LG, cs.AI, cs.CL",table2text,"Generative AI has matured to a point where large-scale models can generate
text that seems indistinguishable from human-written text and remarkably
photorealistic images. Automatically measuring how close the distribution of
generated data is to the target real data distribution is a key step in
diagnosing existing models and developing better models. We present MAUVE, a
family of comparison measures between pairs of distributions such as those
encountered in the generative modeling of text or images. These scores are
statistical summaries of divergence frontiers capturing two types of errors in
generative modeling. We explore four approaches to statistically estimate these
scores: vector quantization, non-parametric estimation, classifier-based
estimation, and parametric Gaussian approximations. We provide statistical
bounds for the vector quantization approach. Empirically, we find that the
proposed scores paired with a range of $f$-divergences and statistical
estimation methods can quantify the gaps between the distributions of
human-written text and those of modern neural language models by correlating
with human judgments and identifying known properties of the generated texts.
We conclude the paper by demonstrating its applications to other AI domains and
discussing practical recommendations.",2022-12-30
"TegFormer: Topic-to-Essay Generation with Good Topic Coverage and High
  Text Coherence",2022-12-27 11:50:14+00:00,http://arxiv.org/abs/2212.13456v1,"Wang Qi, Rui Liu, Yuan Zuo, Yong Chen, Dell Zhang",cs.CL,table2text,"Creating an essay based on a few given topics is a challenging NLP task.
Although several effective methods for this problem, topic-to-essay generation,
have appeared recently, there is still much room for improvement, especially in
terms of the coverage of the given topics and the coherence of the generated
text. In this paper, we propose a novel approach called TegFormer which
utilizes the Transformer architecture where the encoder is enriched with
domain-specific contexts while the decoder is enhanced by a large-scale
pre-trained language model. Specifically, a \emph{Topic-Extension} layer
capturing the interaction between the given topics and their domain-specific
contexts is plugged into the encoder. Since the given topics are usually
concise and sparse, such an additional layer can bring more topic-related
semantics in to facilitate the subsequent natural language generation.
Moreover, an \emph{Embedding-Fusion} module that combines the domain-specific
word embeddings learnt from the given corpus and the general-purpose word
embeddings provided by a GPT-2 model pre-trained on massive text data is
integrated into the decoder. Since GPT-2 is at a much larger scale, it contains
a lot more implicit linguistic knowledge which would help the decoder to
produce more grammatical and readable text. Extensive experiments have shown
that the pieces of text generated by TegFormer have better topic coverage and
higher text coherence than those from SOTA topic-to-essay techniques, according
to automatic and human evaluations. As revealed by ablation studies, both the
Topic-Extension layer and the Embedding-Fusion module contribute substantially
to TegFormer's performance advantage.",2022-12-27
TextBox 2.0: A Text Generation Library with Pre-trained Language Models,2022-12-26 03:50:36+00:00,http://arxiv.org/abs/2212.13005v1,"Tianyi Tang, Junyi Li, Zhipeng Chen, Yiwen Hu, Zhuohao Yu, Wenxun Dai, Zican Dong, Xiaoxue Cheng, Yuhao Wang, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen",cs.CL,table2text,"To facilitate research on text generation, this paper presents a
comprehensive and unified library, TextBox 2.0, focusing on the use of
pre-trained language models (PLMs). To be comprehensive, our library covers
$13$ common text generation tasks and their corresponding $83$ datasets and
further incorporates $45$ PLMs covering general, translation, Chinese,
dialogue, controllable, distilled, prompting, and lightweight PLMs. We also
implement $4$ efficient training strategies and provide $4$ generation
objectives for pre-training new PLMs from scratch. To be unified, we design the
interfaces to support the entire research pipeline (from data loading to
training and evaluation), ensuring that each step can be fulfilled in a unified
way. Despite the rich functionality, it is easy to use our library, either
through the friendly Python API or command line. To validate the effectiveness
of our library, we conduct extensive experiments and exemplify four types of
research scenarios. The project is released at the link:
https://github.com/RUCAIBox/TextBox.",2022-12-26
"CORRPUS: Detecting Story Inconsistencies via Codex-Bootstrapped
  Neurosymbolic Reasoning",2022-12-21 04:21:35+00:00,http://arxiv.org/abs/2212.10754v1,"Yijiang River Dong, Lara J. Martin, Chris Callison-Burch",cs.CL,table2text,"Story generation and understanding -- as with all NLG/NLU tasks -- has seen a
surge in neurosymbolic work. Researchers have recognized that, while large
language models (LLMs) have tremendous utility, they can be augmented with
symbolic means to be even better and to make up for any flaws that the neural
networks might have. However, symbolic methods are extremely costly in terms of
the amount of time and expertise needed to create them. In this work, we
capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use
of symbolic methods for tracking the state of stories and aiding in story
understanding. We show that our CoRRPUS system and abstracted prompting
procedures can beat current state-of-the-art structured LLM techniques on
pre-existing story understanding tasks (bAbI task 2 and Re^3) with minimal hand
engineering. We hope that this work can help highlight the importance of
symbolic representations and specialized prompting for LLMs as these models
require some guidance for performing reasoning tasks properly.",2022-12-21
Tracing and Removing Data Errors in Natural Language Generation Datasets,2022-12-21 02:28:07+00:00,http://arxiv.org/abs/2212.10722v1,"Faisal Ladhak, Esin Durmus, Tatsunori Hashimoto",cs.CL,table2text,"Recent work has identified noisy and misannotated data as a core cause of
hallucinations and unfaithful outputs in Natural Language Generation (NLG)
tasks. Consequently, identifying and removing these examples is a key open
challenge in creating reliable NLG systems. In this work, we introduce a
framework to identify and remove low-quality training instances that lead to
undesirable outputs, such as faithfulness errors in text summarization. We show
that existing approaches for error tracing, such as gradient-based influence
measures, do not perform reliably for detecting faithfulness errors in
summarization. We overcome the drawbacks of existing error tracing methods
through a new, contrast-based estimate that compares undesired generations to
human-corrected outputs. Our proposed method can achieve a mean average
precision of 0.91 across synthetic tasks with known ground truth and can
achieve a two-fold reduction in hallucinations on a real entity hallucination
evaluation on the NYT dataset.",2022-12-21
SimpleStyle: An Adaptable Style Transfer Approach,2022-12-20 18:12:49+00:00,http://arxiv.org/abs/2212.10498v1,"Elron Bandel, Yoav Katz, Noam Slonim, Liat Ein-Dor",cs.CL,table2text,"Attribute Controlled Text Rewriting, also known as text style transfer, has
received significant attention in the natural language generation community due
to its crucial role in controllable natural language generation systems. In
this work we present SimpleStyle a minimalist yet effective approach for
attribute controlled text rewriting based on a simple mechanism composed of two
ingredients. controlled denoising and output filtering. Despite the simplicity
of our approach, which can be succinctly explained with just a few lines of
code, it is competitive with previous state-of-the-art methods both in
automatic and in human evaluations. Additionally, we demonstrate the practical
effectiveness of our system, by applying it to real-world data from social
networks. Additionally, we introduce a soft masking sampling technique that
further improves the performance of the system. We also show that feeding the
output of our system into a text-to-text student model can produce high-quality
results without the need for additional filtering. Finally, we suggest that our
method can solve the fundamental missing baseline absence that holding progress
in the field by offering our protocol as a simple, adaptive and very strong
baseline for works wish to make incremental advancements in the field of
attribute controlled text rewriting.",2022-12-20
"CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data
  Limitation With Contrastive Learning",2022-12-20 15:26:19+00:00,http://arxiv.org/abs/2212.10341v1,"Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Yu Lan, Chao Shen",cs.CL,table2text,"Machine-Generated Text (MGT) detection, a task that discriminates MGT from
Human-Written Text (HWT), plays a crucial role in preventing misuse of text
generative models, which excel in mimicking human writing style recently.
Latest proposed detectors usually take coarse text sequence as input and output
some good results by fine-tune pretrained models with standard cross-entropy
loss. However, these methods fail to consider the linguistic aspect of text
(e.g., coherence) and sentence-level structures. Moreover, they lack the
ability to handle the low-resource problem which could often happen in practice
considering the enormous amount of textual data online. In this paper, we
present a coherence-based contrastive learning model named CoCo to detect the
possible MGT under low-resource scenario. Inspired by the distinctiveness and
permanence properties of linguistic feature, we represent text as a coherence
graph to capture its entity consistency, which is further encoded by the
pretrained model and graph neural network. To tackle the challenges of data
limitations, we employ a contrastive learning framework and propose an improved
contrastive loss for making full use of hard negative samples in training
stage. The experiment results on two public datasets prove our approach
outperforms the state-of-art methods significantly.",2022-12-20
"Toward Human-Like Evaluation for Natural Language Generation with Error
  Analysis",2022-12-20 11:36:22+00:00,http://arxiv.org/abs/2212.10179v1,"Qingyu Lu, Liang Ding, Liping Xie, Kanjian Zhang, Derek F. Wong, Dacheng Tao",cs.CL,table2text,"The state-of-the-art language model-based automatic metrics, e.g. BARTScore,
benefiting from large-scale contextualized pre-training, have been successfully
used in a wide range of natural language generation (NLG) tasks, including
machine translation, text summarization, and data-to-text. Recent studies show
that considering both major errors (e.g. mistranslated tokens) and minor errors
(e.g. imperfections in fluency) can produce high-quality human judgments. This
inspires us to approach the final goal of the evaluation metrics (human-like
evaluations) by automatic error analysis. To this end, we augment BARTScore by
incorporating the human-like error analysis strategies, namely BARTScore++,
where the final score consists of both the evaluations of major errors and
minor errors. Experimental results show that BARTScore++ can consistently
improve the performance of vanilla BARTScore and outperform existing
top-scoring metrics in 20 out of 25 test settings. We hope our technique can
also be extended to other pre-trained model-based metrics. We will release our
code and scripts to facilitate the community.",2022-12-20
"WeCheck: Strong Factual Consistency Checker via Weakly Supervised
  Learning",2022-12-20 08:04:36+00:00,http://arxiv.org/abs/2212.10057v1,"Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Sujian Li, Yajuan Lv",cs.CL,table2text,"A crucial issue of current text generation models is that they often
uncontrollably generate factually inconsistent text with respective of their
inputs. Limited by the lack of annotated data, existing works in evaluating
factual consistency directly transfer the reasoning ability of models trained
on other data-rich upstream tasks like question answering (QA) and natural
language inference (NLI) without any further adaptation. As a result, they
perform poorly on the real generated text and are biased heavily by their
single-source upstream tasks. To alleviate this problem, we propose a weakly
supervised framework that aggregates multiple resources to train a precise and
efficient factual metric, namely WeCheck. WeCheck first utilizes a generative
model to accurately label a real generated sample by aggregating its weak
labels, which are inferred from multiple resources. Then, we train the target
metric model with the weak supervision while taking noises into consideration.
Comprehensive experiments on a variety of tasks demonstrate the strong
performance of WeCheck, which achieves a 3.4\% absolute improvement over
previous state-of-the-art methods on TRUE benchmark on average.",2022-12-20
On the Blind Spots of Model-Based Evaluation Metrics for Text Generation,2022-12-20 06:24:25+00:00,http://arxiv.org/abs/2212.10020v1,"Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James Glass, Yulia Tsvetkov",cs.CL,table2text,"In this work, we explore a useful but often neglected methodology for
robustness analysis of text generation evaluation metrics: stress tests with
synthetic data. Basically, we design and synthesize a wide range of potential
errors and check whether they result in a commensurate drop in the metric
scores. We examine a range of recently proposed evaluation metrics based on
pretrained language models, for the tasks of open-ended generation,
translation, and summarization. Our experiments reveal interesting
insensitivities, biases, or even loopholes in existing metrics. For example, we
find that BERTScore ignores truncation errors in summarization, and MAUVE
(built on top of GPT-2) is insensitive to errors at the beginning of
generations. Further, we investigate the reasons behind these blind spots and
suggest practical workarounds for a more reliable evaluation of text
generation.",2022-12-20
"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",2022-12-19 18:57:05+00:00,http://arxiv.org/abs/2212.09741v2,"Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu",cs.CL,table2text,"We introduce INSTRUCTOR, a new method for computing text embeddings given
task instructions: every text input is embedded together with instructions
explaining the use case (e.g., task and domain descriptions). Unlike encoders
from prior work that are more specialized, INSTRUCTOR is a single embedder that
can generate text embeddings tailored to different downstream tasks and
domains, without any further training. We first annotate instructions for 330
diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive
loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are
unseen during training), ranging from classification and information retrieval
to semantic textual similarity and text generation evaluation. INSTRUCTOR,
while having an order of magnitude fewer parameters than the previous best
model, achieves state-of-the-art performance, with an average improvement of
3.4% compared to the previous best results on the 70 diverse datasets. Our
analysis suggests that INSTRUCTOR is robust to changes in instructions, and
that instruction finetuning mitigates the challenge of training a single model
on diverse datasets. Our model, code, and data are available at
https://instructor-embedding.github.io.",2022-12-19
"Difformer: Empowering Diffusion Model on Embedding Space for Text
  Generation",2022-12-19 12:44:25+00:00,http://arxiv.org/abs/2212.09412v1,"Zhujin Gao, Junliang Guo, Xu Tan, Yongxin Zhu, Fang Zhang, Jiang Bian, Linli Xu","cs.CL, cs.AI, cs.LG",table2text,"Diffusion models have achieved state-of-the-art synthesis quality on visual
and audio tasks, and recent works adapt them to textual data by diffusing on
the embedding space. But the difference between the continuous data space and
the embedding space raises challenges to the diffusion model, which have not
been carefully explored. In this paper, we conduct systematic studies and
analyze the challenges threefold. Firstly, the data distribution is learnable
for embeddings, which may lead to the collapse of the loss function. Secondly,
as the norm of embedding varies between popular and rare words, adding the same
noise scale will lead to sub-optimal results. In addition, we find that noises
sampled from a standard Gaussian distribution may distract the diffusion
process. To solve the above challenges, we propose Difformer, a denoising
diffusion probabilistic model based on Transformer, which consists of three
techniques including utilizing an anchor loss function, a layer normalization
module for embeddings, and a norm factor to the Gaussian noise. All techniques
are complementary to each other and critical to boosting the model performance
together. Experiments are conducted on benchmark datasets over two seminal text
generation tasks including machine translation and text summarization. The
results show that Difformer significantly outperforms the embedding diffusion
baselines, while achieving competitive results with strong autoregressive
baselines.",2022-12-19
SEScore2: Retrieval Augmented Pretraining for Text Generation Evaluation,2022-12-19 09:02:16+00:00,http://arxiv.org/abs/2212.09305v1,"Wenda Xu, Xian Qian, Mingxuan Wang, Lei Li, William Yang Wang",cs.CL,table2text,"Is it possible to leverage large scale raw and raw parallel corpora to build
a general learned metric? Existing learned metrics have gaps to human
judgements, are model-dependent or are limited to the domains or tasks where
human ratings are available. In this paper, we propose SEScore2, a model-based
metric pretrained over million-scale synthetic dataset constructed by our novel
retrieval augmented data synthesis pipeline. SEScore2 achieves high correlation
to human judgements without any human rating supervisions. Importantly, our
unsupervised SEScore2 can outperform supervised metrics, which are trained on
the News human ratings, at the TED domain. We evaluate SEScore2 over four text
generation tasks across three languages. SEScore2 outperforms all prior
unsupervised evaluation metrics in machine translation, speech translation,
data-to-text and dialogue generation, with average Kendall improvements 0.158.
SEScore2 even outperforms SOTA supervised BLEURT at data-to-text, dialogue
generation and overall correlation.",2022-12-19
"Synthesis and Evaluation of a Domain-specific Large Data Set for
  Dungeons & Dragons",2022-12-18 12:54:45+00:00,http://arxiv.org/abs/2212.09080v1,"Akila Peiris, Nisansa de Silva","cs.CL, cs.LG",table2text,"This paper introduces the Forgotten Realms Wiki (FRW) data set and domain
specific natural language generation using FRW along with related analyses.
Forgotten Realms is the de-facto default setting of the popular open ended
tabletop fantasy role playing game, Dungeons & Dragons. The data set was
extracted from the Forgotten Realms Fandom wiki consisting of more than over
45,200 articles. The FRW data set is constituted of 11 sub-data sets in a
number of formats: raw plain text, plain text annotated by article title,
directed link graphs, wiki info-boxes annotated by the wiki article title,
Poincar\'e embedding of first link graph, multiple Word2Vec and Doc2Vec models
of the corpus. This is the first data set of this size for the Dungeons &
Dragons domain. We then present a pairwise similarity comparison benchmark
which utilizes similarity measures. In addition, we perform D&D domain specific
natural language generation using the corpus and evaluate the named entity
classification with respect to the lore of Forgotten Realms.",2022-12-18
RISE: Leveraging Retrieval Techniques for Summarization Evaluation,2022-12-17 01:09:22+00:00,http://arxiv.org/abs/2212.08775v1,"David Uthus, Jianmo Ni",cs.CL,table2text,"Evaluating automatically-generated text summaries is a challenging task.
While there have been many interesting approaches, they still fall short of
human evaluations. We present RISE, a new approach for evaluating summaries by
leveraging techniques from information retrieval. RISE is first trained as a
retrieval task using a dual-encoder retrieval setup, and can then be
subsequently utilized for evaluating a generated summary given an input
document, without gold reference summaries. RISE is especially well suited when
working on new datasets where one may not have reference summaries available
for evaluation. We conduct comprehensive experiments on the SummEval benchmark
(Fabbri et al., 2021) and the results show that RISE has higher correlation
with human evaluations compared to many past approaches to summarization
evaluation. Furthermore, RISE also demonstrates data-efficiency and
generalizability across languages.",2022-12-17
"DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text
  Generation",2022-12-16 21:44:34+00:00,http://arxiv.org/abs/2212.08724v1,"Yuxi Feng, Xiaoyuan Yi, Xiting Wang, Laks V. S. Lakshmanan, Xing Xie",cs.CL,table2text,"Self-training (ST) has prospered again in language understanding by
augmenting the fine-tuning of pre-trained language models when labeled data is
insufficient. However, it remains challenging to incorporate ST into
attribute-controllable language generation. Augmented by only self-generated
pseudo text, generation models over-emphasize exploitation of the previously
learned space, suffering from a constrained generalization boundary. We revisit
ST and propose a novel method, DuNST to alleviate this problem. DuNST jointly
models text generation and classification with a shared Variational AutoEncoder
and corrupts the generated pseudo text by two kinds of flexible noise to
disturb the space. In this way, our model could construct and utilize both
pseudo text from given labels and pseudo labels from available unlabeled text,
which are gradually refined during the ST process. We theoretically demonstrate
that DuNST can be regarded as enhancing exploration towards the potential real
text space, providing a guarantee of improved performance. Experiments on three
controllable generation tasks show that DuNST could significantly boost control
accuracy while maintaining comparable generation fluency and diversity against
several strong baselines.",2022-12-16
"MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text
  Generation",2022-12-16 17:36:23+00:00,http://arxiv.org/abs/2212.08607v1,"Swarnadeep Saha, Xinyan Velocity Yu, Mohit Bansal, Ramakanth Pasunuru, Asli Celikyilmaz","cs.CL, cs.AI, cs.LG",table2text,"Prompting large language models has enabled significant recent progress in
multi-step reasoning over text. However, when applied to text generation from
semi-structured data (e.g., graphs or tables), these methods typically suffer
from low semantic coverage, hallucination, and logical inconsistency. We
propose MURMUR, a neuro-symbolic modular approach to text generation from
semi-structured data with multi-step reasoning. MURMUR is a best-first search
method that generates reasoning paths using: (1) neural and symbolic modules
with specific linguistic and logical skills, (2) a grammar whose production
rules define valid compositions of modules, and (3) value functions that assess
the quality of each reasoning step. We conduct experiments on two diverse
data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in
their data representations (graphs and tables) and span multiple linguistic and
logical skills. MURMUR obtains significant improvements over recent few-shot
baselines like direct prompting and chain-of-thought prompting, while also
achieving comparable performance to fine-tuned GPT-2 on out-of-domain data.
Moreover, human evaluation shows that MURMUR generates highly faithful and
correct reasoning paths that lead to 26% more logically consistent summaries on
LogicNLG, compared to direct prompting.",2022-12-16
"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for
  Programming Languages",2022-12-13 17:21:44+00:00,http://arxiv.org/abs/2212.06742v1,"Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu","cs.CL, cs.LG, cs.PL, cs.SE",table2text,"Software engineers working with the same programming language (PL) may speak
different natural languages (NLs) and vice versa, erecting huge barriers to
communication and working efficiency. Recent studies have demonstrated the
effectiveness of generative pre-training in computer programs, yet they are
always English-centric. In this work, we step towards bridging the gap between
multilingual NLs and multilingual PLs for large language models (LLMs). We
release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.
We employ two methods for universal cross-lingual pre-training: span-corruption
language modeling that learns patterns from monolingual NL or PL; and
pivot-based translation language modeling that relies on parallel data of many
NLs and PLs. Extensive results show that ERNIE-Code outperforms previous
multilingual LLMs for PL or NL across a wide range of end tasks of code
intelligence, including multilingual code-to-text, text-to-code, code-to-code,
and text-to-text generation. We further show its advantage of zero-shot
prompting on multilingual code summarization and text-to-text translation. We
will make our code and pre-trained models publicly available.",2022-12-13
"Collaborating Heterogeneous Natural Language Processing Tasks via
  Federated Learning",2022-12-12 09:27:50+00:00,http://arxiv.org/abs/2212.05789v1,"Chenhe Dong, Yuexiang Xie, Bolin Ding, Ying Shen, Yaliang Li",cs.CL,table2text,"The increasing privacy concerns on personal private text data promote the
development of federated learning (FL) in recent years. However, the existing
studies on applying FL in NLP are not suitable to coordinate participants with
heterogeneous or private learning objectives. In this study, we further broaden
the application scope of FL in NLP by proposing an Assign-Then-Contrast
(denoted as ATC) framework, which enables clients with heterogeneous NLP tasks
to construct an FL course and learn useful knowledge from each other.
Specifically, the clients are suggested to first perform local training with
the unified tasks assigned by the server rather than using their own learning
objectives, which is called the Assign training stage. After that, in the
Contrast training stage, clients train with different local learning objectives
and exchange knowledge with other clients who contribute consistent and useful
model updates. We conduct extensive experiments on six widely-used datasets
covering both Natural Language Understanding (NLU) and Natural Language
Generation (NLG) tasks, and the proposed ATC framework achieves significant
improvements compared with various baseline methods. The source code is
available at
\url{https://github.com/alibaba/FederatedScope/tree/master/federatedscope/nlp/hetero_tasks}.",2022-12-12
T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics,2022-12-12 06:29:04+00:00,http://arxiv.org/abs/2212.05726v1,"Yiwei Qin, Weizhe Yuan, Graham Neubig, Pengfei Liu",cs.CL,table2text,"Modern embedding-based metrics for evaluation of generated text generally
fall into one of two paradigms: discriminative metrics that are trained to
directly predict which outputs are of higher quality according to supervised
human annotations, and generative metrics that are trained to evaluate text
based on the probabilities of a generative model. Both have their advantages;
discriminative metrics are able to directly optimize for the problem of
distinguishing between good and bad outputs, while generative metrics can be
trained using abundant raw text. In this paper, we present a framework that
combines the best of both worlds, using both supervised and unsupervised
signals from whatever data we have available. We operationalize this idea by
training T5Score, a metric that uses these training signals with mT5 as the
backbone. We perform an extensive empirical comparison with other existing
metrics on 5 datasets, 19 languages and 280 systems, demonstrating the utility
of our method. Experimental results show that: T5Score achieves the best
performance on all datasets against existing top-scoring metrics at the segment
level. We release our code and models at https://github.com/qinyiwei/T5Score.",2022-12-12
"The Role of AI in Drug Discovery: Challenges, Opportunities, and
  Strategies",2022-12-08 23:23:39+00:00,http://arxiv.org/abs/2212.08104v1,"Alexandre Blanco-Gonzalez, Alfonso Cabezon, Alejandro Seco-Gonzalez, Daniel Conde-Torres, Paula Antelo-Riveiro, Angel Pineiro, Rebeca Garcia-Fandino","cs.CL, cs.AI, cs.CY",table2text,"Artificial intelligence (AI) has the potential to revolutionize the drug
discovery process, offering improved efficiency, accuracy, and speed. However,
the successful application of AI is dependent on the availability of
high-quality data, the addressing of ethical concerns, and the recognition of
the limitations of AI-based approaches. In this article, the benefits,
challenges and drawbacks of AI in this field are reviewed, and possible
strategies and approaches for overcoming the present obstacles are proposed.
The use of data augmentation, explainable AI, and the integration of AI with
traditional experimental methods, as well as the potential advantages of AI in
pharmaceutical research are also discussed. Overall, this review highlights the
potential of AI in drug discovery and provides insights into the challenges and
opportunities for realizing its potential in this field.
  Note from the human-authors: This article was created to test the ability of
ChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors
in writing review articles. The text generated by the AI following our
instructions (see Supporting Information) was used as a starting point, and its
ability to automatically generate content was evaluated. After conducting a
thorough review, human authors practically rewrote the manuscript, striving to
maintain a balance between the original proposal and scientific criteria. The
advantages and limitations of using AI for this purpose are discussed in the
last section.",2022-12-08
Controlled Language Generation for Language Learning Items,2022-11-28 19:28:12+00:00,http://arxiv.org/abs/2211.15731v1,"Kevin Stowe, Debanjan Ghosh, Mengxuan Zhao","cs.CL, I.2.7",table2text,"This work aims to employ natural language generation (NLG) to rapidly
generate items for English language learning applications: this requires both
language models capable of generating fluent, high-quality English, and to
control the output of the generation to match the requirements of the relevant
items. We experiment with deep pretrained models for this task, developing
novel methods for controlling items for factors relevant in language learning:
diverse sentences for different proficiency levels and argument structure to
test grammar. Human evaluation demonstrates high grammatically scores for all
models (3.4 and above out of 4), and higher length (24%) and complexity (9%)
over the baseline for the advanced proficiency model. Our results show that we
can achieve strong performance while adding additional control to ensure
diverse, tailored content for individual users.",2022-11-28
CodeExp: Explanatory Code Document Generation,2022-11-25 18:05:44+00:00,http://arxiv.org/abs/2211.15395v1,"Haotian Cui, Chenglong Wang, Junjie Huang, Jeevana Priya Inala, Todd Mytkowicz, Bo Wang, Jianfeng Gao, Nan Duan","cs.CL, cs.LG, I.2.2; I.2.7",table2text,"Developing models that can automatically generate detailed code explanation
can greatly benefit software maintenance and programming education. However,
existing code-to-text generation models often produce only high-level summaries
of code that do not capture implementation-level choices essential for these
scenarios. To fill in this gap, we propose the code explanation generation
task. We first conducted a human study to identify the criteria for
high-quality explanatory docstring for code. Based on that, we collected and
refined a large-scale code docstring corpus and formulated automatic evaluation
metrics that best match human assessments. Finally, we present a multi-stage
fine-tuning strategy and baseline models for the task. Our experiments show
that (1) our refined training dataset lets models achieve better performance in
the explanation generation tasks compared to larger unrefined data (15x
larger), and (2) fine-tuned models can generate well-structured long docstrings
comparable to human-written ones. We envision our training dataset,
human-evaluation protocol, recommended metrics, and fine-tuning strategy can
boost future code explanation research. The code and annotated data are
available at https://github.com/subercui/CodeExp.",2022-11-25
"MUSIED: A Benchmark for Event Detection from Multi-Source Heterogeneous
  Informal Texts",2022-11-25 05:05:29+00:00,http://arxiv.org/abs/2211.13896v1,"Xiangyu Xi, Jianwei Lv, Shuaipeng Liu, Wei Ye, Fan Yang, Guanglu Wan",cs.CL,table2text,"Event detection (ED) identifies and classifies event triggers from
unstructured texts, serving as a fundamental task for information extraction.
Despite the remarkable progress achieved in the past several years, most
research efforts focus on detecting events from formal texts (e.g., news
articles, Wikipedia documents, financial announcements). Moreover, the texts in
each dataset are either from a single source or multiple yet relatively
homogeneous sources. With massive amounts of user-generated text accumulating
on the Web and inside enterprises, identifying meaningful events in these
informal texts, usually from multiple heterogeneous sources, has become a
problem of significant practical value. As a pioneering exploration that
expands event detection to the scenarios involving informal and heterogeneous
texts, we propose a new large-scale Chinese event detection dataset based on
user reviews, text conversations, and phone conversations in a leading
e-commerce platform for food service. We carefully investigate the proposed
dataset's textual informality and multi-source heterogeneity characteristics by
inspecting data samples quantitatively and qualitatively. Extensive experiments
with state-of-the-art event detection methods verify the unique challenges
posed by these characteristics, indicating that multi-source informal event
detection remains an open problem and requires further efforts. Our benchmark
and code are released at \url{https://github.com/myeclipse/MUSIED}.",2022-11-25
Retrieval-Augmented Multimodal Language Modeling,2022-11-22 20:26:44+00:00,http://arxiv.org/abs/2211.12561v1,"Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, Wen-tau Yih","cs.CV, cs.CL, cs.LG",table2text,"Recent multimodal models such as DALL-E and CM3 have achieved remarkable
progress in text-to-image and image-to-text generation. However, these models
store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the
model parameters, requiring increasingly larger models and training data to
capture more knowledge. To integrate knowledge in a more scalable and modular
way, we propose a retrieval-augmented multimodal model, which enables a base
multimodal model (generator) to refer to relevant knowledge fetched by a
retriever from external memory (e.g., multimodal documents on the web).
Specifically, we implement a retriever using the pretrained CLIP model and a
generator using the CM3 Transformer architecture, and train this model using
the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3),
is the first multimodal model that can retrieve and generate mixtures of text
and images. We show that RA-CM3 significantly outperforms baseline multimodal
models such as DALL-E and CM3 on both image and caption generation tasks (12
FID and 17 CIDEr improvements on MS-COCO), while requiring much less compute
for training (<30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel
capabilities such as knowledge-intensive image generation and multimodal
in-context learning.",2022-11-22
"How to Describe Images in a More Funny Way? Towards a Modular Approach
  to Cross-Modal Sarcasm Generation",2022-11-20 14:38:24+00:00,http://arxiv.org/abs/2211.10992v1,"Jie Ruan, Yue Wu, Xiaojun Wan, Yuesheng Zhu","cs.CV, cs.CL",table2text,"Sarcasm generation has been investigated in previous studies by considering
it as a text-to-text generation problem, i.e., generating a sarcastic sentence
for an input sentence. In this paper, we study a new problem of cross-modal
sarcasm generation (CMSG), i.e., generating a sarcastic description for a given
image. CMSG is challenging as models need to satisfy the characteristics of
sarcasm, as well as the correlation between different modalities. In addition,
there should be some inconsistency between the two modalities, which requires
imagination. Moreover, high-quality training data is insufficient. To address
these problems, we take a step toward generating sarcastic descriptions from
images without paired training data and propose an
Extraction-Generation-Ranking based Modular method (EGRM) for cross-model
sarcasm generation. Specifically, EGRM first extracts diverse information from
an image at different levels and uses the obtained image tags, sentimental
descriptive caption, and commonsense-based consequence to generate candidate
sarcastic texts. Then, a comprehensive ranking algorithm, which considers
image-text relation, sarcasticness, and grammaticality, is proposed to select a
final text from the candidate texts. Human evaluation at five criteria on a
total of 1200 generated image-text pairs from eight systems and auxiliary
automatic evaluation show the superiority of our method.",2022-11-20
"GENIUS: Sketch-based Language Model Pre-training via Extreme and
  Selective Masking for Text Generation and Augmentation",2022-11-18 16:39:45+00:00,http://arxiv.org/abs/2211.10330v1,"Biyang Guo, Yeyun Gong, Yelong Shen, Songqiao Han, Hailiang Huang, Nan Duan, Weizhu Chen",cs.CL,table2text,"We introduce GENIUS: a conditional text generation model using sketches as
input, which can fill in the missing contexts for a given sketch (key
information consisting of textual spans, phrases, or words, concatenated by
mask tokens). GENIUS is pre-trained on a large-scale textual corpus with a
novel reconstruction from sketch objective using an extreme and selective
masking strategy, enabling it to generate diverse and high-quality texts given
sketches. Comparison with other competitive conditional language models (CLMs)
reveals the superiority of GENIUS's text generation quality. We further show
that GENIUS can be used as a strong and ready-to-use data augmentation tool for
various natural language processing (NLP) tasks. Most existing textual data
augmentation methods are either too conservative, by making small changes to
the original text, or too aggressive, by creating entirely new samples. With
GENIUS, we propose GeniusAug, which first extracts the target-aware sketches
from the original training set and then generates new samples based on the
sketches. Empirical experiments on 6 text classification datasets show that
GeniusAug significantly improves the models' performance in both
in-distribution (ID) and out-of-distribution (OOD) settings. We also
demonstrate the effectiveness of GeniusAug on named entity recognition (NER)
and machine reading comprehension (MRC) tasks. (Code and models are publicly
available at https://github.com/microsoft/SCGLab and
https://github.com/beyondguo/genius)",2022-11-18
"Towards Computationally Verifiable Semantic Grounding for Language
  Models",2022-11-16 17:35:52+00:00,http://arxiv.org/abs/2211.09070v1,"Chris Alberti, Kuzman Ganchev, Michael Collins, Sebastian Gehrmann, Ciprian Chelba",cs.CL,table2text,"The paper presents an approach to semantic grounding of language models (LMs)
that conceptualizes the LM as a conditional model generating text given a
desired semantic message formalized as a set of entity-relationship triples. It
embeds the LM in an auto-encoder by feeding its output to a semantic parser
whose output is in the same representation domain as the input message.
Compared to a baseline that generates text using greedy search, we demonstrate
two techniques that improve the fluency and semantic accuracy of the generated
text: The first technique samples multiple candidate text sequences from which
the semantic parser chooses. The second trains the language model while keeping
the semantic parser frozen to improve the semantic accuracy of the
auto-encoder. We carry out experiments on the English WebNLG 3.0 data set,
using BLEU to measure the fluency of generated text and standard parsing
metrics to measure semantic accuracy. We show that our proposed approaches
significantly improve on the greedy search baseline. Human evaluation
corroborates the results of the automatic evaluation experiments.",2022-11-16
Reward Gaming in Conditional Text Generation,2022-11-16 07:10:02+00:00,http://arxiv.org/abs/2211.08714v1,"Richard Yuanzhe Pang, Vishakh Padmakumar, Thibault Sellam, Ankur P. Parikh, He He","cs.CL, cs.AI, cs.LG",table2text,"To align conditional text generation model outputs with desired behaviors,
there has been an increasing focus on training the model using reinforcement
learning (RL) with reward functions learned from human annotations. Under this
framework, we identify three common cases where high rewards are incorrectly
assigned to undesirable patterns: noise-induced spurious correlation, naturally
occurring spurious correlation, and covariate shift. We show that even though
learned metrics achieve high performance on the distribution of the data used
to train the reward function, the undesirable patterns may be amplified during
RL training of the text generation model. While there has been discussion about
reward gaming in the RL or safety community, in this short discussion piece, we
would like to highlight reward gaming in the NLG community using concrete
conditional text generation examples and discuss potential fixes and areas for
future work.",2022-11-16
"Follow the Wisdom of the Crowd: Effective Text Generation via Minimum
  Bayes Risk Decoding",2022-11-14 18:57:37+00:00,http://arxiv.org/abs/2211.07634v1,"Mirac Suzgun, Luke Melas-Kyriazi, Dan Jurafsky","cs.CL, cs.LG",table2text,"In open-ended natural-language generation, existing text decoding methods
typically struggle to produce text which is both diverse and high-quality.
Greedy and beam search are known to suffer from text degeneration and
linguistic diversity issues, while temperature, top-k, and nucleus sampling
often yield diverse but low-quality outputs. In this work, we present crowd
sampling, a family of decoding methods based on Bayesian risk minimization, to
address this diversity-quality trade-off. Inspired by the principle of ""the
wisdom of the crowd,"" crowd sampling seeks to select a candidate from a pool of
candidates that has the least expected risk (i.e., highest expected reward)
under a generative model according to a given utility function. Crowd sampling
can be seen as a generalization of numerous existing methods, including
majority voting, and in practice, it can be used as a drop-in replacement for
existing sampling methods. Extensive experiments show that crowd sampling
delivers improvements of 3-7 ROUGE and BLEU points across a wide range of
tasks, including summarization, data-to-text, translation, and textual style
transfer, while achieving new state-of-the-art results on WebNLG and WMT'16.",2022-11-14
"Evade the Trap of Mediocrity: Promoting Diversity and Novelty in Text
  Generation via Concentrating Attention",2022-11-14 07:53:16+00:00,http://arxiv.org/abs/2211.07164v1,"Wenhao Li, Xiaoyuan Yi, Jinyi Hu, Maosong Sun, Xing Xie",cs.CL,table2text,"Recently, powerful Transformer architectures have proven superior in
generating high-quality sentences. Nevertheless, these models tend to produce
dull high-frequency phrases, severely hurting the diversity and novelty of
generated text. In this work, we dig into the intrinsic mechanism of this
problem and found that sparser attention values in Transformer could improve
diversity. To understand such a phenomenon, we first conduct both empirical and
theoretical analysis and then attribute it to representation degeneration
caused by the attentive mixture of the hidden states during training. We term
this process the Trap of Mediocrity. To escape from such a trap, we introduce a
novel attention regularization loss to control the sharpness of the attention
distribution, which is transparent to model structures and can be easily
implemented within 20 lines of python code. We prove that this method could be
mathematically regarded as learning a Bayesian approximation of posterior
attention. Experiments show that our method improved the diversity and novelty
of the generated text while maintaining comparable quality on a variety of
conditional and unconditional generation tasks.",2022-11-14
Controllable Citation Text Generation,2022-11-14 01:54:08+00:00,http://arxiv.org/abs/2211.07066v1,"Nianlong Gu, Richard H. R. Hahnloser",cs.CL,table2text,"The aim of citation generation is usually to automatically generate a
citation sentence that refers to a chosen paper in the context of a manuscript.
However, a rigid citation generation process is at odds with an author's desire
to control the generated text based on certain attributes, such as 1) the
citation intent of e.g. either introducing background information or comparing
results; 2) keywords that should appear in the citation text; or 3) specific
sentences in the cited paper that characterize the citation content. To provide
these degrees of freedom, we present a controllable citation generation system.
In data from a large corpus, we first parse the attributes of each citation
sentence and use these as additional input sources during training of the
BART-based abstractive summarizer. We further develop an attribute suggestion
module that infers the citation intent and suggests relevant keywords and
sentences that users can select to tune the generation. Our framework gives
users more control over generated citations, outperforming citation generation
models without attribute awareness in both ROUGE and human evaluations.",2022-11-14
Self-conditioned Embedding Diffusion for Text Generation,2022-11-08 13:30:27+00:00,http://arxiv.org/abs/2211.04236v1,"Robin Strudel, Corentin Tallec, Florent Altché, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, Rémi Leblond","cs.CL, cs.LG",table2text,"Can continuous diffusion models bring the same performance breakthrough on
natural language they did for image generation? To circumvent the discrete
nature of text data, we can simply project tokens in a continuous space of
embeddings, as is standard in language modeling. We propose Self-conditioned
Embedding Diffusion, a continuous diffusion mechanism that operates on token
embeddings and allows to learn flexible and scalable diffusion models for both
conditional and unconditional text generation. Through qualitative and
quantitative evaluation, we show that our text diffusion models generate
samples comparable with those produced by standard autoregressive language
models - while being in theory more efficient on accelerator hardware at
inference time. Our work paves the way for scaling up diffusion models for
text, similarly to autoregressive models, and for improving performance with
recent refinements to continuous diffusion.",2022-11-08
Generative Transformers for Design Concept Generation,2022-11-07 11:29:10+00:00,http://arxiv.org/abs/2211.03468v1,"Qihao Zhu, Jianxi Luo",cs.CL,table2text,"Generating novel and useful concepts is essential during the early design
stage to explore a large variety of design opportunities, which usually
requires advanced design thinking ability and a wide range of knowledge from
designers. Growing works on computer-aided tools have explored the retrieval of
knowledge and heuristics from design data. However, they only provide stimuli
to inspire designers from limited aspects. This study explores the recent
advance of the natural language generation (NLG) technique in the artificial
intelligence (AI) field to automate the early-stage design concept generation.
Specifically, a novel approach utilizing the generative pre-trained transformer
(GPT) is proposed to leverage the knowledge and reasoning from textual data and
transform them into new concepts in understandable language. Three concept
generation tasks are defined to leverage different knowledge and reasoning:
domain knowledge synthesis, problem-driven synthesis, and analogy-driven
synthesis. The experiments with both human and data-driven evaluation show good
performance in generating novel and useful concepts.",2022-11-07
"Human-Machine Collaboration Approaches to Build a Dialogue Dataset for
  Hate Speech Countering",2022-11-07 10:37:13+00:00,http://arxiv.org/abs/2211.03433v1,"Helena Bonaldi, Sara Dellantonio, Serra Sinem Tekiroglu, Marco Guerini","cs.CL, cs.CY",table2text,"Fighting online hate speech is a challenge that is usually addressed using
Natural Language Processing via automatic detection and removal of hate
content. Besides this approach, counter narratives have emerged as an effective
tool employed by NGOs to respond to online hate on social media platforms. For
this reason, Natural Language Generation is currently being studied as a way to
automatize counter narrative writing. However, the existing resources necessary
to train NLG models are limited to 2-turn interactions (a hate speech and a
counter narrative as response), while in real life, interactions can consist of
multiple turns. In this paper, we present a hybrid approach for dialogical data
collection, which combines the intervention of human expert annotators over
machine generated dialogues obtained using 19 different configurations. The
result of this work is DIALOCONAN, the first dataset comprising over 3000
fictitious multi-turn dialogues between a hater and an NGO operator, covering 6
targets of hate.",2022-11-07
Time-aware Prompting for Text Generation,2022-11-03 22:10:25+00:00,http://arxiv.org/abs/2211.02162v1,"Shuyang Cao, Lu Wang",cs.CL,table2text,"In this paper, we study the effects of incorporating timestamps, such as
document creation dates, into generation systems. Two types of time-aware
prompts are investigated: (1) textual prompts that encode document timestamps
in natural language sentences; and (2) linear prompts that convert timestamps
into continuous vectors. To explore extrapolation to future data points, we
further introduce a new data-to-text generation dataset, TempWikiBio,
containing more than 4 millions of chronologically ordered revisions of
biographical articles from English Wikipedia, each paired with structured
personal profiles. Through data-to-text generation on TempWikiBio, text-to-text
generation on the content transfer dataset, and summarization on XSum, we show
that linear prompts on encoder and textual prompts improve the generation
quality on all datasets. Despite having less performance drop when testing on
data drawn from a later time, linear prompts focus more on non-temporal
information and are less sensitive to the given timestamps, according to human
evaluations and sensitivity analyses. Meanwhile, textual prompts establish the
association between the given timestamps and the output dates, yielding more
factual temporal information in the output.",2022-11-03
TaTa: A Multilingual Table-to-Text Dataset for African Languages,2022-10-31 21:05:42+00:00,http://arxiv.org/abs/2211.00142v1,"Sebastian Gehrmann, Sebastian Ruder, Vitaly Nikolaev, Jan A. Botha, Michael Chavinda, Ankur Parikh, Clara Rivera","cs.CL, cs.LG",table2text,"Existing data-to-text generation datasets are mostly limited to English. To
address this lack of data, we create Table-to-Text in African languages (TaTa),
the first large multilingual table-to-text dataset with a focus on African
languages. We created TaTa by transcribing figures and accompanying text in
bilingual reports by the Demographic and Health Surveys Program, followed by
professional translation to make the dataset fully parallel. TaTa includes
8,700 examples in nine languages including four African languages (Hausa, Igbo,
Swahili, and Yor\`ub\'a) and a zero-shot test language (Russian). We
additionally release screenshots of the original figures for future research on
multilingual multi-modal approaches. Through an in-depth human evaluation, we
show that TaTa is challenging for current models and that less than half the
outputs from an mT5-XXL-based model are understandable and attributable to the
source data. We further demonstrate that existing metrics perform poorly for
TaTa and introduce learned metrics that achieve a high correlation with human
judgments. We release all data and annotations at
https://github.com/google-research/url-nlp.",2022-10-31
DiffusER: Discrete Diffusion via Edit-based Reconstruction,2022-10-30 16:55:23+00:00,http://arxiv.org/abs/2210.16886v1,"Machel Reid, Vincent J. Hellendoorn, Graham Neubig","cs.CL, cs.LG",table2text,"In text generation, models that generate text from scratch one token at a
time are currently the dominant paradigm. Despite being performant, these
models lack the ability to revise existing text, which limits their usability
in many practical scenarios. We look to address this, with DiffusER (Diffusion
via Edit-based Reconstruction), a new edit-based generative model for text
based on denoising diffusion models -- a class of models that use a Markov
chain of denoising steps to incrementally generate data. DiffusER is not only a
strong generative model in general, rivalling autoregressive models on several
tasks spanning machine translation, summarization, and style transfer; it can
also perform other varieties of generation that standard autoregressive models
are not well-suited for. For instance, we demonstrate that DiffusER makes it
possible for a user to condition generation on a prototype, or an incomplete
sequence, and continue revising based on previous edit steps.",2022-10-30
"Diverse Parallel Data Synthesis for Cross-Database Adaptation of
  Text-to-SQL Parsers",2022-10-29 14:30:53+00:00,http://arxiv.org/abs/2210.16613v1,"Abhijeet Awasthi, Ashutosh Sathe, Sunita Sarawagi","cs.CL, cs.AI, cs.LG",table2text,"Text-to-SQL parsers typically struggle with databases unseen during the train
time. Adapting parsers to new databases is a challenging problem due to the
lack of natural language queries in the new schemas. We present ReFill, a
framework for synthesizing high-quality and textually diverse parallel datasets
for adapting a Text-to-SQL parser to a target schema. ReFill learns to
retrieve-and-edit text queries from the existing schemas and transfers them to
the target schema. We show that retrieving diverse existing text, masking their
schema-specific tokens, and refilling with tokens relevant to the target
schema, leads to significantly more diverse text queries than achievable by
standard SQL-to-Text generation methods. Through experiments spanning multiple
databases, we demonstrate that fine-tuning parsers on datasets synthesized
using ReFill consistently outperforms the prior data-augmentation methods.",2022-10-29
Nearest Neighbor Language Models for Stylistic Controllable Generation,2022-10-27 20:46:12+00:00,http://arxiv.org/abs/2210.15762v1,"Severino Trotta, Lucie Flek, Charles Welch",cs.CL,table2text,"Recent language modeling performance has been greatly improved by the use of
external memory. This memory encodes the context so that similar contexts can
be recalled during decoding. This similarity depends on how the model learns to
encode context, which can be altered to include other attributes, such as
style. We construct and evaluate an architecture for this purpose, using
corpora annotated for politeness, formality, and toxicity. Through extensive
experiments and human evaluation we demonstrate the potential of our method to
generate text while controlling style. We find that style-specific datastores
improve generation performance, though results vary greatly across styles, and
the effect of pretraining data and specific styles should be explored in future
work.",2022-10-27
Categorical SDEs with Simplex Diffusion,2022-10-26 15:27:43+00:00,http://arxiv.org/abs/2210.14784v1,"Pierre H. Richemond, Sander Dieleman, Arnaud Doucet",cs.LG,table2text,"Diffusion models typically operate in the standard framework of generative
modelling by producing continuously-valued datapoints. To this end, they rely
on a progressive Gaussian smoothing of the original data distribution, which
admits an SDE interpretation involving increments of a standard Brownian
motion. However, some applications such as text generation or reinforcement
learning might naturally be better served by diffusing categorical-valued data,
i.e., lifting the diffusion to a space of probability distributions. To this
end, this short theoretical note proposes Simplex Diffusion, a means to
directly diffuse datapoints located on an n-dimensional probability simplex. We
show how this relates to the Dirichlet distribution on the simplex and how the
analogous SDE is realized thanks to a multi-dimensional Cox-Ingersoll-Ross
process (abbreviated as CIR), previously used in economics and mathematical
finance. Finally, we make remarks as to the numerical implementation of
trajectories of the CIR process, and discuss some limitations of our approach.",2022-10-26
SentBS: Sentence-level Beam Search for Controllable Summarization,2022-10-26 06:21:01+00:00,http://arxiv.org/abs/2210.14502v1,"Chenhui Shen, Liying Cheng, Lidong Bing, Yang You, Luo Si",cs.CL,table2text,"A wide range of control perspectives have been explored in controllable text
generation. Structure-controlled summarization is recently proposed as a useful
and interesting research direction. However, current structure-controlling
methods have limited effectiveness in enforcing the desired structure. To
address this limitation, we propose a sentence-level beam search generation
method (SentBS), where evaluation is conducted throughout the generation
process to select suitable sentences for subsequent generations. We experiment
with different combinations of decoding methods to be used as subcomponents by
SentBS and evaluate results on the structure-controlled dataset MReD.
Experiments show that all explored combinations for SentBS can improve the
agreement between the generated text and the desired structure, with the best
method significantly reducing the structural discrepancies suffered by the
existing model, by approximately 68%.",2022-10-26
On the Effectiveness of Automated Metrics for Text Generation Systems,2022-10-24 08:15:28+00:00,http://arxiv.org/abs/2210.13025v1,"Pius von Däniken, Jan Deriu, Don Tuggener, Mark Cieliebak","cs.CL, cs.AI",table2text,"A major challenge in the field of Text Generation is evaluation because we
lack a sound theory that can be leveraged to extract guidelines for evaluation
campaigns. In this work, we propose a first step towards such a theory that
incorporates different sources of uncertainty, such as imperfect automated
metrics and insufficiently sized test sets. The theory has practical
applications, such as determining the number of samples needed to reliably
distinguish the performance of a set of Text Generation systems in a given
setting. We showcase the application of the theory on the WMT 21 and
Spot-The-Bot evaluation data and outline how it can be leveraged to improve the
evaluation protocol regarding the reliability, robustness, and significance of
the evaluation outcome.",2022-10-24
"Finding Memo: Extractive Memorization in Constrained Sequence Generation
  Tasks",2022-10-24 03:01:52+00:00,http://arxiv.org/abs/2210.12929v1,"Vikas Raunak, Arul Menezes","cs.CL, cs.AI, cs.LG",table2text,"Memorization presents a challenge for several constrained Natural Language
Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the
proclivity of neural models to memorize noisy and atypical samples reacts
adversely with the noisy (web crawled) datasets. However, previous studies of
memorization in constrained NLG tasks have only focused on counterfactual
memorization, linking it to the problem of hallucinations. In this work, we
propose a new, inexpensive algorithm for extractive memorization (exact
training data generation under insufficient context) in constrained sequence
generation tasks and use it to study extractive memorization and its effects in
NMT. We demonstrate that extractive memorization poses a serious threat to NMT
reliability by qualitatively and quantitatively characterizing the memorized
samples as well as the model behavior in their vicinity. Based on empirical
observations, we develop a simple algorithm which elicits non-memorized
translations of memorized samples from the same model, for a large fraction of
such samples. Finally, we show that the proposed algorithm could also be
leveraged to mitigate memorization in the model through finetuning. We have
released the code to reproduce our results at
https://github.com/vyraun/Finding-Memo.",2022-10-24
"Mapping Process for the Task: Wikidata Statements to Text as Wikipedia
  Sentences",2022-10-23 08:34:33+00:00,http://arxiv.org/abs/2210.12659v1,"Hoang Thang Ta, Alexander Gelbukha, Grigori Sidorov","cs.CL, cs.AI",table2text,"Acknowledged as one of the most successful online cooperative projects in
human society, Wikipedia has obtained rapid growth in recent years and desires
continuously to expand content and disseminate knowledge values for everyone
globally. The shortage of volunteers brings to Wikipedia many issues, including
developing content for over 300 languages at the present. Therefore, the
benefit that machines can automatically generate content to reduce human
efforts on Wikipedia language projects could be considerable. In this paper, we
propose our mapping process for the task of converting Wikidata statements to
natural language text (WS2T) for Wikipedia projects at the sentence level. The
main step is to organize statements, represented as a group of quadruples and
triples, and then to map them to corresponding sentences in English Wikipedia.
We evaluate the output corpus in various aspects: sentence structure analysis,
noise filtering, and relationships between sentence components based on word
embedding models. The results are helpful not only for the data-to-text
generation task but also for other relevant works in the field.",2022-10-23
"Hard Gate Knowledge Distillation -- Leverage Calibration for Robust and
  Reliable Language Model",2022-10-22 11:57:10+00:00,http://arxiv.org/abs/2210.12427v1,"Dongkyu Lee, Zhiliang Tian, Yingxiu Zhao, Ka Chun Cheung, Nevin L. Zhang","cs.CL, cs.AI",table2text,"In knowledge distillation, a student model is trained with supervisions from
both knowledge from a teacher and observations drawn from a training data
distribution. Knowledge of a teacher is considered a subject that holds
inter-class relations which send a meaningful supervision to a student; hence,
much effort has been put to find such knowledge to be distilled. In this paper,
we explore a question that has been given little attention: ""when to distill
such knowledge."" The question is answered in our work with the concept of model
calibration; we view a teacher model not only as a source of knowledge but also
as a gauge to detect miscalibration of a student. This simple and yet novel
view leads to a hard gate knowledge distillation scheme that switches between
learning from a teacher model and training data. We verify the gating mechanism
in the context of natural language generation at both the token-level and the
sentence-level. Empirical comparisons with strong baselines show that hard gate
knowledge distillation not only improves model generalization, but also
significantly lowers model calibration error.",2022-10-22
"Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in
  Transformer-Based Variational AutoEncoder for Diverse Text Generation",2022-10-22 10:25:35+00:00,http://arxiv.org/abs/2210.12409v2,"Jinyi Hu, Xiaoyuan Yi, Wenhao Li, Maosong Sun, Xing Xie",cs.CL,table2text,"Variational Auto-Encoder (VAE) has been widely adopted in text generation.
Among many variants, recurrent VAE learns token-wise latent variables with each
conditioned on the preceding ones, which captures sequential variability better
in the era of RNN. However, it is unclear how to incorporate such recurrent
dynamics into the recently dominant Transformer due to its parallelism. In this
work, we propose TRACE, a Transformer-based recurrent VAE structure. TRACE
imposes recurrence on segment-wise latent variables with arbitrarily separated
text segments and constructs the posterior distribution with residual
parameterization. Besides, we design an acceleration method by approximating
idempotent matrices, which allows parallelism while maintaining the conditional
dependence of latent variables. We demonstrate that TRACE could enhance the
entanglement of each segment and preceding latent variables and deduce a
non-zero lower bound of the KL term, providing a theoretical guarantee of
generation diversity. Experiments on two unconditional and one conditional
generation tasks show that TRACE achieves significantly improved diversity
while maintaining satisfactory generation quality.",2022-10-22
"ReasTAP: Injecting Table Reasoning Skills During Pre-training via
  Synthetic Reasoning Examples",2022-10-22 07:04:02+00:00,http://arxiv.org/abs/2210.12374v1,"Yilun Zhao, Linyong Nan, Zhenting Qi, Rui Zhang, Dragomir Radev",cs.CL,table2text,"Reasoning over tabular data requires both table structure understanding and a
broad set of table reasoning skills. Current models with table-specific
architectures and pre-training methods perform well on understanding table
structures, but they still struggle with tasks that require various table
reasoning skills. In this work, we develop ReasTAP to show that high-level
table reasoning skills can be injected into models during pre-training without
a complex table-specific architecture design. We define 7 table reasoning
skills, such as numerical operation, temporal comparison, and conjunction. Each
reasoning skill is associated with one example generator, which synthesizes
questions over semi-structured tables according to the sampled templates. We
model the table pre-training task as a sequence generation task and pre-train
ReasTAP to generate precise answers to the synthetic examples. ReasTAP is
evaluated on four benchmarks covering three downstream tasks including: 1)
WikiSQL and WTQ for Table Question Answering; 2) TabFact for Table Fact
Verification; and 3) LogicNLG for Faithful Table-to-Text Generation.
Experimental results demonstrate that ReasTAP achieves new state-of-the-art
performance on all benchmarks and delivers a significant improvement on
low-resource setting. Our code is publicly available at
https://github.com/Yale-LILY/ReasTAP.",2022-10-22
"The University of Edinburgh's Submission to the WMT22 Code-Mixing Shared
  Task (MixMT)",2022-10-20 14:40:10+00:00,http://arxiv.org/abs/2210.11309v1,"Faheem Kirefu, Vivek Iyer, Pinzhen Chen, Laurie Burchell",cs.CL,table2text,"The University of Edinburgh participated in the WMT22 shared task on
code-mixed translation. This consists of two subtasks: i) generating code-mixed
Hindi/English (Hinglish) text generation from parallel Hindi and English
sentences and ii) machine translation from Hinglish to English. As both
subtasks are considered low-resource, we focused our efforts on careful data
generation and curation, especially the use of backtranslation from monolingual
resources. For subtask 1 we explored the effects of constrained decoding on
English and transliterated subwords in order to produce Hinglish. For subtask
2, we investigated different pretraining techniques, namely comparing simple
initialisation from existing machine translation models and aligned
augmentation. For both subtasks, we found that our baseline systems worked
best. Our systems for both subtasks were one of the overall top-performing
submissions.",2022-10-20
Image Semantic Relation Generation,2022-10-19 16:15:19+00:00,http://arxiv.org/abs/2210.11253v1,Mingzhe Du,"cs.CV, cs.CL",table2text,"Scene graphs provide structured semantic understanding beyond images. For
downstream tasks, such as image retrieval, visual question answering, visual
relationship detection, and even autonomous vehicle technology, scene graphs
can not only distil complex image information but also correct the bias of
visual models using semantic-level relations, which has broad application
prospects. However, the heavy labour cost of constructing graph annotations may
hinder the application of PSG in practical scenarios. Inspired by the
observation that people usually identify the subject and object first and then
determine the relationship between them, we proposed to decouple the scene
graphs generation task into two sub-tasks: 1) an image segmentation task to
pick up the qualified objects. 2) a restricted auto-regressive text generation
task to generate the relation between given objects. Therefore, in this work,
we introduce image semantic relation generation (ISRG), a simple but effective
image-to-text model, which achieved 31 points on the OpenPSG dataset and
outperforms strong baselines respectively by 16 points (ResNet-50) and 5 points
(CLIP).",2022-10-19
NGEP: A Graph-based Event Planning Framework for Story Generation,2022-10-19 14:49:27+00:00,http://arxiv.org/abs/2210.10602v1,"Chen Tang, Zhihao Zhang, Tyler Loakman, Chenghua Lin, Frank Guerin","cs.CL, cs.AI",table2text,"To improve the performance of long text generation, recent studies have
leveraged automatically planned event structures (i.e. storylines) to guide
story generation. Such prior works mostly employ end-to-end neural generation
models to predict event sequences for a story. However, such generation models
struggle to guarantee the narrative coherence of separate events due to the
hallucination problem, and additionally the generated event sequences are often
hard to control due to the end-to-end nature of the models. To address these
challenges, we propose NGEP, an novel event planning framework which generates
an event sequence by performing inference on an automatically constructed event
graph and enhances generalisation ability through a neural event advisor. We
conduct a range of experiments on multiple criteria, and the results
demonstrate that our graph-based neural framework outperforms the
state-of-the-art (SOTA) event planning approaches, considering both the
performance of event sequence generation and the effectiveness on the
downstream task of story generation.",2022-10-19
"Attribution and Obfuscation of Neural Text Authorship: A Data Mining
  Perspective",2022-10-19 11:53:13+00:00,http://arxiv.org/abs/2210.10488v2,"Adaku Uchendu, Thai Le, Dongwon Lee","cs.CL, cs.LG",table2text,"Two interlocking research questions of growing interest and importance in
privacy research are Authorship Attribution (AA) and Authorship Obfuscation
(AO). Given an artifact, especially a text t in question, an AA solution aims
to accurately attribute t to its true author out of many candidate authors
while an AO solution aims to modify t to hide its true authorship.
Traditionally, the notion of authorship and its accompanying privacy concern is
only toward human authors. However, in recent years, due to the explosive
advancements in Neural Text Generation (NTG) techniques in NLP, capable of
synthesizing human-quality open-ended texts (so-called ""neural texts""), one has
to now consider authorships by humans, machines, or their combination. Due to
the implications and potential threats of neural texts when used maliciously,
it has become critical to understand the limitations of traditional AA/AO
solutions and develop novel AA/AO solutions in dealing with neural texts. In
this survey, therefore, we make a comprehensive review of recent literature on
the attribution and obfuscation of neural text authorship from a Data Mining
perspective, and share our view on their limitations and promising research
directions.",2022-10-19
"Team Flow at DRC2022: Pipeline System for Travel Destination
  Recommendation Task in Spoken Dialogue",2022-10-18 01:11:16+00:00,http://arxiv.org/abs/2210.09518v1,"Ryu Hirai, Atsumoto Ohashi, Ao Guo, Hideki Shiroma, Xulin Zhou, Yukihiko Tone, Shinya Iizuka, Ryuichiro Higashinaka","cs.CL, cs.AI, cs.RO",table2text,"To improve the interactive capabilities of a dialogue system, e.g., to adapt
to different customers, the Dialogue Robot Competition (DRC2022) was held. As
one of the teams, we built a dialogue system with a pipeline structure
containing four modules. The natural language understanding (NLU) and natural
language generation (NLG) modules were GPT-2 based models, and the dialogue
state tracking (DST) and policy modules were designed on the basis of
hand-crafted rules. After the preliminary round of the competition, we found
that the low variation in training examples for the NLU and failed
recommendation due to the policy used were probably the main reasons for the
limited performance of the system.",2022-10-18
Table-To-Text generation and pre-training with TabT5,2022-10-17 15:05:53+00:00,http://arxiv.org/abs/2210.09162v1,"Ewa Andrejczuk, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Yasemin Altun","cs.CL, cs.LG",table2text,"Encoder-only transformer models have been successfully applied to different
table understanding tasks, as in TAPAS (Herzig et al., 2020). A major
limitation of these architectures is that they are constrained to
classification-like tasks such as cell selection or entailment detection. We
present TABT5, an encoder-decoder model that generates natural language text
based on tables and textual inputs. TABT5 overcomes the encoder-only limitation
by incorporating a decoder component and leverages the input structure with
table specific embeddings and pre-training. TABT5 achieves new state-of-the-art
results on several domains, including spreadsheet formula prediction with a 15%
increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and
data-to-text generation with a 2.5% increase in BLEU.",2022-10-17
Model Criticism for Long-Form Text Generation,2022-10-16 04:35:58+00:00,http://arxiv.org/abs/2210.08444v1,"Yuntian Deng, Volodymyr Kuleshov, Alexander M. Rush","cs.CL, cs.LG, stat.ML",table2text,"Language models have demonstrated the ability to generate highly fluent text;
however, it remains unclear whether their output retains coherent high-level
structure (e.g., story progression). Here, we propose to apply a statistical
tool, model criticism in latent space, to evaluate the high-level structure of
the generated text. Model criticism compares the distributions between real and
generated data in a latent space obtained according to an assumptive generative
process. Different generative processes identify specific failure modes of the
underlying model. We perform experiments on three representative aspects of
high-level discourse -- coherence, coreference, and topicality -- and find that
transformer-based language models are able to capture topical structures but
have a harder time maintaining structural coherence or modeling coreference.",2022-10-16
"LEATHER: A Framework for Learning to Generate Human-like Text in
  Dialogue",2022-10-14 13:05:11+00:00,http://arxiv.org/abs/2210.07777v1,"Anthony Sicilia, Malihe Alikhani","cs.CL, cs.LG",table2text,"Algorithms for text-generation in dialogue can be misguided. For example, in
task-oriented settings, reinforcement learning that optimizes only task-success
can lead to abysmal lexical diversity. We hypothesize this is due to poor
theoretical understanding of the objectives in text-generation and their
relation to the learning process (i.e., model training). To this end, we
propose a new theoretical framework for learning to generate text in dialogue.
Compared to existing theories of learning, our framework allows for analysis of
the multi-faceted goals inherent to text-generation. We use our framework to
develop theoretical guarantees for learners that adapt to unseen data. As an
example, we apply our theory to study data-shift within a cooperative learning
algorithm proposed for the GuessWhat?! visual dialogue game. From this insight,
we propose a new algorithm, and empirically, we demonstrate our proposal
improves both task-success and human-likeness of the generated text. Finally,
we show statistics from our theory are empirically predictive of multiple
qualities of the generated dialogue, suggesting our theory is useful for
model-selection when human evaluations are not available.",2022-10-14
Towards a Unified Multi-Dimensional Evaluator for Text Generation,2022-10-13 17:17:03+00:00,http://arxiv.org/abs/2210.07197v1,"Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, Jiawei Han",cs.CL,table2text,"Multi-dimensional evaluation is the dominant paradigm for human evaluation in
Natural Language Generation (NLG), i.e., evaluating the generated text from
multiple explainable dimensions, such as coherence and fluency. However,
automatic evaluation in NLG is still dominated by similarity-based metrics, and
we lack a reliable framework for a more comprehensive evaluation of advanced
models. In this paper, we propose a unified multi-dimensional evaluator UniEval
for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task,
and by guiding the model with different questions, we can use one evaluator to
evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean
QA format, we are able to introduce an intermediate learning phase that enables
UniEval to incorporate external knowledge from multiple related tasks and gain
further improvement. Experiments on three typical NLG tasks show that UniEval
correlates substantially better with human judgments than existing metrics.
Specifically, compared to the top-performing unified evaluators, UniEval
achieves a 23% higher correlation on text summarization, and over 43% on
dialogue response generation. Also, UniEval demonstrates a strong zero-shot
learning ability for unseen evaluation dimensions and tasks. Source code, data
and all pre-trained evaluators are available on our GitHub repository
(https://github.com/maszhongming/UniEval).",2022-10-13
"Scaling Back-Translation with Domain Text Generation for Sign Language
  Gloss Translation",2022-10-13 14:25:08+00:00,http://arxiv.org/abs/2210.07054v1,"Jinhui Ye, Wenxiang Jiao, Xing Wang, Zhaopeng Tu","cs.CL, cs.AI",table2text,"Sign language gloss translation aims to translate the sign glosses into
spoken language texts, which is challenging due to the scarcity of labeled
gloss-text parallel data. Back translation (BT), which generates
pseudo-parallel data by translating in-domain spoken language texts into sign
glosses, has been applied to alleviate the data scarcity problem. However, the
lack of large-scale high-quality domain spoken language text data limits the
effect of BT. In this paper, to overcome the limitation, we propose a Prompt
based domain text Generation (PGEN) approach to produce the large-scale
in-domain spoken language text data. Specifically, PGEN randomly concatenates
sentences from the original in-domain spoken language text data as prompts to
induce a pre-trained language model (i.e., GPT-2) to generate spoken language
texts in a similar style. Experimental results on three benchmarks of sign
language gloss translation in varied languages demonstrate that BT with spoken
language texts generated by PGEN significantly outperforms the compared
methods. In addition, as the scale of spoken language texts generated by PGEN
increases, the BT technique can achieve further improvements, demonstrating the
effectiveness of our approach. We release the code and data for facilitating
future research in this field.",2022-10-13
Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis,2022-10-12 23:38:57+00:00,http://arxiv.org/abs/2210.06629v1,"Siddharth Varia, Shuai Wang, Kishaloy Halder, Robert Vacareanu, Miguel Ballesteros, Yassine Benajiba, Neha Anna John, Rishita Anubhai, Smaranda Muresan, Dan Roth",cs.CL,table2text,"Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis
task which involves four elements from user-generated texts: aspect term,
aspect category, opinion term, and sentiment polarity. Most computational
approaches focus on some of the ABSA sub-tasks such as tuple (aspect term,
sentiment polarity) or triplet (aspect term, opinion term, sentiment polarity)
extraction using either pipeline or joint modeling approaches. Recently,
generative approaches have been proposed to extract all four elements as (one
or more) quadruplets from text as a single task. In this work, we take a step
further and propose a unified framework for solving ABSA, and the associated
sub-tasks to improve the performance in few-shot scenarios. To this end, we
fine-tune a T5 model with instructional prompts in a multi-task learning
fashion covering all the sub-tasks, as well as the entire quadruple prediction
task. In experiments with multiple benchmark data sets, we show that the
proposed multi-task prompting approach brings performance boost (by absolute
$6.75$ F1) in the few-shot learning setting.",2022-10-12
DATScore: Evaluating Translation with Data Augmented Translations,2022-10-12 20:31:42+00:00,http://arxiv.org/abs/2210.06576v1,"Moussa Kamal Eddine, Guokan Shang, Michalis Vazirgiannis",cs.CL,table2text,"The rapid development of large pretrained language models has revolutionized
not only the field of Natural Language Generation (NLG) but also its
evaluation. Inspired by the recent work of BARTScore: a metric leveraging the
BART language model to evaluate the quality of generated text from various
aspects, we introduce DATScore. DATScore uses data augmentation techniques to
improve the evaluation of machine translation. Our main finding is that
introducing data augmented translations of the source and reference texts is
greatly helpful in evaluating the quality of the generated translation. We also
propose two novel score averaging and term weighting strategies to improve the
original score computing process of BARTScore. Experimental results on WMT show
that DATScore correlates better with human meta-evaluations than the other
recent state-of-the-art metrics, especially for low-resource languages.
Ablation studies demonstrate the value added by our new scoring strategies.
Moreover, we report in our extended experiments the performance of DATScore on
3 NLG tasks other than translation.",2022-10-12
ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models,2022-10-09 19:17:43+00:00,http://arxiv.org/abs/2210.04325v2,"Jiannan Xiang, Zhengzhong Liu, Yucheng Zhou, Eric P. Xing, Zhiting Hu",cs.CL,table2text,"Data-to-text generation is challenging due to the great variety of the input
data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse
predicates). Recent end-to-end neural methods thus require substantial training
examples to learn to disambiguate and describe the data. Yet, real-world
data-to-text problems often suffer from various data-scarce issues: one may
have access to only a handful of or no training examples, and/or have to rely
on examples in a different domain or schema. To fill this gap, we propose
Any-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse
settings by making efficient use of any given (or no) examples. ASDOT consists
of two steps, data disambiguation and sentence fusion, both of which are
amenable to be solved with off-the-shelf pretrained language models (LMs) with
optional finetuning. In the data disambiguation stage, we employ the prompted
GPT-3 model to understand possibly ambiguous triples from the input data and
convert each into a short sentence with reduced ambiguity. The sentence fusion
stage then uses an LM like T5 to fuse all the resulting sentences into a
coherent paragraph as the final description. We evaluate extensively on various
datasets in different scenarios, including the zero-/few-/full-shot settings,
and generalization to unseen predicates and out-of-domain data. Experimental
results show that ASDOT consistently achieves significant improvement over
baselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot
setting.",2022-10-09
"FAST: Improving Controllability for Text Generation with Feedback Aware
  Self-Training",2022-10-06 19:00:51+00:00,http://arxiv.org/abs/2210.03167v1,"Junyi Chai, Reid Pryzant, Victor Ye Dong, Konstantin Golobokov, Chenguang Zhu, Yi Liu",cs.CL,table2text,"Controllable text generation systems often leverage control codes to direct
various properties of the output like style and length. Inspired by recent work
on causal inference for NLP, this paper reveals a previously overlooked flaw in
these control code-based conditional text generation algorithms. Spurious
correlations in the training data can lead models to incorrectly rely on parts
of the input other than the control code for attribute selection, significantly
undermining downstream generation quality and controllability. We demonstrate
the severity of this issue with a series of case studies and then propose two
simple techniques to reduce these correlations in training sets. The first
technique is based on resampling the data according to an example's propensity
towards each linguistic attribute (IPS). The second produces multiple
counterfactual versions of each example and then uses an additional feedback
mechanism to remove noisy examples (feedback aware self-training, FAST). We
evaluate on 3 tasks -- news headline, meta review, and search ads generation --
and demonstrate that FAST can significantly improve the controllability and
language quality of generated outputs when compared to state-of-the-art
controllable text generation approaches.",2022-10-06
A Distributional Lens for Multi-Aspect Controllable Text Generation,2022-10-06 13:08:04+00:00,http://arxiv.org/abs/2210.02889v1,"Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Lingyuan Zhang, Heng Gong, Bing Qin",cs.CL,table2text,"Multi-aspect controllable text generation is a more challenging and practical
task than single-aspect control. Existing methods achieve complex multi-aspect
control by fusing multiple controllers learned from single-aspect, but suffer
from attribute degeneration caused by the mutual interference of these
controllers. To address this, we provide observations on attribute fusion from
a distributional perspective and propose to directly search for the
intersection areas of multiple attribute distributions as their combination for
generation. Our method first estimates the attribute space with an autoencoder
structure. Afterward, we iteratively approach the intersections by jointly
minimizing distances to points representing different attributes. Finally, we
map them to attribute-relevant sentences with a prefix-tuning-based decoder.
Experiments on the three-aspect control task, including sentiment, topic, and
detoxification aspects, reveal that our method outperforms several strong
baselines on attribute relevance and text quality and achieves the SOTA.
Further analysis also supplies some explanatory support for the effectiveness
of our approach.",2022-10-06
"Unsupervised Sentence Textual Similarity with Compositional Phrase
  Semantics",2022-10-05 14:14:04+00:00,http://arxiv.org/abs/2210.02284v1,"Zihao Wang, Jiaheng Dou, Yong Zhang",cs.CL,table2text,"Measuring Sentence Textual Similarity (STS) is a classic task that can be
applied to many downstream NLP applications such as text generation and
retrieval. In this paper, we focus on unsupervised STS that works on various
domains but only requires minimal data and computational resources.
Theoretically, we propose a light-weighted Expectation-Correction (EC)
formulation for STS computation. EC formulation unifies unsupervised STS
approaches including the cosine similarity of Additively Composed (AC) sentence
embeddings, Optimal Transport (OT), and Tree Kernels (TK). Moreover, we propose
the Recursive Optimal Transport Similarity (ROTS) algorithm to capture the
compositional phrase semantics by composing multiple recursive EC formulations.
ROTS finishes in linear time and is faster than its predecessors. ROTS is
empirically more effective and scalable than previous approaches. Extensive
experiments on 29 STS tasks under various settings show the clear advantage of
ROTS over existing approaches. Detailed ablation studies demonstrate the
effectiveness of our approaches.",2022-10-05
CodeDSI: Differentiable Code Search,2022-10-01 17:39:57+00:00,http://arxiv.org/abs/2210.00328v1,"Usama Nadeem, Noah Ziems, Shaoen Wu","cs.SE, cs.IR",table2text,"Reimplementing solutions to previously solved software engineering problems
is not only inefficient but also introduces inadequate and error-prone code.
Many existing methods achieve impressive performance on this issue by using
autoregressive text-generation models trained on code. However, these methods
are not without their flaws. The generated code from these models can be buggy,
lack documentation, and introduce vulnerabilities that may go unnoticed by
developers. An alternative to code generation -- neural code search -- is a
field of machine learning where a model takes natural language queries as input
and, in turn, relevant code samples from a database are returned. Due to the
nature of this pre-existing database, code samples can be documented, tested,
licensed, and checked for vulnerabilities before being used by developers in
production. In this work, we present CodeDSI, an end-to-end unified approach to
code search. CodeDSI is trained to directly map natural language queries to
their respective code samples, which can be retrieved later. In an effort to
improve the performance of code search, we have investigated docid
representation strategies, impact of tokenization on docid structure, and
dataset sizes on overall code search performance. Our results demonstrate
CodeDSI strong performance, exceeding conventional robust baselines by 2-6%
across varying dataset sizes.",2022-10-01
Calibrating Sequence likelihood Improves Conditional Language Generation,2022-09-30 19:16:16+00:00,http://arxiv.org/abs/2210.00045v1,"Yao Zhao, Misha Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, Peter J. Liu",cs.CL,table2text,"Conditional language models are predominantly trained with maximum likelihood
estimation (MLE), giving probability mass to sparsely observed target
sequences. While MLE trained models assign high probability to plausible
sequences given the context, the model probabilities often do not accurately
rank-order generated sequences by quality. This has been empirically observed
in beam search decoding as output quality degrading with large beam sizes, and
decoding strategies benefiting from heuristics such as length normalization and
repetition-blocking. In this work, we introduce sequence likelihood calibration
(SLiC) where the likelihood of model generated sequences are calibrated to
better align with reference sequences in the model's latent space. With SLiC,
decoding heuristics become unnecessary and decoding candidates' quality
significantly improves regardless of the decoding method. Furthermore, SLiC
shows no sign of diminishing returns with model scale, and presents alternative
ways to improve quality with limited training and inference budgets. With SLiC,
we exceed or match SOTA results on a wide range of generation tasks spanning
abstractive summarization, question generation, abstractive question answering
and data-to-text generation, even with modest-sized models.",2022-09-30
"Co-Writing Screenplays and Theatre Scripts with Language Models: An
  Evaluation by Industry Professionals",2022-09-29 17:26:22+00:00,http://arxiv.org/abs/2209.14958v1,"Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, Richard Evans","cs.HC, cs.CL",table2text,"Language models are increasingly attracting interest from writers. However,
such models lack long-range semantic coherence, limiting their usefulness for
longform creative writing. We address this limitation by applying language
models hierarchically, in a system we call Dramatron. By building structural
context via prompt chaining, Dramatron can generate coherent scripts and
screenplays complete with title, characters, story beats, location
descriptions, and dialogue. We illustrate Dramatron's usefulness as an
interactive co-creative system with a user study of 15 theatre and film
industry professionals. Participants co-wrote theatre scripts and screenplays
with Dramatron and engaged in open-ended interviews. We report critical
reflections both from our interviewees and from independent reviewers who
watched stagings of the works to illustrate how both Dramatron and hierarchical
text generation could be useful for human-machine co-creativity. Finally, we
discuss the suitability of Dramatron for co-creativity, ethical considerations
-- including plagiarism and bias -- and participatory models for the design and
deployment of such tools.",2022-09-29
"DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language
  Processing",2022-09-29 16:05:53+00:00,http://arxiv.org/abs/2209.14901v1,"Yanjun Gao, Dmitriy Dligach, Timothy Miller, John Caskey, Brihat Sharma, Matthew M Churpek, Majid Afshar","cs.CL, cs.AI",table2text,"The meaningful use of electronic health records (EHR) continues to progress
in the digital era with clinical decision support systems augmented by
artificial intelligence. A priority in improving provider experience is to
overcome information overload and reduce the cognitive burden so fewer medical
errors and cognitive biases are introduced during patient care. One major type
of medical error is diagnostic error due to systematic or predictable errors in
judgment that rely on heuristics. The potential for clinical natural language
processing (cNLP) to model diagnostic reasoning in humans with forward
reasoning from data to diagnosis and potentially reduce the cognitive burden
and medical error has not been investigated. Existing tasks to advance the
science in cNLP have largely focused on information extraction and named entity
recognition through classification tasks. We introduce a novel suite of tasks
coined as Diagnostic Reasoning Benchmarks, DR.BENCH, as a new benchmark for
developing and evaluating cNLP models with clinical diagnostic reasoning
ability. The suite includes six tasks from ten publicly available datasets
addressing clinical text understanding, medical knowledge reasoning, and
diagnosis generation. DR.BENCH is the first clinical suite of tasks designed to
be a natural language generation framework to evaluate pre-trained language
models. Experiments with state-of-the-art pre-trained generative language
models using large general domain models and models that were continually
trained on a medical corpus demonstrate opportunities for improvement when
evaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab
repository with a systematic approach to load and evaluate models for the cNLP
community.",2022-09-29
Informative Text Generation from Knowledge Triples,2022-09-26 14:35:57+00:00,http://arxiv.org/abs/2209.12733v1,"Zihao Fu, Yijiang River Dong, Lidong Bing, Wai Lam",cs.CL,table2text,"As the development of the encoder-decoder architecture, researchers are able
to study the text generation tasks with broader types of data. Among them,
KB-to-text aims at converting a set of knowledge triples into human readable
sentences. In the original setting, the task assumes that the input triples and
the text are exactly aligned in the perspective of the embodied
knowledge/information. In this paper, we extend this setting and explore how to
facilitate the trained model to generate more informative text, namely,
containing more information about the triple entities but not conveyed by the
input triples. To solve this problem, we propose a novel memory augmented
generator that employs a memory network to memorize the useful knowledge
learned during the training and utilizes such information together with the
input triples to generate text in the operational or testing phase. We derive a
dataset from WebNLG for our new setting and conduct extensive experiments to
investigate the effectiveness of our model as well as uncover the intrinsic
characteristics of the setting.",2022-09-26
Controllable Text Generation for Open-Domain Creativity and Fairness,2022-09-24 22:40:01+00:00,http://arxiv.org/abs/2209.12099v1,Nanyun Peng,"cs.CL, cs.AI",table2text,"Recent advances in large pre-trained language models have demonstrated strong
results in generating natural languages and significantly improved performances
for many natural language generation (NLG) applications such as machine
translation and text summarization. However, when the generation tasks are more
open-ended and the content is under-specified, existing techniques struggle to
generate long-term coherent and creative content. Moreover, the models exhibit
and even amplify social biases that are learned from the training corpora. This
happens because the generation models are trained to capture the surface
patterns (i.e. sequences of words), instead of capturing underlying semantics
and discourse structures, as well as background knowledge including social
norms. In this paper, I introduce our recent works on controllable text
generation to enhance the creativity and fairness of language generation
models. We explore hierarchical generation and constrained decoding, with
applications to creative language generation including story, poetry, and
figurative languages, and bias mitigation for generation models.",2022-09-24
XF2T: Cross-lingual Fact-to-Text Generation for Low-Resource Languages,2022-09-22 18:01:27+00:00,http://arxiv.org/abs/2209.11252v1,"Shivprasad Sagare, Tushar Abhishek, Bhavyajeet Singh, Anubhav Sharma, Manish Gupta, Vasudeva Varma",cs.CL,table2text,"Multiple business scenarios require an automated generation of descriptive
human-readable text from structured input data. Hence, fact-to-text generation
systems have been developed for various downstream tasks like generating soccer
reports, weather and financial reports, medical reports, person biographies,
etc. Unfortunately, previous work on fact-to-text (F2T) generation has focused
primarily on English mainly due to the high availability of relevant datasets.
Only recently, the problem of cross-lingual fact-to-text (XF2T) was proposed
for generation across multiple languages alongwith a dataset, XALIGN for eight
languages. However, there has been no rigorous work on the actual XF2T
generation problem. We extend XALIGN dataset with annotated data for four more
languages: Punjabi, Malayalam, Assamese and Oriya. We conduct an extensive
study using popular Transformer-based text generation models on our extended
multi-lingual dataset, which we call XALIGNV2. Further, we investigate the
performance of different text generation strategies: multiple variations of
pretraining, fact-aware embeddings and structure-aware input encoding. Our
extensive experiments show that a multi-lingual mT5 model which uses fact-aware
embeddings with structure-aware input encoding leads to best results on average
across the twelve languages. We make our code, dataset and model publicly
available, and hope that this will help advance further research in this
critical area.",2022-09-22
Selective Token Generation for Few-shot Natural Language Generation,2022-09-17 00:48:52+00:00,http://arxiv.org/abs/2209.08206v1,"Daejin Jo, Taehwan Kwon, Eun-Sol Kim, Sungwoong Kim","cs.CL, cs.LG",table2text,"Natural language modeling with limited training data is a challenging
problem, and many algorithms make use of large-scale pretrained language models
(PLMs) for this due to its great generalization ability. Among them, additive
learning that incorporates a task-specific adapter on top of the fixed
large-scale PLM has been popularly used in the few-shot setting. However, this
added adapter is still easy to disregard the knowledge of the PLM especially
for few-shot natural language generation (NLG) since an entire sequence is
usually generated by only the newly trained adapter. Therefore, in this work,
we develop a novel additive learning algorithm based on reinforcement learning
(RL) that selectively outputs language tokens between the task-general PLM and
the task-specific adapter during both training and inference. This output token
selection over the two generators allows the adapter to take into account
solely the task-relevant parts in sequence generation, and therefore makes it
more robust to overfitting as well as more stable in RL training. In addition,
to obtain the complementary adapter from the PLM for each few-shot task, we
exploit a separate selecting module that is also simultaneously trained using
RL. Experimental results on various few-shot NLG tasks including question
answering, data-to-text generation and text summarization demonstrate that the
proposed selective token generation significantly outperforms the previous
additive learning algorithms based on the PLMs.",2022-09-17
"Adaptive Natural Language Generation for Task-oriented Dialogue via
  Reinforcement Learning",2022-09-16 12:08:57+00:00,http://arxiv.org/abs/2209.07873v1,"Atsumoto Ohashi, Ryuichiro Higashinaka","cs.CL, cs.AI",table2text,"When a natural language generation (NLG) component is implemented in a
real-world task-oriented dialogue system, it is necessary to generate not only
natural utterances as learned on training data but also utterances adapted to
the dialogue environment (e.g., noise from environmental sounds) and the user
(e.g., users with low levels of understanding ability). Inspired by recent
advances in reinforcement learning (RL) for language generation tasks, we
propose ANTOR, a method for Adaptive Natural language generation for
Task-Oriented dialogue via Reinforcement learning. In ANTOR, a natural language
understanding (NLU) module, which corresponds to the user's understanding of
system utterances, is incorporated into the objective function of RL. If the
NLG's intentions are correctly conveyed to the NLU, which understands a
system's utterances, the NLG is given a positive reward. We conducted
experiments on the MultiWOZ dataset, and we confirmed that ANTOR could generate
adaptive utterances against speech recognition errors and the different
vocabulary levels of users.",2022-09-16
"TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for
  Multilingual Tweet Representations",2022-09-15 19:01:21+00:00,http://arxiv.org/abs/2209.07562v1,"Xinyang Zhang, Yury Malkov, Omar Florez, Serim Park, Brian McWilliams, Jiawei Han, Ahmed El-Kishky",cs.CL,table2text,"We present TwHIN-BERT, a multilingual language model trained on in-domain
data from the popular social network Twitter. TwHIN-BERT differs from prior
pre-trained language models as it is trained with not only text-based
self-supervision, but also with a social objective based on the rich social
engagements within a Twitter heterogeneous information network (TwHIN). Our
model is trained on 7 billion tweets covering over 100 distinct languages
providing a valuable representation to model short, noisy, user-generated text.
We evaluate our model on a variety of multilingual social recommendation and
semantic understanding tasks and demonstrate significant metric improvement
over established pre-trained language models. We will freely open-source
TwHIN-BERT and our curated hashtag prediction and social engagement benchmark
datasets to the research community.",2022-09-15
Distribution Aware Metrics for Conditional Natural Language Generation,2022-09-15 17:58:13+00:00,http://arxiv.org/abs/2209.07518v1,"David M Chan, Yiming Ni, Austin Myers, Sudheendra Vijayanarasimhan, David A Ross, John Canny","cs.CL, cs.AI, cs.CV, cs.LG",table2text,"Traditional automated metrics for evaluating conditional natural language
generation use pairwise comparisons between a single generated text and the
best-matching gold-standard ground truth text. When multiple ground truths are
available, scores are aggregated using an average or max operation across
references. While this approach works well when diversity in the ground truth
data (i.e. dispersion of the distribution of conditional texts) can be ascribed
to noise, such as in automated speech recognition, it does not allow for robust
evaluation in the case where diversity in the ground truths represents signal
for the model. In this work we argue that existing metrics are not appropriate
for domains such as visual description or summarization where ground truths are
semantically diverse, and where the diversity in those captions captures useful
additional information about the context. We propose a novel paradigm for
multi-candidate evaluation of conditional language generation models, and a new
family of metrics that compare the distributions of reference and
model-generated caption sets using small sample sets of each. We demonstrate
the utility of our approach with a case study in visual description: where we
show that existing models optimize for single-description quality over
diversity, and gain some insights into how sampling methods and temperature
impact description quality and diversity.",2022-09-15
vec2text with Round-Trip Translations,2022-09-14 17:20:18+00:00,http://arxiv.org/abs/2209.06792v1,"Geoffrey Cideron, Sertan Girgin, Anton Raichuk, Olivier Pietquin, Olivier Bachem, Léonard Hussenot","cs.CL, cs.LG",table2text,"We investigate models that can generate arbitrary natural language text (e.g.
all English sentences) from a bounded, convex and well-behaved control space.
We call them universal vec2text models. Such models would allow making semantic
decisions in the vector space (e.g. via reinforcement learning) while the
natural language generation is handled by the vec2text model. We propose four
desired properties: universality, diversity, fluency, and semantic structure,
that such vec2text models should possess and we provide quantitative and
qualitative methods to assess them. We implement a vec2text model by adding a
bottleneck to a 250M parameters Transformer model and training it with an
auto-encoding objective on 400M sentences (10B tokens) extracted from a massive
web corpus. We propose a simple data augmentation technique based on round-trip
translations and show in extensive experiments that the resulting vec2text
model surprisingly leads to vector spaces that fulfill our four desired
properties and that this model strongly outperforms both standard and denoising
auto-encoders.",2022-09-14
LibertyMFD: A Lexicon to Assess the Moral Foundation of Liberty,2022-09-14 16:14:54+00:00,http://arxiv.org/abs/2209.06750v1,"Oscar Araque, Lorenzo Gatti, Kyriaki Kalimeri",cs.CL,table2text,"Quantifying the moral narratives expressed in the user-generated text, news,
or public discourses is fundamental for understanding individuals' concerns and
viewpoints and preventing violent protests and social polarisation. The Moral
Foundation Theory (MFT) was developed to operationalise morality in a
five-dimensional scale system. Recent developments of the theory urged for the
introduction of a new foundation, the Liberty Foundation. Being only recently
added to the theory, there are no available linguistic resources to assess
whether liberty is present in text corpora. Given its importance to current
social issues such as the vaccination debate, we propose two data-driven
approaches, deriving two candidate lexicons generated based on aligned
documents from online news sources with different worldviews. After extensive
experimentation, we contribute to the research community a novel lexicon that
assesses the liberty moral foundation in the way individuals with contrasting
viewpoints express themselves through written text. The LibertyMFD dictionary
can be a valuable tool for policymakers to understand diverse viewpoints on
controversial social issues such as vaccination, abortion, or even uprisings,
as they happen and on a large scale.",2022-09-14
"OPAL: Ontology-Aware Pretrained Language Model for End-to-End
  Task-Oriented Dialogue",2022-09-10 04:38:27+00:00,http://arxiv.org/abs/2209.04595v1,"Zhi Chen, Yuncong Liu, Lu Chen, Su Zhu, Mengyue Wu, Kai Yu",cs.CL,table2text,"This paper presents an ontology-aware pretrained language model (OPAL) for
end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models,
task-oriented dialogue models fulfill at least two task-specific modules:
dialogue state tracker (DST) and response generator (RG). The dialogue state
consists of the domain-slot-value triples, which are regarded as the user's
constraints to search the domain-related databases. The large-scale
task-oriented dialogue data with the annotated structured dialogue state
usually are inaccessible. It prevents the development of the pretrained
language model for the task-oriented dialogue. We propose a simple yet
effective pretraining method to alleviate this problem, which consists of two
pretraining phases. The first phase is to pretrain on large-scale contextual
text data, where the structured information of the text is extracted by the
information extracting tool. To bridge the gap between the pretraining method
and downstream tasks, we design two pretraining tasks: ontology-like triple
recovery and next-text generation, which simulates the DST and RG,
respectively. The second phase is to fine-tune the pretrained model on the TOD
data. The experimental results show that our proposed method achieves an
exciting boost and get competitive performance even without any TOD data on
CamRest676 and MultiWOZ benchmarks.",2022-09-10
"Layer or Representation Space: What makes BERT-based Evaluation Metrics
  Robust?",2022-09-06 09:10:54+00:00,http://arxiv.org/abs/2209.02317v2,"Doan Nam Long Vu, Nafise Sadat Moosavi, Steffen Eger",cs.CL,table2text,"The evaluation of recent embedding-based evaluation metrics for text
generation is primarily based on measuring their correlation with human
evaluations on standard benchmarks. However, these benchmarks are mostly from
similar domains to those used for pretraining word embeddings. This raises
concerns about the (lack of) generalization of embedding-based metrics to new
and noisy domains that contain a different vocabulary than the pretraining
data. In this paper, we examine the robustness of BERTScore, one of the most
popular embedding-based metrics for text generation. We show that (a) an
embedding-based metric that has the highest correlation with human evaluations
on a standard benchmark can have the lowest correlation if the amount of input
noise or unknown tokens increases, (b) taking embeddings from the first layer
of pretrained models improves the robustness of all metrics, and (c) the
highest robustness is achieved when using character-level embeddings, instead
of token-based embeddings, from the first layer of the pretrained model.",2022-09-06
"Every picture tells a story: Image-grounded controllable stylistic story
  generation",2022-09-04 15:07:53+00:00,http://arxiv.org/abs/2209.01638v1,"Holy Lovenia, Bryan Wilie, Romain Barraud, Samuel Cahyawijaya, Willy Chung, Pascale Fung",cs.CL,table2text,"Generating a short story out of an image is arduous. Unlike image captioning,
story generation from an image poses multiple challenges: preserving the story
coherence, appropriately assessing the quality of the story, steering the
generated story into a certain style, and addressing the scarcity of
image-story pair reference datasets limiting supervision during training. In
this work, we introduce Plug-and-Play Story Teller (PPST) and improve
image-to-story generation by: 1) alleviating the data scarcity problem by
incorporating large pre-trained models, namely CLIP and GPT-2, to facilitate a
fluent image-to-text generation with minimal supervision, and 2) enabling a
more style-relevant generation by incorporating stylistic adapters to control
the story generation. We conduct image-to-story generation experiments with
non-styled, romance-styled, and action-styled PPST approaches and compare our
generated stories with those of previous work over three aspects, i.e., story
coherence, image-story relevance, and style fitness, using both automatic and
human evaluation. The results show that PPST improves story coherence and has
better image-story relevance, but has yet to be adequately stylistic.",2022-09-04
Multi-Modal Experience Inspired AI Creation,2022-09-02 11:50:41+00:00,http://arxiv.org/abs/2209.02427v1,"Qian Cao, Xu Chen, Ruihua Song, Hao Jiang, Guang Yang, Zhao Cao",cs.AI,table2text,"AI creation, such as poem or lyrics generation, has attracted increasing
attention from both industry and academic communities, with many promising
models proposed in the past few years. Existing methods usually estimate the
outputs based on single and independent visual or textual information. However,
in reality, humans usually make creations according to their experiences, which
may involve different modalities and be sequentially correlated. To model such
human capabilities, in this paper, we define and solve a novel AI creation
problem based on human experiences. More specifically, we study how to generate
texts based on sequential multi-modal information. Compared with the previous
works, this task is much more difficult because the designed model has to well
understand and adapt the semantics among different modalities and effectively
convert them into the output in a sequential manner. To alleviate these
difficulties, we firstly design a multi-channel sequence-to-sequence
architecture equipped with a multi-modal attention network. For more effective
optimization, we then propose a curriculum negative sampling strategy tailored
for the sequential inputs. To benchmark this problem and demonstrate the
effectiveness of our model, we manually labeled a new multi-modal experience
dataset. With this dataset, we conduct extensive experiments by comparing our
model with a series of representative baselines, where we can demonstrate
significant improvements in our model based on both automatic and
human-centered metrics. The code and data are available at:
\url{https://github.com/Aman-4-Real/MMTG}.",2022-09-02
A Spanish dataset for Targeted Sentiment Analysis of political headlines,2022-08-30 01:30:30+00:00,http://arxiv.org/abs/2208.13947v1,"Tomás Alves Salgueiro, Emilio Recart Zapata, Damián Furman, Juan Manuel Pérez, Pablo Nicolás Fernández Larrosa",cs.CL,table2text,"Subjective texts have been studied by several works as they can induce
certain behaviours in their users. Most work focuses on user-generated texts in
social networks, but some other texts also comprise opinions on certain topics
and could influence judgement criteria during political decisions. In this
work, we address the task of Targeted Sentiment Analysis for the domain of news
headlines, published by the main outlets during the 2019 Argentinean
Presidential Elections. For this purpose, we present a polarity dataset of
1,976 headlines mentioning candidates in the 2019 elections at the target
level. Preliminary experiments with state-of-the-art classification algorithms
based on pre-trained linguistic models suggest that target information is
helpful for this task. We make our data and pre-trained models publicly
available.",2022-08-30
"StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse
  Representations and Content Enhancing",2022-08-29 08:47:49+00:00,http://arxiv.org/abs/2208.13423v1,"Xuekai Zhu, Jian Guan, Minlie Huang, Juan Liu",cs.CL,table2text,"Non-parallel text style transfer is an important task in natural language
generation. However, previous studies concentrate on the token or sentence
level, such as sentence sentiment and formality transfer, but neglect long
style transfer at the discourse level. Long texts usually involve more
complicated author linguistic preferences such as discourse structures than
sentences. In this paper, we formulate the task of non-parallel story
author-style transfer, which requires transferring an input story into a
specified author style while maintaining source semantics. To tackle this
problem, we propose a generation model, named StoryTrans, which leverages
discourse representations to capture source content information and transfer
them to target styles with learnable style embeddings. We use an additional
training objective to disentangle stylistic features from the learned discourse
representation to prevent the model from degenerating to an auto-encoder.
Moreover, to enhance content preservation, we design a mask-and-fill framework
to explicitly fuse style-specific keywords of source texts into generation.
Furthermore, we constructed new datasets for this task in Chinese and English,
respectively. Extensive experiments show that our model outperforms strong
baselines in overall performance of style transfer and content preservation.",2022-08-29
Nearest Neighbor Non-autoregressive Text Generation,2022-08-26 08:21:21+00:00,http://arxiv.org/abs/2208.12496v1,"Ayana Niwa, Sho Takase, Naoaki Okazaki",cs.CL,table2text,"Non-autoregressive (NAR) models can generate sentences with less computation
than autoregressive models but sacrifice generation quality. Previous studies
addressed this issue through iterative decoding. This study proposes using
nearest neighbors as the initial state of an NAR decoder and editing them
iteratively. We present a novel training strategy to learn the edit operations
on neighbors to improve NAR text generation. Experimental results show that the
proposed method (NeighborEdit) achieves higher translation quality (1.69 points
higher than the vanilla Transformer) with fewer decoding iterations
(one-eighteenth fewer iterations) on the JRC-Acquis En-De dataset, the common
benchmark dataset for machine translation using nearest neighbors. We also
confirm the effectiveness of the proposed method on a data-to-text task
(WikiBio). In addition, the proposed method outperforms an NAR baseline on the
WMT'14 En-De dataset. We also report analysis on neighbor examples used in the
proposed method.",2022-08-26
"GenTUS: Simulating User Behaviour and Language in Task-oriented
  Dialogues with Generative Transformers",2022-08-23 09:01:17+00:00,http://arxiv.org/abs/2208.10817v1,"Hsien-Chin Lin, Christian Geishauser, Shutong Feng, Nurul Lubis, Carel van Niekerk, Michael Heck, Milica Gašić",cs.CL,table2text,"User simulators (USs) are commonly used to train task-oriented dialogue
systems (DSs) via reinforcement learning. The interactions often take place on
semantic level for efficiency, but there is still a gap from semantic actions
to natural language, which causes a mismatch between training and deployment
environment. Incorporating a natural language generation (NLG) module with USs
during training can partly deal with this problem. However, since the policy
and NLG of USs are optimised separately, these simulated user utterances may
not be natural enough in a given context. In this work, we propose a generative
transformer-based user simulator (GenTUS). GenTUS consists of an
encoder-decoder structure, which means it can optimise both the user policy and
natural language generation jointly. GenTUS generates both semantic actions and
natural language utterances, preserving interpretability and enhancing language
variation. In addition, by representing the inputs and outputs as word
sequences and by using a large pre-trained language model we can achieve
generalisability in feature representation. We evaluate GenTUS with automatic
metrics and human evaluation. Our results show that GenTUS generates more
natural language and is able to transfer to an unseen ontology in a zero-shot
fashion. In addition, its behaviour can be further shaped with reinforcement
learning opening the door to training specialised user simulators.",2022-08-23
Few-Shot Table-to-Text Generation with Prefix-Controlled Generator,2022-08-23 03:23:26+00:00,http://arxiv.org/abs/2208.10709v1,"Yutao Luo, Menghua Lu, Gongshen Liu, Shilin Wang",cs.CL,table2text,"Neural table-to-text generation approaches are data-hungry, limiting their
adaptation for low-resource real-world applications. Previous works mostly
resort to Pre-trained Language Models (PLMs) to generate fluent summaries of a
table. However, they often contain hallucinated contents due to the
uncontrolled nature of PLMs. Moreover, the topological differences between
tables and sequences are rarely studied. Last but not least, fine-tuning on
PLMs with a handful of instances may lead to over-fitting and catastrophic
forgetting. To alleviate these problems, we propose a prompt-based approach,
Prefix-Controlled Generator (i.e., PCG), for few-shot table-to-text generation.
We prepend a task-specific prefix for a PLM to make the table structure better
fit the pre-trained input. In addition, we generate an input-specific prefix to
control the factual contents and word order of the generated text. Both
automatic and human evaluations on different domains (humans, books and songs)
of the Wikibio dataset show substantial improvements over baseline approaches.",2022-08-23
Automatic tagging of knowledge points for K12 math problems,2022-08-21 11:11:30+00:00,http://arxiv.org/abs/2208.09867v1,"Xiaolu Wang, Ziqi Ding, Liangyu Chen",cs.CL,table2text,"Automatic tagging of knowledge points for practice problems is the basis for
managing question bases and improving the automation and intelligence of
education. Therefore, it is of great practical significance to study the
automatic tagging technology for practice problems. However, there are few
studies on the automatic tagging of knowledge points for math problems. Math
texts have more complex structures and semantics compared with general texts
because they contain unique elements such as symbols and formulas. Therefore,
it is difficult to meet the accuracy requirement of knowledge point prediction
by directly applying the text classification techniques in general domains. In
this paper, K12 math problems taken as the research object, the LABS model
based on label-semantic attention and multi-label smoothing combining textual
features is proposed to improve the automatic tagging of knowledge points for
math problems. The model combines the text classification techniques in general
domains and the unique features of math texts. The results show that the models
using label-semantic attention or multi-label smoothing perform better on
precision, recall, and F1-score metrics than the traditional BiLSTM model,
while the LABS model using both performs best. It can be seen that label
information can guide the neural networks to extract meaningful information
from the problem text, which improves the text classification performance of
the model. Moreover, multi-label smoothing combining textual features can fully
explore the relationship between text and labels, improve the model's
prediction ability for new data and improve the model's classification
accuracy.",2022-08-21
"Beyond Text Generation: Supporting Writers with Continuous Automatic
  Text Summaries",2022-08-19 13:09:56+00:00,http://arxiv.org/abs/2208.09323v1,"Hai Dang, Karim Benharrak, Florian Lehmann, Daniel Buschek","cs.HC, cs.CL, H.5.2; I.2.7",table2text,"We propose a text editor to help users plan, structure and reflect on their
writing process. It provides continuously updated paragraph-wise summaries as
margin annotations, using automatic text summarization. Summary levels range
from full text, to selected (central) sentences, down to a collection of
keywords. To understand how users interact with this system during writing, we
conducted two user studies (N=4 and N=8) in which people wrote analytic essays
about a given topic and article. As a key finding, the summaries gave users an
external perspective on their writing and helped them to revise the content and
scope of their drafted paragraphs. People further used the tool to quickly gain
an overview of the text and developed strategies to integrate insights from the
automated summaries. More broadly, this work explores and highlights the value
of designing AI tools for writers, with Natural Language Processing (NLP)
capabilities that go beyond direct text generation and correction.",2022-08-19
"Coarse-to-Fine: Hierarchical Multi-task Learning for Natural Language
  Understanding",2022-08-19 02:46:20+00:00,http://arxiv.org/abs/2208.09129v1,"Zhaoye Fei, Yu Tian, Yongkang Wu, Xinyu Zhang, Yutao Zhu, Zheng Liu, Jiawen Wu, Dejiang Kong, Ruofei Lai, Zhao Cao, Zhicheng Dou, Xipeng Qiu",cs.CL,table2text,"Generalized text representations are the foundation of many natural language
understanding tasks. To fully utilize the different corpus, it is inevitable
that models need to understand the relevance among them. However, many methods
ignore the relevance and adopt a single-channel model (a coarse paradigm)
directly for all tasks, which lacks enough rationality and interpretation. In
addition, some existing works learn downstream tasks by stitches skill block(a
fine paradigm), which might cause irrationalresults due to its redundancy and
noise. Inthis work, we first analyze the task correlation through three
different perspectives, i.e., data property, manual design, and model-based
relevance, based on which the similar tasks are grouped together. Then, we
propose a hierarchical framework with a coarse-to-fine paradigm, with the
bottom level shared to all the tasks, the mid-level divided to different
groups, and the top-level assigned to each of the tasks. This allows our model
to learn basic language properties from all tasks, boost performance on
relevant tasks, and reduce the negative impact from irrelevant tasks. Our
experiments on 13 benchmark datasets across five natural language understanding
tasks demonstrate the superiority of our method.",2022-08-19
"Performance Optimization for Semantic Communications: An Attention-based
  Reinforcement Learning Approach",2022-08-17 11:39:16+00:00,http://arxiv.org/abs/2208.08239v1,"Yining Wang, Mingzhe Chen, Tao Luo, Walid Saad, Dusit Niyato, H. Vincent Poor, Shuguang Cui","cs.IT, cs.AI, math.IT",table2text,"In this paper, a semantic communication framework is proposed for textual
data transmission. In the studied model, a base station (BS) extracts the
semantic information from textual data, and transmits it to each user. The
semantic information is modeled by a knowledge graph (KG) that consists of a
set of semantic triples. After receiving the semantic information, each user
recovers the original text using a graph-to-text generation model. To measure
the performance of the considered semantic communication framework, a metric of
semantic similarity (MSS) that jointly captures the semantic accuracy and
completeness of the recovered text is proposed. Due to wireless resource
limitations, the BS may not be able to transmit the entire semantic information
to each user and satisfy the transmission delay constraint. Hence, the BS must
select an appropriate resource block for each user as well as determine and
transmit part of the semantic information to the users. As such, we formulate
an optimization problem whose goal is to maximize the total MSS by jointly
optimizing the resource allocation policy and determining the partial semantic
information to be transmitted. To solve this problem, a
proximal-policy-optimization-based reinforcement learning (RL) algorithm
integrated with an attention network is proposed. The proposed algorithm can
evaluate the importance of each triple in the semantic information using an
attention network and then, build a relationship between the importance
distribution of the triples in the semantic information and the total MSS.
Compared to traditional RL algorithms, the proposed algorithm can dynamically
adjust its learning rate thus ensuring convergence to a locally optimal
solution.",2022-08-17
High Recall Data-to-text Generation with Progressive Edit,2022-08-09 06:22:05+00:00,http://arxiv.org/abs/2208.04558v1,"Choonghan Kim, Gary Geunbae Lee","cs.CL, cs.AI",table2text,"Data-to-text (D2T) generation is the task of generating texts from structured
inputs. We observed that when the same target sentence was repeated twice,
Transformer (T5) based model generates an output made up of asymmetric
sentences from structured inputs. In other words, these sentences were
different in length and quality. We call this phenomenon ""Asymmetric
Generation"" and we exploit this in D2T generation. Once asymmetric sentences
are generated, we add the first part of the output with a no-repeated-target.
As this goes through progressive edit (ProEdit), the recall increases. Hence,
this method better covers structured inputs than before editing. ProEdit is a
simple but effective way to improve performance in D2T generation and it
achieves the new stateof-the-art result on the ToTTo dataset",2022-08-09
"Suggestion Lists vs. Continuous Generation: Interaction Design for
  Writing with Generative Models on Mobile Devices Affect Text Length, Wording
  and Perceived Authorship",2022-08-01 13:57:11+00:00,http://arxiv.org/abs/2208.00870v1,"Florian Lehmann, Niklas Markert, Hai Dang, Daniel Buschek","cs.HC, cs.AI",table2text,"Neural language models have the potential to support human writing. However,
questions remain on their integration and influence on writing and output. To
address this, we designed and compared two user interfaces for writing with AI
on mobile devices, which manipulate levels of initiative and control: 1)
Writing with continuously generated text, the AI adds text word-by-word and
user steers. 2) Writing with suggestions, the AI suggests phrases and user
selects from a list. In a supervised online study (N=18), participants used
these prototypes and a baseline without AI. We collected touch interactions,
ratings on inspiration and authorship, and interview data. With AI suggestions,
people wrote less actively, yet felt they were the author. Continuously
generated text reduced this perceived authorship, yet increased editing
behavior. In both designs, AI increased text length and was perceived to
influence wording. Our findings add new empirical evidence on the impact of UI
design decisions on user experience and output with co-creative systems.",2022-08-01
"LaKo: Knowledge-driven Visual Question Answering via Late
  Knowledge-to-Text Injection",2022-07-26 13:29:51+00:00,http://arxiv.org/abs/2207.12888v1,"Zhuo Chen, Yufeng Huang, Jiaoyan Chen, Yuxia Geng, Yin Fang, Jeff Pan, Ningyu Zhang, Wen Zhang","cs.CV, cs.AI",table2text,"Visual question answering (VQA) often requires an understanding of visual
concepts and language semantics, which relies on external knowledge. Most
existing methods exploit pre-trained language models or/and unstructured text,
but the knowledge in these resources are often incomplete and noisy. Some
methods prefer to use knowledge graphs (KGs) which often have intensive
structured knowledge, but the research is still quite preliminary. In this
paper, we propose LaKo, a knowledge-driven VQA method via Late
Knowledge-to-text Injection. To effectively incorporate an external KG, we
transfer triples into text and propose a late injection mechanism. Finally we
address VQA as a text generation task with an effective encoder-decoder
paradigm. In the evaluation with OKVQA datasets, our method achieves
state-of-the-art results.",2022-07-26
Innovations in Neural Data-to-text Generation,2022-07-25 23:21:48+00:00,http://arxiv.org/abs/2207.12571v1,"Mandar Sharma, Ajay Gogineni, Naren Ramakrishnan",cs.CL,table2text,"The neural boom that has sparked natural language processing (NLP) research
through the last decade has similarly led to significant innovations in
data-to-text generation (DTG). This survey offers a consolidated view into the
neural DTG paradigm with a structured examination of the approaches, benchmark
datasets, and evaluation protocols. This survey draws boundaries separating DTG
from the rest of the natural language generation (NLG) landscape, encompassing
an up-to-date synthesis of the literature, and highlighting the stages of
technological adoption from within and outside the greater NLG umbrella. With
this holistic view, we highlight promising avenues for DTG research that not
only focus on the design of linguistically capable systems but also systems
that exhibit fairness and accountability.",2022-07-25
"Leveraging Natural Supervision for Language Representation Learning and
  Generation",2022-07-21 17:26:03+00:00,http://arxiv.org/abs/2207.10617v1,Mingda Chen,cs.CL,table2text,"Recent breakthroughs in Natural Language Processing (NLP) have been driven by
language models trained on a massive amount of plain text. While powerful,
deriving supervision from textual resources is still an open question. For
example, language model pretraining often neglects the rich, freely-available
structures in textual data. In this thesis, we describe three lines of work
that seek to improve the training and evaluation of neural models using
naturally-occurring supervision.
  We first investigate self-supervised training losses to help enhance the
performance of pretrained language models for various NLP tasks. Specifically,
we alter the sentence prediction loss to make it better suited to other
pretraining losses and more challenging to solve. We design an intermediate
finetuning step that uses self-supervised training to promote models' ability
in cross-task generalization.
  Then we describe methods to leverage the structures in Wikipedia and
paraphrases. In particular, we propose training losses to exploit hyperlinks,
article structures, and article category graphs for entity-, discourse-,
entailment-related knowledge. We propose a framework that uses paraphrase pairs
to disentangle semantics and syntax in sentence representations. We extend the
framework for a novel generation task that controls the syntax of output text
with a sentential exemplar.
  Lastly, we discuss our work on tailoring textual resources for establishing
challenging evaluation tasks. We introduce three datasets by defining novel
tasks using various fan-contributed websites, including a long-form
data-to-text generation dataset, a screenplay summarization dataset, and a
long-form story generation dataset. These datasets have unique characteristics
offering challenges to future work in their respective task settings.",2022-07-21
"Neural Data-to-Text Generation Based on Small Datasets: Comparing the
  Added Value of Two Semi-Supervised Learning Approaches on Top of a Large
  Language Model",2022-07-14 11:53:04+00:00,http://arxiv.org/abs/2207.06839v1,"Chris van der Lee, Thiago Castro Ferreira, Chris Emmery, Travis Wiltshire, Emiel Krahmer",cs.CL,table2text,"This study discusses the effect of semi-supervised learning in combination
with pretrained language models for data-to-text generation. It is not known
whether semi-supervised learning is still helpful when a large-scale language
model is also supplemented. This study aims to answer this question by
comparing a data-to-text system only supplemented with a language model, to two
data-to-text systems that are additionally enriched by a data augmentation or a
pseudo-labeling semi-supervised learning approach.
  Results show that semi-supervised learning results in higher scores on
diversity metrics. In terms of output quality, extending the training set of a
data-to-text system with a language model using the pseudo-labeling approach
did increase text quality scores, but the data augmentation approach yielded
similar scores to the system without training set extension. These results
indicate that semi-supervised learning approaches can bolster output quality
and diversity, even when a language model is also present.",2022-07-14
"Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent
  Variable Inference for Text Generation",2022-07-13 11:27:46+00:00,http://arxiv.org/abs/2207.06130v1,"Jinyi Hu, Xiaoyuan Yi, Wenhao Li, Maosong Sun, Xing Xie",cs.CL,table2text,"The past several years have witnessed Variational Auto-Encoder's superiority
in various text generation tasks. However, due to the sequential nature of the
text, auto-regressive decoders tend to ignore latent variables and then reduce
to simple language models, known as the KL vanishing problem, which would
further deteriorate when VAE is combined with Transformer-based structures. To
ameliorate this problem, we propose DELLA, a novel variational Transformer
framework. DELLA learns a series of layer-wise latent variables with each
inferred from those of lower layers and tightly coupled with the hidden states
by low-rank tensor product. In this way, DELLA forces these posterior latent
variables to be fused deeply with the whole computation path and hence
incorporate more information. We theoretically demonstrate that our method can
be regarded as entangling latent variables to avoid posterior information
decrease through layers, enabling DELLA to get higher non-zero KL values even
without any annealing or thresholding tricks. Experiments on four unconditional
and three conditional generation tasks show that DELLA could better alleviate
KL vanishing and improve both quality and diversity compared to several strong
baselines.",2022-07-13
Towards Multimodal Vision-Language Models Generating Non-Generic Text,2022-07-09 01:56:35+00:00,http://arxiv.org/abs/2207.04174v1,"Wes Robbins, Zanyar Zohourianshahzadi, Jugal Kalita","cs.CV, cs.AI",table2text,"Vision-language models can assess visual context in an image and generate
descriptive text. While the generated text may be accurate and syntactically
correct, it is often overly general. To address this, recent work has used
optical character recognition to supplement visual information with text
extracted from an image. In this work, we contend that vision-language models
can benefit from additional information that can be extracted from an image,
but are not used by current models. We modify previous multimodal frameworks to
accept relevant information from any number of auxiliary classifiers. In
particular, we focus on person names as an additional set of tokens and create
a novel image-caption dataset to facilitate captioning with person names. The
dataset, Politicians and Athletes in Captions (PAC), consists of captioned
images of well-known people in context. By fine-tuning pretrained models with
this dataset, we demonstrate a model that can naturally integrate facial
recognition tokens into generated text by training on limited data. For the PAC
dataset, we provide a discussion on collection and baseline benchmark scores.",2022-07-09
"TalkToModel: Understanding Machine Learning Models With Open Ended
  Dialogues",2022-07-08 23:42:56+00:00,http://arxiv.org/abs/2207.04154v1,"Dylan Slack, Satyapriya Krishna, Himabindu Lakkaraju, Sameer Singh","cs.LG, cs.AI, cs.CL",table2text,"Machine Learning (ML) models are increasingly used to make critical decisions
in real-world applications, yet they have also become more complex, making them
harder to understand. To this end, several techniques to explain model
predictions have been proposed. However, practitioners struggle to leverage
explanations because they often do not know which to use, how to interpret the
results, and may have insufficient data science experience to obtain
explanations. In addition, most current works focus on generating one-shot
explanations and do not allow users to follow up and ask fine-grained questions
about the explanations, which can be frustrating. In this work, we address
these challenges by introducing TalkToModel: an open-ended dialogue system for
understanding machine learning models. Specifically, TalkToModel comprises
three key components: 1) a natural language interface for engaging in
dialogues, making understanding ML models highly accessible, 2) a dialogue
engine that adapts to any tabular model and dataset, interprets natural
language, maps it to appropriate operations (e.g., feature importance
explanations, counterfactual explanations, showing model errors), and generates
text responses, and 3) an execution component that run the operations and
ensures explanations are accurate. We carried out quantitative and human
subject evaluations of TalkToModel. We found the system understands user
questions on novel datasets and models with high accuracy, demonstrating the
system's capacity to generalize to new situations. In human evaluations, 73% of
healthcare workers (e.g., doctors and nurses) agreed they would use TalkToModel
over baseline point-and-click systems, and 84.6% of ML graduate students agreed
TalkToModel was easier to use.",2022-07-08
Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk,2022-07-02 04:30:07+00:00,http://arxiv.org/abs/2207.00735v1,"Benyou Wang, Xiangbo Wu, Xiaokang Liu, Jianquan Li, Prayag Tiwari, Qianqian Xie",cs.CL,table2text,"Language is the principal tool for human communication, in which humor is one
of the most attractive parts. Producing natural language like humans using
computers, a.k.a, Natural Language Generation (NLG), has been widely used for
dialogue systems, chatbots, machine translation, as well as computer-aid
creation e.g., idea generations, scriptwriting. However, the humor aspect of
natural language is relatively under-investigated, especially in the age of
pre-trained language models. In this work, we aim to preliminarily test whether
NLG can generate humor as humans do. We build a new dataset consisting of
numerous digitized Chinese Comical Crosstalk scripts (called C$^3$ in short),
which is for a popular Chinese performing art called `Xiangsheng' since 1800s.
(For convenience for non-Chinese speakers, we called `crosstalk' for
`Xiangsheng' in this paper.) We benchmark various generation approaches
including training-from-scratch Seq2seq, fine-tuned middle-scale PLMs, and
large-scale PLMs (with and without fine-tuning). Moreover, we also conduct a
human assessment, showing that 1) large-scale pretraining largely improves
crosstalk generation quality; and 2) even the scripts generated from the best
PLM is far from what we expect, with only 65% quality of human-created
crosstalk. We conclude, humor generation could be largely improved using
large-scaled PLMs, but it is still in its infancy.
  The data and benchmarking code is publicly available in
\url{https://github.com/anonNo2/crosstalk-generation}.",2022-07-02
"Syntax Controlled Knowledge Graph-to-Text Generation with Order and
  Semantic Consistency",2022-07-02 02:42:14+00:00,http://arxiv.org/abs/2207.00719v1,"Jin Liu, Chongfeng Fan, Fengyu Zhou, Huijuan Xu",cs.AI,table2text,"The knowledge graph (KG) stores a large amount of structural knowledge, while
it is not easy for direct human understanding. Knowledge graph-to-text
(KG-to-text) generation aims to generate easy-to-understand sentences from the
KG, and at the same time, maintains semantic consistency between generated
sentences and the KG. Existing KG-to-text generation methods phrase this task
as a sequence-to-sequence generation task with linearized KG as input and
consider the consistency issue of the generated texts and KG through a simple
selection between decoded sentence word and KG node word at each time step.
However, the linearized KG order is commonly obtained through a heuristic
search without data-driven optimization. In this paper, we optimize the
knowledge description order prediction under the order supervision extracted
from the caption and further enhance the consistency of the generated sentences
and KG through syntactic and semantic regularization. We incorporate the
Part-of-Speech (POS) syntactic tags to constrain the positions to copy words
from the KG and employ a semantic context scoring function to evaluate the
semantic fitness for each word in its local context when decoding each word in
the generated sentence. Extensive experiments are conducted on two datasets,
WebNLG and DART, and achieve state-of-the-art performances.",2022-07-02
Mapping the Design Space of Human-AI Interaction in Text Summarization,2022-06-29 19:03:25+00:00,http://arxiv.org/abs/2206.14863v1,"Ruijia Cheng, Alison Smith-Renner, Ke Zhang, Joel R. Tetreault, Alejandro Jaimes",cs.HC,table2text,"Automatic text summarization systems commonly involve humans for preparing
data or evaluating model performance, yet, there lacks a systematic
understanding of humans' roles, experience, and needs when interacting with or
being assisted by AI. From a human-centered perspective, we map the design
opportunities and considerations for human-AI interaction in text summarization
and broader text generation tasks. We first conducted a systematic literature
review of 70 papers, developing a taxonomy of five interactions in AI-assisted
text generation and relevant design dimensions. We designed text summarization
prototypes for each interaction. We then interviewed 16 users, aided by the
prototypes, to understand their expectations, experience, and needs regarding
efficiency, control, and trust with AI in text summarization and propose design
considerations accordingly.",2022-06-29
Joint Generator-Ranker Learning for Natural Language Generation,2022-06-28 12:58:30+00:00,http://arxiv.org/abs/2206.13974v1,"Weizhou Shen, Yeyun Gong, Yelong Shen, Song Wang, Xiaojun Quan, Nan Duan, Weizhu Chen",cs.CL,table2text,"Due to exposure bias, most existing natural language generation (NLG) models
trained by maximizing the likelihood objective predict poor text results during
the inference stage. In this paper, to tackle this problem, we revisit the
generate-then-rank framework and propose a joint generator-ranker (JGR)
training algorithm for text generation tasks. In JGR, the generator model is
trained by maximizing two objectives: the likelihood of the training corpus and
the expected reward given by the ranker model. Meanwhile, the ranker model
takes input samples from the generator model and learns to distinguish good
samples from the generation pool. The generator and ranker models are
alternately optimized till convergence. In the empirical study, the proposed
JGR model achieves new state-of-the-art performance on five public benchmarks
covering three popular generation tasks: summarization, question generation,
and response generation. We will make code, data, and models available at
https://github.com/microsoft/AdvNLG.",2022-06-28
Megapixel Image Generation with Step-Unrolled Denoising Autoencoders,2022-06-24 15:47:42+00:00,http://arxiv.org/abs/2206.12351v1,"Alex F. McKinney, Chris G. Willcocks","cs.CV, cs.LG",table2text,"An ongoing trend in generative modelling research has been to push sample
resolutions higher whilst simultaneously reducing computational requirements
for training and sampling. We aim to push this trend further via the
combination of techniques - each component representing the current pinnacle of
efficiency in their respective areas. These include vector-quantized GAN
(VQ-GAN), a vector-quantization (VQ) model capable of high levels of lossy -
but perceptually insignificant - compression; hourglass transformers, a highly
scaleable self-attention model; and step-unrolled denoising autoencoders
(SUNDAE), a non-autoregressive (NAR) text generative model. Unexpectedly, our
method highlights weaknesses in the original formulation of hourglass
transformers when applied to multidimensional data. In light of this, we
propose modifications to the resampling mechanism, applicable in any task
applying hierarchical transformers to multidimensional data. Additionally, we
demonstrate the scalability of SUNDAE to long sequence lengths - four times
longer than prior work. Our proposed framework scales to high-resolutions
($1024 \times 1024$) and trains quickly (2-4 days). Crucially, the trained
model produces diverse and realistic megapixel samples in approximately 2
seconds on a consumer-grade GPU (GTX 1080Ti). In general, the framework is
flexible: supporting an arbitrary number of sampling steps, sample-wise
self-stopping, self-correction capabilities, conditional generation, and a NAR
formulation that allows for arbitrary inpainting masks. We obtain FID scores of
10.56 on FFHQ256 - close to the original VQ-GAN in less than half the sampling
steps - and 21.85 on FFHQ1024 in only 100 sampling steps.",2022-06-24
MVP: Multi-task Supervised Pre-training for Natural Language Generation,2022-06-24 07:49:47+00:00,http://arxiv.org/abs/2206.12131v1,"Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen",cs.CL,table2text,"Pre-trained language models (PLMs) have achieved notable success in natural
language generation (NLG) tasks. Up to now, most of the PLMs are pre-trained in
an unsupervised manner using large-scale general corpus. In the meanwhile, an
increasing number of models pre-trained with less labeled data showcase
superior performance compared to unsupervised models. Motivated by the success
of supervised pre-training, we propose Multi-task superVised Pre-training (MVP)
for natural language generation. For pre-training the text generation model
MVP, we collect a labeled pre-training corpus from 45 datasets over seven
generation tasks. For each task, we further pre-train specific soft prompts to
stimulate the model capacity in performing a specific task. Extensive
experiments have demonstrated the effectiveness of our supervised pre-training
in a number of NLG tasks, and our general methods achieve state-of-the-art
performance on 12 of 17 datasets.",2022-06-24
"Comparing informativeness of an NLG chatbot vs graphical app in
  diet-information domain",2022-06-23 07:15:58+00:00,http://arxiv.org/abs/2206.13435v1,"Simone Balloccu, Ehud Reiter","cs.CL, cs.AI",table2text,"Visual representation of data like charts and tables can be challenging to
understand for readers. Previous work showed that combining visualisations with
text can improve the communication of insights in static contexts, but little
is known about interactive ones. In this work we present an NLG chatbot that
processes natural language queries and provides insights through a combination
of charts and text. We apply it to nutrition, a domain communication quality is
critical. Through crowd-sourced evaluation we compare the informativeness of
our chatbot against traditional, static diet-apps. We find that the
conversational context significantly improved users' understanding of dietary
data in various tasks, and that users considered the chatbot as more useful and
quick to use than traditional apps.",2022-06-23
GEMv2: Multilingual NLG Benchmarking in a Single Line of Code,2022-06-22 17:52:30+00:00,http://arxiv.org/abs/2206.11249v3,"Sebastian Gehrmann, Abhik Bhattacharjee, Abinaya Mahendiran, Alex Wang, Alexandros Papangelis, Aman Madaan, Angelina McMillan-Major, Anna Shvets, Ashish Upadhyay, Bingsheng Yao, Bryan Wilie, Chandra Bhagavatula, Chaobin You, Craig Thomson, Cristina Garbacea, Dakuo Wang, Daniel Deutsch, Deyi Xiong, Di Jin, Dimitra Gkatzia, Dragomir Radev, Elizabeth Clark, Esin Durmus, Faisal Ladhak, Filip Ginter, Genta Indra Winata, Hendrik Strobelt, Hiroaki Hayashi, Jekaterina Novikova, Jenna Kanerva, Jenny Chim, Jiawei Zhou, Jordan Clive, Joshua Maynez, João Sedoc, Juraj Juraska, Kaustubh Dhole, Khyathi Raghavi Chandu, Laura Perez-Beltrachini, Leonardo F. R. Ribeiro, Lewis Tunstall, Li Zhang, Mahima Pushkarna, Mathias Creutz, Michael White, Mihir Sanjay Kale, Moussa Kamal Eddine, Nico Daheim, Nishant Subramani, Ondrej Dusek, Paul Pu Liang, Pawan Sasanka Ammanamanchi, Qi Zhu, Ratish Puduppully, Reno Kriz, Rifat Shahriyar, Ronald Cardenas, Saad Mahamood, Salomey Osei, Samuel Cahyawijaya, Sanja Štajner, Sebastien Montella, Shailza, Shailza Jolly, Simon Mille, Tahmid Hasan, Tianhao Shen, Tosin Adewumi, Vikas Raunak, Vipul Raheja, Vitaly Nikolaev, Vivian Tsai, Yacine Jernite, Ying Xu, Yisi Sang, Yixin Liu, Yufang Hou","cs.CL, cs.AI, cs.LG",table2text,"Evaluation in machine learning is usually informed by past choices, for
example which datasets or metrics to use. This standardization enables the
comparison on equal footing using leaderboards, but the evaluation choices
become sub-optimal as better alternatives arise. This problem is especially
pertinent in natural language generation which requires ever-improving suites
of datasets, metrics, and human evaluation to make definitive claims. To make
following best model evaluation practices easier, we introduce GEMv2. The new
version of the Generation, Evaluation, and Metrics Benchmark introduces a
modular infrastructure for dataset, model, and metric developers to benefit
from each others work. GEMv2 supports 40 documented datasets in 51 languages.
Models for all datasets can be evaluated online and our interactive data card
creation and rendering tools make it easier to add new datasets to the living
benchmark.",2022-06-22
"BenchCLAMP: A Benchmark for Evaluating Language Models on Semantic
  Parsing",2022-06-21 18:34:11+00:00,http://arxiv.org/abs/2206.10668v1,"Subhro Roy, Sam Thomson, Tongfei Chen, Richard Shin, Adam Pauls, Jason Eisner, Benjamin Van Durme",cs.CL,table2text,"We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model
Parsing, which produces semantic outputs based on the analysis of input text
through constrained decoding of a prompted or fine-tuned language model.
Developers of pretrained language models currently benchmark on classification,
span extraction and free-text generation tasks. Semantic parsing is neglected
in language model evaluation because of the complexity of handling
task-specific architectures and representations. Recent work has shown that
generation from a prompted or fine-tuned language model can perform well at
semantic parsing when the output is constrained to be a valid semantic
representation. BenchCLAMP includes context-free grammars for six semantic
parsing datasets with varied output meaning representations, as well as a
constrained decoding interface to generate outputs covered by these grammars.
We provide low, medium, and high resource splits for each dataset, allowing
accurate comparison of various language models under different data regimes.
Our benchmark supports both prompt-based learning as well as fine-tuning, and
provides an easy-to-use toolkit for language model developers to evaluate on
semantic parsing.",2022-06-21
Prefix Language Models are Unified Modal Learners,2022-06-15 17:49:38+00:00,http://arxiv.org/abs/2206.07699v1,"Shizhe Diao, Wangchunshu Zhou, Xinsong Zhang, Jiawei Wang","cs.CV, cs.CL, cs.LG",table2text,"With the success of vision-language pre-training, we have witnessed the
state-of-the-art has been pushed on multi-modal understanding and generation.
However, the current pre-training paradigm is either incapable of targeting all
modalities at once (e.g., text generation and image generation), or requires
multi-fold well-designed tasks which significantly limits the scalability. We
demonstrate that a unified modal model could be learned with a prefix language
modeling objective upon text and image sequences. Thanks to the simple but
powerful pre-training paradigm, our proposed model, DaVinci, is simple to
train, scalable to huge data, and adaptable to a variety of downstream tasks
across modalities (language / vision / vision+language), types (understanding /
generation) and settings (e.g., zero-shot, fine-tuning, linear evaluation) with
a single unified architecture. DaVinci achieves the competitive performance on
a wide range of 26 understanding / generation tasks, and outperforms previous
unified vision-language models on most tasks, including ImageNet classification
(+1.6%), VQAv2 (+1.4%), COCO caption generation (BLEU@4 +1.1%, CIDEr +1.5%) and
COCO image generation (IS +0.9%, FID -1.0%), at the comparable model and data
scale. Furthermore, we offer a well-defined benchmark for future research by
reporting the performance on different scales of the pre-training dataset on a
heterogeneous and wide distribution coverage. Our results establish new,
stronger baselines for future comparisons at different data scales and shed
light on the difficulties of comparing VLP models more generally.",2022-06-15
A Benchmark for Federated Hetero-Task Learning,2022-06-07 16:43:09+00:00,http://arxiv.org/abs/2206.03436v2,"Liuyi Yao, Dawei Gao, Zhen Wang, Yuexiang Xie, Weirui Kuang, Daoyuan Chen, Haohui Wang, Chenhe Dong, Bolin Ding, Yaliang Li",cs.LG,table2text,"To investigate the heterogeneity in federated learning in real-world
scenarios, we generalize the classic federated learning to federated
hetero-task learning, which emphasizes the inconsistency across the
participants in federated learning in terms of both data distribution and
learning tasks. We also present B-FHTL, a federated hetero-task learning
benchmark consisting of simulation dataset, FL protocols and a unified
evaluation mechanism. B-FHTL dataset contains three well-designed federated
learning tasks with increasing heterogeneity. Each task simulates the clients
with different non-IID data and learning tasks. To ensure fair comparison among
different FL algorithms, B-FHTL builds in a full suite of FL protocols by
providing high-level APIs to avoid privacy leakage, and presets most common
evaluation metrics spanning across different learning tasks, such as
regression, classification, text generation and etc. Furthermore, we compare
the FL algorithms in fields of federated multi-task learning, federated
personalization and federated meta learning within B-FHTL, and highlight the
influence of heterogeneity and difficulties of federated hetero-task learning.
Our benchmark, including the federated dataset, protocols, the evaluation
mechanism and the preliminary experiment, is open-sourced at
https://github.com/alibaba/FederatedScope/tree/master/benchmark/B-FHTL",2022-06-07
DeepCAVE: An Interactive Analysis Tool for Automated Machine Learning,2022-06-07 12:59:39+00:00,http://arxiv.org/abs/2206.03493v1,"René Sass, Eddie Bergman, André Biedenkapp, Frank Hutter, Marius Lindauer",cs.LG,table2text,"Automated Machine Learning (AutoML) is used more than ever before to support
users in determining efficient hyperparameters, neural architectures, or even
full machine learning pipelines. However, users tend to mistrust the
optimization process and its results due to a lack of transparency, making
manual tuning still widespread. We introduce DeepCAVE, an interactive framework
to analyze and monitor state-of-the-art optimization procedures for AutoML
easily and ad hoc. By aiming for full and accessible transparency, DeepCAVE
builds a bridge between users and AutoML and contributes to establishing trust.
Our framework's modular and easy-to-extend nature provides users with
automatically generated text, tables, and graphic visualizations. We show the
value of DeepCAVE in an exemplary use-case of outlier detection, in which our
framework makes it easy to identify problems, compare multiple runs and
interpret optimization processes. The package is freely available on GitHub
https://github.com/automl/DeepCAVE.",2022-06-07
Plot Writing From Pre-Trained Language Models,2022-06-07 05:30:46+00:00,http://arxiv.org/abs/2206.03021v1,"Yiping Jin, Vishakha Kadam, Dittaya Wanvarie",cs.CL,table2text,"Pre-trained language models (PLMs) fail to generate long-form narrative text
because they do not consider global structure. As a result, the generated texts
are often incohesive, repetitive, or lack content. Recent work in story
generation reintroduced explicit content planning in the form of prompts,
keywords, or semantic frames. Trained on large parallel corpora, these models
can generate more logical event sequences and thus more contentful stories.
However, these intermediate representations are often not in natural language
and cannot be utilized by PLMs without fine-tuning. We propose generating story
plots using off-the-shelf PLMs while maintaining the benefit of content
planning to generate cohesive and contentful stories. Our proposed method,
ScratchPlot, first prompts a PLM to compose a content plan. Then, we generate
the story's body and ending conditioned on the content plan. Furthermore, we
take a generate-and-rank approach by using additional PLMs to rank the
generated (story, ending) pairs. We benchmark our method with various baselines
and achieved superior results in both human and automatic evaluation.",2022-06-07
"Curriculum-Based Self-Training Makes Better Few-Shot Learners for
  Data-to-Text Generation",2022-06-06 16:11:58+00:00,http://arxiv.org/abs/2206.02712v1,"Pei Ke, Haozhe Ji, Zhenyu Yang, Yi Huang, Junlan Feng, Xiaoyan Zhu, Minlie Huang",cs.CL,table2text,"Despite the success of text-to-text pre-trained models in various natural
language generation (NLG) tasks, the generation performance is largely
restricted by the number of labeled data in downstream tasks, particularly in
data-to-text generation tasks. Existing works mostly utilize abundant unlabeled
structured data to conduct unsupervised pre-training for task adaption, which
fail to model the complex relationship between source structured data and
target texts. Thus, we introduce self-training as a better few-shot learner
than task-adaptive pre-training, which explicitly captures this relationship
via pseudo-labeled data generated by the pre-trained model. To alleviate the
side-effect of low-quality pseudo-labeled data during self-training, we propose
a novel method called Curriculum-Based Self-Training (CBST) to effectively
leverage unlabeled data in a rearranged order determined by the difficulty of
text generation. Experimental results show that our method can outperform
fine-tuning and task-adaptive pre-training methods, and achieve
state-of-the-art performance in the few-shot setting of data-to-text
generation.",2022-06-06
"Learning to Break the Loop: Analyzing and Mitigating Repetitions for
  Neural Text Generation",2022-06-06 05:51:12+00:00,http://arxiv.org/abs/2206.02369v1,"Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, Jian Li",cs.CL,table2text,"While large-scale neural language models, such as GPT2 and BART, have
achieved impressive results on various text generation tasks, they tend to get
stuck in undesirable sentence-level loops with maximization-based decoding
algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive
since there are few consecutive sentence-level repetitions in human corpora
(e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for
generating consecutive sentence-level repetitions, we study the relationship
between the probabilities of the repetitive tokens and their previous
repetitions in the context. Through our quantitative experiments, we find that
1) Language models have a preference to repeat the previous sentence; 2) The
sentence-level repetitions have a \textit{self-reinforcement effect}: the more
times a sentence is repeated in the context, the higher the probability of
continuing to generate that sentence; 3) The sentences with higher initial
probabilities usually have a stronger self-reinforcement effect. Motivated by
our findings, we propose a simple and effective training method \textbf{DITTO}
(Pseu\underline{D}o-Repet\underline{IT}ion
Penaliza\underline{T}i\underline{O}n), where the model learns to penalize
probabilities of sentence-level repetitions from pseudo repetitive data.
Although our method is motivated by mitigating repetitions, experiments show
that DITTO not only mitigates the repetition issue without sacrificing
perplexity, but also achieves better generation quality. Extensive experiments
on open-ended text generation (Wikitext-103) and text summarization
(CNN/DailyMail) demonstrate the generality and effectiveness of our method.",2022-06-06
"Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for
  Text-to-Speech",2022-06-05 10:50:34+00:00,http://arxiv.org/abs/2206.02147v1,"Ziyue Jiang, Su Zhe, Zhou Zhao, Qian Yang, Yi Ren, Jinglin Liu, Zhenhui Ye","eess.AS, cs.CL, cs.SD",table2text,"Polyphone disambiguation aims to capture accurate pronunciation knowledge
from natural text sequences for reliable Text-to-speech (TTS) systems. However,
previous approaches require substantial annotated training data and additional
efforts from language experts, making it difficult to extend high-quality
neural TTS systems to out-of-domain daily conversations and countless languages
worldwide. This paper tackles the polyphone disambiguation problem from a
concise and novel perspective: we propose Dict-TTS, a semantic-aware generative
text-to-speech model with an online website dictionary (the existing prior
information in the natural language). Specifically, we design a
semantics-to-pronunciation attention (S2PA) module to match the semantic
patterns between the input text sequence and the prior semantics in the
dictionary and obtain the corresponding pronunciations; The S2PA module can be
easily trained with the end-to-end TTS model without any annotated phoneme
labels. Experimental results in three languages show that our model outperforms
several strong baseline models in terms of pronunciation accuracy and improves
the prosody modeling of TTS systems. Further extensive analyses with different
linguistic encoders demonstrate that each design in Dict-TTS is effective.
Audio samples are available at \url{https://dicttts.github.io/DictTTS-Demo/}.",2022-06-05
CoNT: Contrastive Neural Text Generation,2022-05-29 15:18:37+00:00,http://arxiv.org/abs/2205.14690v1,"Chenxin An, Jiangtao Feng, Kai Lv, Lingpeng Kong, Xipeng Qiu, Xuanjing Huang",cs.CL,table2text,"Recently, contrastive learning attracts increasing interests in neural text
generation as a new solution to alleviate the exposure bias problem. It
introduces a sequence-level training signal which is crucial to generation
tasks that always rely on auto-regressive decoding. However, previous methods
using contrastive learning in neural text generation usually lead to inferior
performance. In this paper, we analyse the underlying reasons and propose a new
Contrastive Neural Text generation framework, CoNT. CoNT addresses bottlenecks
that prevent contrastive learning from being widely adopted in generation tasks
from three aspects -- the construction of contrastive examples, the choice of
the contrastive loss, and the strategy in decoding. We validate CoNT on five
generation tasks with ten benchmarks, including machine translation,
summarization, code comment generation, data-to-text generation and commonsense
generation. Experimental results show that CoNT clearly outperforms the
conventional training framework on all the ten benchmarks with a convincing
margin. Especially, CoNT surpasses previous the most competitive contrastive
learning method for text generation, by 1.50 BLEU on machine translation and
1.77 ROUGE-1 on summarization, respectively. It achieves new state-of-the-art
on summarization, code comment generation (without external data) and
data-to-text generation.",2022-05-29
Controllable Text Generation with Neurally-Decomposed Oracle,2022-05-27 20:17:53+00:00,http://arxiv.org/abs/2205.14219v1,"Tao Meng, Sidi Lu, Nanyun Peng, Kai-Wei Chang",cs.CL,table2text,"We propose a general and efficient framework to control auto-regressive
generation models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained
base language model and a sequence-level boolean oracle function, we propose to
decompose the oracle function into token-level guidance to steer the base model
in text generation. Specifically, the token-level guidance is approximated by a
neural model trained with examples sampled from the base model, demanding no
additional auxiliary labeled data. We present the closed-form optimal solution
to incorporate the token-level guidance into the base model for controllable
generation. We further provide a theoretical analysis of how the approximation
quality of NADO affects the controllable generation results. Experiments
conducted on two applications: (1) text generation with lexical constraints and
(2) machine translation with formality control demonstrate that our framework
efficiently guides the base model towards the given oracle while maintaining
high generation quality.",2022-05-27
Diffusion-LM Improves Controllable Text Generation,2022-05-27 20:12:09+00:00,http://arxiv.org/abs/2205.14217v1,"Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, Tatsunori B. Hashimoto","cs.CL, cs.AI, cs.LG",table2text,"Controlling the behavior of language models (LMs) without re-training is a
major open problem in natural language generation. While recent works have
demonstrated successes on controlling simple sentence attributes (e.g.,
sentiment), there has been little progress on complex, fine-grained controls
(e.g., syntactic structure). To address this challenge, we develop a new
non-autoregressive language model based on continuous diffusions that we call
Diffusion-LM. Building upon the recent successes of diffusion models in
continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian
vectors into word vectors, yielding a sequence of intermediate latent
variables. The continuous, hierarchical nature of these intermediate variables
enables a simple gradient-based algorithm to perform complex, controllable
generation tasks. We demonstrate successful control of Diffusion-LM for six
challenging fine-grained control tasks, significantly outperforming prior work.",2022-05-27
Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach,2022-05-26 06:36:53+00:00,http://arxiv.org/abs/2205.13183v1,"Chao Zhao, Faeze Brahman, Tenghao Huang, Snigdha Chaturvedi",cs.CL,table2text,"Pre-trained models (PTMs) have lead to great improvements in natural language
generation (NLG). However, it is still unclear how much commonsense knowledge
they possess. With the goal of evaluating commonsense knowledge of NLG models,
recent work has proposed the problem of generative commonsense reasoning, e.g.,
to compose a logical sentence given a set of unordered concepts. Existing
approaches to this problem hypothesize that PTMs lack sufficient parametric
knowledge for this task, which can be overcome by introducing external
knowledge or task-specific pre-training objectives. Different from this trend,
we argue that PTM's inherent ability for generative commonsense reasoning is
underestimated due to the order-agnostic property of its input. In particular,
we hypothesize that the order of the input concepts can affect the PTM's
ability to utilize its commonsense knowledge. To this end, we propose a
pre-ordering approach to elaborately manipulate the order of the given concepts
before generation. Experiments show that our approach can outperform the more
sophisticated models that have access to a lot of external data and resources.",2022-05-26
"Automatic question generation based on sentence structure analysis using
  machine learning approach",2022-05-25 14:35:29+00:00,http://arxiv.org/abs/2205.12811v1,"Miroslav Blšták, Viera Rozinajová","cs.CL, cs.AI",table2text,"Automatic question generation is one of the most challenging tasks of Natural
Language Processing. It requires ""bidirectional"" language processing: firstly,
the system has to understand the input text (Natural Language Understanding)
and it then has to generate questions also in the form of text (Natural
Language Generation). In this article, we introduce our framework for
generating the factual questions from unstructured text in the English
language. It uses a combination of traditional linguistic approaches based on
sentence patterns with several machine learning methods. We firstly obtain
lexical, syntactic and semantic information from an input text and we then
construct a hierarchical set of patterns for each sentence. The set of features
is extracted from the patterns and it is then used for automated learning of
new transformation rules. Our learning process is totally data-driven because
the transformation rules are obtained from a set of initial sentence-question
pairs. The advantages of this approach lie in a simple expansion of new
transformation rules which allows us to generate various types of questions and
also in the continuous improvement of the system by reinforcement learning. The
framework also includes a question evaluation module which estimates the
quality of generated questions. It serves as a filter for selecting the best
questions and eliminating incorrect ones or duplicates. We have performed
several experiments to evaluate the correctness of generated questions and we
have also compared our system with several state-of-the-art systems. Our
results indicate that the quality of generated questions outperforms the
state-of-the-art systems and our questions are also comparable to questions
created by humans. We have also created and published an interface with all
created datasets and evaluated questions, so it is possible to follow up on our
work.",2022-05-25
PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation,2022-05-25 11:55:54+00:00,http://arxiv.org/abs/2205.12697v1,"Ao Liu, Haoyu Dong, Naoaki Okazaki, Shi Han, Dongmei Zhang",cs.CL,table2text,"Logical table-to-text generation is a task that involves generating logically
faithful sentences from tables, which requires models to derive logical level
facts from table records via logical inference. It raises a new challenge on
the logical-level content planning of table-to-text models. However, directly
learning the logical inference knowledge from table-text pairs is very
difficult for neural models because of the ambiguity of natural language and
the scarcity of parallel data. Hence even large-scale pre-trained language
models present low logical fidelity on logical table-to-text. In this work, we
propose a PLOG (Pretrained Logical Form Generator) framework to improve the
generation fidelity. Specifically, PLOG is first pretrained on a
table-to-logic-form generation (table-to-logic) task, then finetuned on
downstream table-to-text tasks. The formal definition of logical forms enables
us to collect large amount of accurate logical forms from tables without human
annotation. In addition, PLOG can learn logical inference from table-logic
pairs much more definitely than from table-text pairs. To evaluate our model,
we further collect a controlled logical table-to-text dataset CONTLOG based on
an existing dataset. On two benchmarks, LOGICNLG and CONTLOG, PLOG outperforms
strong baselines by a large margin on the logical fidelity, demonstrating the
effectiveness of table-to-logic pretraining.",2022-05-25
