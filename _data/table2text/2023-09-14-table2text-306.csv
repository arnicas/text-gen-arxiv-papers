title,pubdate,id,authors,categories,search,abstract,displaydate
"CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain
  Performance and Calibration",2023-09-14 16:16:40+00:00,http://arxiv.org/abs/2309.07822v1,"Rachneet Sachdeva, Martin Tutek, Iryna Gurevych",cs.CL,table2text,"In recent years, large language models (LLMs) have shown remarkable
capabilities at scale, particularly at generating text conditioned on a prompt.
In our work, we investigate the use of LLMs to augment training data of small
language models~(SLMs) with automatically generated counterfactual~(CF)
instances -- i.e. minimally altered inputs -- in order to improve
out-of-domain~(OOD) performance of SLMs in the extractive question
answering~(QA) setup. We show that, across various LLM generators, such data
augmentation consistently enhances OOD performance and improves model
calibration for both confidence-based and rationale-augmented calibrator
models. Furthermore, these performance improvements correlate with higher
diversity of CF instances in terms of their surface form and semantic content.
Finally, we show that CF augmented models which are easier to calibrate also
exhibit much lower entropy when assigning importance, indicating that
rationale-augmented calibrators prefer concise explanations.",2023-09-14
Semantic reconstruction of continuous language from MEG signals,2023-09-14 13:19:53+00:00,http://arxiv.org/abs/2309.07701v1,"Bo Wang, Xiran Xu, Longxiang Zhang, Boda Xiao, Xihong Wu, Jing Chen","cs.HC, eess.SP, q-bio.NC",table2text,"Decoding language from neural signals holds considerable theoretical and
practical importance. Previous research has indicated the feasibility of
decoding text or speech from invasive neural signals. However, when using
non-invasive neural signals, significant challenges are encountered due to
their low quality. In this study, we proposed a data-driven approach for
decoding semantic of language from Magnetoencephalography (MEG) signals
recorded while subjects were listening to continuous speech. First, a
multi-subject decoding model was trained using contrastive learning to
reconstruct continuous word embeddings from MEG data. Subsequently, a beam
search algorithm was adopted to generate text sequences based on the
reconstructed word embeddings. Given a candidate sentence in the beam, a
language model was used to predict the subsequent words. The word embeddings of
the subsequent words were correlated with the reconstructed word embedding.
These correlations were then used as a measure of the probability for the next
word. The results showed that the proposed continuous word embedding model can
effectively leverage both subject-specific and subject-shared information.
Additionally, the decoded text exhibited significant similarity to the target
text, with an average BERTScore of 0.816, a score comparable to that in the
previous fMRI study.",2023-09-14
Auto-Regressive Next-Token Predictors are Universal Learners,2023-09-13 14:15:03+00:00,http://arxiv.org/abs/2309.06979v1,Eran Malach,"cs.LG, cs.CL",table2text,"Large language models display remarkable capabilities in logical and
mathematical reasoning, allowing them to solve complex tasks. Interestingly,
these abilities emerge in networks trained on the simple task of next-token
prediction. In this work, we present a theoretical framework for studying
auto-regressive next-token predictors. We demonstrate that even simple models
such as linear next-token predictors, trained on Chain-of-Thought (CoT) data,
can approximate any function efficiently computed by a Turing machine. We
introduce a new complexity measure -- length complexity -- which measures the
number of intermediate tokens in a CoT sequence required to approximate some
target function, and analyze the interplay between length complexity and other
notions of complexity. Finally, we show experimentally that simple next-token
predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs),
display non-trivial performance on text generation and arithmetic tasks. Our
results demonstrate that the power of language models can be attributed, to a
great extent, to the auto-regressive next-token training scheme, and not
necessarily to a particular choice of architecture.",2023-09-13
Scaled Prompt-Tuning for Few-Shot Natural Language Generation,2023-09-13 07:12:31+00:00,http://arxiv.org/abs/2309.06759v1,"Ting Hu, Christoph Meinel, Haojin Yang",cs.CL,table2text,"The increasingly Large Language Models (LLMs) demonstrate stronger language
understanding and generation capabilities, while the memory demand and
computation cost of fine-tuning LLMs on downstream tasks are non-negligible.
Besides, fine-tuning generally requires a certain amount of data from
individual tasks whilst data collection cost is another issue to consider in
real-world applications. In this work, we focus on Parameter-Efficient
Fine-Tuning (PEFT) methods for few-shot Natural Language Generation (NLG),
which freeze most parameters in LLMs and tune a small subset of parameters in
few-shot cases so that memory footprint, training cost, and labeling cost are
reduced while maintaining or even improving the performance. We propose a
Scaled Prompt-Tuning (SPT) method which surpasses conventional PT with better
performance and generalization ability but without an obvious increase in
training cost. Further study on intermediate SPT suggests the superior
transferability of SPT in few-shot scenarios, providing a recipe for
data-deficient and computation-limited circumstances. Moreover, a comprehensive
comparison of existing PEFT methods reveals that certain approaches exhibiting
decent performance with modest training cost such as Prefix-Tuning in prior
study could struggle in few-shot NLG tasks, especially on challenging datasets.",2023-09-13
"Text Encoders Lack Knowledge: Leveraging Generative LLMs for
  Domain-Specific Semantic Textual Similarity",2023-09-12 19:32:45+00:00,http://arxiv.org/abs/2309.06541v1,"Joseph Gatto, Omar Sharif, Parker Seegmiller, Philip Bohlman, Sarah Masud Preum",cs.CL,table2text,"Amidst the sharp rise in the evaluation of large language models (LLMs) on
various tasks, we find that semantic textual similarity (STS) has been
under-explored. In this study, we show that STS can be cast as a text
generation problem while maintaining strong performance on multiple STS
benchmarks. Additionally, we show generative LLMs significantly outperform
existing encoder-based STS models when characterizing the semantic similarity
between two texts with complex semantic relationships dependent on world
knowledge. We validate this claim by evaluating both generative LLMs and
existing encoder-based STS models on three newly collected STS challenge sets
which require world knowledge in the domains of Health, Politics, and Sports.
All newly collected data is sourced from social media content posted after May
2023 to ensure the performance of closed-source models like ChatGPT cannot be
credited to memorization. Our results show that, on average, generative LLMs
outperform the best encoder-only baselines by an average of 22.3% on STS tasks
requiring world knowledge. Our results suggest generative language models with
STS-specific prompting strategies achieve state-of-the-art performance in
complex, domain-specific STS tasks.",2023-09-12
"Neural Latent Geometry Search: Product Manifold Inference via
  Gromov-Hausdorff-Informed Bayesian Optimization",2023-09-09 14:29:22+00:00,http://arxiv.org/abs/2309.04810v1,"Haitz Saez de Ocariz Borde, Alvaro Arroyo, Ismael Morales, Ingmar Posner, Xiaowen Dong","cs.LG, stat.ML",table2text,"Recent research indicates that the performance of machine learning models can
be improved by aligning the geometry of the latent space with the underlying
data structure. Rather than relying solely on Euclidean space, researchers have
proposed using hyperbolic and spherical spaces with constant curvature, or
combinations thereof, to better model the latent space and enhance model
performance. However, little attention has been given to the problem of
automatically identifying the optimal latent geometry for the downstream task.
We mathematically define this novel formulation and coin it as neural latent
geometry search (NLGS). More specifically, we introduce a principled method
that searches for a latent geometry composed of a product of constant curvature
model spaces with minimal query evaluations. To accomplish this, we propose a
novel notion of distance between candidate latent geometries based on the
Gromov-Hausdorff distance from metric geometry. In order to compute the
Gromov-Hausdorff distance, we introduce a mapping function that enables the
comparison of different manifolds by embedding them in a common
high-dimensional ambient space. Finally, we design a graph search space based
on the calculated distances between candidate manifolds and use Bayesian
optimization to search for the optimal latent geometry in a query-efficient
manner. This is a general method which can be applied to search for the optimal
latent geometry for a variety of models and downstream tasks. Extensive
experiments on synthetic and real-world datasets confirm the efficacy of our
method in identifying the optimal latent geometry for multiple machine learning
problems.",2023-09-09
"EPA: Easy Prompt Augmentation on Large Language Models via Multiple
  Sources and Multiple Targets",2023-09-09 09:03:50+00:00,http://arxiv.org/abs/2309.04725v1,"Hongyuan Lu, Wai Lam",cs.CL,table2text,"Large language models (LLMs) have shown promising performance on various NLP
tasks via task prompting. And their performance can be further improved by
appending task demonstrations to the head of the prompt. And usually, a better
performance can be achieved with more demonstrations. However, asking the users
to write the demonstrations can be cumbersome. As a simple yet cost-effective
workaround, this paper proposes a novel method called EPA (\textbf{E}asy
\textbf{P}rompt \textbf{A}ugmentation)\footnote{While this paper considers
augmenting prompts via demonstrations, we name it EPA as the name EDA is
already taken by a well-known NLP method \citep{wei-zou-2019-eda}.} that
effectively minimizes user efforts in writing demonstrations while improving
the model performance at the same time. EPA achieves these goals by
automatically augmenting the demonstrations with multiple sources/targets,
where each of them paraphrases each other. This is well motivated as augmenting
data via paraphrasing effectively improves neural language models. EPA thus
employs paraphrasing as an augmentation method for in-context learning.
Extensive experiments indicate that EPA effectively improves both NLU and NLG
tasks, covering from natural language inference to machine translation in
translating tens of languages.\footnote{Code and data will be released upon
publication.}",2023-09-09
ConDA: Contrastive Domain Adaptation for AI-generated Text Detection,2023-09-07 19:51:30+00:00,http://arxiv.org/abs/2309.03992v1,"Amrita Bhattacharjee, Tharindu Kumarage, Raha Moraffah, Huan Liu","cs.CL, cs.AI, cs.LG",table2text,"Large language models (LLMs) are increasingly being used for generating text
in a variety of use cases, including journalistic news articles. Given the
potential malicious nature in which these LLMs can be used to generate
disinformation at scale, it is important to build effective detectors for such
AI-generated text. Given the surge in development of new LLMs, acquiring
labeled training data for supervised detectors is a bottleneck. However, there
might be plenty of unlabeled text data available, without information on which
generator it came from. In this work we tackle this data problem, in detecting
AI-generated news text, and frame the problem as an unsupervised domain
adaptation task. Here the domains are the different text generators, i.e. LLMs,
and we assume we have access to only the labeled source data and unlabeled
target data. We develop a Contrastive Domain Adaptation framework, called
ConDA, that blends standard domain adaptation techniques with the
representation power of contrastive learning to learn domain invariant
representations that are effective for the final unsupervised detection task.
Our experiments demonstrate the effectiveness of our framework, resulting in
average performance gains of 31.7% from the best performing baselines, and
within 0.8% margin of a fully supervised detector. All our code and data is
available at https://github.com/AmritaBh/ConDA-gen-text-detection.",2023-09-07
"Parameter Efficient Audio Captioning With Faithful Guidance Using
  Audio-text Shared Latent Representation",2023-09-06 19:42:52+00:00,http://arxiv.org/abs/2309.03340v1,"Arvind Krishna Sridhar, Yinyi Guo, Erik Visser, Rehana Mahfuz","cs.CL, cs.MM, cs.SD",table2text,"There has been significant research on developing pretrained transformer
architectures for multimodal-to-text generation tasks. Albeit performance
improvements, such models are frequently overparameterized, hence suffer from
hallucination and large memory footprint making them challenging to deploy on
edge devices. In this paper, we address both these issues for the application
of automated audio captioning. First, we propose a data augmentation technique
for generating hallucinated audio captions and show that similarity based on an
audio-text shared latent space is suitable for detecting hallucination. Then,
we propose a parameter efficient inference time faithful decoding algorithm
that enables smaller audio captioning models with performance equivalent to
larger models trained with more data. During the beam decoding step, the
smaller model utilizes an audio-text shared latent representation to
semantically align the generated text with corresponding input audio. Faithful
guidance is introduced into the beam probability by incorporating the cosine
similarity between latent representation projections of greedy rolled out
intermediate beams and audio clip. We show the efficacy of our algorithm on
benchmark datasets and evaluate the proposed scheme against baselines using
conventional audio captioning and semantic similarity metrics while
illustrating tradeoffs between performance and complexity.",2023-09-06
Persona-aware Generative Model for Code-mixed Language,2023-09-06 11:20:41+00:00,http://arxiv.org/abs/2309.02915v1,"Ayan Sengupta, Md Shad Akhtar, Tanmoy Chakraborty","cs.CL, cs.LG",table2text,"Code-mixing and script-mixing are prevalent across online social networks and
multilingual societies. However, a user's preference toward code-mixing depends
on the socioeconomic status, demographics of the user, and the local context,
which existing generative models mostly ignore while generating code-mixed
texts. In this work, we make a pioneering attempt to develop a persona-aware
generative model to generate texts resembling real-life code-mixed texts of
individuals. We propose a Persona-aware Generative Model for Code-mixed
Generation, PARADOX, a novel Transformer-based encoder-decoder model that
encodes an utterance conditioned on a user's persona and generates code-mixed
texts without monolingual reference data. We propose an alignment module that
re-calibrates the generated sequence to resemble real-life code-mixed texts.
PARADOX generates code-mixed texts that are semantically more meaningful and
linguistically more valid. To evaluate the personification capabilities of
PARADOX, we propose four new metrics -- CM BLEU, CM Rouge-1, CM Rouge-L and CM
KS. On average, PARADOX achieves 1.6 points better CM BLEU, 47% better
perplexity and 32% better semantic coherence than the non-persona-based
counterparts.",2023-09-06
"Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction
  Tuning",2023-09-05 21:27:27+00:00,http://arxiv.org/abs/2309.02591v1,"Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, Armen Aghajanyan","cs.LG, cs.CL, cs.CV",table2text,"We present CM3Leon (pronounced ""Chameleon""), a retrieval-augmented,
token-based, decoder-only multi-modal language model capable of generating and
infilling both text and images. CM3Leon uses the CM3 multi-modal architecture
but additionally shows the extreme benefits of scaling up and tuning on more
diverse instruction-style data. It is the first multi-modal model trained with
a recipe adapted from text-only language models, including a large-scale
retrieval-augmented pre-training stage and a second multi-task supervised
fine-tuning (SFT) stage. It is also a general-purpose model that can do both
text-to-image and image-to-text generation, allowing us to introduce
self-contained contrastive decoding methods that produce high-quality outputs.
Extensive experiments demonstrate that this recipe is highly effective for
multi-modal models. CM3Leon achieves state-of-the-art performance in
text-to-image generation with 5x less training compute than comparable methods
(zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon can also demonstrate
unprecedented levels of controllability in tasks ranging from language-guided
image editing to image-controlled generation and segmentation.",2023-09-05
PromptTTS 2: Describing and Generating Voices with Text Prompt,2023-09-05 14:45:27+00:00,http://arxiv.org/abs/2309.02285v1,"Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao, Tao Qin, Jiang Bian","eess.AS, cs.CL, cs.LG, cs.SD",table2text,"Speech conveys more information than just text, as the same word can be
uttered in various voices to convey diverse information. Compared to
traditional text-to-speech (TTS) methods relying on speech prompts (reference
speech) for voice variability, using text prompts (descriptions) is more
user-friendly since speech prompts can be hard to find or may not exist at all.
TTS approaches based on the text prompt face two challenges: 1) the one-to-many
problem, where not all details about voice variability can be described in the
text prompt, and 2) the limited availability of text prompt datasets, where
vendors and large cost of data labeling are required to write text prompt for
speech. In this work, we introduce PromptTTS 2 to address these challenges with
a variation network to provide variability information of voice not captured by
text prompts, and a prompt generation pipeline to utilize the large language
models (LLM) to compose high quality text prompts. Specifically, the variation
network predicts the representation extracted from the reference speech (which
contains full information about voice) based on the text prompt representation.
For the prompt generation pipeline, it generates text prompts for speech with a
speech understanding model to recognize voice attributes (e.g., gender, speed)
from speech and a large language model to formulate text prompt based on the
recognition results. Experiments on a large-scale (44K hours) speech dataset
demonstrate that compared to the previous works, PromptTTS 2 generates voices
more consistent with text prompts and supports the sampling of diverse voice
variability, thereby offering users more choices on voice generation.
Additionally, the prompt generation pipeline produces high-quality prompts,
eliminating the large labeling cost. The demo page of PromptTTS 2 is available
online\footnote{https://speechresearch.github.io/prompttts2}.",2023-09-05
"Studying the impacts of pre-training using ChatGPT-generated text on
  downstream tasks",2023-09-02 12:56:15+00:00,http://arxiv.org/abs/2309.05668v1,Sarthak Anand,"cs.CL, cs.AI",table2text,"In recent times, significant advancements have been witnessed in the field of
language models, particularly with the emergence of Large Language Models
(LLMs) that are trained on vast amounts of data extracted from internet
archives. These LLMs, such as ChatGPT, have become widely accessible, allowing
users to generate text for various purposes including articles, essays, jokes,
and poetry. Given that LLMs are trained on a diverse range of text sources,
encompassing platforms like Reddit and Twitter, it is foreseeable that future
training datasets will also incorporate text generated by previous iterations
of the models themselves. In light of this development, our research aims to
investigate the influence of artificial text in the pre-training phase of
language models. Specifically, we conducted a comparative analysis between a
language model, RoBERTa, pre-trained using CNN/DailyMail news articles, and
ChatGPT, which employed the same articles for its training and evaluated their
performance on three downstream tasks as well as their potential gender bias,
using sentiment analysis as a metric. Through a series of experiments, we
demonstrate that the utilization of artificial text during pre-training does
not have a significant impact on either the performance of the models in
downstream tasks or their gender bias. In conclusion, our findings suggest that
the inclusion of text generated by LLMs in their own pre-training process does
not yield substantial effects on the subsequent performance of the models in
downstream tasks or their potential gender bias.",2023-09-02
"BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment
  of Continuation Writing",2023-09-02 11:46:05+00:00,http://arxiv.org/abs/2309.00916v1,"Chen Wang, Minpeng Liao, Zhongqiang Huang, Jinliang Lu, Junhong Wu, Yuchen Liu, Chengqing Zong, Jiajun Zhang","cs.CL, cs.SD, eess.AS",table2text,"The emergence of large language models (LLMs) has sparked significant
interest in extending their remarkable language capabilities to speech.
However, modality alignment between speech and text still remains an open
problem. Current solutions can be categorized into two strategies. One is a
cascaded approach where outputs (tokens or states) of a separately trained
speech recognition system are used as inputs for LLMs, which limits their
potential in modeling alignment between speech and text. The other is an
end-to-end approach that relies on speech instruction data, which is very
difficult to collect in large quantities. In this paper, we address these
issues and propose the BLSP approach that Bootstraps Language-Speech
Pre-training via behavior alignment of continuation writing. We achieve this by
learning a lightweight modality adapter between a frozen speech encoder and an
LLM, ensuring that the LLM exhibits the same generation behavior regardless of
the modality of input: a speech segment or its transcript. The training process
can be divided into two steps. The first step prompts an LLM to generate texts
with speech transcripts as prefixes, obtaining text continuations. In the
second step, these continuations are used as supervised signals to train the
modality adapter in an end-to-end manner. We demonstrate that this
straightforward process can extend the capabilities of LLMs to speech, enabling
speech recognition, speech translation, spoken language understanding, and
speech conversation, even in zero-shot cross-lingual scenarios.",2023-09-02
Bias and Fairness in Large Language Models: A Survey,2023-09-02 00:32:55+00:00,http://arxiv.org/abs/2309.00770v1,"Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed","cs.CL, cs.AI, cs.CY, cs.LG",table2text,"Rapid advancements of large language models (LLMs) have enabled the
processing, understanding, and generation of human-like text, with increasing
integration into systems that touch our social sphere. Despite this success,
these models can learn, perpetuate, and amplify harmful social biases. In this
paper, we present a comprehensive survey of bias evaluation and mitigation
techniques for LLMs. We first consolidate, formalize, and expand notions of
social bias and fairness in natural language processing, defining distinct
facets of harm and introducing several desiderata to operationalize fairness
for LLMs. We then unify the literature by proposing three intuitive taxonomies,
two for bias evaluation, namely metrics and datasets, and one for mitigation.
Our first taxonomy of metrics for bias evaluation disambiguates the
relationship between metrics and evaluation datasets, and organizes metrics by
the different levels at which they operate in a model: embeddings,
probabilities, and generated text. Our second taxonomy of datasets for bias
evaluation categorizes datasets by their structure as counterfactual inputs or
prompts, and identifies the targeted harms and social groups; we also release a
consolidation of publicly-available datasets for improved access. Our third
taxonomy of techniques for bias mitigation classifies methods by their
intervention during pre-processing, in-training, intra-processing, and
post-processing, with granular subcategories that elucidate research trends.
Finally, we identify open problems and challenges for future work. Synthesizing
a wide range of recent research, we aim to provide a clear guide of the
existing literature that empowers researchers and practitioners to better
understand and prevent the propagation of bias in LLMs.",2023-09-02
Reinforcement Learning for Generative AI: A Survey,2023-08-28 06:15:14+00:00,http://arxiv.org/abs/2308.14328v2,"Yuanjiang Cao, Quan Z. Sheng, Julian McAuley, Lina Yao","cs.LG, cs.AI",table2text,"Deep Generative AI has been a long-standing essential topic in the machine
learning community, which can impact a number of application areas like text
generation and computer vision. The major paradigm to train a generative model
is maximum likelihood estimation, which pushes the learner to capture and
approximate the target data distribution by decreasing the divergence between
the model distribution and the target distribution. This formulation
successfully establishes the objective of generative tasks, while it is
incapable of satisfying all the requirements that a user might expect from a
generative model. Reinforcement learning, serving as a competitive option to
inject new training signals by creating new objectives that exploit novel
signals, has demonstrated its power and flexibility to incorporate human
inductive bias from multiple angles, such as adversarial learning,
hand-designed rules and learned reward model to build a performant model.
Thereby, reinforcement learning has become a trending research field and has
stretched the limits of generative AI in both model design and application. It
is reasonable to summarize and conclude advances in recent years with a
comprehensive review. Although there are surveys in different application areas
recently, this survey aims to shed light on a high-level review that spans a
range of application areas. We provide a rigorous taxonomy in this area and
make sufficient coverage on various models and applications. Notably, we also
surveyed the fast-developing large language model area. We conclude this survey
by showing the potential directions that might tackle the limit of current
models and expand the frontiers for generative AI.",2023-08-28
"MedAlign: A Clinician-Generated Dataset for Instruction Following with
  Electronic Medical Records",2023-08-27 12:24:39+00:00,http://arxiv.org/abs/2308.14089v1,"Scott L. Fleming, Alejandro Lozano, William J. Haberkorn, Jenelle A. Jindal, Eduardo P. Reis, Rahul Thapa, Louis Blankemeier, Julian Z. Genkins, Ethan Steinberg, Ashwin Nayak, Birju S. Patel, Chia-Chun Chiang, Alison Callahan, Zepeng Huo, Sergios Gatidis, Scott J. Adams, Oluseyi Fayanju, Shreya J. Shah, Thomas Savage, Ethan Goh, Akshay S. Chaudhari, Nima Aghaeepour, Christopher Sharp, Michael A. Pfeffer, Percy Liang, Jonathan H. Chen, Keith E. Morse, Emma P. Brunskill, Jason A. Fries, Nigam H. Shah","cs.CL, cs.AI, cs.LG",table2text,"The ability of large language models (LLMs) to follow natural language
instructions with human-level fluency suggests many opportunities in healthcare
to reduce administrative burden and improve quality of care. However,
evaluating LLMs on realistic text generation tasks for healthcare remains
challenging. Existing question answering datasets for electronic health record
(EHR) data fail to capture the complexity of information needs and
documentation burdens experienced by clinicians. To address these challenges,
we introduce MedAlign, a benchmark dataset of 983 natural language instructions
for EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes
clinician-written reference responses for 303 instructions, and provides 276
longitudinal EHRs for grounding instruction-response pairs. We used MedAlign to
evaluate 6 general domain LLMs, having clinicians rank the accuracy and quality
of each LLM response. We found high error rates, ranging from 35% (GPT-4) to
68% (MPT-7B-Instruct), and an 8.3% drop in accuracy moving from 32k to 2k
context lengths for GPT-4. Finally, we report correlations between clinician
rankings and automated natural language generation metrics as a way to rank
LLMs without human review. We make MedAlign available under a research data use
agreement to enable LLM evaluations on tasks aligned with clinician needs and
preferences.",2023-08-27
"Planning with Logical Graph-based Language Model for Instruction
  Generation",2023-08-26 06:28:14+00:00,http://arxiv.org/abs/2308.13782v1,"Fan Zhang, Kebing Jin, Hankz Hankui Zhuo","cs.CL, cs.AI",table2text,"Despite the superior performance of large language models to generate natural
language texts, it is hard to generate texts with correct logic according to a
given task, due to the difficulties for neural models to capture implied rules
from free-form texts. In this paper, we propose a novel graph-based language
model, Logical-GLM, to infuse logic into language models for more valid text
generation and interpretability. Specifically, we first capture information
from natural language instructions and construct logical bayes graphs that
generally describe domains. Next, we generate logical skeletons to guide
language model training, infusing domain knowledge into language models.
Finally, we alternately optimize the searching policy of graphs and language
models until convergence. The experimental results show that Logical-GLM is
both effective and efficient compared with traditional language models, despite
using smaller-scale training data and fewer parameters. Our approach can
generate instructional texts with more correct logic owing to the internalized
domain knowledge. Moreover, the usage of logical graphs reflects the inner
mechanism of the language models, which improves the interpretability of
black-box models.",2023-08-26
1.5 million materials narratives generated by chatbots,2023-08-25 22:00:53+00:00,http://arxiv.org/abs/2308.13687v1,"Yang Jeong Park, Sung Eun Jerng, Jin-Sung Park, Choah Kwon, Chia-Wei Hsu, Zhichu Ren, Sungroh Yoon, Ju Li","cond-mat.mtrl-sci, cs.CL",table2text,"The advent of artificial intelligence (AI) has enabled a comprehensive
exploration of materials for various applications. However, AI models often
prioritize frequently encountered materials in the scientific literature,
limiting the selection of suitable candidates based on inherent physical and
chemical properties. To address this imbalance, we have generated a dataset of
1,494,017 natural language-material paragraphs based on combined OQMD,
Materials Project, JARVIS, COD and AFLOW2 databases, which are dominated by ab
initio calculations and tend to be much more evenly distributed on the periodic
table. The generated text narratives were then polled and scored by both human
experts and ChatGPT-4, based on three rubrics: technical accuracy, language and
structure, and relevance and depth of content, showing similar scores but with
human-scored depth of content being the most lagging. The merger of
multi-modality data sources and large language model (LLM) holds immense
potential for AI frameworks to help the exploration and discovery of
solid-state materials for specific applications.",2023-08-25
"ChatGPT as Data Augmentation for Compositional Generalization: A Case
  Study in Open Intent Detection",2023-08-25 17:51:23+00:00,http://arxiv.org/abs/2308.13517v1,"Yihao Fang, Xianzhi Li, Stephen W. Thomas, Xiaodan Zhu","cs.CL, cs.AI",table2text,"Open intent detection, a crucial aspect of natural language understanding,
involves the identification of previously unseen intents in user-generated
text. Despite the progress made in this field, challenges persist in handling
new combinations of language components, which is essential for compositional
generalization. In this paper, we present a case study exploring the use of
ChatGPT as a data augmentation technique to enhance compositional
generalization in open intent detection tasks. We begin by discussing the
limitations of existing benchmarks in evaluating this problem, highlighting the
need for constructing datasets for addressing compositional generalization in
open intent detection tasks. By incorporating synthetic data generated by
ChatGPT into the training process, we demonstrate that our approach can
effectively improve model performance. Rigorous evaluation of multiple
benchmarks reveals that our method outperforms existing techniques and
significantly enhances open intent detection capabilities. Our findings
underscore the potential of large language models like ChatGPT for data
augmentation in natural language understanding tasks.",2023-08-25
"GeoExplainer: A Visual Analytics Framework for Spatial Modeling
  Contextualization and Report Generation",2023-08-25 16:55:33+00:00,http://arxiv.org/abs/2308.13588v1,"Fan Lei, Yuxin Ma, Stewart Fotheringham, Elizabeth Mack, Ziqi Li, Mehak Sachdeva, Sarah Bardin, Ross Maciejewski","cs.HC, cs.LG",table2text,"Geographic regression models of various descriptions are often applied to
identify patterns and anomalies in the determinants of spatially distributed
observations. These types of analyses focus on answering why questions about
underlying spatial phenomena, e.g., why is crime higher in this locale, why do
children in one school district outperform those in another, etc.? Answers to
these questions require explanations of the model structure, the choice of
parameters, and contextualization of the findings with respect to their
geographic context. This is particularly true for local forms of regression
models which are focused on the role of locational context in determining human
behavior. In this paper, we present GeoExplainer, a visual analytics framework
designed to support analysts in creating explanative documentation that
summarizes and contextualizes their spatial analyses. As analysts create their
spatial models, our framework flags potential issues with model parameter
selections, utilizes template-based text generation to summarize model outputs,
and links with external knowledge repositories to provide annotations that help
to explain the model results. As analysts explore the model results, all
visualizations and annotations can be captured in an interactive report
generation widget. We demonstrate our framework using a case study modeling the
determinants of voting in the 2016 US Presidential Election.",2023-08-25
Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection,2023-08-22 01:55:03+00:00,http://arxiv.org/abs/2308.11119v1,Masato Tamura,"cs.CV, cs.LG",table2text,"This paper presents a novel method that leverages a visual-language model,
CLIP, as a data source for zero-shot anomaly detection. Tremendous efforts have
been put towards developing anomaly detectors due to their potential industrial
applications. Considering the difficulty in acquiring various anomalous samples
for training, most existing methods train models with only normal samples and
measure discrepancies from the distribution of normal samples during inference,
which requires training a model for each object category. The problem of this
inefficient training requirement has been tackled by designing a CLIP-based
anomaly detector that applies prompt-guided classification to each part of an
image in a sliding window manner. However, the method still suffers from the
labor of careful prompt ensembling with known object categories. To overcome
the issues above, we propose leveraging CLIP as a data source for training. Our
method generates text embeddings with the text encoder in CLIP with typical
prompts that include words of normal and anomaly. In addition to these words,
we insert several randomly generated words into prompts, which enables the
encoder to generate a diverse set of normal and anomalous samples. Using the
generated embeddings as training data, a feed-forward neural network learns to
extract features of normal and anomaly from CLIP's embeddings, and as a result,
a category-agnostic anomaly detector can be obtained without any training
images. Experimental results demonstrate that our method achieves
state-of-the-art performance without laborious prompt ensembling in zero-shot
setups.",2023-08-22
"Data-to-text Generation for Severely Under-Resourced Languages with
  GPT-3.5: A Bit of Help Needed from Google Translate",2023-08-19 09:19:34+00:00,http://arxiv.org/abs/2308.09957v1,"Michela Lorandi, Anya Belz","cs.CL, cs.AI",table2text,"LLMs like GPT are great at tasks involving English which dominates in their
training data. In this paper, we look at how they cope with tasks involving
languages that are severely under-represented in their training data, in the
context of data-to-text generation for Irish, Maltese, Welsh and Breton. During
the prompt-engineering phase we tested a range of prompt types and formats on
GPT-3.5 and~4 with a small sample of example input/output pairs. We then fully
evaluated the two most promising prompts in two scenarios: (i) direct
generation into the under-resourced language, and (ii) generation into English
followed by translation into the under-resourced language. We find that
few-shot prompting works better for direct generation into under-resourced
languages, but that the difference disappears when pivoting via English. The
few-shot + translation system variants were submitted to the WebNLG 2023 shared
task where they outperformed competitor systems by substantial margins in all
languages on all metrics. We conclude that good performance on under-resourced
languages can be achieved out-of-the box with state-of-the-art LLMs. However,
our best results (for Welsh) remain well below the lowest ranked English system
at WebNLG'20.",2023-08-19
Mirror Diffusion Models,2023-08-11 18:31:54+00:00,http://arxiv.org/abs/2308.06342v2,Jaesung Tae,cs.LG,table2text,"Diffusion models have successfully been applied to generative tasks in
various continuous domains. However, applying diffusion to discrete categorical
data remains a non-trivial task. Moreover, generation in continuous domains
often requires clipping in practice, which motivates the need for a theoretical
framework for adapting diffusion to constrained domains. Inspired by the mirror
Langevin algorithm for the constrained sampling problem, in this theoretical
report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the
context of simplex diffusion and propose natural extensions to popular domains
such as image and text generation.",2023-08-11
"Few-Shot Data-to-Text Generation via Unified Representation and
  Multi-Source Learning",2023-08-10 03:09:12+00:00,http://arxiv.org/abs/2308.05317v1,"Alexander Hanbo Li, Mingyue Shang, Evangelia Spiliopoulou, Jie Ma, Patrick Ng, Zhiguo Wang, Bonan Min, William Wang, Kathleen McKeown, Vittorio Castelli, Dan Roth, Bing Xiang",cs.CL,table2text,"We present a novel approach for structured data-to-text generation that
addresses the limitations of existing methods that primarily focus on specific
types of structured data. Our proposed method aims to improve performance in
multi-task training, zero-shot and few-shot scenarios by providing a unified
representation that can handle various forms of structured data such as tables,
knowledge graph triples, and meaning representations. We demonstrate that our
proposed approach can effectively adapt to new structured forms, and can
improve performance in comparison to current methods. For example, our method
resulted in a 66% improvement in zero-shot BLEU scores when transferring models
trained on table inputs to a knowledge graph dataset. Our proposed method is an
important step towards a more general data-to-text generation framework.",2023-08-10
"Emotion-Conditioned Text Generation through Automatic Prompt
  Optimization",2023-08-09 10:42:38+00:00,http://arxiv.org/abs/2308.04857v1,"Yarik Menchaca Resendiz, Roman Klinger",cs.CL,table2text,"Conditional natural language generation methods often require either
expensive fine-tuning or training a large language model from scratch. Both are
unlikely to lead to good results without a substantial amount of data and
computational resources. Prompt learning without changing the parameters of a
large language model presents a promising alternative. It is a cost-effective
approach, while still achieving competitive results. While this procedure is
now established for zero- and few-shot text classification and structured
prediction, it has received limited attention in conditional text generation.
We present the first automatic prompt optimization approach for
emotion-conditioned text generation with instruction-fine-tuned models. Our
method uses an iterative optimization procedure that changes the prompt by
adding, removing, or replacing tokens. As objective function, we only require a
text classifier that measures the realization of the conditional variable in
the generated text. We evaluate the method on emotion-conditioned text
generation with a focus on event reports and compare it to manually designed
prompts that also act as the seed for the optimization procedure. The optimized
prompts achieve 0.75 macro-average F1 to fulfill the emotion condition in
contrast to manually designed seed prompts with only 0.22 macro-average F1.",2023-08-09
"DataTales: Investigating the use of Large Language Models for Authoring
  Data-Driven Articles",2023-08-08 06:21:58+00:00,http://arxiv.org/abs/2308.04076v1,"Nicole Sultanum, Arjun Srinivasan","cs.HC, cs.CL",table2text,"Authoring data-driven articles is a complex process requiring authors to not
only analyze data for insights but also craft a cohesive narrative that
effectively communicates the insights. Text generation capabilities of
contemporary large language models (LLMs) present an opportunity to assist the
authoring of data-driven articles and expedite the writing process. In this
work, we investigate the feasibility and perceived value of leveraging LLMs to
support authors of data-driven articles. We designed a prototype system,
DataTales, that leverages a LLM to generate textual narratives accompanying a
given chart. Using DataTales as a design probe, we conducted a qualitative
study with 11 professionals to evaluate the concept, from which we distilled
affordances and opportunities to further integrate LLMs as valuable data-driven
article authoring assistants.",2023-08-08
Generative Forests,2023-08-07 14:58:53+00:00,http://arxiv.org/abs/2308.03648v1,"Richard Nock, Mathieu Guillame-Bert","cs.LG, I.2.6",table2text,"Tabular data represents one of the most prevalent form of data. When it comes
to data generation, many approaches would learn a density for the data
generation process, but would not necessarily end up with a sampler, even less
so being exact with respect to the underlying density. A second issue is on
models: while complex modeling based on neural nets thrives in image or text
generation (etc.), less is known for powerful generative models on tabular
data. A third problem is the visible chasm on tabular data between training
algorithms for supervised learning with remarkable properties (e.g. boosting),
and a comparative lack of guarantees when it comes to data generation. In this
paper, we tackle the three problems, introducing new tree-based generative
models convenient for density modeling and tabular data generation that improve
on modeling capabilities of recent proposals, and a training algorithm which
simplifies the training setting of previous approaches and displays
boosting-compliant convergence. This algorithm has the convenient property to
rely on a supervised training scheme that can be implemented by a few tweaks to
the most popular induction scheme for decision tree induction with two classes.
Experiments are provided on missing data imputation and comparing generated
data to real data, displaying the quality of the results obtained by our
approach, in particular against state of the art.",2023-08-07
"Boosting Chinese ASR Error Correction with Dynamic Error Scaling
  Mechanism",2023-08-07 09:19:59+00:00,http://arxiv.org/abs/2308.03423v1,"Jiaxin Fan, Yong Zhang, Hanzhang Li, Jianzong Wang, Zhitao Li, Sheng Ouyang, Ning Cheng, Jing Xiao","cs.CL, cs.AI",table2text,"Chinese Automatic Speech Recognition (ASR) error correction presents
significant challenges due to the Chinese language's unique features, including
a large character set and borderless, morpheme-based structure. Current
mainstream models often struggle with effectively utilizing word-level features
and phonetic information. This paper introduces a novel approach that
incorporates a dynamic error scaling mechanism to detect and correct
phonetically erroneous text generated by ASR output. This mechanism operates by
dynamically fusing word-level features and phonetic information, thereby
enriching the model with additional semantic data. Furthermore, our method
implements unique error reduction and amplification strategies to address the
issues of matching wrong words caused by incorrect characters. Experimental
results indicate substantial improvements in ASR error correction,
demonstrating the effectiveness of our proposed method and yielding promising
results on established datasets.",2023-08-07
"Towards Multiple References Era -- Addressing Data Leakage and Limited
  Reference Diversity in NLG Evaluation",2023-08-06 14:49:26+00:00,http://arxiv.org/abs/2308.03131v4,"Xianfeng Zeng, Yijin Liu, Fandong Meng, Jie Zhou",cs.CL,table2text,"N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely
utilized across a range of natural language generation (NLG) tasks. However,
recent studies have revealed a weak correlation between these matching-based
metrics and human evaluations, especially when compared with neural-based
metrics like BLEURT. In this paper, we conjecture that the performance
bottleneck in matching-based metrics may be caused by the limited diversity of
references. To address this issue, we propose to utilize \textit{multiple
references} to enhance the consistency between these metrics and human
evaluations. Within the WMT Metrics benchmarks, we observe that the
multi-references F200spBLEU surpasses the conventional single-reference one by
an accuracy improvement of 7.2\%. Remarkably, it also exceeds the neural-based
BERTscore by an accuracy enhancement of 3.9\%. Moreover, we observe that the
data leakage issue in large language models (LLMs) can be mitigated to a large
extent by our multi-reference metric. We release the code and data at
\url{https://github.com/SefaZeng/LLM-Ref}",2023-08-06
Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?,2023-08-02 17:11:37+00:00,http://arxiv.org/abs/2308.01284v1,"Amrita Bhattacharjee, Huan Liu","cs.CL, cs.AI",table2text,"Large language models (LLMs) such as ChatGPT are increasingly being used for
various use cases, including text content generation at scale. Although
detection methods for such AI-generated text exist already, we investigate
ChatGPT's performance as a detector on such AI-generated text, inspired by
works that use ChatGPT as a data labeler or annotator. We evaluate the
zero-shot performance of ChatGPT in the task of human-written vs. AI-generated
text detection, and perform experiments on publicly available datasets. We
empirically investigate if ChatGPT is symmetrically effective in detecting
AI-generated or human-written text. Our findings provide insight on how ChatGPT
and similar LLMs may be leveraged in automated detection pipelines by simply
focusing on solving a specific aspect of the problem and deriving the rest from
that solution. All code and data is available at
\url{https://github.com/AmritaBh/ChatGPT-as-Detector}.",2023-08-02
Feature-aware conditional GAN for category text generation,2023-08-02 04:43:54+00:00,http://arxiv.org/abs/2308.00939v1,"Xinze Li, Kezhi Mao, Fanfan Lin, Zijian Feng","cs.CL, cs.AI",table2text,"Category text generation receives considerable attentions since it is
beneficial for various natural language processing tasks. Recently, the
generative adversarial network (GAN) has attained promising performance in text
generation, attributed to its adversarial training process. However, there are
several issues in text GANs, including discreteness, training instability, mode
collapse, lack of diversity and controllability etc. To address these issues,
this paper proposes a novel GAN framework, the feature-aware conditional GAN
(FA-GAN), for controllable category text generation. In FA-GAN, the generator
has a sequence-to-sequence structure for improving sentence diversity, which
consists of three encoders including a special feature-aware encoder and a
category-aware encoder, and one relational-memory-core-based decoder with the
Gumbel SoftMax activation function. The discriminator has an additional
category classification head. To generate sentences with specified categories,
the multi-class classification loss is supplemented in the adversarial
training. Comprehensive experiments have been conducted, and the results show
that FA-GAN consistently outperforms 10 state-of-the-art text generation
approaches on 6 text classification datasets. The case study demonstrates that
the synthetic sentences generated by FA-GAN can match the required categories
and are aware of the features of conditioned sentences, with good readability,
fluency, and text authenticity.",2023-08-02
"CoSMo: A constructor specification language for Abstract Wikipedia's
  content selection process",2023-08-01 13:57:23+00:00,http://arxiv.org/abs/2308.02539v1,"Kutz Arrieta, Pablo R. Fillottrani, C. Maria Keet","cs.CL, I.2.4; H.2.3",table2text,"Representing snippets of information abstractly is a task that needs to be
performed for various purposes, such as database view specification and the
first stage in the natural language generation pipeline for generative AI from
structured input, i.e., the content selection stage to determine what needs to
be verbalised. For the Abstract Wikipedia project, requirements analysis
revealed that such an abstract representation requires multilingual modelling,
content selection covering declarative content and functions, and both classes
and instances. There is no modelling language that meets either of the three
features, let alone a combination. Following a rigorous language design process
inclusive of broad stakeholder consultation, we created CoSMo, a novel {\sc
Co}ntent {\sc S}election {\sc Mo}deling language that meets these and other
requirements so that it may be useful both in Abstract Wikipedia as well as
other contexts. We describe the design process, rationale and choices, the
specification, and preliminary evaluation of the language.",2023-08-01
Tackling Hallucinations in Neural Chart Summarization,2023-08-01 09:26:40+00:00,http://arxiv.org/abs/2308.00399v1,"Saad Obaid ul Islam, Iza krjanec, Ondej Duek, Vera Demberg","cs.CL, cs.LG",table2text,"Hallucinations in text generation occur when the system produces text that is
not grounded in the input. In this work, we tackle the problem of
hallucinations in neural chart summarization. Our analysis shows that the
target side of chart summarization training datasets often contains additional
information, leading to hallucinations. We propose a natural language inference
(NLI) based method to preprocess the training data and show through human
evaluation that our method significantly reduces hallucinations. We also found
that shortening long-distance dependencies in the input sequence and adding
chart-related information like title and legends improves the overall
performance.",2023-08-01
"Learning Multi-modal Representations by Watching Hundreds of Surgical
  Video Lectures",2023-07-27 22:38:12+00:00,http://arxiv.org/abs/2307.15220v1,"Kun Yuan, Vinkle Srivastav, Tong Yu, Joel Lavanchy, Pietro Mascagni, Nassir Navab, Nicolas Padoy","cs.CV, cs.AI",table2text,"Recent advancements in surgical computer vision applications have been driven
by fully-supervised methods, primarily using only visual data. These methods
rely on manually annotated surgical videos to predict a fixed set of object
categories, limiting their generalizability to unseen surgical procedures and
downstream tasks. In this work, we put forward the idea that the surgical video
lectures available through open surgical e-learning platforms can provide
effective supervisory signals for multi-modal representation learning without
relying on manual annotations. We address the surgery-specific linguistic
challenges present in surgical video lectures by employing multiple
complementary automatic speech recognition systems to generate text
transcriptions. We then present a novel method, SurgVLP - Surgical Vision
Language Pre-training, for multi-modal representation learning. SurgVLP
constructs a new contrastive learning objective to align video clip embeddings
with the corresponding multiple text embeddings by bringing them together
within a joint latent space. To effectively show the representation capability
of the learned joint latent space, we introduce several vision-and-language
tasks for surgery, such as text-based video retrieval, temporal activity
grounding, and video captioning, as benchmarks for evaluation. We further
demonstrate that without using any labeled ground truth, our approach can be
employed for traditional vision-only surgical downstream tasks, such as
surgical tool, phase, and triplet recognition. The code will be made available
at https://github.com/CAMMA-public/SurgVLP",2023-07-27
Evaluating Generative Models for Graph-to-Text Generation,2023-07-27 09:03:05+00:00,http://arxiv.org/abs/2307.14712v1,"Shuzhou Yuan, Michael Frber","cs.CL, cs.AI",table2text,"Large language models (LLMs) have been widely employed for graph-to-text
generation tasks. However, the process of finetuning LLMs requires significant
training resources and annotation work. In this paper, we explore the
capability of generative models to generate descriptive text from graph data in
a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two
graph-to-text datasets and compare their performance with that of finetuned LLM
models such as T5 and BART. Our results demonstrate that generative models are
capable of generating fluent and coherent text, achieving BLEU scores of 10.57
and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error
analysis reveals that generative models still struggle with understanding the
semantic relations between entities, and they also tend to generate text with
hallucinations or irrelevant information. As a part of error analysis, we
utilize BERT to detect machine-generated text and achieve high macro-F1 scores.
We have made the text generated by generative models publicly available.",2023-07-27
Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts,2023-07-21 15:49:59+00:00,http://arxiv.org/abs/2307.11661v1,"Mayug Maniparambil, Chris Vorster, Derek Molloy, Noel Murphy, Kevin McGuinness, Noel E. O'Connor","cs.CV, cs.AI, cs.CL, cs.LG",table2text,"Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have
revolutionized visual representation learning by providing good performance on
downstream datasets. VLMs are 0-shot adapted to a downstream dataset by
designing prompts that are relevant to the dataset. Such prompt engineering
makes use of domain expertise and a validation dataset. Meanwhile, recent
developments in generative pretrained models like GPT-4 mean they can be used
as advanced internet search tools. They can also be manipulated to provide
visual information in any structure. In this work, we show that GPT-4 can be
used to generate text that is visually descriptive and how this can be used to
adapt CLIP to downstream tasks. We show considerable improvements in 0-shot
transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD
(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.
We also design a simple few-shot adapter that learns to choose the best
possible sentences to construct generalizable classifiers that outperform the
recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized
fine-grained datasets. We will release the code, prompts, and auxiliary text
dataset upon acceptance.",2023-07-21
OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?,2023-07-21 14:58:44+00:00,http://arxiv.org/abs/2307.11636v1,"Runjia Li, Shuyang Sun, Mohamed Elhoseiny, Philip Torr","cs.CV, cs.CL",table2text,"This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scale
dataset for humour generation and understanding. Humour is an abstract,
subjective, and context-dependent cognitive construct involving several
cognitive factors, making it a challenging task to generate and interpret.
Hence, humour generation and understanding can serve as a new task for
evaluating the ability of deep-learning methods to process abstract and
subjective information. Due to the scarcity of data, humour-related generation
tasks such as captioning remain under-explored. To address this gap,
OxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores to
train a generalizable humour captioning model. Contrary to existing captioning
datasets, OxfordTVG-HIC features a wide range of emotional and semantic
diversity resulting in out-of-context examples that are particularly conducive
to generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive
content. We also show how OxfordTVG-HIC can be leveraged for evaluating the
humour of a generated text. Through explainability analysis of the trained
models, we identify the visual and linguistic cues influential for evoking
humour prediction (and generation). We observe qualitatively that these cues
are aligned with the benign violation theory of humour in cognitive psychology.",2023-07-21
"Jina Embeddings: A Novel Set of High-Performance Sentence Embedding
  Models",2023-07-20 20:37:24+00:00,http://arxiv.org/abs/2307.11224v1,"Michael Gnther, Louis Milliken, Jonathan Geuter, Georgios Mastrapas, Bo Wang, Han Xiao","cs.CL, cs.AI, cs.IR, cs.LG, 68T50, H.3.1; H.3.3; I.2.7; I.5.4",table2text,"Jina Embeddings constitutes a set of high-performance sentence embedding
models adept at translating various textual inputs into numerical
representations, thereby capturing the semantic essence of the text. While
these models are not exclusively designed for text generation, they excel in
applications such as dense retrieval and semantic textual similarity. This
paper details the development of Jina Embeddings, starting with the creation of
a high-quality pairwise and triplet dataset. It underlines the crucial role of
data cleaning in dataset preparation, gives in-depth insights into the model
training process, and concludes with a comprehensive performance evaluation
using the Massive Textual Embedding Benchmark (MTEB).",2023-07-20
"Visual Flow-based Programming Plugin for Brain Computer Interface in
  Computer-Aided Design",2023-07-20 16:50:39+00:00,http://arxiv.org/abs/2307.11023v1,"Tong Bill Xu, Saleh Kalantari","cs.HC, cs.SE",table2text,"Over the last half century, the main application of Brain Computer
Interfaces, BCIs has been controlling wheelchairs and neural prostheses or
generating text or commands for people with restricted mobility. There has been
very limited attention in the field to applications for computer aided design,
despite the potential of BCIs to provide a new form of environmental
interaction. In this paper we introduce the development and application of
Neuron, a novel BCI tool that enables designers with little experience in
neuroscience or computer programming to gain access to neurological data, along
with established metrics relevant to design, create BCI interaction prototypes,
both with digital onscreen objects and physical devices, and evaluate designs
based on neurological information and record measurements for further analysis.
After discussing the BCI tool development, the article presents its
capabilities through two case studies, along with a brief evaluation of the
tool performance and a discussion of implications, limitations, and future
improvement.",2023-07-20
Generative Language Models on Nucleotide Sequences of Human Genes,2023-07-20 06:59:02+00:00,http://arxiv.org/abs/2307.10634v1,"Musa Nuri Ihtiyar, Arzucan Ozgur","q-bio.GN, cs.CL, cs.LG",table2text,"Language models, primarily transformer-based ones, obtained colossal success
in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3
for NLG are very crucial. DNA sequences are very close to natural language in
terms of structure, so if the DNA-related bioinformatics domain is concerned,
discriminative models, like DNABert, exist. Yet, the generative side of the
coin is mainly unexplored to the best of our knowledge. Consequently, we
focused on developing an autoregressive generative language model like GPT-3
for DNA sequences. Because working with whole DNA sequences is challenging
without substantial computational resources, we decided to carry out our study
on a smaller scale, focusing on nucleotide sequences of human genes, unique
parts in DNA with specific functionalities, instead of the whole DNA. This
decision did not change the problem structure a lot due to the fact that both
DNA and genes can be seen as 1D sequences consisting of four different
nucleotides without losing much information and making too much simplification.
First of all, we systematically examined an almost entirely unexplored problem
and observed that RNNs performed the best while simple techniques like N-grams
were also promising. Another beneficial point was learning how to work with
generative models on languages we do not understand, unlike natural language.
How essential using real-life tasks beyond the classical metrics such as
perplexity is observed. Furthermore, checking whether the data-hungry nature of
these models can be changed through selecting a language with minimal
vocabulary size, four owing to four different types of nucleotides, is
examined. The reason for reviewing this was that choosing such a language might
make the problem easier. However, what we observed in this study was it did not
provide that much of a change in the amount of data needed.",2023-07-20
"FinGPT: Democratizing Internet-scale Data for Financial Large Language
  Models",2023-07-19 22:43:57+00:00,http://arxiv.org/abs/2307.10485v1,"Xiao-Yang Liu, Guoxuan Wang, Daochen Zha","cs.CL, cs.LG, q-fin.GN",table2text,"Large language models (LLMs) have demonstrated remarkable proficiency in
understanding and generating human-like texts, which may potentially
revolutionize the finance industry. However, existing LLMs often fall short in
the financial field, which is mainly attributed to the disparities between
general text data and financial text data. Unfortunately, there is only a
limited number of financial text datasets available (quite small size), and
BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the
training logs were released). In light of this, we aim to democratize
Internet-scale financial data for LLMs, which is an open challenge due to
diverse data sources, low signal-to-noise ratio, and high time-validity. To
address the challenges, we introduce an open-sourced and data-centric
framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that
automates the collection and curation of real-time financial data from >34
diverse sources on the Internet, providing researchers and practitioners with
accessible and transparent resources to develop their FinLLMs. Additionally, we
propose a simple yet effective strategy for fine-tuning FinLLM using the
inherent feedback from the market, dubbed Reinforcement Learning with Stock
Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that
enables users to customize their own FinLLMs from open-source general-purpose
LLMs at a low cost. Finally, we showcase several FinGPT applications, including
robo-advisor, sentiment analysis for algorithmic trading, and low-code
development. FinGPT aims to democratize FinLLMs, stimulate innovation, and
unlock new opportunities in open finance. The codes are available at
https://github.com/AI4Finance-Foundation/FinGPT and
https://github.com/AI4Finance-Foundation/FinNLP",2023-07-19
"Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and
  Addressing Sociological Implications",2023-07-18 11:38:45+00:00,http://arxiv.org/abs/2307.09162v1,Vishesh Thakur,cs.CL,table2text,"Gender bias in artificial intelligence (AI) and natural language processing
has garnered significant attention due to its potential impact on societal
perceptions and biases. This research paper aims to analyze gender bias in
Large Language Models (LLMs) with a focus on multiple comparisons between GPT-2
and GPT-3.5, some prominent language models, to better understand its
implications. Through a comprehensive literature review, the study examines
existing research on gender bias in AI language models and identifies gaps in
the current knowledge. The methodology involves collecting and preprocessing
data from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis
techniques to evaluate gender bias in the generated text. The findings shed
light on gendered word associations, language usage, and biased narratives
present in the outputs of these Large Language Models. The discussion explores
the ethical implications of gender bias and its potential consequences on
social perceptions and marginalized communities. Additionally, the paper
presents strategies for reducing gender bias in LLMs, including algorithmic
approaches and data augmentation techniques. The research highlights the
importance of interdisciplinary collaborations and the role of sociological
studies in mitigating gender bias in AI models. By addressing these issues, we
can pave the way for more inclusive and unbiased AI systems that have a
positive impact on society.",2023-07-18
COLLIE: Systematic Construction of Constrained Text Generation Tasks,2023-07-17 17:48:51+00:00,http://arxiv.org/abs/2307.08689v1,"Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, Karthik Narasimhan","cs.CL, cs.AI, cs.LG",table2text,"Text generation under constraints have seen increasing interests in natural
language processing, especially with the rapidly improving capabilities of
large language models. However, existing benchmarks for constrained generation
usually focus on fixed constraint types (e.g.,generate a sentence containing
certain words) that have proved to be easy for state-of-the-art models like
GPT-4. We present COLLIE, a grammar-based framework that allows the
specification of rich, compositional constraints with diverse generation levels
(word, sentence, paragraph, passage) and modeling challenges (e.g.,language
understanding, logical reasoning, counting, semantic planning). We also develop
tools for automatic extraction of task instances given a constraint structure
and a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 2080
instances comprising 13 constraint structures. We perform systematic
experiments across five state-of-the-art instruction-tuned language models and
analyze their performances to reveal shortcomings. COLLIE is designed to be
extensible and lightweight, and we hope the community finds it useful to
develop more complex constraints and evaluations in the future.",2023-07-17
Fast Quantum Algorithm for Attention Computation,2023-07-16 14:00:42+00:00,http://arxiv.org/abs/2307.08045v1,"Yeqi Gao, Zhao Song, Xin Yang, Ruizhe Zhang","quant-ph, cs.LG",table2text,"Large language models (LLMs) have demonstrated exceptional performance across
a wide range of tasks. These models, powered by advanced deep learning
techniques, have revolutionized the field of natural language processing (NLP)
and have achieved remarkable results in various language-related tasks.
  LLMs have excelled in tasks such as machine translation, sentiment analysis,
question answering, text generation, text classification, language modeling,
and more. They have proven to be highly effective in capturing complex
linguistic patterns, understanding context, and generating coherent and
contextually relevant text. The attention scheme plays a crucial role in the
architecture of large language models (LLMs). It is a fundamental component
that enables the model to capture and utilize contextual information during
language processing tasks effectively. Making the attention scheme computation
faster is one of the central questions to speed up the LLMs computation. It is
well-known that quantum machine has certain computational advantages compared
to the classical machine. However, it is currently unknown whether quantum
computing can aid in LLM.
  In this work, we focus on utilizing Grover's Search algorithm to compute a
sparse attention computation matrix efficiently. We achieve a polynomial
quantum speed-up over the classical method. Moreover, the attention matrix
outputted by our quantum algorithm exhibits an extra low-rank structure that
will be useful in obtaining a faster training algorithm for LLMs. Additionally,
we present a detailed analysis of the algorithm's error analysis and time
complexity within the context of computing the attention matrix.",2023-07-16
"Using Large Language Models for Zero-Shot Natural Language Generation
  from Knowledge Graphs",2023-07-14 12:45:03+00:00,http://arxiv.org/abs/2307.07312v1,"Agnes Axelsson, Gabriel Skantze","cs.CL, 68T50, I.2.7; I.2.4",table2text,"In any system that uses structured knowledge graph (KG) data as its
underlying knowledge representation, KG-to-text generation is a useful tool for
turning parts of the graph data into text that can be understood by humans.
Recent work has shown that models that make use of pretraining on large amounts
of text data can perform well on the KG-to-text task even with relatively small
sets of training data on the specific graph-to-text task. In this paper, we
build on this concept by using large language models to perform zero-shot
generation based on nothing but the model's understanding of the triple
structure from what it can read. We show that ChatGPT achieves near
state-of-the-art performance on some measures of the WebNLG 2020 challenge, but
falls behind on others. Additionally, we compare factual, counter-factual and
fictional statements, and show that there is a significant connection between
what the LLM already knows about the data it is parsing and the quality of the
output text.",2023-07-14
Generating Efficient Training Data via LLM-based Attribute Manipulation,2023-07-14 00:10:03+00:00,http://arxiv.org/abs/2307.07099v1,"Letian Peng, Yuwei Zhang, Jingbo Shang",cs.CL,table2text,"In this paper, we propose a novel method, Chain-of-Thoughts Attribute
Manipulation (CoTAM), to guide few-shot learning by carefully crafted data from
Large Language Models (LLMs). The main idea is to create data with changes only
in the attribute targeted by the task. Inspired by facial attribute
manipulation, our approach generates label-switched data by leveraging LLMs to
manipulate task-specific attributes and reconstruct new sentences in a
controlled manner. Instead of conventional latent representation controlling,
we implement chain-of-thoughts decomposition and reconstruction to adapt the
procedure to LLMs. Extensive results on text classification and other tasks
verify the advantage of CoTAM over other LLM-based text generation methods with
the same number of training examples. Analysis visualizes the attribute
manipulation effectiveness of CoTAM and presents the potential of LLM-guided
learning with even less supervision.",2023-07-14
"DIALGEN: Collaborative Human-LM Generated Dialogues for Improved
  Understanding of Human-Human Conversations",2023-07-13 20:02:50+00:00,http://arxiv.org/abs/2307.07047v1,"Bo-Ru Lu, Nikita Haduong, Chia-Hsuan Lee, Zeqiu Wu, Hao Cheng, Paul Koester, Jean Utke, Tao Yu, Noah A. Smith, Mari Ostendorf",cs.CL,table2text,"Applications that could benefit from automatic understanding of human-human
conversations often come with challenges associated with private information in
real-world data such as call center or clinical conversations. Working with
protected data also increases costs of annotation, which limits technology
development. To address these challenges, we propose DIALGEN, a
human-in-the-loop semi-automated dialogue generation framework. DIALGEN uses a
language model (ChatGPT) that can follow schema and style specifications to
produce fluent conversational text, generating a complex conversation through
iteratively generating subdialogues and using human feedback to correct
inconsistencies or redirect the flow. In experiments on structured
summarization of agent-client information gathering calls, framed as dialogue
state tracking, we show that DIALGEN data enables significant improvement in
model performance.",2023-07-13
Reading Radiology Imaging Like The Radiologist,2023-07-12 05:36:47+00:00,http://arxiv.org/abs/2307.05921v3,Yuhao Wang,"cs.CV, cs.AI",table2text,"Automated radiology report generation aims to generate radiology reports that
contain rich, fine-grained descriptions of radiology imaging. Compared with
image captioning in the natural image domain, medical images are very similar
to each other, with only minor differences in the occurrence of diseases. Given
the importance of these minor differences in the radiology report, it is
crucial to encourage the model to focus more on the subtle regions of disease
occurrence. Secondly, the problem of visual and textual data biases is serious.
Not only do normal cases make up the majority of the dataset, but sentences
describing areas with pathological changes also constitute only a small part of
the paragraph. Lastly, generating medical image reports involves the challenge
of long text generation, which requires more expertise and empirical training
in medical knowledge. As a result, the difficulty of generating such reports is
increased. To address these challenges, we propose a disease-oriented retrieval
framework that utilizes similar reports as prior knowledge references. We
design a factual consistency captioning generator to generate more accurate and
factually consistent disease descriptions. Our framework can find most similar
reports for a given disease from the CXR database by retrieving a
disease-oriented mask consisting of the position and morphological
characteristics. By referencing the disease-oriented similar report and the
visual features, the factual consistency model can generate a more accurate
radiology report.",2023-07-12
"PatternGPT :A Pattern-Driven Framework for Large Language Model Text
  Generation",2023-07-02 04:32:41+00:00,http://arxiv.org/abs/2307.00470v4,"Le Xiao, Xin Shan","cs.CL, cs.AI",table2text,"Large language models(LLMS)have shown excellent text generation capabilities,
capable of generating fluent human-like responses for many downstream tasks.
However, applying large language models to real-world critical tasks remains
challenging due to their susceptibility to hallucinations and inability to
directly use external knowledge. To cope with the above challenges, this paper
proposes PatternGPT, a pattern-driven text generation framework for Large
Language Models. Firstly, the framework utilizes the extraction capability of
Large Language Models to generate rich and diversified structured and
formalized patterns, which facilitates the introduction of external knowledge
to do the computation, and then draws on the idea of federated learning to use
multiple agents to achieve the sharing in order to obtain more diversified
patterns, and finally uses judgment criteria and optimization algorithm to
search for high-quality patterns to guide the generation of models. Finally,
external knowledge such as judgment criteria and optimization algorithms are
used to search for high-quality patterns, and the searched patterns are used to
guide model generation. This framework has the advantages of generating
diversified patterns, protecting data privacy, combining external knowledge,
and improving the quality of generation, which provides an effective method to
optimize the text generation capability of large language models, and make it
better applied to the field of intelligent dialogue and content generation.",2023-07-02
"Benchmarking Large Language Model Capabilities for Conditional
  Generation",2023-06-29 08:59:40+00:00,http://arxiv.org/abs/2306.16793v1,"Joshua Maynez, Priyanka Agrawal, Sebastian Gehrmann",cs.CL,table2text,"Pre-trained large language models (PLMs) underlie most new developments in
natural language processing. They have shifted the field from
application-specific model pipelines to a single model that is adapted to a
wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside
techniques like few-shot learning, have additionally shifted the output
modality to generation instead of classification or regression. Despite their
ubiquitous use, the generation quality of language models is rarely evaluated
when these models are introduced. Additionally, it is unclear how existing
generation tasks--while they can be used to compare systems at a high
level--relate to the real world use cases for which people have been adopting
them. In this work, we discuss how to adapt existing application-specific
generation benchmarks to PLMs and provide an in-depth, empirical study of the
limitations and capabilities of PLMs in natural language generation tasks along
dimensions such as scale, architecture, input and output language. Our results
show that PLMs differ in their applicability to different data regimes and
their generalization to multiple languages and inform which PLMs to use for a
given generation task setup. We share best practices to be taken into
consideration when benchmarking generation capabilities during the development
of upcoming PLMs.",2023-06-29
Joint Level Generation and Translation Using Gameplay Videos,2023-06-29 03:46:44+00:00,http://arxiv.org/abs/2306.16662v1,"Negar Mirgati, Matthew Guzdial","cs.CV, cs.LG",table2text,"Procedural Content Generation via Machine Learning (PCGML) faces a
significant hurdle that sets it apart from other fields, such as image or text
generation, which is limited annotated data. Many existing methods for
procedural level generation via machine learning require a secondary
representation besides level images. However, the current methods for obtaining
such representations are laborious and time-consuming, which contributes to
this problem. In this work, we aim to address this problem by utilizing
gameplay videos of two human-annotated games to develop a novel multi-tail
framework that learns to perform simultaneous level translation and generation.
The translation tail of our framework can convert gameplay video frames to an
equivalent secondary representation, while its generation tail can produce
novel level segments. Evaluation results and comparisons between our framework
and baselines suggest that combining the level generation and translation tasks
can lead to an overall improved performance regarding both tasks. This
represents a possible solution to limited annotated level data, and we
demonstrate the potential for future versions to generalize to unseen games.",2023-06-29
"You Can Generate It Again: Data-to-text Generation with Verification and
  Correction Prompting",2023-06-28 05:34:25+00:00,http://arxiv.org/abs/2306.15933v1,"Xuan Ren, Lingqiao Liu","cs.CL, cs.AI, cs.LG",table2text,"Despite significant advancements in existing models, generating text
descriptions from structured data input, known as data-to-text generation,
remains a challenging task. In this paper, we propose a novel approach that
goes beyond traditional one-shot generation methods by introducing a multi-step
process consisting of generation, verification, and correction stages. Our
approach, VCP(Verification and Correction Prompting), begins with the model
generating an initial output. We then proceed to verify the correctness of
different aspects of the generated text. The observations from the verification
step are converted into a specialized error-indication prompt, which instructs
the model to regenerate the output while considering the identified errors. To
enhance the model's correction ability, we have developed a carefully designed
training procedure. This procedure enables the model to incorporate feedback
from the error-indication prompt, resulting in improved output generation.
Through experimental results, we demonstrate that our approach effectively
reduces slot error rates while maintaining the overall quality of the generated
text.",2023-06-28
Knowledge Graph-Augmented Korean Generative Commonsense Reasoning,2023-06-26 07:23:47+00:00,http://arxiv.org/abs/2306.14470v1,"Dahyun Jung, Jaehyung Seo, Jaewook Lee, Chanjun Park, Heuiseok Lim","cs.CL, cs.AI",table2text,"Generative commonsense reasoning refers to the task of generating acceptable
and logical assumptions about everyday situations based on commonsense
understanding. By utilizing an existing dataset such as Korean CommonGen,
language generation models can learn commonsense reasoning specific to the
Korean language. However, language models often fail to consider the
relationships between concepts and the deep knowledge inherent to concepts. To
address these limitations, we propose a method to utilize the Korean knowledge
graph data for text generation. Our experimental result shows that the proposed
method can enhance the efficiency of Korean commonsense inference, thereby
underlining the significance of employing supplementary data.",2023-06-26
AudioPaLM: A Large Language Model That Can Speak and Listen,2023-06-22 14:37:54+00:00,http://arxiv.org/abs/2306.12925v1,"Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zaln Borsos, Flix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo Velimirovi, Damien Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka, Christian Frank","cs.CL, cs.AI, cs.SD, eess.AS, stat.ML",table2text,"We introduce AudioPaLM, a large language model for speech understanding and
generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2
[Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified
multimodal architecture that can process and generate text and speech with
applications including speech recognition and speech-to-speech translation.
AudioPaLM inherits the capability to preserve paralinguistic information such
as speaker identity and intonation from AudioLM and the linguistic knowledge
present only in text large language models such as PaLM-2. We demonstrate that
initializing AudioPaLM with the weights of a text-only large language model
improves speech processing, successfully leveraging the larger quantity of text
training data used in pretraining to assist with the speech tasks. The
resulting model significantly outperforms existing systems for speech
translation tasks and has the ability to perform zero-shot speech-to-text
translation for many languages for which input/target language combinations
were not seen in training. AudioPaLM also demonstrates features of audio
language models, such as transferring a voice across languages based on a short
spoken prompt. We release examples of our method at
https://google-research.github.io/seanet/audiopalm/examples",2023-06-22
Open-Domain Text Evaluation via Meta Distribution Modeling,2023-06-20 20:37:54+00:00,http://arxiv.org/abs/2306.11879v1,"Sidi Lu, Asli Celikyilmaz, Tianlu Wang, Nanyun Peng",cs.CL,table2text,"Recent advances in open-domain text generation models powered by large
pre-trained language models (LLMs) have achieved remarkable performance.
However, evaluating and controlling these models for desired attributes remains
a challenge, as traditional reference-based metrics such as BLEU, ROUGE, and
METEOR are insufficient for open-ended generation tasks. Similarly, while
trainable discriminator-based evaluation metrics show promise, obtaining
high-quality training data is a non-trivial task. In this paper, we introduce a
novel approach to evaluate open-domain generation - the Meta-Distribution
Methods (MDM). Drawing on the correlation between the rising parameter counts
and the improving performance of LLMs, MDM creates a mapping from the contrast
of two probabilistic distributions -- one known to be superior to the other --
to quality measures, which can be viewed as a distribution of distributions
i.e. Meta-Distribution. We investigate MDM for open-domain text generation
evaluation under two paradigms: 1) \emph{Generative} MDM, which leverages the
Meta-Distribution Methods to generate in-domain negative samples for training
discriminator-based metrics; 2) \emph{Discriminative} MDM, which directly uses
distribution discrepancies between two language models for evaluation. Our
experiments on multi-turn dialogue and factuality in abstractive summarization
demonstrate that MDMs correlate better with human judgment than existing
automatic evaluation metrics on both tasks, highlighting the strong performance
and generalizability of such methods.",2023-06-20
"ChatGPT is not Enough: Enhancing Large Language Models with Knowledge
  Graphs for Fact-aware Language Modeling",2023-06-20 12:21:06+00:00,http://arxiv.org/abs/2306.11489v1,"Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, Xindong Wu","cs.CL, cs.AI",table2text,"Recently, ChatGPT, a representative large language model (LLM), has gained
considerable attention due to its powerful emergent abilities. Some researchers
suggest that LLMs could potentially replace structured knowledge bases like
knowledge graphs (KGs) and function as parameterized knowledge bases. However,
while LLMs are proficient at learning probabilistic language patterns based on
large corpus and engaging in conversations with humans, they, like previous
smaller pre-trained language models (PLMs), still have difficulty in recalling
facts while generating knowledge-grounded contents. To overcome these
limitations, researchers have proposed enhancing data-driven PLMs with
knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus
improving their performance to generate texts requiring factual knowledge and
providing more informed responses to user queries. This paper reviews the
studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced
pre-trained language models (KGPLMs) as well as their applications. Inspired by
existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by
developing knowledge graph-enhanced large language models (KGLLMs). KGLLM
provides a solution to enhance LLMs' factual reasoning ability, opening up new
avenues for LLM research.",2023-06-20
Explicit Syntactic Guidance for Neural Text Generation,2023-06-20 12:16:31+00:00,http://arxiv.org/abs/2306.11485v2,"Yafu Li, Leyang Cui, Jianhao Yan, Yongjing Yin, Wei Bi, Shuming Shi, Yue Zhang",cs.CL,table2text,"Most existing text generation models follow the sequence-to-sequence
paradigm. Generative Grammar suggests that humans generate natural language
texts by learning language grammar. We propose a syntax-guided generation
schema, which generates the sequence guided by a constituency parse tree in a
top-down direction. The decoding process can be decomposed into two parts: (1)
predicting the infilling texts for each constituent in the lexicalized syntax
context given the source sentence; (2) mapping and expanding each constituent
to construct the next-level syntax context. Accordingly, we propose a
structural beam search method to find possible syntax structures
hierarchically. Experiments on paraphrase generation and machine translation
show that the proposed method outperforms autoregressive baselines, while also
demonstrating effectiveness in terms of interpretability, controllability, and
diversity.",2023-06-20
"Semi-supervised Relation Extraction via Data Augmentation and
  Consistency-training",2023-06-16 19:45:42+00:00,http://arxiv.org/abs/2306.10153v1,Komal K. Teru,"cs.CL, cs.IR",table2text,"Due to the semantic complexity of the Relation extraction (RE) task,
obtaining high-quality human labelled data is an expensive and noisy process.
To improve the sample efficiency of the models, semi-supervised learning (SSL)
methods aim to leverage unlabelled data in addition to learning from limited
labelled data points. Recently, strong data augmentation combined with
consistency-based semi-supervised learning methods have advanced the state of
the art in several SSL tasks. However, adapting these methods to the RE task
has been challenging due to the difficulty of data augmentation for RE. In this
work, we leverage the recent advances in controlled text generation to perform
high quality data augmentation for the RE task. We further introduce small but
significant changes to model architecture that allows for generation of more
training data by interpolating different data points in their latent space.
These data augmentations along with consistency training result in very
competitive results for semi-supervised relation extraction on four benchmark
datasets.",2023-06-16
"Building blocks for complex tasks: Robust generative event extraction
  for radiology reports under domain shifts",2023-06-15 23:16:58+00:00,http://arxiv.org/abs/2306.09544v1,"Sitong Zhou, Meliha Yetisgen, Mari Ostendorf",cs.CL,table2text,"This paper explores methods for extracting information from radiology reports
that generalize across exam modalities to reduce requirements for annotated
data. We demonstrate that multi-pass T5-based text-to-text generative models
exhibit better generalization across exam modalities compared to approaches
that employ BERT-based task-specific classification layers. We then develop
methods that reduce the inference cost of the model, making large-scale corpus
processing more feasible for clinical applications. Specifically, we introduce
a generative technique that decomposes complex tasks into smaller subtask
blocks, which improves a single-pass model when combined with multitask
training. In addition, we leverage target-domain contexts during inference to
enhance domain adaptation, enabling use of smaller models. Analyses offer
insights into the benefits of different cost reduction strategies.",2023-06-15
"Opportunities and Challenges for ChatGPT and Large Language Models in
  Biomedicine and Health",2023-06-15 20:19:08+00:00,http://arxiv.org/abs/2306.10070v1,"Shubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai, Qingqing Zhu, Xiuying Chen, Yifan Yang, Qingyu Chen, Won Kim, Donald C. Comeau, Rezarta Islamaj, Aadit Kapoor, Xin Gao, Zhiyong Lu","cs.CY, cs.AI, cs.CL, q-bio.QM",table2text,"ChatGPT has drawn considerable attention from both the general public and
domain experts with its remarkable text generation capabilities. This has
subsequently led to the emergence of diverse applications in the field of
biomedicine and health. In this work, we examine the diverse applications of
large language models (LLMs), such as ChatGPT, in biomedicine and health.
Specifically we explore the areas of biomedical information retrieval, question
answering, medical text summarization, information extraction, and medical
education, and investigate whether LLMs possess the transformative power to
revolutionize these tasks or whether the distinct complexities of biomedical
domain presents unique challenges. Following an extensive literature survey, we
find that significant advances have been made in the field of text generation
tasks, surpassing the previous state-of-the-art methods. For other
applications, the advances have been modest. Overall, LLMs have not yet
revolutionized the biomedicine, but recent rapid progress indicates that such
methods hold great potential to provide valuable means for accelerating
discovery and improving health. We also find that the use of LLMs, like
ChatGPT, in the fields of biomedicine and health entails various risks and
challenges, including fabricated information in its generated responses, as
well as legal and privacy concerns associated with sensitive patient data. We
believe this first-of-its-kind survey can provide a comprehensive overview to
biomedical researchers and healthcare practitioners on the opportunities and
challenges associated with using ChatGPT and other LLMs for transforming
biomedicine and health.",2023-06-15
DiffuDetox: A Mixed Diffusion Model for Text Detoxification,2023-06-14 13:41:23+00:00,http://arxiv.org/abs/2306.08505v1,"Griffin Floto, Mohammad Mahdi Abdollah Pour, Parsa Farinneya, Zhenwei Tang, Ali Pesaranghader, Manasa Bharadwaj, Scott Sanner","cs.CL, cs.LG",table2text,"Text detoxification is a conditional text generation task aiming to remove
offensive content from toxic text. It is highly useful for online forums and
social media, where offensive content is frequently encountered. Intuitively,
there are diverse ways to detoxify sentences while preserving their meanings,
and we can select from detoxified sentences before displaying text to users.
Conditional diffusion models are particularly suitable for this task given
their demonstrated higher generative diversity than existing conditional text
generation models based on language models. Nonetheless, text fluency declines
when they are trained with insufficient data, which is the case for this task.
In this work, we propose DiffuDetox, a mixed conditional and unconditional
diffusion model for text detoxification. The conditional model takes toxic text
as the condition and reduces its toxicity, yielding a diverse set of detoxified
sentences. The unconditional model is trained to recover the input text, which
allows the introduction of additional fluent text for training and thus ensures
text fluency. Extensive experimental results and in-depth analysis demonstrate
the effectiveness of our proposed DiffuDetox.",2023-06-14
Unifying Large Language Models and Knowledge Graphs: A Roadmap,2023-06-14 07:15:26+00:00,http://arxiv.org/abs/2306.08302v2,"Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu","cs.CL, cs.AI",table2text,"Large language models (LLMs), such as ChatGPT and GPT4, are making new waves
in the field of natural language processing and artificial intelligence, due to
their emergent ability and generalizability. However, LLMs are black-box
models, which often fall short of capturing and accessing factual knowledge. In
contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are
structured knowledge models that explicitly store rich factual knowledge. KGs
can enhance LLMs by providing external knowledge for inference and
interpretability. Meanwhile, KGs are difficult to construct and evolving by
nature, which challenges the existing methods in KGs to generate new facts and
represent unseen knowledge. Therefore, it is complementary to unify LLMs and
KGs together and simultaneously leverage their advantages. In this article, we
present a forward-looking roadmap for the unification of LLMs and KGs. Our
roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,
which incorporate KGs during the pre-training and inference phases of LLMs, or
for the purpose of enhancing understanding of the knowledge learned by LLMs; 2)
LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,
completion, construction, graph-to-text generation, and question answering; and
3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a
mutually beneficial way to enhance both LLMs and KGs for bidirectional
reasoning driven by both data and knowledge. We review and summarize existing
efforts within these three frameworks in our roadmap and pinpoint their future
research directions.",2023-06-14
"Large Language Models Sometimes Generate Purely Negatively-Reinforced
  Text",2023-06-13 06:40:37+00:00,http://arxiv.org/abs/2306.07567v2,Fabien Roger,"cs.LG, cs.CL",table2text,"When using adversarial training, it is common practice to train against the
most egregious failures. However, this might imply using examples with
sensitive information (such as leaked passwords or security vulnerabilities) as
training data. One might assume that language models trained with gradient
descent never generate text snippets which were only present in examples
associated with the lowest possible reward. In this paper, we show that this
assumption is wrong: in some situations, large language models do learn from
such negatively-reinforced examples. We present a specific training setup that
enables Pythia-160M to guess passwords 13% more often than it would by guessing
randomly, despite only showing it these passwords on examples where the model
is incentivized to not output these passwords. Our code is available at
www.github.com/FabienRoger/Learning-From-Negative-Examples",2023-06-13
"SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling
  with Backtracking",2023-06-08 17:59:58+00:00,http://arxiv.org/abs/2306.05426v1,"Chris Cundy, Stefano Ermon","cs.LG, cs.AI",table2text,"In many domains, autoregressive models can achieve low log-likelihood on the
task of predicting the next observation. However, this maximum-likelihood (MLE)
objective does not necessarily match a downstream use-case of autoregressively
generating high-quality sequences. The MLE objective weights sequences
proportionally to their frequency under the data distribution, with no guidance
for the model's behaviour out of distribution (OOD): leading to compounding
error during autoregressive generation. In order to address this compounding
error problem, we formulate sequence generation as an imitation learning (IL)
problem. This allows us to minimize a variety of divergences between the
distribution of sequences generated by an autoregressive model and sequences
from a dataset, including divergences with weight on OOD generated sequences.
The IL framework also allows us to incorporate backtracking by introducing a
backspace action into the generation process. This further mitigates the
compounding error problem by allowing the model to revert a sampled token if it
takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented
without adversarial training or major architectural changes. We identify the
SequenceMatch-$\chi^2$ divergence as a more suitable training objective for
autoregressive models which are used for generation. We show that empirically,
SequenceMatch training leads to improvements over MLE on text generation with
language models.",2023-06-08
"Increasing Diversity While Maintaining Accuracy: Text Data Generation
  with Large Language Models and Human Interventions",2023-06-07 04:27:09+00:00,http://arxiv.org/abs/2306.04140v1,"John Joon Young Chung, Ece Kamar, Saleema Amershi",cs.CL,table2text,"Large language models (LLMs) can be used to generate text data for training
and evaluating other models. However, creating high-quality datasets with LLMs
can be challenging. In this work, we explore human-AI partnerships to
facilitate high diversity and accuracy in LLM-based text data generation. We
first examine two approaches to diversify text generation: 1) logit
suppression, which minimizes the generation of languages that have already been
frequently generated, and 2) temperature sampling, which flattens the token
sampling probability. We found that diversification approaches can increase
data diversity but often at the cost of data accuracy (i.e., text and labels
being appropriate for the target domain). To address this issue, we examined
two human interventions, 1) label replacement (LR), correcting misaligned
labels, and 2) out-of-scope filtering (OOSF), removing instances that are out
of the user's domain of interest or to which no considered label applies. With
oracle studies, we found that LR increases the absolute accuracy of models
trained with diversified datasets by 14.4%. Moreover, we found that some models
trained with data generated with LR interventions outperformed LLM-based
few-shot classification. In contrast, OOSF was not effective in increasing
model accuracy, implying the need for future work in human-in-the-loop text
data generation.",2023-06-07
Structured Voronoi Sampling,2023-06-05 17:32:35+00:00,http://arxiv.org/abs/2306.03061v1,"Afra Amini, Li Du, Ryan Cotterell","cs.CL, cs.AI",table2text,"Recently, there has been a growing interest in the development of
gradient-based sampling algorithms for text generation, especially in the
context of controlled generation. However, there exists a lack of theoretically
grounded and principled approaches for this task. In this paper, we take an
important step toward building a principled approach for sampling from language
models with gradient-based methods. We use discrete distributions given by
language models to define densities and develop an algorithm based on
Hamiltonian Monte Carlo to sample from them. We name our gradient-based
technique Structured Voronoi Sampling (SVS). In an experimental setup where the
reference distribution is known, we show that the empirical distribution of SVS
samples is closer to the reference distribution compared to alternative
sampling schemes. Furthermore, in a controlled generation task, SVS is able to
generate fluent and diverse samples while following the control targets
significantly better than other methods.",2023-06-05
"Adaptive and Personalized Exercise Generation for Online Language
  Learning",2023-06-04 20:18:40+00:00,http://arxiv.org/abs/2306.02457v1,"Peng Cui, Mrinmaya Sachan","cs.CL, cs.AI",table2text,"Adaptive learning aims to provide customized educational activities (e.g.,
exercises) to address individual learning needs. However, manual construction
and delivery of such activities is a laborious process. Thus, in this paper, we
study a novel task of adaptive and personalized exercise generation for online
language learning. To this end, we combine a knowledge tracing model that
estimates each student's evolving knowledge states from their learning history
and a controlled text generation model that generates exercise sentences based
on the student's current estimated knowledge state and instructor requirements
of desired properties (e.g., domain knowledge and difficulty). We train and
evaluate our model on real-world learner interaction data from Duolingo and
demonstrate that LMs guided by student states can generate superior exercises.
Then, we discuss the potential use of our model in educational applications
using various simulations. These simulations show that our model can adapt to
students' individual abilities and can facilitate their learning efficiency by
personalizing learning sequences.",2023-06-04
Exposing Bias in Online Communities through Large-Scale Language Models,2023-06-04 08:09:26+00:00,http://arxiv.org/abs/2306.02294v1,"Celine Wald, Lukas Pfahler","cs.CL, cs.CY, cs.LG",table2text,"Progress in natural language generation research has been shaped by the
ever-growing size of language models. While large language models pre-trained
on web data can generate human-sounding text, they also reproduce social biases
and contribute to the propagation of harmful stereotypes. This work utilises
the flaw of bias in language models to explore the biases of six different
online communities. In order to get an insight into the communities'
viewpoints, we fine-tune GPT-Neo 1.3B with six social media datasets. The bias
of the resulting models is evaluated by prompting the models with different
demographics and comparing the sentiment and toxicity values of these
generations. Together, these methods reveal that bias differs in type and
intensity for the various models. This work not only affirms how easily bias is
absorbed from training data but also presents a scalable method to identify and
compare the bias of different datasets or communities. Additionally, the
examples generated for this work demonstrate the limitations of using automated
sentiment and toxicity classifiers in bias research.",2023-06-04
"Exploring semantic information in disease: Simple Data Augmentation
  Techniques for Chinese Disease Normalization",2023-06-02 22:12:05+00:00,http://arxiv.org/abs/2306.01931v1,"Wenqian Cui, Shaohui Liu, Xiangling Fu, Xien Liu, Ji Wu","cs.CL, cs.AI",table2text,"The disease is a core concept in the medical field, and the task of
normalizing disease names is the basis of all disease-related tasks. However,
due to the multi-axis and multi-grain nature of disease names, incorrect
information is often injected and harms the performance when using general text
data augmentation techniques. To address the above problem, we propose a set of
data augmentation techniques that work together as an augmented training task
for disease normalization. Our data augmentation methods are based on both the
clinical disease corpus and standard disease corpus derived from ICD-10 coding.
Extensive experiments are conducted to show the effectiveness of our proposed
methods. The results demonstrate that our methods can have up to 3\%
performance gain compared to non-augmented counterparts, and they can work even
better on smaller datasets.",2023-06-02
"Fine-Grained Human Feedback Gives Better Rewards for Language Model
  Training",2023-06-02 17:11:37+00:00,http://arxiv.org/abs/2306.01693v1,"Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, Hannaneh Hajishirzi",cs.CL,table2text,"Language models (LMs) often exhibit undesirable text generation behaviors,
including generating false, toxic, or irrelevant outputs. Reinforcement
learning from human feedback (RLHF) - where human preference judgments on LM
outputs are transformed into a learning signal - has recently shown promise in
addressing these issues. However, such holistic feedback conveys limited
information on long text outputs; it does not indicate which aspects of the
outputs influenced user preference; e.g., which parts contain what type(s) of
errors. In this paper, we use fine-grained human feedback (e.g., which sentence
is false, which sub-sentence is irrelevant) as an explicit training signal. We
introduce Fine-Grained RLHF, a framework that enables training and learning
from reward functions that are fine-grained in two respects: (1) density,
providing a reward after every segment (e.g., a sentence) is generated; and (2)
incorporating multiple reward models associated with different feedback types
(e.g., factual incorrectness, irrelevance, and information incompleteness). We
conduct experiments on detoxification and long-form question answering to
illustrate how learning with such reward functions leads to improved
performance, supported by both automatic and human evaluation. Additionally, we
show that LM behaviors can be customized using different combinations of
fine-grained reward models. We release all data, collected human feedback, and
codes at https://FineGrainedRLHF.github.io.",2023-06-02
Preference-grounded Token-level Guidance for Language Model Fine-tuning,2023-06-01 07:00:07+00:00,http://arxiv.org/abs/2306.00398v1,"Shentao Yang, Shujian Zhang, Congying Xia, Yihao Feng, Caiming Xiong, Mingyuan Zhou",cs.CL,table2text,"Aligning language models (LMs) with preferences is an important problem in
natural language generation. A key challenge is that preferences are typically
provided at the sequence level while LM training and generation both occur at
the token level. There is, therefore, a granularity mismatch between the
preference and the LM training losses, which may complicate the learning
problem. In this paper, we address this issue by developing an alternate
training process, where we iterate between grounding the sequence-level
preference into token-level training guidance, and improving the LM with the
learned guidance. For guidance learning, we design a framework that extends the
pairwise-preference learning in imitation learning to both variable-length LM
generation and utilizing the preference among multiple generations. For LM
training, based on the amount of supervised data, we present two minimalist
learning objectives that utilize the learned guidance. In experiments, our
method performs competitively on two distinct representative LM tasks --
discrete-prompt generation and text summarization.",2023-06-01
Learning to Imagine: Visually-Augmented Natural Language Generation,2023-05-26 13:59:45+00:00,http://arxiv.org/abs/2305.16944v2,"Tianyi Tang, Yushuo Chen, Yifan Du, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen",cs.CL,table2text,"People often imagine relevant scenes to aid in the writing process. In this
work, we aim to utilize visual information for composition in the same manner
as humans. We propose a method, LIVE, that makes pre-trained language models
(PLMs) Learn to Imagine for Visuallyaugmented natural language gEneration.
First, we imagine the scene based on the text: we use a diffusion model to
synthesize high-quality images conditioned on the input texts. Second, we use
CLIP to determine whether the text can evoke the imagination in a posterior
way. Finally, our imagination is dynamic, and we conduct synthesis for each
sentence rather than generate only one image for an entire paragraph.
Technically, we propose a novel plug-and-play fusion layer to obtain
visually-augmented representations for each text. Our vision-text fusion layer
is compatible with Transformerbased architecture. We have conducted extensive
experiments on four generation tasks using BART and T5, and the automatic
results and human evaluation demonstrate the effectiveness of our proposed
method. We will release the code, model, and data at the link:
https://github.com/RUCAIBox/LIVE.",2023-05-26
RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting,2023-05-25 03:26:26+00:00,http://arxiv.org/abs/2305.15685v1,"Lei Shu, Liangchen Luo, Jayakumar Hoskere, Yun Zhu, Canoee Liu, Simon Tong, Jindong Chen, Lei Meng","cs.CL, cs.AI",table2text,"Large Language Models (LLMs) have demonstrated impressive zero-shot
capabilities in long-form text generation tasks expressed through natural
language instructions. However, user expectations for long-form text rewriting
is high, and unintended rewrites (''hallucinations'') produced by the model can
negatively impact its overall performance. Existing evaluation benchmarks
primarily focus on limited rewriting styles and sentence-level rewriting rather
than long-form open-ended rewriting.We introduce OpenRewriteEval, a novel
benchmark that covers a wide variety of rewriting types expressed through
natural language instructions. It is specifically designed to facilitate the
evaluation of open-ended rewriting of long-form texts. In addition, we propose
a strong baseline model, RewriteLM, an instruction-tuned large language model
for long-form text rewriting. We develop new strategies that facilitate the
generation of diverse instructions and preference data with minimal human
intervention. We conduct empirical experiments and demonstrate that our model
outperforms the current state-of-the-art LLMs in text rewriting. Specifically,
it excels in preserving the essential content and meaning of the source text,
minimizing the generation of ''hallucinated'' content, while showcasing the
ability to generate rewrites with diverse wording and structures.",2023-05-25
"Balancing Effect of Training Dataset Distribution of Multiple Styles for
  Multi-Style Text Transfer",2023-05-24 21:36:15+00:00,http://arxiv.org/abs/2305.15582v1,"Debarati Das, David Ma, Dongyeop Kang",cs.CL,table2text,"Text style transfer is an exciting task within the field of natural language
generation that is often plagued by the need for high-quality paired datasets.
Furthermore, training a model for multi-attribute text style transfer requires
datasets with sufficient support across all combinations of the considered
stylistic attributes, adding to the challenges of training a style transfer
model. This paper explores the impact of training data input diversity on the
quality of the generated text from the multi-style transfer model. We construct
a pseudo-parallel dataset by devising heuristics to adjust the style
distribution in the training samples. We balance our training dataset using
marginal and joint distributions to train our style transfer models. We observe
that a balanced dataset produces more effective control effects over multiple
styles than an imbalanced or skewed one. Through quantitative analysis, we
explore the impact of multiple style distributions in training data on
style-transferred output. These findings will better inform the design of
style-transfer datasets.",2023-05-24
"Peek Across: Improving Multi-Document Modeling via Cross-Document
  Question-Answering",2023-05-24 17:48:40+00:00,http://arxiv.org/abs/2305.15387v1,"Avi Caciularu, Matthew E. Peters, Jacob Goldberger, Ido Dagan, Arman Cohan","cs.CL, cs.AI",table2text,"The integration of multi-document pre-training objectives into language
models has resulted in remarkable improvements in multi-document downstream
tasks. In this work, we propose extending this idea by pre-training a generic
multi-document model from a novel cross-document question answering
pre-training objective. To that end, given a set (or cluster) of
topically-related documents, we systematically generate semantically-oriented
questions from a salient sentence in one document and challenge the model,
during pre-training, to answer these questions while ""peeking"" into other
topically-related documents. In a similar manner, the model is also challenged
to recover the sentence from which the question was generated, again while
leveraging cross-document information. This novel multi-document QA formulation
directs the model to better recover cross-text informational relations, and
introduces a natural augmentation that artificially increases the pre-training
data. Further, unlike prior multi-document models that focus on either
classification or summarization tasks, our pre-training objective formulation
enables the model to perform tasks that involve both short text generation
(e.g., QA) and long text generation (e.g., summarization). Following this
scheme, we pre-train our model -- termed QAmden -- and evaluate its performance
across several multi-document tasks, including multi-document QA,
summarization, and query-focused summarization, yielding improvements of up to
7%, and significantly outperforms zero-shot GPT-3.5 and GPT-4.",2023-05-24
"Not All Metrics Are Guilty: Improving NLG Evaluation with LLM
  Paraphrasing",2023-05-24 11:53:29+00:00,http://arxiv.org/abs/2305.15067v1,"Tianyi Tang, Hongyuan Lu, Yuchen Eleanor Jiang, Haoyang Huang, Dongdong Zhang, Wayne Xin Zhao, Furu Wei",cs.CL,table2text,"Most research about natural language generation (NLG) relies on evaluation
benchmarks with limited references for a sample, which may result in poor
correlations with human judgements. The underlying reason is that one semantic
meaning can actually be expressed in different forms, and the evaluation with a
single or few references may not accurately reflect the quality of the model's
hypotheses. To address this issue, this paper presents a novel method, named
Para-Ref, to enhance existing evaluation benchmarks by enriching the number of
references. We leverage large language models (LLMs) to paraphrase a single
reference into multiple high-quality ones in diverse expressions. Experimental
results on representative NLG tasks of machine translation, text summarization,
and image caption demonstrate that our method can effectively improve the
correlation with human evaluation for sixteen automatic evaluation metrics by
+7.82% in ratio. We release the code and data at
https://github.com/RUCAIBox/Para-Ref.",2023-05-24
Ghostbuster: Detecting Text Ghostwritten by Large Language Models,2023-05-24 11:37:10+00:00,http://arxiv.org/abs/2305.15047v1,"Vivek Verma, Eve Fleisig, Nicholas Tomlin, Dan Klein","cs.CL, cs.AI",table2text,"We introduce Ghostbuster, a state-of-the-art system for detecting
AI-generated text. Our method works by passing documents through a series of
weaker language models and running a structured search over possible
combinations of their features, then training a classifier on the selected
features to determine if the target document was AI-generated. Crucially,
Ghostbuster does not require access to token probabilities from the target
model, making it useful for detecting text generated by black-box models or
unknown model versions. In conjunction with our model, we release three new
datasets of human and AI-generated text as detection benchmarks that cover
multiple domains (student essays, creative fiction, and news) and task setups:
document-level detection, author identification, and a challenge task of
paragraph-level detection. Ghostbuster averages 99.1 F1 across all three
datasets on document-level detection, outperforming previous approaches such as
GPTZero and DetectGPT by up to 32.7 F1.",2023-05-24
Active Learning for Natural Language Generation,2023-05-24 11:27:53+00:00,http://arxiv.org/abs/2305.15040v1,"Yotam Perlitz, Ariel Gera, Michal Shmueli-Scheuer, Dafna Sheinwald, Noam Slonim, Liat Ein-Dor",cs.CL,table2text,"The field of text generation suffers from a severe shortage of labeled data
due to the extremely expensive and time consuming process involved in manual
annotation. A natural approach for coping with this problem is active learning
(AL), a well-known machine learning technique for improving annotation
efficiency by selectively choosing the most informative examples to label.
However, while AL has been well-researched in the context of text
classification, its application to text generation remained largely unexplored.
In this paper, we present a first systematic study of active learning for text
generation, considering a diverse set of tasks and multiple leading AL
strategies. Our results indicate that existing AL strategies, despite their
success in classification, are largely ineffective for the text generation
scenario, and fail to consistently surpass the baseline of random example
selection. We highlight some notable differences between the classification and
generation scenarios, and analyze the selection behaviors of existing AL
strategies. Our findings motivate exploring novel approaches for applying AL to
NLG tasks.",2023-05-24
LLMDet: A Large Language Models Detection Tool,2023-05-24 10:45:16+00:00,http://arxiv.org/abs/2305.15004v1,"Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, Tat-Seng Chua",cs.CL,table2text,"With the advancement of generative language models, the generated text has
come remarkably close to high-quality human-authored text in terms of fluency
and diversity. This calls for a highly practical detection tool that can
identify the source of text, preferably pinpointing the language model it
originates from. However, existing detection tools typically require access to
language models and can only differentiate between machine-generated and
human-authored text, failing to meet the requirements of rapid detection and
text tracing. Therefore, in this paper, we propose an efficient, secure, and
scalable detection tool called LLMDet, which calculates the proxy perplexity of
text by utilizing the prior information of the model's next-token
probabilities, obtained through pre-training. Subsequently, we use the
self-watermarking information of the model, as measured by proxy perplexity, to
detect the source of the text. We found that our method demonstrates impressive
detection performance while ensuring speed and security, particularly achieving
a recognition accuracy of 97.97\% for human-authored text. Furthermore, our
detection tool also shows promising results in identifying the large language
model (e.g., GPT-2, OPT, LLaMA, Vicuna...) responsible for the text. We release
the code and processed data at \url{https://github.com/TrustedLLM/LLMDet}.",2023-05-24
The ACL OCL Corpus: advancing Open science in Computational Linguistics,2023-05-24 10:35:56+00:00,http://arxiv.org/abs/2305.14996v1,"Shaurya Rohatgi, Yanxia Qin, Benjamin Aw, Niranjana Unnithan, Min-Yen Kan","cs.CL, cs.DL",table2text,"We present a scholarly corpus from the ACL Anthology to assist Open
scientific research in the Computational Linguistics domain, named as ACL OCL.
Compared with previous ARC and AAN versions, ACL OCL includes structured
full-texts with logical sections, references to figures, and links to a large
knowledge resource (semantic scholar). ACL OCL contains 74k scientific papers,
together with 210k figures extracted up to September 2022. To observe the
development in the computational linguistics domain, we detect the topics of
all OCL papers with a supervised neural model. We observe ''Syntax: Tagging,
Chunking and Parsing'' topic is significantly shrinking and ''Natural Language
Generation'' is resurging. Our dataset is open and available to download from
HuggingFace in https://huggingface.co/datasets/ACL-OCL/ACL-OCL-Corpus.",2023-05-24
"Large Language Models are Effective Table-to-Text Generators,
  Evaluators, and Feedback Providers",2023-05-24 10:22:30+00:00,http://arxiv.org/abs/2305.14987v1,"Yilun Zhao, Haowei Zhang, Shengyun Si, Linyong Nan, Xiangru Tang, Arman Cohan",cs.CL,table2text,"Large language models (LLMs) have shown remarkable ability on controllable
text generation. However, the potential of LLMs in generating text from
structured tables remains largely under-explored. In this paper, we study the
capabilities of LLMs for table-to-text generation tasks, particularly aiming to
investigate their performance in generating natural language statements that
can be logically entailed by a provided table. First, we investigate how LLMs
compare to state-of-the-art table-to-text fine-tuned models, and demonstrate
that LLMs can generate statements with higher faithfulness compared with
previous state-of-the-art fine-tuned models. Given this finding, we next
explore whether LLMs can serve as faithfulness-level automated evaluation
metrics. Through human evaluation, we show that evaluation metrics adopted from
LLMs correlates better with human judgments compared with existing
faithfulness-level metrics. Finally, we demonstrate that LLMs using
chain-of-thought prompting can generate high-fidelity natural language feedback
for other table-to-text models' generations, provide insights for future work
regarding the distillation of text generation capabilities from LLMs to smaller
models.",2023-05-24
Universal Self-adaptive Prompting,2023-05-24 09:09:48+00:00,http://arxiv.org/abs/2305.14926v1,"Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Hanjun Dai, Julian Martin Eisenschlos, Sercan O. Arik, Tomas Pfister","cs.CL, cs.AI, cs.LG",table2text,"A hallmark of modern large language models (LLMs) is their impressive general
zero-shot and few-shot abilities, often elicited through prompt-based and/or
in-context learning. However, while highly coveted and being the most general,
zero-shot performances in LLMs are still typically weaker due to the lack of
guidance and the difficulty of applying existing automatic prompt design
methods in general tasks when ground-truth labels are unavailable. In this
study, we address this by presenting Universal Self-adaptive Prompting (USP),
an automatic prompt design approach specifically tailored for zero-shot
learning (while compatible with few-shot). Requiring only a small amount of
unlabeled data & an inference-only LLM, USP is highly versatile: to achieve
universal prompting, USP categorizes a possible NLP task into one of the three
possible task types, and then uses a corresponding selector to select the most
suitable queries & zero-shot model-generated responses as
pseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a
fully automated way. We evaluate zero-shot USP with two PaLM models, and
demonstrate performances that are considerably stronger than standard zero-shot
baselines and are comparable to or even superior than few-shot baselines across
more than 20 natural language understanding (NLU) and natural language
generation (NLG) tasks.",2023-05-24
Faithful Low-Resource Data-to-Text Generation through Cycle Training,2023-05-24 06:44:42+00:00,http://arxiv.org/abs/2305.14793v1,"Zhuoer Wang, Marcus Collins, Nikhita Vedula, Simone Filice, Shervin Malmasi, Oleg Rokhlenko",cs.CL,table2text,"Methods to generate text from structured data have advanced significantly in
recent years, primarily due to fine-tuning of pre-trained language models on
large datasets. However, such models can fail to produce output faithful to the
input data, particularly on out-of-domain data. Sufficient annotated data is
often not available for specific domains, leading us to seek an unsupervised
approach to improve the faithfulness of output text. Since the problem is
fundamentally one of consistency between the representations of the structured
data and text, we evaluate the effectiveness of cycle training in this work.
Cycle training uses two models which are inverses of each other: one that
generates text from structured data, and one which generates the structured
data from natural language text. We show that cycle training, when initialized
with a small amount of supervised data (100 samples in our case), achieves
nearly the same performance as fully supervised approaches for the data-to-text
generation task on the WebNLG, E2E, WTQ, and WSQL datasets. We perform
extensive empirical analysis with automated evaluation metrics and a newly
designed human evaluation schema to reveal different cycle training strategies'
effectiveness of reducing various types of generation errors. Our code is
publicly available at https://github.com/Edillower/CycleNLG.",2023-05-24
In-Context Demonstration Selection with Cross Entropy Difference,2023-05-24 05:04:00+00:00,http://arxiv.org/abs/2305.14726v1,"Dan Iter, Reid Pryzant, Ruochen Xu, Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu","cs.CL, cs.AI",table2text,"Large language models (LLMs) can use in-context demonstrations to improve
performance on zero-shot tasks. However, selecting the best in-context examples
is challenging because model performance can vary widely depending on the
selected examples. We present a cross-entropy difference (CED) method for
selecting in-context demonstrations. Our method is based on the observation
that the effectiveness of in-context demonstrations negatively correlates with
the perplexity of the test example by a language model that was finetuned on
that demonstration. We utilize parameter efficient finetuning to train small
models on training data that are used for computing the cross-entropy
difference between a test example and every candidate in-context demonstration.
This metric is used to rank and select in-context demonstrations independently
for each test input. We evaluate our method on a mix-domain dataset that
combines 8 benchmarks, representing 4 text generation tasks, showing that CED
for in-context demonstration selection can improve performance for a variety of
LLMs.",2023-05-24
Diffusion Models in NLP: A Survey,2023-05-24 03:25:32+00:00,http://arxiv.org/abs/2305.14671v1,"Hao Zou, Zae Myung Kim, Dongyeop Kang",cs.CL,table2text,"This survey paper provides a comprehensive review of the use of diffusion
models in natural language processing (NLP). Diffusion models are a class of
mathematical models that aim to capture the diffusion of information or signals
across a network or manifold. In NLP, diffusion models have been used in a
variety of applications, such as natural language generation, sentiment
analysis, topic modeling, and machine translation. This paper discusses the
different formulations of diffusion models used in NLP, their strengths and
limitations, and their applications. We also perform a thorough comparison
between diffusion models and alternative generative models, specifically
highlighting the autoregressive (AR) models, while also examining how diverse
architectures incorporate the Transformer in conjunction with diffusion models.
Compared to AR models, diffusion models have significant advantages for
parallel generation, text interpolation, token-level controls such as syntactic
structures and semantic contents, and robustness. Exploring further
permutations of integrating Transformers into diffusion models would be a
valuable pursuit. Also, the development of multimodal diffusion models and
large-scale diffusion language models with notable capabilities for few-shot
learning would be important directions for the future advance of diffusion
models in NLP.",2023-05-24
QTSumm: A New Benchmark for Query-Focused Table Summarization,2023-05-23 17:43:51+00:00,http://arxiv.org/abs/2305.14303v1,"Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin Liu, Weijin Zou, Simeng Han, Xiangru Tang, Yumo Xu, Arman Cohan, Dragomir Radev",cs.CL,table2text,"People primarily consult tables to conduct data analysis or answer specific
questions. Text generation systems that can provide accurate table summaries
tailored to users' information needs can facilitate more efficient access to
relevant data insights. However, existing table-to-text generation studies
primarily focus on converting tabular data into coherent statements, rather
than addressing information-seeking purposes. In this paper, we define a new
query-focused table summarization task, where text generation models have to
perform human-like reasoning and analysis over the given table to generate a
tailored summary, and we introduce a new benchmark named QTSumm for this task.
QTSumm consists of 5,625 human-annotated query-summary pairs over 2,437 tables
on diverse topics. Moreover, we investigate state-of-the-art models (i.e., text
generation, table-to-text generation, and large language models) on the QTSumm
dataset. Experimental results and manual analysis reveal that our benchmark
presents significant challenges in table-to-text generation for future
research.",2023-05-23
"INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with
  Automatic Feedback",2023-05-23 17:27:22+00:00,http://arxiv.org/abs/2305.14282v1,"Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, Lei Li","cs.CL, cs.AI",table2text,"The field of automatic evaluation of text generation made tremendous progress
in the last few years. In particular, since the advent of neural metrics, like
COMET, BLEURT, and SEScore2, the newest generation of metrics show a high
correlation with human judgment. Unfortunately, quality scores generated with
neural metrics are not interpretable, and it is unclear which part of the
generation output is criticized by the metrics. To address this limitation, we
present INSTRUCTSCORE, an open-source, explainable evaluation metric for text
generation. By harnessing both explicit human instruction and the implicit
knowledge of GPT4, we fine-tune a LLAMA model to create an evaluative metric
that can produce a diagnostic report aligned with human judgment. We evaluate
INSTRUCTSCORE on the WMT22 Zh-En translation task, where our 7B model surpasses
other LLM-based baselines, including those based on 175B GPT3. Impressively,
our INSTRUCTSCORE, even without direct supervision from human-rated data,
achieves performance levels on par with state-of-the-art metrics like COMET22,
which was fine-tuned on human ratings.",2023-05-23
"Process-To-Text: A Framework for the Quantitative Description of
  Processes in Natural Language",2023-05-23 13:14:34+00:00,http://arxiv.org/abs/2305.14044v1,"Yago Fontenla-Seco, Alberto Bugarn-Diz, Manuel Lama",cs.CL,table2text,"In this paper we present the Process-To-Text (P2T) framework for the
automatic generation of textual descriptive explanations of processes. P2T
integrates three AI paradigms: process mining for extracting temporal and
structural information from a process, fuzzy linguistic protoforms for
modelling uncertain terms, and natural language generation for building the
explanations. A real use-case in the cardiology domain is presented, showing
the potential of P2T for providing natural language explanations addressed to
specialists.",2023-05-23
STOAT: Structured Data to Analytical Text With Controls,2023-05-19 17:03:09+00:00,http://arxiv.org/abs/2305.11826v1,"Deepanway Ghosal, Preksha Nema, Aravindan Raghuveer","cs.CL, cs.AI",table2text,"Recent language models have made tremendous progress in the structured data
to text generation task. However, these models still give sub-optimal
performance where logical inference is required to generate the descriptions.
In this work, we specifically focus on analytical text generation from
structured data such as tables. Building on the taxonomy proposed in (Gupta et
al., 2020) we focus on controllable table to text generation for the following
reasoning categories: numerical reasoning, commonsense reasoning, temporal
reasoning, table knowledge, and entity knowledge. We propose STOAT model, which
is table and reasoning aware, with vector-quantization to infuse the given
reasoning categories in the output. We observe that our model provides 10.19%,
1.13% improvement on the PARENT metric in iToTTo and Infotabs for the
analytical sentence task. We also found that our model generates 15.3% more
faithful and analytical descriptions as compared to the baseline models in
human evaluation. We curate and release two reasoning category annotated
table-to-interesting text generation datasets based on the ToTTo (Parikh et
al., 2020) and InfoTabs datasets (Gupta et al.,2020).",2023-05-19
"Generating Visual Spatial Description via Holistic 3D Scene
  Understanding",2023-05-19 15:53:56+00:00,http://arxiv.org/abs/2305.11768v1,"Yu Zhao, Hao Fei, Wei Ji, Jianguo Wei, Meishan Zhang, Min Zhang, Tat-Seng Chua","cs.CV, cs.CL",table2text,"Visual spatial description (VSD) aims to generate texts that describe the
spatial relations of the given objects within images. Existing VSD work merely
models the 2D geometrical vision features, thus inevitably falling prey to the
problem of skewed spatial understanding of target objects. In this work, we
investigate the incorporation of 3D scene features for VSD. With an external 3D
scene extractor, we obtain the 3D objects and scene features for input images,
based on which we construct a target object-centered 3D spatial scene graph
(Go3D-S2G), such that we model the spatial semantics of target objects within
the holistic 3D scenes. Besides, we propose a scene subgraph selecting
mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the
diverse local structure features are navigated to yield spatially-diversified
text generation. Experimental results on two VSD datasets demonstrate that our
framework outperforms the baselines significantly, especially improving on the
cases with complex visual spatial relations. Meanwhile, our method can produce
more spatially-diversified generation. Code is available at
https://github.com/zhaoyucs/VSD.",2023-05-19
"What Comes Next? Evaluating Uncertainty in Neural Text Generators
  Against Human Production Variability",2023-05-19 14:41:55+00:00,http://arxiv.org/abs/2305.11707v1,"Mario Giulianelli, Joris Baan, Wilker Aziz, Raquel Fernndez, Barbara Plank","cs.CL, cs.AI, cs.LG",table2text,"In Natural Language Generation (NLG) tasks, for any input, multiple
communicative goals are plausible, and any goal can be put into words, or
produced, in multiple ways. We characterise the extent to which human
production varies lexically, syntactically, and semantically across four NLG
tasks, connecting human production variability to aleatoric or data
uncertainty. We then inspect the space of output strings shaped by a generation
system's predicted probability distribution and decoding algorithm to probe its
uncertainty. For each test input, we measure the generator's calibration to
human production variability. Following this instance-level approach, we
analyse NLG models and decoding strategies, demonstrating that probing a
generator with multiple samples and, when possible, multiple references,
provides the level of detail necessary to gain understanding of a model's
representation of uncertainty.",2023-05-19
Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model,2023-05-18 17:35:28+00:00,http://arxiv.org/abs/2305.11140v1,"Chantal Amrhein, Florian Schottmann, Rico Sennrich, Samuel Lubli","cs.CL, I.2.7",table2text,"Natural language generation models reproduce and often amplify the biases
present in their training data. Previous research explored using
sequence-to-sequence rewriting models to transform biased model outputs (or
original texts) into more gender-fair language by creating pseudo training data
through linguistic rules. However, this approach is not practical for languages
with more complex morphology than English. We hypothesise that creating
training data in the reverse direction, i.e. starting from gender-fair text, is
easier for morphologically complex languages and show that it matches the
performance of state-of-the-art rewriting models for English. To eliminate the
rule-based nature of data creation, we instead propose using machine
translation models to create gender-biased text from real gender-fair text via
round-trip translation. Our approach allows us to train a rewriting model for
German without the need for elaborate handcrafted rules. The outputs of this
model increased gender-fairness as shown in a human evaluation study.",2023-05-18
"Cross-modality Data Augmentation for End-to-End Sign Language
  Translation",2023-05-18 16:34:18+00:00,http://arxiv.org/abs/2305.11096v1,"Jinhui Ye, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Hui Xiong",cs.CL,table2text,"End-to-end sign language translation (SLT) aims to convert sign language
videos into spoken language texts directly without intermediate
representations. It has been a challenging task due to the modality gap between
sign videos and texts and the data scarcity of labeled data. To tackle these
challenges, we propose a novel Cross-modality Data Augmentation (XmDA)
framework to transfer the powerful gloss-to-text translation capabilities to
end-to-end sign language translation (i.e. video-to-text) by exploiting pseudo
gloss-text pairs from the sign gloss translation model. Specifically, XmDA
consists of two key components, namely, cross-modality mix-up and
cross-modality knowledge distillation. The former explicitly encourages the
alignment between sign video features and gloss embeddings to bridge the
modality gap. The latter utilizes the generation knowledge from gloss-to-text
teacher models to guide the spoken language text generation. Experimental
results on two widely used SLT datasets, i.e., PHOENIX-2014T and CSL-Daily,
demonstrate that the proposed XmDA framework significantly and consistently
outperforms the baseline models. Extensive analyses confirm our claim that XmDA
enhances spoken language text generation by reducing the representation
distance between videos and texts, as well as improving the processing of
low-frequency words and long sentences.",2023-05-18
"ReGen: Zero-Shot Text Classification via Training Data Generation with
  Progressive Dense Retrieval",2023-05-18 04:30:09+00:00,http://arxiv.org/abs/2305.10703v1,"Yue Yu, Yuchen Zhuang, Rongzhi Zhang, Yu Meng, Jiaming Shen, Chao Zhang","cs.CL, cs.IR, cs.LG",table2text,"With the development of large language models (LLMs), zero-shot learning has
attracted much attention for various NLP tasks. Different from prior works that
generate training data with billion-scale natural language generation (NLG)
models, we propose a retrieval-enhanced framework to create training data from
a general-domain unlabeled corpus. To realize this, we first conduct
contrastive pretraining to learn an unsupervised dense retriever for extracting
the most relevant documents using class-descriptive verbalizers. We then
further propose two simple strategies, namely Verbalizer Augmentation with
Demonstrations and Self-consistency Guided Filtering to improve the topic
coverage of the dataset while removing noisy examples. Experiments on nine
datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines
and saves around 70% of the time compared to baselines using large NLG models.
Besides, REGEN can be naturally integrated with recently proposed large
language models to boost performance.",2023-05-18
Equivariant Few-Shot Learning from Pretrained Models,2023-05-17 02:20:34+00:00,http://arxiv.org/abs/2305.09900v1,"Sourya Basu, Pulkit Katdare, Prasanna Sattigeri, Vijil Chenthamarakshan, Katherine Driggs-Campbell, Payel Das, Lav R. Varshney","cs.LG, cs.AI, cs.CL, cs.CV",table2text,"Efficient transfer learning algorithms are key to the success of foundation
models on diverse downstream tasks even with limited data. Recent works of
\cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging
(\textit{equitune}) and optimization-based methods, respectively, over features
from group-transformed inputs to obtain equivariant outputs from
non-equivariant neural networks. While \cite{kaba2022equivariance} are only
concerned with training from scratch, we find that equitune performs poorly on
equivariant zero-shot tasks despite good finetuning results. We hypothesize
that this is because pretrained models provide better quality features for
certain transformations than others and simply averaging them is deleterious.
Hence, we propose $\lambda$-\textit{equitune} that averages the features using
\textit{importance weights}, $\lambda$s. These weights are learned directly
from the data using a small neural network, leading to excellent zero-shot and
finetuned results that outperform equitune. Further, we prove that
$\lambda$-equitune is equivariant and a universal approximator of equivariant
functions. Additionally, we show that the method of \cite{kaba2022equivariance}
used with appropriate loss functions, which we call \textit{equizero}, also
gives excellent zero-shot and finetuned performance. Both equitune and equizero
are special cases of $\lambda$-equitune. To show the simplicity and generality
of our method, we validate on a wide range of diverse applications and models
such as 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in
natural language generation (NLG), 4) compositional generalization in
languages, and 5) image classification using pretrained CNNs such as Resnet and
Alexnet.",2023-05-17
"Smaller Language Models are Better Black-box Machine-Generated Text
  Detectors",2023-05-17 00:09:08+00:00,http://arxiv.org/abs/2305.09859v1,"Fatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, Taylor Berg-Kirkpatrick","cs.CL, cs.LG",table2text,"With the advent of fluent generative language models that can produce
convincing utterances very similar to those written by humans, distinguishing
whether a piece of text is machine-generated or human-written becomes more
challenging and more important, as such models could be used to spread
misinformation, fake news, fake reviews and to mimic certain authors and
figures. To this end, there have been a slew of methods proposed to detect
machine-generated text. Most of these methods need access to the logits of the
target model or need the ability to sample from the target. One such black-box
detection method relies on the observation that generated text is locally
optimal under the likelihood function of the generator, while human-written
text is not. We find that overall, smaller and partially-trained models are
better universal text detectors: they can more precisely detect text generated
from both small and larger models. Interestingly, we find that whether the
detector and generator were trained on the same data is not critically
important to the detection success. For instance the OPT-125M model has an AUC
of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT
family, GPTJ-6B, has AUC of 0.45.",2023-05-17
Boosting Event Extraction with Denoised Structure-to-Text Augmentation,2023-05-16 16:52:07+00:00,http://arxiv.org/abs/2305.09598v1,"bo wang, Heyan Huang, Xiaochi Wei, Ge Shi, Xiao Liu, Chong Feng, Tong Zhou, Shuaiqiang Wang, Dawei Yin",cs.CL,table2text,"Event extraction aims to recognize pre-defined event triggers and arguments
from texts, which suffer from the lack of high-quality annotations. In most NLP
applications, involving a large scale of synthetic training data is a practical
and effective approach to alleviate the problem of data scarcity. However, when
applying to the task of event extraction, recent data augmentation methods
often neglect the problem of grammatical incorrectness, structure misalignment,
and semantic drifting, leading to unsatisfactory performances. In order to
solve these problems, we propose a denoised structure-to-text augmentation
framework for event extraction DAEE, which generates additional training data
through the knowledge-based structure-to-text generation model and selects the
effective subset from the generated data iteratively with a deep reinforcement
learning agent. Experimental results on several datasets demonstrate that the
proposed method generates more diverse text representations for event
extraction and achieves comparable results with the state-of-the-art.",2023-05-16
"Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice
  and Feedback",2023-05-15 19:48:59+00:00,http://arxiv.org/abs/2305.08982v1,"Shang-Ling Hsu, Raj Sanjay Shah, Prathik Senthil, Zahra Ashktorab, Casey Dugan, Werner Geyer, Diyi Yang","cs.HC, cs.CL",table2text,"Millions of users come to online peer counseling platforms to seek support on
diverse topics ranging from relationship stress to anxiety. However, studies
show that online peer support groups are not always as effective as expected
largely due to users' negative experiences with unhelpful counselors. Peer
counselors are key to the success of online peer counseling platforms, but most
of them often do not have systematic ways to receive guidelines or supervision.
In this work, we introduce CARE: an interactive AI-based tool to empower peer
counselors through automatic suggestion generation. During the practical
training stage, CARE helps diagnose which specific counseling strategies are
most suitable in the given context and provides tailored example responses as
suggestions. Counselors can choose to select, modify, or ignore any suggestion
before replying to the support seeker. Building upon the Motivational
Interviewing framework, CARE utilizes large-scale counseling conversation data
together with advanced natural language generation techniques to achieve these
functionalities. We demonstrate the efficacy of CARE by performing both
quantitative evaluations and qualitative user studies through simulated chats
and semi-structured interviews. We also find that CARE especially helps novice
counselors respond better in challenging situations.",2023-05-15
Creative Data Generation: A Review Focusing on Text and Poetry,2023-05-15 09:50:15+00:00,http://arxiv.org/abs/2305.08493v1,"Mohamad Elzohbi, Richard Zhao",cs.CL,table2text,"The rapid advancement in machine learning has led to a surge in automatic
data generation, making it increasingly challenging to differentiate between
naturally or human-generated data and machine-generated data. Despite these
advancements, the generation of creative data remains a challenge. This paper
aims to investigate and comprehend the essence of creativity, both in general
and within the context of natural language generation. We review various
approaches to creative writing devices and tasks, with a specific focus on the
generation of poetry. We aim to shed light on the challenges and opportunities
in the field of creative data generation.",2023-05-15
"Taxi1500: A Multilingual Dataset for Text Classification in 1500
  Languages",2023-05-15 09:43:32+00:00,http://arxiv.org/abs/2305.08487v1,"Chunlan Ma, Ayyoob ImaniGooghari, Haotian Ye, Ehsaneddin Asgari, Hinrich Schtze",cs.CL,table2text,"While natural language processing tools have been developed extensively for
some of the world's languages, a significant portion of the world's over 7000
languages are still neglected. One reason for this is that evaluation datasets
do not yet cover a wide range of languages, including low-resource and
endangered ones. We aim to address this issue by creating a text classification
dataset encompassing a large number of languages, many of which currently have
little to no annotated data available. We leverage parallel translations of the
Bible to construct such a dataset by first developing applicable topics and
employing a crowdsourcing tool to collect annotated data. By annotating the
English side of the data and projecting the labels onto other languages through
aligned verses, we generate text classification datasets for more than 1500
languages. We extensively benchmark several existing multilingual language
models using our dataset. To facilitate the advancement of research in this
area, we will release our dataset and code.",2023-05-15
"MatSci-NLP: Evaluating Scientific Language Models on Materials Science
  Language Tasks Using Text-to-Schema Modeling",2023-05-14 22:01:24+00:00,http://arxiv.org/abs/2305.08264v1,"Yu Song, Santiago Miret, Bang Liu","cs.CL, cond-mat.mtrl-sci, cs.AI",table2text,"We present MatSci-NLP, a natural language benchmark for evaluating the
performance of natural language processing (NLP) models on materials science
text. We construct the benchmark from publicly available materials science text
data to encompass seven different NLP tasks, including conventional NLP tasks
like named entity recognition and relation classification, as well as NLP tasks
specific to materials science, such as synthesis action retrieval which relates
to creating synthesis procedures for materials. We study various BERT-based
models pretrained on different scientific text corpora on MatSci-NLP to
understand the impact of pretraining strategies on understanding materials
science text. Given the scarcity of high-quality annotated data in the
materials science domain, we perform our fine-tuning experiments with limited
training data to encourage the generalize across MatSci-NLP tasks. Our
experiments in this low-resource training setting show that language models
pretrained on scientific text outperform BERT trained on general text. MatBERT,
a model pretrained specifically on materials science journals, generally
performs best for most tasks. Moreover, we propose a unified text-to-schema for
multitask learning on \benchmark and compare its performance with traditional
fine-tuning methods. In our analysis of different training methods, we find
that our proposed text-to-schema methods inspired by question-answering
consistently outperform single and multitask NLP fine-tuning methods. The code
and datasets are publicly available at
\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23}.",2023-05-14
"Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive
  Text Generation",2023-05-06 13:20:31+00:00,http://arxiv.org/abs/2305.04044v1,"Kun Zhou, Yifan Li, Wayne Xin Zhao, Ji-Rong Wen",cs.CL,table2text,"Recently, continuous diffusion models (CDM) have been introduced into
non-autoregressive (NAR) text-to-text generation. However, the discrete nature
of text increases the difficulty of CDM to generate coherent and fluent texts,
and also causes the incompatibility problem between CDM and advanced NLP
techniques, especially the popular pre-trained language models~(PLMs). To solve
it, we propose Diffusion-NAT, which introduces discrete diffusion models~(DDM)
into NAR text-to-text generation and integrates BART to improve the
performance. By revising the decoding process of BART and the typical settings
of DDM, we unify the inference process of BART and the denoising process of DDM
into the same NAR masked tokens recovering task. In this way, DDM can rely on
BART to perform denoising, which can benefit from both the rich pre-learned
knowledge of BART and the iterative refining paradigm of DDM. Besides, we also
propose the iterative self-prompting strategy to further improve the generation
quality. Experimental results on 7 datasets show that our approach can
outperform competitive NAR methods, and even surpass autoregressive methods.
Our code and data will be publicly released.",2023-05-06
Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain,2023-05-05 03:02:41+00:00,http://arxiv.org/abs/2305.03256v1,"Liqiang Jing, Xuemeng Song, Xuming Lin, Zhongzhou Zhao, Wei Zhou, Liqiang Nie",cs.CL,table2text,"Existing data-to-text generation efforts mainly focus on generating a
coherent text from non-linguistic input data, such as tables and
attribute-value pairs, but overlook that different application scenarios may
require texts of different styles. Inspired by this, we define a new task,
namely stylized data-to-text generation, whose aim is to generate coherent text
for the given non-linguistic data according to a specific style. This task is
non-trivial, due to three challenges: the logic of the generated text,
unstructured style reference, and biased training samples. To address these
challenges, we propose a novel stylized data-to-text generation model, named
StyleD2T, comprising three components: logic planning-enhanced data embedding,
mask-based style embedding, and unbiased stylized text generation. In the first
component, we introduce a graph-guided logic planner for attribute organization
to ensure the logic of generated text. In the second component, we devise
feature-level mask-based style embedding to extract the essential style signal
from the given unstructured style reference. In the last one, pseudo triplet
augmentation is utilized to achieve unbiased text generation, and a
multi-condition based confidence assignment function is designed to ensure the
quality of pseudo samples. Extensive experiments on a newly collected dataset
from Taobao have been conducted, and the results show the superiority of our
model over existing methods.",2023-05-05
VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation,2023-05-04 23:27:21+00:00,http://arxiv.org/abs/2305.03204v1,"Xilun Chen, Lili Yu, Wenhan Xiong, Barlas Ouz, Yashar Mehdad, Wen-tau Yih","cs.CV, cs.CL",table2text,"We propose a new two-stage pre-training framework for video-to-text
generation tasks such as video captioning and video question answering: A
generative encoder-decoder model is first jointly pre-trained on massive
image-text data to learn fundamental vision-language concepts, and then adapted
to video data in an intermediate video-text pre-training stage to learn
video-specific skills such as spatio-temporal reasoning. As a result, our
VideoOFA model achieves new state-of-the-art performance on four Video
Captioning benchmarks, beating prior art by an average of 9.7 points in CIDEr
score. It also outperforms existing models on two open-ended Video Question
Answering datasets, showcasing its generalization capability as a universal
video-to-text model.",2023-05-04
"Can LLM Already Serve as A Database Interface? A BIg Bench for
  Large-Scale Database Grounded Text-to-SQLs",2023-05-04 19:02:29+00:00,http://arxiv.org/abs/2305.03111v1,"Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Chenhao Ma, Kevin C. C. Chang, Fei Huang, Reynold Cheng, Yongbin Li",cs.CL,table2text,"Text-to-SQL parsing, which aims at converting natural language instructions
into executable SQLs, has gained increasing attention in recent years. In
particular, Codex and ChatGPT have shown impressive results in this task.
However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on
database schema with few rows of database contents leaving the gap between
academic study and real-world applications. To mitigate this gap, we present
Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks,
containing 12,751 pairs of text-to-SQL data and 95 databases with a total size
of 33.4 GB, spanning 37 professional domains. Our emphasis on database values
highlights the new challenges of dirty database contents, external knowledge
between NL questions and database contents, and SQL efficiency, particularly in
the context of massive databases. To solve these problems, text-to-SQL models
must feature database value comprehension in addition to semantic parsing. The
experimental results demonstrate the significance of database values in
generating accurate text-to-SQLs for big databases. Furthermore, even the most
effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution
accuracy, which is still far from the human result of 92.96%, proving that
challenges still stand. Besides, we also provide an efficiency analysis to
offer insights into generating text-to-efficient-SQLs that are beneficial to
industries. We believe that BIRD will contribute to advancing real-world
applications of text-to-SQL research. The leaderboard and source code are
available: https://bird-bench.github.io/.",2023-05-04
"How to Choose Pretrained Handwriting Recognition Models for Single
  Writer Fine-Tuning",2023-05-04 07:00:28+00:00,http://arxiv.org/abs/2305.02593v1,"Vittorio Pippi, Silvia Cascianelli, Christopher Kermorvant, Rita Cucchiara","cs.CV, cs.DL",table2text,"Recent advancements in Deep Learning-based Handwritten Text Recognition (HTR)
have led to models with remarkable performance on both modern and historical
manuscripts in large benchmark datasets. Nonetheless, those models struggle to
obtain the same performance when applied to manuscripts with peculiar
characteristics, such as language, paper support, ink, and author handwriting.
This issue is very relevant for valuable but small collections of documents
preserved in historical archives, for which obtaining sufficient annotated
training data is costly or, in some cases, unfeasible. To overcome this
challenge, a possible solution is to pretrain HTR models on large datasets and
then fine-tune them on small single-author collections. In this paper, we take
into account large, real benchmark datasets and synthetic ones obtained with a
styled Handwritten Text Generation model. Through extensive experimental
analysis, also considering the amount of fine-tuning lines, we give a
quantitative indication of the most relevant characteristics of such data for
obtaining an HTR model able to effectively transcribe manuscripts in small
collections with as little as five real fine-tuning lines.",2023-05-04
"Governance of the AI, by the AI, and for the AI",2023-05-04 03:29:07+00:00,http://arxiv.org/abs/2305.03719v1,"Andrew W. Torrance, Bill Tomlinson","cs.CY, cs.AI",table2text,"Over the past half century, there have been several false dawns during which
the ""arrival"" of world-changing artificial intelligence (AI) has been heralded.
Tempting fate, the authors believe the age of AI has, indeed, finally arrived.
Powerful image generators, such as DALL-E2 and Midjourney have suddenly allowed
anyone with access the ability easily to create rich and complex art. In a
similar vein, text generators, such as GPT3.5 (including ChatGPT) and BLOOM,
allow users to compose detailed written descriptions of many topics of
interest. And, it is even possible now for a person without extensive expertise
in writing software to use AI to generate code capable of myriad applications.
While AI will continue to evolve and improve, probably at a rapid rate, the
current state of AI is already ushering in profound changes to many different
sectors of society. Every new technology challenges the ability of humanity to
govern it wisely. However, governance is usually viewed as both possible and
necessary due to the disruption new technology often poses to social
structures, industries, the environment, and other important human concerns. In
this article, we offer an analysis of a range of interactions between AI and
governance, with the hope that wise decisions may be made that maximize
benefits and minimize costs. The article addresses two main aspects of this
relationship: the governance of AI by humanity, and the governance of humanity
by AI. The approach we have taken is itself informed by AI, as this article was
written collaboratively by the authors and ChatGPT.",2023-05-04
Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System,2023-05-04 00:17:49+00:00,http://arxiv.org/abs/2305.02468v1,"Namo Bang, Jeehyun Lee, Myoung-Wan Koo",cs.CL,table2text,"Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks
by tracking dialogue states and generating appropriate responses to help users
achieve defined goals. Recently, end-to-end dialogue models pre-trained based
on large datasets have shown promising performance in the conversational
system. However, they share the same parameters to train tasks of the dialogue
system (NLU, DST, NLG), so debugging each task is challenging. Also, they
require a lot of effort to fine-tune large parameters to create a task-oriented
chatbot, making it difficult for non-experts to handle. Therefore, we intend to
train relatively lightweight and fast models compared to PLM. In this paper, we
propose an End-to-end TOD system with Task-Optimized Adapters which learn
independently per task, adding only small number of parameters after fixed
layers of pre-trained network. We also enhance the performance of the DST and
NLG modules through reinforcement learning, overcoming the learning curve that
has lacked at the adapter learning and enabling the natural and consistent
response generation that is appropriate for the goal. Our method is a
model-agnostic approach and does not require prompt-tuning as only input data
without a prompt. As results of the experiment, our method shows competitive
performance on the MultiWOZ benchmark compared to the existing end-to-end
models. In particular, we attain state-of-the-art performance on the DST task
of 2.2 dataset.",2023-05-04
"A Systematic Study of Knowledge Distillation for Natural Language
  Generation with Pseudo-Target Training",2023-05-03 10:49:38+00:00,http://arxiv.org/abs/2305.02031v1,"Nitay Calderon, Subhabrata Mukherjee, Roi Reichart, Amir Kantor","cs.CL, cs.AI",table2text,"Modern Natural Language Generation (NLG) models come with massive
computational and storage requirements. In this work, we study the potential of
compressing them, which is crucial for real-world applications serving millions
of users. We focus on Knowledge Distillation (KD) techniques, in which a small
student model learns to imitate a large teacher model, allowing to transfer
knowledge from the teacher to the student. In contrast to much of the previous
work, our goal is to optimize the model for a specific NLG task and a specific
dataset. Typically, in real-world applications, in addition to labeled data
there is abundant unlabeled task-specific data, which is crucial for attaining
high compression rates via KD. In this work, we conduct a systematic study of
task-specific KD techniques for various NLG tasks under realistic assumptions.
We discuss the special characteristics of NLG distillation and particularly the
exposure bias problem. Following, we derive a family of Pseudo-Target (PT)
augmentation methods, substantially extending prior work on sequence-level KD.
We propose the Joint-Teaching method for NLG distillation, which applies
word-level KD to multiple PTs generated by both the teacher and the student.
Our study provides practical model design observations and demonstrates the
effectiveness of PT training for task-specific KD in NLG.",2023-05-03
Towards Summarizing Multiple Documents with Hierarchical Relationships,2023-05-02 15:18:18+00:00,http://arxiv.org/abs/2305.01498v1,"Miao Li, Eduard Hovy, Jey Han Lau","cs.CL, cs.AI",table2text,"Most existing multi-document summarization (MDS) datasets lack
human-generated and genuine (i.e., not synthetic) summaries or source documents
with explicit inter-document relationships that a summary must capture. To
enhance the capabilities of MDS systems we present PeerSum, a novel dataset for
generating meta-reviews of scientific papers, where the meta-reviews are highly
abstractive and genuine summaries of reviews and corresponding discussions.
These source documents have rich inter-document relationships of an explicit
hierarchical structure with cross-references and often feature conflicts. As
there is a scarcity of research that incorporates hierarchical relationships
into MDS systems through attention manipulation on pre-trained language models,
we additionally present Rammer (Relationship-aware Multi-task Meta-review
Generator), a meta-review generation model that uses sparse attention based on
the hierarchical relationships and a multi-task objective that predicts several
metadata features in addition to the standard text generation objective. Our
experimental results show that PeerSum is a challenging dataset, and Rammer
outperforms other strong baseline MDS models under various evaluation metrics.",2023-05-02
"Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural
  Language Generation",2023-05-01 17:36:06+00:00,http://arxiv.org/abs/2305.00955v1,"Patrick Fernandes, Aman Madaan, Emmy Liu, Antnio Farinhas, Pedro Henrique Martins, Amanda Bertsch, Jos G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, Andr F. T. Martins","cs.CL, cs.AI, cs.LG",table2text,"Many recent advances in natural language generation have been fueled by
training large language models on internet-scale data. However, this paradigm
can lead to models that generate toxic, inaccurate, and unhelpful content, and
automatic evaluation metrics often fail to identify these behaviors. As models
become more capable, human feedback is an invaluable signal for evaluating and
improving models. This survey aims to provide an overview of the recent
research that has leveraged human feedback to improve natural language
generation. First, we introduce an encompassing formalization of feedback, and
identify and organize existing research into a taxonomy following this
formalization. Next, we discuss how feedback can be described by its format and
objective, and cover the two approaches proposed to use feedback (either for
training or decoding): directly using the feedback or training feedback models.
We also discuss existing datasets for human-feedback data collection, and
concerns surrounding feedback collection. Finally, we provide an overview of
the nascent field of AI feedback, which exploits large language models to make
judgments based on a set of principles and minimize the need for human
intervention.",2023-05-01
"A Comprehensive AI Policy Education Framework for University Teaching
  and Learning",2023-04-29 15:35:39+00:00,http://arxiv.org/abs/2305.00280v1,Cecilia Ka Yuk Chan,"cs.CY, cs.AI",table2text,"This study aims to develop an AI education policy for higher education by
examining the perceptions and implications of text generative AI technologies.
Data was collected from 457 students and 180 teachers and staff across various
disciplines in Hong Kong universities, using both quantitative and qualitative
research methods. Based on the findings, the study proposes an AI Ecological
Education Policy Framework to address the multifaceted implications of AI
integration in university teaching and learning. This framework is organized
into three dimensions: Pedagogical, Governance, and Operational. The
Pedagogical dimension concentrates on using AI to improve teaching and learning
outcomes, while the Governance dimension tackles issues related to privacy,
security, and accountability. The Operational dimension addresses matters
concerning infrastructure and training. The framework fosters a nuanced
understanding of the implications of AI integration in academic settings,
ensuring that stakeholders are aware of their responsibilities and can take
appropriate actions accordingly.",2023-04-29
"CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to
  Guardrail Models for Virtual Assistants",2023-04-27 17:39:11+00:00,http://arxiv.org/abs/2304.14364v1,"Albert Yu Sun, Varun Nair, Elliot Schumacher, Anitha Kannan","cs.CL, cs.AI, cs.LG",table2text,"A wave of new task-based virtual assistants has been fueled by increasingly
powerful large language models, such as GPT-4. These conversational agents can
be customized to serve customer-specific use cases, but ensuring that
agent-generated text conforms to designer-specified rules included in prompt
instructions alone is challenging. Therefore, chatbot designers often use
another model, called a guardrail model, to verify that the agent output aligns
with their rules and constraints. We explore using a distillation approach to
guardrail models to monitor the output of the first model using training data
from GPT-4. We find two crucial steps to our CONSCENDI process:
scenario-augmented generation and contrastive training examples. When
generating conversational data, we generate a set of rule-breaking scenarios,
which enumerate a diverse set of high-level ways a rule can be violated. This
scenario-guided approach produces a diverse training set of rule-violating
conversations, and it provides chatbot designers greater control over the
classification process. We also prompt GPT-4 to also generate contrastive
examples by altering conversations with violations into acceptable
conversations. This set of borderline, contrastive examples enables the
distilled model to learn finer-grained distinctions between what is acceptable
and what is not. We find that CONSCENDI results in guardrail models that
improve over baselines.",2023-04-27
Controlled Text Generation with Natural Language Instructions,2023-04-27 15:56:34+00:00,http://arxiv.org/abs/2304.14293v1,"Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, Mrinmaya Sachan","cs.CL, cs.AI, cs.LG",table2text,"Large language models generate fluent texts and can follow natural language
instructions to solve a wide range of tasks without task-specific training.
Nevertheless, it is notoriously difficult to control their generation to
satisfy the various constraints required by different applications. In this
work, we present InstructCTG, a controlled text generation framework that
incorporates different constraints by conditioning on natural language
descriptions and demonstrations of the constraints. In particular, we first
extract the underlying constraints of natural texts through a combination of
off-the-shelf NLP tools and simple heuristics. We then verbalize the
constraints into natural language instructions to form weakly supervised
training data. By prepending natural language descriptions of the constraints
and a few demonstrations, we fine-tune a pre-trained language model to
incorporate various types of constraints. Compared to existing search-based or
score-based methods, InstructCTG is more flexible to different constraint types
and has a much smaller impact on the generation quality and speed because it
does not modify the decoding procedure. Additionally, InstructCTG allows the
model to adapt to new constraints without re-training through the use of
few-shot task generalization and in-context learning abilities of
instruction-tuned language models.",2023-04-27
"SweCTRL-Mini: a data-transparent Transformer-based large language model
  for controllable text generation in Swedish",2023-04-27 07:32:37+00:00,http://arxiv.org/abs/2304.13994v1,"Dmytro Kalpakchi, Johan Boye",cs.CL,table2text,"We present SweCTRL-Mini, a large Swedish language model that can be used for
inference and fine-tuning on a single consumer-grade GPU. The model is based on
the CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019),
which means that users of the SweCTRL-Mini model can control the genre of the
generated text by inserting special tokens in the generation prompts.
SweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a
set of Swedish novels. In this article, we provide (1) a detailed account of
the utilized training data and text pre-processing steps, to the extent that it
is possible to check whether a specific phrase/source was a part of the
training data, and (2) an evaluation of the model on both discriminative tasks,
using automatic evaluation methods, and generative tasks, using human referees.
We also compare the generative capabilities of the model with those of GPT-3.
SweCTRL-Mini is fully open and available for download.",2023-04-27
Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,2023-04-26 17:52:30+00:00,http://arxiv.org/abs/2304.13712v2,"Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu","cs.CL, cs.AI, cs.LG",table2text,"This paper presents a comprehensive and practical guide for practitioners and
end-users working with Large Language Models (LLMs) in their downstream natural
language processing (NLP) tasks. We provide discussions and insights into the
usage of LLMs from the perspectives of models, data, and downstream tasks.
Firstly, we offer an introduction and brief summary of current GPT- and
BERT-style LLMs. Then, we discuss the influence of pre-training data, training
data, and test data. Most importantly, we provide a detailed discussion about
the use and non-use cases of large language models for various natural language
processing tasks, such as knowledge-intensive tasks, traditional natural
language understanding tasks, natural language generation tasks, emergent
abilities, and considerations for specific tasks.We present various use cases
and non-use cases to illustrate the practical applications and limitations of
LLMs in real-world scenarios. We also try to understand the importance of data
and the specific challenges associated with each NLP task. Furthermore, we
explore the impact of spurious biases on LLMs and delve into other essential
considerations, such as efficiency, cost, and latency, to ensure a
comprehensive understanding of deploying LLMs in practice. This comprehensive
guide aims to provide researchers and practitioners with valuable insights and
best practices for working with LLMs, thereby enabling the successful
implementation of these models in a wide range of NLP tasks. A curated list of
practical guide resources of LLMs, regularly updated, can be found at
\url{https://github.com/Mooler0410/LLMsPracticalGuide}.",2023-04-26
Evaluating Inter-Bilingual Semantic Parsing for Indian Languages,2023-04-25 17:24:32+00:00,http://arxiv.org/abs/2304.13005v1,"Divyanshu Aggarwal, Vivek Gupta, Anoop Kunchukuttan",cs.CL,table2text,"Despite significant progress in Natural Language Generation for Indian
languages (IndicNLP), there is a lack of datasets around complex structured
tasks such as semantic parsing. One reason for this imminent gap is the
complexity of the logical form, which makes English to multilingual translation
difficult. The process involves alignment of logical forms, intents and slots
with translated unstructured utterance. To address this, we propose an
Inter-bilingual Seq2seq Semantic parsing dataset IE-SEMPARSE for 11 distinct
Indian languages. We highlight the proposed task's practicality, and evaluate
existing multilingual seq2seq models across several train-test strategies. Our
experiment reveals a high correlation across performance of original
multilingual semantic parsing datasets (such as mTOP, multilingual TOP and
multiATIS++) and our proposed IE-SEMPARSE suite.",2023-04-25
"Which Factors Predict the Chat Experience of a Natural Language
  Generation Dialogue Service?",2023-04-21 07:29:07+00:00,http://arxiv.org/abs/2304.10785v1,Eason Chen,"cs.CL, cs.HC",table2text,"In this paper, we proposed a conceptual model to predict the chat experience
in a natural language generation dialog system. We evaluated the model with 120
participants with Partial Least Squares Structural Equation Modeling (PLS-SEM)
and obtained an R-square (R2) with 0.541. The model considers various factors,
including the prompts used for generation; coherence, sentiment, and similarity
in the conversation; and users' perceived dialog agents' favorability. We then
further explore the effectiveness of the subset of our proposed model. The
results showed that users' favorability and coherence, sentiment, and
similarity in the dialogue are positive predictors of users' chat experience.
Moreover, we found users may prefer dialog agents with characteristics of
Extroversion, Openness, Conscientiousness, Agreeableness, and Non-Neuroticism.
Through our research, an adaptive dialog system might use collected data to
infer factors in our model, predict the chat experience for users through these
factors, and optimize it by adjusting prompts.",2023-04-21
"Multi-aspect Repetition Suppression and Content Moderation of Large
  Language Models",2023-04-20 19:17:49+00:00,http://arxiv.org/abs/2304.10611v1,"Minghui Zhang, Alex Sokolov, Weixin Cai, Si-Qing Chen","cs.CL, cs.LG",table2text,"Natural language generation is one of the most impactful fields in NLP, and
recent years have witnessed its evolution brought about by large language
models (LLMs). As the key instrument for writing assistance applications, they
are generally prone to replicating or extending offensive content provided in
the input. In low-resource data regime, they can also lead to repetitive
outputs (Holtzman et al., 2019) [1]. Usually, offensive content and repetitions
are mitigated with post-hoc methods, including n-gram level blocklists, top-k
and nucleus sampling. In this paper, we introduce a combination of exact and
non-exact repetition suppression using token and sequence level unlikelihood
loss, repetition penalty during training, inference, and post-processing
respectively. We further explore multi-level unlikelihood loss to the extent
that it endows the model with abilities to avoid generating offensive words and
phrases from the beginning. Finally, with comprehensive experiments, we
demonstrate that our proposed methods work exceptionally in controlling the
repetition and content quality of LLM outputs.",2023-04-20
GPT-NER: Named Entity Recognition via Large Language Models,2023-04-20 16:17:26+00:00,http://arxiv.org/abs/2304.10428v1,"Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, Guoyin Wang",cs.CL,table2text,"Despite the fact that large-scale Language Models (LLM) have achieved SOTA
performances on a variety of NLP tasks, its performance on NER is still
significantly below supervised baselines. This is due to the gap between the
two tasks the NER and LLMs: the former is a sequence labeling task in nature
while the latter is a text-generation model.
  In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the
gap by transforming the sequence labeling task to a generation task that can be
easily adapted by LLMs e.g., the task of finding location entities in the input
text ""Columbus is a city"" is transformed to generate the text sequence
""@@Columbus## is a city"", where special tokens @@## marks the entity to
extract. To efficiently address the ""hallucination"" issue of LLMs, where LLMs
have a strong inclination to over-confidently label NULL inputs as entities, we
propose a self-verification strategy by prompting LLMs to ask itself whether
the extracted entities belong to a labeled entity tag.
  We conduct experiments on five widely adopted NER datasets, and GPT-NER
achieves comparable performances to fully supervised baselines, which is the
first time as far as we are concerned. More importantly, we find that GPT-NER
exhibits a greater ability in the low-resource and few-shot setups, when the
amount of training data is extremely scarce, GPT-NER performs significantly
better than supervised models. This demonstrates the capabilities of GPT-NER in
real-world NER applications where the number of labeled examples is limited.",2023-04-20
"Towards Zero-Shot Personalized Table-to-Text Generation with Contrastive
  Persona Distillation",2023-04-18 11:32:33+00:00,http://arxiv.org/abs/2304.08911v1,"Haolan Zhan, Xuming Lin, Shaobo Cui, Zhongzhou Zhao, Wei Zhou, Haiqing Chen",cs.CL,table2text,"Existing neural methods have shown great potentials towards generating
informative text from structured tabular data as well as maintaining high
content fidelity. However, few of them shed light on generating personalized
expressions, which often requires well-aligned persona-table-text datasets that
are difficult to obtain. To overcome these obstacles, we explore personalized
table-to-text generation under a zero-shot setting, by assuming no well-aligned
persona-table-text triples are required during training. To this end, we
firstly collect a set of unpaired persona information and then propose a
semi-supervised approach with contrastive persona distillation (S2P-CPD) to
generate personalized context. Specifically, tabular data and persona
information are firstly represented as latent variables separately. Then, we
devise a latent space fusion technique to distill persona information into the
table representation. Besides, a contrastive-based discriminator is employed to
guarantee the style consistency between the generated context and its
corresponding persona. Experimental results on two benchmarks demonstrate
S2P-CPD's ability on keeping both content fidelity and personalized
expressions.",2023-04-18
"LongForm: Optimizing Instruction Tuning for Long Text Generation with
  Corpus Extraction",2023-04-17 17:36:35+00:00,http://arxiv.org/abs/2304.08460v1,"Abdullatif Kksal, Timo Schick, Anna Korhonen, Hinrich Schtze","cs.CL, cs.AI, cs.LG",table2text,"Instruction tuning enables language models to generalize more effectively and
better follow user intent. However, obtaining instruction data can be costly
and challenging. Prior works employ methods such as expensive human annotation,
crowd-sourced datasets with alignment issues, or generating noisy examples via
LLMs. We introduce the LongForm dataset, which is created by leveraging English
corpus examples with augmented instructions. We select a diverse set of
human-written documents from existing corpora such as C4 and Wikipedia and
generate instructions for the given documents via LLMs. This approach provides
a cheaper and cleaner instruction-tuning dataset and one suitable for long text
generation. We finetune T5, OPT, and LLaMA models on our dataset and show that
even smaller LongForm models have good generalization capabilities for text
generation. Our models outperform 10x larger language models without
instruction tuning on various tasks such as story/recipe generation and
long-form question answering. Moreover, LongForm models outperform prior
instruction-tuned models such as FLAN-T5 and Alpaca by a large margin. Finally,
our models can effectively follow and answer multilingual instructions; we
demonstrate this for news generation. We publicly release our data and models:
https://github.com/akoksal/LongForm.",2023-04-17
"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and
  Dataset",2023-04-17 15:08:15+00:00,http://arxiv.org/abs/2304.08345v1,"Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang, Jing Liu","cs.LG, cs.CL, cs.CV, cs.MM, eess.AS",table2text,"In this paper, we propose a Vision-Audio-Language Omni-peRception pretraining
model (VALOR) for multi-modal understanding and generation. Different from
widely-studied vision-language pretraining models, VALOR jointly models
relationships of vision, audio and language in an end-to-end manner. It
contains three separate encoders for single modality representations, and a
decoder for multimodal conditional text generation. We design two pretext tasks
to pretrain VALOR model, including Multimodal Grouping Alignment (MGA) and
Multimodal Grouping Captioning (MGC). MGA projects vision, language and audio
to the same common space, building vision-language, audio-language and
audiovisual-language alignment simultaneously. MGC learns how to generate text
tokens in conditions of vision, audio or their both. To promote
vision-audio-language pretraining research, we construct a large-scale
high-quality tri-modality dataset named VALOR-1M, which contains 1M audiable
videos with human annotated audiovisual captions. Extensive experiments show
that VALOR can learn strong multimodal correlations and be generalized to
various downstream tasks (e.g., retrieval, captioning and question answering),
with different input modalities (e.g., vision-language, audio-language and
audiovisual-language). VALOR achieves new state-of-the-art performances on
series of public cross-modality benchmarks. Code and data are available at
project page https://casia-iva-group.github.io/projects/VALOR.",2023-04-17
"VISAR: A Human-AI Argumentative Writing Assistant with Visual
  Programming and Rapid Draft Prototyping",2023-04-16 15:29:03+00:00,http://arxiv.org/abs/2304.07810v1,"Zheng Zhang, Jie Gao, Ranjodh Singh Dhaliwal, Toby Jia-Jun Li","cs.HC, cs.AI, cs.CL, cs.LG",table2text,"In argumentative writing, writers must brainstorm hierarchical writing goals,
ensure the persuasiveness of their arguments, and revise and organize their
plans through drafting. Recent advances in large language models (LLMs) have
made interactive text generation through a chat interface (e.g., ChatGPT)
possible. However, this approach often neglects implicit writing context and
user intent, lacks support for user control and autonomy, and provides limited
assistance for sensemaking and revising writing plans. To address these
challenges, we introduce VISAR, an AI-enabled writing assistant system designed
to help writers brainstorm and revise hierarchical goals within their writing
context, organize argument structures through synchronized text editing and
visual programming, and enhance persuasiveness with argumentation spark
recommendations. VISAR allows users to explore, experiment with, and validate
their writing plans using automatic draft prototyping. A controlled lab study
confirmed the usability and effectiveness of VISAR in facilitating the
argumentative writing planning process.",2023-04-16
"ArguGPT: evaluating, understanding and identifying argumentative essays
  generated by GPT models",2023-04-16 01:50:26+00:00,http://arxiv.org/abs/2304.07666v1,"Yikang Liu, Ziyin Zhang, Wanyang Zhang, Shisen Yue, Xiaojing Zhao, Xinyuan Cheng, Yiwen Zhang, Hai Hu",cs.CL,table2text,"AI generated content (AIGC) presents considerable challenge to educators
around the world. Instructors need to be able to detect such text generated by
large language models, either with the naked eye or with the help of some
tools. There is also growing need to understand the lexical, syntactic and
stylistic features of AIGC. To address these challenges in English language
teaching, we first present ArguGPT, a balanced corpus of 4,038 argumentative
essays generated by 7 GPT models in response to essay prompts from three
sources: (1) in-class or homework exercises, (2) TOEFL and (3) GRE writing
tasks. Machine-generated texts are paired with roughly equal number of
human-written essays with three score levels matched in essay prompts. We then
hire English instructors to distinguish machine essays from human ones. Results
show that when first exposed to machine-generated essays, the instructors only
have an accuracy of 61% in detecting them. But the number rises to 67% after
one round of minimal self-training. Next, we perform linguistic analyses of
these essays, which show that machines produce sentences with more complex
syntactic structures while human essays tend to be lexically more complex.
Finally, we test existing AIGC detectors and build our own detectors using SVMs
and RoBERTa. Results suggest that a RoBERTa fine-tuned with the training set of
ArguGPT achieves above 90% accuracy in both essay- and sentence-level
classification. To the best of our knowledge, this is the first comprehensive
analysis of argumentative essays produced by generative large language models.
Machine-authored essays in ArguGPT and our models will be made publicly
available at https://github.com/huhailinguist/ArguGPT",2023-04-16
"ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large
  Language Models in Multilingual Learning",2023-04-12 05:08:52+00:00,http://arxiv.org/abs/2304.05613v1,"Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, Thien Huu Nguyen","cs.CL, cs.AI",table2text,"Over the last few years, large language models (LLMs) have emerged as the
most important breakthroughs in natural language processing (NLP) that
fundamentally transform research and developments in the field. ChatGPT
represents one of the most exciting LLM systems developed recently to showcase
impressive skills for language generation and highly attract public attention.
Among various exciting applications discovered for ChatGPT in English, the
model can process and generate texts for multiple languages due to its
multilingual training data. Given the broad adoption of ChatGPT for English in
different problems and areas, a natural question is whether ChatGPT can also be
applied effectively for other languages or it is necessary to develop more
language-specific technologies. The answer to this question requires a thorough
evaluation of ChatGPT over multiple tasks with diverse languages and large
datasets (i.e., beyond reported anecdotes), which is still missing or limited
in current research. Our work aims to fill this gap for the evaluation of
ChatGPT and similar LLMs to provide more comprehensive information for
multilingual NLP applications. While this work will be an ongoing effort to
include additional experiments in the future, our current paper evaluates
ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,
low, and extremely low resources. We also focus on the zero-shot learning
setting for ChatGPT to improve reproducibility and better simulate the
interactions of general users. Compared to the performance of previous models,
our extensive experimental results demonstrate a worse performance of ChatGPT
for different NLP tasks and languages, calling for further research to develop
better models and understanding for multilingual learning.",2023-04-12
Automated Reading Passage Generation with OpenAI's Large Language Model,2023-04-10 14:30:39+00:00,http://arxiv.org/abs/2304.04616v1,"Ummugul Bezirhan, Matthias von Davier","cs.CL, cs.AI",table2text,"The widespread usage of computer-based assessments and individualized
learning platforms has resulted in an increased demand for the rapid production
of high-quality items. Automated item generation (AIG), the process of using
item models to generate new items with the help of computer technology, was
proposed to reduce reliance on human subject experts at each step of the
process. AIG has been used in test development for some time. Still, the use of
machine learning algorithms has introduced the potential to improve the
efficiency and effectiveness of the process greatly. The approach presented in
this paper utilizes OpenAI's latest transformer-based language model, GPT-3, to
generate reading passages. Existing reading passages were used in carefully
engineered prompts to ensure the AI-generated text has similar content and
structure to a fourth-grade reading passage. For each prompt, we generated
multiple passages, the final passage was selected according to the Lexile score
agreement with the original passage. In the final round, the selected passage
went through a simple revision by a human editor to ensure the text was free of
any grammatical and factual errors. All AI-generated passages, along with
original passages were evaluated by human judges according to their coherence,
appropriateness to fourth graders, and readability.",2023-04-10
"Decoder-Only or Encoder-Decoder? Interpreting Language Model as a
  Regularized Encoder-Decoder",2023-04-08 15:44:29+00:00,http://arxiv.org/abs/2304.04052v1,"Zihao Fu, Wai Lam, Qian Yu, Anthony Man-Cho So, Shengding Hu, Zhiyuan Liu, Nigel Collier","cs.CL, cs.AI, cs.LG",table2text,"The sequence-to-sequence (seq2seq) task aims at generating the target
sequence based on the given input source sequence. Traditionally, most of the
seq2seq task is resolved by the Encoder-Decoder framework which requires an
encoder to encode the source sequence and a decoder to generate the target
text. Recently, a bunch of new approaches have emerged that apply decoder-only
language models directly to the seq2seq task. Despite the significant
advancements in applying language models to the seq2seq task, there is still a
lack of thorough analysis on the effectiveness of the decoder-only language
model architecture. This paper aims to address this gap by conducting a
detailed comparison between the encoder-decoder architecture and the
decoder-only language model framework through the analysis of a regularized
encoder-decoder structure. This structure is designed to replicate all
behaviors in the classical decoder-only language model but has an encoder and a
decoder making it easier to be compared with the classical encoder-decoder
structure. Based on the analysis, we unveil the attention degeneration problem
in the language model, namely, as the generation step number grows, less and
less attention is focused on the source sequence. To give a quantitative
understanding of this problem, we conduct a theoretical sensitivity analysis of
the attention output with respect to the source input. Grounded on our
analysis, we propose a novel partial attention language model to solve the
attention degeneration problem. Experimental results on machine translation,
summarization, and data-to-text generation tasks support our analysis and
demonstrate the effectiveness of our proposed model.",2023-04-08
"Beyond Privacy: Navigating the Opportunities and Challenges of Synthetic
  Data",2023-04-07 16:38:40+00:00,http://arxiv.org/abs/2304.03722v1,"Boris van Breugel, Mihaela van der Schaar",cs.LG,table2text,"Generating synthetic data through generative models is gaining interest in
the ML community and beyond. In the past, synthetic data was often regarded as
a means to private data release, but a surge of recent papers explore how its
potential reaches much further than this -- from creating more fair data to
data augmentation, and from simulation to text generated by ChatGPT. In this
perspective we explore whether, and how, synthetic data may become a dominant
force in the machine learning world, promising a future where datasets can be
tailored to individual needs. Just as importantly, we discuss which fundamental
challenges the community needs to overcome for wider relevance and application
of synthetic data -- the most important of which is quantifying how much we can
trust any finding or prediction drawn from synthetic data.",2023-04-07
Measuring and Manipulating Knowledge Representations in Language Models,2023-04-03 06:24:10+00:00,http://arxiv.org/abs/2304.00740v1,"Evan Hernandez, Belinda Z. Li, Jacob Andreas",cs.CL,table2text,"Neural language models (LMs) represent facts about the world described by
text. Sometimes these facts derive from training data (in most LMs, a
representation of the word banana encodes the fact that bananas are fruits).
Sometimes facts derive from input text itself (a representation of the sentence
""I poured out the bottle"" encodes the fact that the bottle became empty). Tools
for inspecting and modifying LM fact representations would be useful almost
everywhere LMs are used: making it possible to update them when the world
changes, to localize and remove sources of bias, and to identify errors in
generated text. We describe REMEDI, an approach for querying and modifying
factual knowledge in LMs. REMEDI learns a map from textual queries to fact
encodings in an LM's internal representation system. These encodings can be
used as knowledge editors: by adding them to LM hidden representations, we can
modify downstream generation to be consistent with new facts. REMEDI encodings
can also be used as model probes: by comparing them to LM representations, we
can ascertain what properties LMs attribute to mentioned entities, and predict
when they will generate outputs that conflict with background knowledge or
input text. REMEDI thus links work on probing, prompting, and model editing,
and offers steps toward general tools for fine-grained inspection and control
of knowledge in LMs.",2023-04-03
Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts,2023-03-31 20:33:03+00:00,http://arxiv.org/abs/2304.00121v1,"Ryan Koo, Anna Martin, Linghe Wang, Dongyeop Kang","cs.CL, cs.HC",table2text,"Scholarly writing presents a complex space that generally follows a
methodical procedure to plan and produce both rationally sound and creative
compositions. Recent works involving large language models (LLM) demonstrate
considerable success in text generation and revision tasks; however, LLMs still
struggle to provide structural and creative feedback on the document level that
is crucial to academic writing. In this paper, we introduce a novel taxonomy
that categorizes scholarly writing behaviors according to intention, writer
actions, and the information types of the written data. We also provide
ManuScript, an original dataset annotated with a simplified version of our
taxonomy to show writer actions and the intentions behind them. Motivated by
cognitive writing theory, our taxonomy for scientific papers includes three
levels of categorization in order to trace the general writing flow and
identify the distinct writer activities embedded within each higher-level
process. ManuScript intends to provide a complete picture of the scholarly
writing process by capturing the linearity and non-linearity of writing
trajectory, such that writing assistants can provide stronger feedback and
suggestions on an end-to-end level. The collected writing trajectories are
viewed at https://minnesotanlp.github.io/REWARD_demo/",2023-03-31
Assessing Language Model Deployment with Risk Cards,2023-03-31 16:45:42+00:00,http://arxiv.org/abs/2303.18190v1,"Leon Derczynski, Hannah Rose Kirk, Vidhisha Balachandran, Sachin Kumar, Yulia Tsvetkov, M. R. Leiser, Saif Mohammad",cs.CL,table2text,"This paper introduces RiskCards, a framework for structured assessment and
documentation of risks associated with an application of language models. As
with all language, text generated by language models can be harmful, or used to
bring about harm. Automating language generation adds both an element of scale
and also more subtle or emergent undesirable tendencies to the generated text.
Prior work establishes a wide variety of language model harms to many different
actors: existing taxonomies identify categories of harms posed by language
models; benchmarks establish automated tests of these harms; and documentation
standards for models, tasks and datasets encourage transparent reporting.
However, there is no risk-centric framework for documenting the complexity of a
landscape in which some risks are shared across models and contexts, while
others are specific, and where certain conditions may be required for risks to
manifest as harms. RiskCards address this methodological gap by providing a
generic framework for assessing the use of a given language model in a given
scenario. Each RiskCard makes clear the routes for the risk to manifest harm,
their placement in harm taxonomies, and example prompt-output pairs. While
RiskCards are designed to be open-source, dynamic and participatory, we present
a ""starter set"" of RiskCards taken from a broad literature survey, each of
which details a concrete risk presentation. Language model RiskCards initiate a
community knowledge base which permits the mapping of risks and harms to a
specific model or its application scenario, ultimately contributing to a
better, safer and shared understanding of the risk landscape.",2023-03-31
Prefix tuning for automated audio captioning,2023-03-30 16:01:28+00:00,http://arxiv.org/abs/2303.17489v2,"Minkyu Kim, Kim Sung-Bin, Tae-Hyun Oh","eess.AS, cs.MM, cs.SD",table2text,"Audio captioning aims to generate text descriptions from environmental
sounds. One challenge of audio captioning is the difficulty of the
generalization due to the lack of audio-text paired training data. In this
work, we propose a simple yet effective method of dealing with small-scaled
datasets by leveraging a pre-trained language model. We keep the language model
frozen to maintain the expressivity for text generation, and we only learn to
extract global and temporal features from the input audio. To bridge a modality
gap between the audio features and the language model, we employ mapping
networks that translate audio features to the continuous vectors the language
model can understand, called prefixes. We evaluate our proposed method on the
Clotho and AudioCaps dataset and show our method outperforms prior arts in
diverse experimental settings.",2023-03-30
"Humans in Humans Out: On GPT Converging Toward Common Sense in both
  Success and Failure",2023-03-30 10:32:18+00:00,http://arxiv.org/abs/2303.17276v1,"Philipp Koralus, Vincent Wang-Macianica","cs.AI, cs.CL, cs.HC, cs.LG, 00, 68, I.2.0; I.2.6",table2text,"Increase in computational scale and fine-tuning has seen a dramatic
improvement in the quality of outputs of large language models (LLMs) like GPT.
Given that both GPT-3 and GPT-4 were trained on large quantities of
human-generated text, we might ask to what extent their outputs reflect
patterns of human thinking, both for correct and incorrect cases. The Erotetic
Theory of Reason (ETR) provides a symbolic generative model of both human
success and failure in thinking, across propositional, quantified, and
probabilistic reasoning, as well as decision-making. We presented GPT-3,
GPT-3.5, and GPT-4 with 61 central inference and judgment problems from a
recent book-length presentation of ETR, consisting of experimentally verified
data-points on human judgment and extrapolated data-points predicted by ETR,
with correct inference patterns as well as fallacies and framing effects (the
ETR61 benchmark). ETR61 includes classics like Wason's card task, illusory
inferences, the decoy effect, and opportunity-cost neglect, among others. GPT-3
showed evidence of ETR-predicted outputs for 59% of these examples, rising to
77% in GPT-3.5 and 75% in GPT-4. Remarkably, the production of human-like
fallacious judgments increased from 18% in GPT-3 to 33% in GPT-3.5 and 34% in
GPT-4. This suggests that larger and more advanced LLMs may develop a tendency
toward more human-like mistakes, as relevant thought patterns are inherent in
human-produced training data. According to ETR, the same fundamental patterns
are involved both in successful and unsuccessful ordinary reasoning, so that
the ""bad"" cases could paradoxically be learned from the ""good"" cases. We
further present preliminary evidence that ETR-inspired prompt engineering could
reduce instances of these mistakes.",2023-03-30
Foundation Models and Fair Use,2023-03-28 03:58:40+00:00,http://arxiv.org/abs/2303.15715v1,"Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, Percy Liang","cs.CY, cs.AI, cs.LG",table2text,"Existing foundation models are trained on copyrighted material. Deploying
these models can pose both legal and ethical risks when data creators fail to
receive appropriate attribution or compensation. In the United States and
several other countries, copyrighted content may be used to build foundation
models without incurring liability due to the fair use doctrine. However, there
is a caveat: If the model produces output that is similar to copyrighted data,
particularly in scenarios that affect the market of that data, fair use may no
longer apply to the output of the model. In this work, we emphasize that fair
use is not guaranteed, and additional work may be necessary to keep model
development and deployment squarely in the realm of fair use. First, we survey
the potential risks of developing and deploying foundation models based on
copyrighted content. We review relevant U.S. case law, drawing parallels to
existing and potential applications for generating text, source code, and
visual art. Experiments confirm that popular foundation models can generate
content considerably similar to copyrighted material. Second, we discuss
technical mitigations that can help foundation models stay in line with fair
use. We argue that more research is needed to align mitigation strategies with
the current state of the law. Lastly, we suggest that the law and technical
mitigations should co-evolve. For example, coupled with other policy
mechanisms, the law could more explicitly consider safe harbors when strong
technical tools are used to mitigate infringement harms. This co-evolution may
help strike a balance between intellectual property and innovation, which
speaks to the original goal of fair use. But we emphasize that the strategies
we describe here are not a panacea and more work is needed to develop policies
that address the potential harms of foundation models.",2023-03-28
GPT is becoming a Turing machine: Here are some ways to program it,2023-03-25 00:43:41+00:00,http://arxiv.org/abs/2303.14310v1,"Ana Jojic, Zhen Wang, Nebojsa Jojic",cs.CL,table2text,"We demonstrate that, through appropriate prompting, GPT-3 family of models
can be triggered to perform iterative behaviours necessary to execute (rather
than just write or recall) programs that involve loops, including several
popular algorithms found in computer science curricula or software developer
interviews. We trigger execution and description of Iterations by Regimenting
Self-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong
repetitive structure in an example of an execution path of a target program for
one particular input, 2) Prompting with fragments of execution paths, and 3)
Explicitly forbidding (skipping) self-attention to parts of the generated text.
On a dynamic program execution, IRSA leads to larger accuracy gains than
replacing the model with the much more powerful GPT-4. IRSA has promising
applications in education, as the prompts and responses resemble student
assignments in data structures and algorithms classes. Our findings hold
implications for evaluating LLMs, which typically target the in-context
learning: We show that prompts that may not even cover one full task example
can trigger algorithmic behaviour, allowing solving problems previously thought
of as hard for LLMs, such as logical puzzles. Consequently, prompt design plays
an even more critical role in LLM performance than previously recognized.",2023-03-25
CoBIT: A Contrastive Bi-directional Image-Text Generation Model,2023-03-23 17:24:31+00:00,http://arxiv.org/abs/2303.13455v1,"Haoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, Jiahui Yu","cs.CV, cs.CL",table2text,"The field of vision and language has witnessed a proliferation of pre-trained
foundation models. Most existing methods are independently pre-trained with
contrastive objective like CLIP, image-to-text generative objective like PaLI,
or text-to-image generative objective like Parti. However, the three objectives
can be pre-trained on the same data, image-text pairs, and intuitively they
complement each other as contrasting provides global alignment capacity and
generation grants fine-grained understanding. In this work, we present a
Contrastive Bi-directional Image-Text generation model (CoBIT), which attempts
to unify the three pre-training objectives in one framework. Specifically,
CoBIT employs a novel unicoder-decoder structure, consisting of an image
unicoder, a text unicoder and a cross-modal decoder. The image/text unicoders
can switch between encoding and decoding in different tasks, enabling
flexibility and shared knowledge that benefits both image-to-text and
text-to-image generations. CoBIT achieves superior performance in image
understanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)
and text-based content creation, particularly in zero-shot scenarios. For
instance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in
zero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.",2023-03-23
"Paraphrasing evades detectors of AI-generated text, but retrieval is an
  effective defense",2023-03-23 16:29:27+00:00,http://arxiv.org/abs/2303.13408v1,"Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer","cs.CL, cs.CR, cs.LG",table2text,"To detect the deployment of large language models for malicious use cases
(e.g., fake content creation or academic plagiarism), several approaches have
recently been proposed for identifying AI-generated text via watermarks or
statistical irregularities. How robust are these detection algorithms to
paraphrases of AI-generated text? To stress test these detectors, we first
train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase
paragraphs, optionally leveraging surrounding text (e.g., user-written prompts)
as context. DIPPER also uses scalar knobs to control the amount of lexical
diversity and reordering in the paraphrases. Paraphrasing text generated by
three large language models (including GPT3.5-davinci-003) with DIPPER
successfully evades several detectors, including watermarking, GPTZero,
DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the
detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false
positive rate of 1%), without appreciably modifying the input semantics. To
increase the robustness of AI-generated text detection to paraphrase attacks,
we introduce a simple defense that relies on retrieving semantically-similar
generations and must be maintained by a language model API provider. Given a
candidate text, our algorithm searches a database of sequences previously
generated by the API, looking for sequences that match the candidate text
within a certain threshold. We empirically verify our defense using a database
of 15M generations from a fine-tuned T5-XXL model and find that it can detect
80% to 97% of paraphrased generations across different settings, while only
classifying 1% of human-written sequences as AI-generated. We will open source
our code, model and data for future research.",2023-03-23
Compositional Zero-Shot Domain Transfer with Text-to-Text Models,2023-03-23 15:58:41+00:00,http://arxiv.org/abs/2303.13386v1,"Fangyu Liu, Qianchu Liu, Shruthi Bannur, Fernando Prez-Garca, Naoto Usuyama, Sheng Zhang, Tristan Naumann, Aditya Nori, Hoifung Poon, Javier Alvarez-Valle, Ozan Oktay, Stephanie L. Hyland","cs.CL, cs.LG",table2text,"Label scarcity is a bottleneck for improving task performance in specialised
domains. We propose a novel compositional transfer learning framework (DoT5 -
domain compositional zero-shot T5) for zero-shot domain transfer. Without
access to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of
unlabelled in-domain free text) and task knowledge (from task training on more
readily available general-domain data) in a multi-task manner. To improve the
transferability of task training, we design a strategy named NLGU: we
simultaneously train NLG for in-domain label-to-data generation which enables
data augmentation for self-finetuning and NLU for label prediction. We evaluate
DoT5 on the biomedical domain and the resource-lean subdomain of radiology,
focusing on NLI, text summarisation and embedding learning. DoT5 demonstrates
the effectiveness of compositional transfer learning through multi-task
learning. In particular, DoT5 outperforms the current SOTA in zero-shot
transfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with
ablations and a case study demonstrating its ability to solve challenging NLI
examples requiring in-domain expertise.",2023-03-23
JaCoText: A Pretrained Model for Java Code-Text Generation,2023-03-22 19:01:25+00:00,http://arxiv.org/abs/2303.12869v1,"Jessica Lpez Espejel, Mahaman Sanoussi Yahaya Alassan, Walid Dahhane, El Hassane Ettifouri",cs.CL,table2text,"Pretrained transformer-based models have shown high performance in natural
language generation task. However, a new wave of interest has surged: automatic
programming language generation. This task consists of translating natural
language instructions to a programming code. Despite the fact that well-known
pretrained models on language generation have achieved good performance in
learning programming languages, effort is still needed in automatic code
generation. In this paper, we introduce JaCoText, a model based on Transformers
neural network. It aims to generate java source code from natural language
text. JaCoText leverages advantages of both natural language and code
generation models. More specifically, we study some findings from the state of
the art and use them to (1) initialize our model from powerful pretrained
models, (2) explore additional pretraining on our java dataset, (3) carry out
experiments combining the unimodal and bimodal data in the training, and (4)
scale the input and output length during the fine-tuning of the model.
Conducted experiments on CONCODE dataset show that JaCoText achieves new
state-of-the-art results.",2023-03-22
"Chinese Intermediate English Learners outdid ChatGPT in deep cohesion:
  Evidence from English narrative writing",2023-03-21 12:55:54+00:00,http://arxiv.org/abs/2303.11812v1,"Tongquan Zhou, Siyi Cao, Siruo Zhou, Yao Zhang, Aijing He",cs.CL,table2text,"ChatGPT is a publicly available chatbot that can quickly generate texts on
given topics, but it is unknown whether the chatbot is really superior to human
writers in all aspects of writing and whether its writing quality can be
prominently improved on the basis of updating commands. Consequently, this
study compared the writing performance on a narrative topic by ChatGPT and
Chinese intermediate English (CIE) learners so as to reveal the chatbot's
advantage and disadvantage in writing. The data were analyzed in terms of five
discourse components using Coh-Metrix (a special instrument for analyzing
language discourses), and the results revealed that ChatGPT performed better
than human writers in narrativity, word concreteness, and referential cohesion,
but worse in syntactic simplicity and deep cohesion in its initial version.
After more revision commands were updated, while the resulting version was
facilitated in syntactic simplicity, yet it is still lagged far behind CIE
learners' writing in deep cohesion. In addition, the correlation analysis of
the discourse components suggests that narrativity was correlated with
referential cohesion in both ChatGPT and human writers, but the correlations
varied within each group.",2023-03-21
Code-Switching Text Generation and Injection in Mandarin-English ASR,2023-03-20 09:13:27+00:00,http://arxiv.org/abs/2303.10949v1,"Haibin Yu, Yuxuan Hu, Yao Qian, Ma Jin, Linquan Liu, Shujie Liu, Yu Shi, Yanmin Qian, Edward Lin, Michael Zeng","eess.AS, cs.CL, cs.SD",table2text,"Code-switching speech refers to a means of expression by mixing two or more
languages within a single utterance. Automatic Speech Recognition (ASR) with
End-to-End (E2E) modeling for such speech can be a challenging task due to the
lack of data. In this study, we investigate text generation and injection for
improving the performance of an industry commonly-used streaming model,
Transformer-Transducer (T-T), in Mandarin-English code-switching speech
recognition. We first propose a strategy to generate code-switching text data
and then investigate injecting generated text into T-T model explicitly by
Text-To-Speech (TTS) conversion or implicitly by tying speech and text latent
spaces. Experimental results on the T-T model trained with a dataset containing
1,800 hours of real Mandarin-English code-switched speech show that our
approaches to inject generated code-switching text significantly boost the
performance of T-T models, i.e., 16% relative Token-based Error Rate (TER)
reduction averaged on three evaluation sets, and the approach of tying speech
and text latent spaces is superior to that of TTS conversion on the evaluation
set which contains more homogeneous data with the training set.",2023-03-20
"SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language
  Models",2023-03-18 17:56:01+00:00,http://arxiv.org/abs/2303.10464v1,"Vithursan Thangarasa, Abhay Gupta, William Marshall, Tianda Li, Kevin Leong, Dennis DeCoste, Sean Lie, Shreyas Saxena","cs.LG, cs.CL",table2text,"The pre-training and fine-tuning paradigm has contributed to a number of
breakthroughs in Natural Language Processing (NLP). Instead of directly
training on a downstream task, language models are first pre-trained on large
datasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then
fine-tuned on task-specific data (e.g., natural language generation, text
summarization, etc.). Scaling the model and dataset size has helped improve the
performance of LLMs, but unfortunately, this also leads to highly prohibitive
computational costs. Pre-training LLMs often require orders of magnitude more
FLOPs than fine-tuning and the model capacity often remains the same between
the two phases. To achieve training efficiency w.r.t training FLOPs, we propose
to decouple the model capacity between the two phases and introduce Sparse
Pre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits
of using unstructured weight sparsity to train only a subset of weights during
pre-training (Sparse Pre-training) and then recover the representational
capacity by allowing the zeroed weights to learn (Dense Fine-tuning). We
demonstrate that we can induce up to 75% sparsity into a 1.3B parameter GPT-3
XL model resulting in a 2.5x reduction in pre-training FLOPs, without a
significant loss in accuracy on the downstream tasks relative to the dense
baseline. By rigorously evaluating multiple downstream tasks, we also establish
a relationship between sparsity, task complexity, and dataset size. Our work
presents a promising direction to train large GPT models at a fraction of the
training FLOPs using weight sparsity while retaining the benefits of
pre-trained textual representations for downstream tasks.",2023-03-18
HIVE: Harnessing Human Feedback for Instructional Visual Editing,2023-03-16 19:47:41+00:00,http://arxiv.org/abs/2303.09618v1,"Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu","cs.CV, cs.AI, cs.CL, cs.HC, cs.LG",table2text,"Incorporating human feedback has been shown to be crucial to align text
generated by large language models to human preferences. We hypothesize that
state-of-the-art instructional image editing models, where outputs are
generated based on an input image and an editing instruction, could similarly
benefit from human feedback, as their outputs may not adhere to the correct
instructions and preferences of users. In this paper, we present a novel
framework to harness human feedback for instructional visual editing (HIVE).
Specifically, we collect human feedback on the edited images and learn a reward
function to capture the underlying user preferences. We then introduce scalable
diffusion model fine-tuning methods that can incorporate human preferences
based on the estimated reward. Besides, to mitigate the bias brought by the
limitation of data, we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset to boost the
performance of instructional image editing. We conduct extensive empirical
experiments quantitatively and qualitatively, showing that HIVE is favored over
previous state-of-the-art instructional image editing approaches by a large
margin.",2023-03-16
Input-length-shortening and text generation via attention values,2023-03-14 02:11:24+00:00,http://arxiv.org/abs/2303.07585v1,"Neet zkan Tan, Alex Yuxuan Peng, Joshua Bensemann, Qiming Bao, Tim Hartill, Mark Gahegan, Michael Witbrock",cs.CL,table2text,"Identifying words that impact a task's performance more than others is a
challenge in natural language processing. Transformers models have recently
addressed this issue by incorporating an attention mechanism that assigns
greater attention (i.e., relevance) scores to some words than others. Because
of the attention mechanism's high computational cost, transformer models
usually have an input-length limitation caused by hardware constraints. This
limitation applies to many transformers, including the well-known bidirectional
encoder representations of the transformer (BERT) model. In this paper, we
examined BERT's attention assignment mechanism, focusing on two questions: (1)
How can attention be employed to reduce input length? (2) How can attention be
used as a control mechanism for conditional text generation? We investigated
these questions in the context of a text classification task. We discovered
that BERT's early layers assign more critical attention scores for text
classification tasks compared to later layers. We demonstrated that the first
layer's attention sums could be used to filter tokens in a given sequence,
considerably decreasing the input length while maintaining good test accuracy.
We also applied filtering, which uses a compute-efficient semantic similarities
algorithm, and discovered that retaining approximately 6\% of the original
sequence is sufficient to obtain 86.5\% accuracy. Finally, we showed that we
could generate data in a stable manner and indistinguishable from the original
one by only using a small percentage (10\%) of the tokens with high attention
scores according to BERT's first layer.",2023-03-14
Diffusion Models for Non-autoregressive Text Generation: A Survey,2023-03-12 05:11:09+00:00,http://arxiv.org/abs/2303.06574v1,"Yifan Li, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen",cs.CL,table2text,"Non-autoregressive (NAR) text generation has attracted much attention in the
field of natural language processing, which greatly reduces the inference
latency but has to sacrifice the generation accuracy. Recently, diffusion
models, a class of latent variable generative models, have been introduced into
NAR text generation, showing improved generation quality. In this survey, we
review the recent progress in diffusion models for NAR text generation. As the
background, we first present the general definition of diffusion models and the
text diffusion models, and then discuss their merits for NAR generation. As the
core content, we further introduce two mainstream diffusion models in existing
text diffusion works, and review the key designs of the diffusion process.
Moreover, we discuss the utilization of pre-trained language models (PLMs) for
text diffusion models and introduce optimization techniques for text data.
Finally, we discuss several promising directions and conclude this paper. Our
survey aims to provide researchers with a systematic reference of related
research on text diffusion models for NAR generation.",2023-03-12
"ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and
  Multilingual Natural Language Generation",2023-03-11 17:14:33+00:00,http://arxiv.org/abs/2303.06458v1,"Bang Yang, Fenglin Liu, Yuexian Zou, Xian Wu, Yaowei Wang, David A. Clifton","cs.CL, cs.AI, cs.CV",table2text,"Natural Language Generation (NLG) accepts input data in the form of images,
videos, or text and generates corresponding natural language text as output.
Existing NLG methods mainly adopt a supervised approach and rely heavily on
coupled data-to-text pairs. However, for many targeted scenarios and for
non-English languages, sufficient quantities of labeled data are often not
available. To relax the dependency on labeled data of downstream tasks, we
propose an intuitive and effective zero-shot learning framework, ZeroNLG, which
can deal with multiple NLG tasks, including image-to-text (image captioning),
video-to-text (video captioning), and text-to-text (neural machine
translation), across English, Chinese, German, and French within a unified
framework. ZeroNLG does not require any labeled downstream pairs for training.
During training, ZeroNLG (i) projects different domains (across modalities and
languages) to corresponding coordinates in a shared common latent space; (ii)
bridges different domains by aligning their corresponding coordinates in this
space; and (iii) builds an unsupervised multilingual auto-encoder to learn to
generate text by reconstructing the input text given its coordinate in shared
latent space. Consequently, during inference, based on the data-to-text
pipeline, ZeroNLG can generate target sentences across different languages
given the coordinate of input data in the common space. Within this unified
framework, given visual (imaging or video) data as input, ZeroNLG can perform
zero-shot visual captioning; given textual sentences as input, ZeroNLG can
perform zero-shot machine translation. We present the results of extensive
experiments on twelve NLG tasks, showing that, without using any labeled
downstream pairs for training, ZeroNLG generates high-quality and believable
outputs and significantly outperforms existing zero-shot methods.",2023-03-11
"Reinforcement Learning-based Counter-Misinformation Response Generation:
  A Case Study of COVID-19 Vaccine Misinformation",2023-03-11 15:55:01+00:00,http://arxiv.org/abs/2303.06433v1,"Bing He, Mustaque Ahamad, Srijan Kumar","cs.SI, cs.LG",table2text,"The spread of online misinformation threatens public health, democracy, and
the broader society. While professional fact-checkers form the first line of
defense by fact-checking popular false claims, they do not engage directly in
conversations with misinformation spreaders. On the other hand, non-expert
ordinary users act as eyes-on-the-ground who proactively counter misinformation
-- recent research has shown that 96% counter-misinformation responses are made
by ordinary users. However, research also found that 2/3 times, these responses
are rude and lack evidence. This work seeks to create a counter-misinformation
response generation model to empower users to effectively correct
misinformation. This objective is challenging due to the absence of datasets
containing ground-truth of ideal counter-misinformation responses, and the lack
of models that can generate responses backed by communication theories. In this
work, we create two novel datasets of misinformation and counter-misinformation
response pairs from in-the-wild social media and crowdsourcing from
college-educated students. We annotate the collected data to distinguish poor
from ideal responses that are factual, polite, and refute misinformation. We
propose MisinfoCorrect, a reinforcement learning-based framework that learns to
generate counter-misinformation responses for an input misinformation post. The
model rewards the generator to increase the politeness, factuality, and
refutation attitude while retaining text fluency and relevancy. Quantitative
and qualitative evaluation shows that our model outperforms several baselines
by generating high-quality counter-responses. This work illustrates the promise
of generative text models for social good -- here, to help create a safe and
reliable information ecosystem. The code and data is accessible on
https://github.com/claws-lab/MisinfoCorrect.",2023-03-11
An Overview on Language Models: Recent Developments and Outlook,2023-03-10 07:55:00+00:00,http://arxiv.org/abs/2303.05759v1,"Chengwei Wei, Yun-Cheng Wang, Bin Wang, C. -C. Jay Kuo",cs.CL,table2text,"Language modeling studies the probability distributions over strings of
texts. It is one of the most fundamental tasks in natural language processing
(NLP). It has been widely used in text generation, speech recognition, machine
translation, etc. Conventional language models (CLMs) aim to predict the
probability of linguistic sequences in a causal manner. In contrast,
pre-trained language models (PLMs) cover broader concepts and can be used in
both causal sequential modeling and fine-tuning for downstream applications.
PLMs have their own training paradigms (usually self-supervised) and serve as
foundation models in modern NLP systems. This overview paper provides an
introduction to both CLMs and PLMs from five aspects, i.e., linguistic units,
structures, training methods, evaluation methods, and applications.
Furthermore, we discuss the relationship between CLMs and PLMs and shed light
on the future directions of language modeling in the pre-trained era.",2023-03-10
Is ChatGPT a Good NLG Evaluator? A Preliminary Study,2023-03-07 16:57:20+00:00,http://arxiv.org/abs/2303.04048v1,"Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou","cs.CL, cs.AI",table2text,"Recently, the emergence of ChatGPT has attracted wide attention from the
computational linguistics community. Many prior studies have shown that ChatGPT
achieves remarkable performance on various NLP tasks in terms of automatic
evaluation metrics. However, the ability of ChatGPT to serve as an evaluation
metric is still underexplored. Considering assessing the quality of NLG models
is an arduous task and previous statistical metrics notoriously show their poor
correlation with human judgments, we wonder whether ChatGPT is a good NLG
evaluation metric. In this report, we provide a preliminary meta-evaluation on
ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT
as a human evaluator and give task-specific (e.g., summarization) and
aspect-specific (e.g., relevance) instruction to prompt ChatGPT to score the
generation of NLG models. We conduct experiments on three widely-used NLG
meta-evaluation datasets (including summarization, story generation and
data-to-text tasks). Experimental results show that compared with previous
automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation
with golden human judgments. We hope our preliminary study could prompt the
emergence of a general-purposed reliable NLG metric.",2023-03-07
"Large Language Models as Zero-Shot Human Models for Human-Robot
  Interaction",2023-03-06 23:16:24+00:00,http://arxiv.org/abs/2303.03548v1,"Bowen Zhang, Harold Soh","cs.RO, cs.CL, cs.HC, cs.LG",table2text,"Human models play a crucial role in human-robot interaction (HRI), enabling
robots to consider the impact of their actions on people and plan their
behavior accordingly. However, crafting good human models is challenging;
capturing context-dependent human behavior requires significant prior knowledge
and/or large amounts of interaction data, both of which are difficult to
obtain. In this work, we explore the potential of large-language models (LLMs)
-- which have consumed vast amounts of human-generated text data -- to act as
zero-shot human models for HRI. Our experiments on three social datasets yield
promising results; the LLMs are able to achieve performance comparable to
purpose-built models. That said, we also discuss current limitations, such as
sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our
findings, we demonstrate how LLM-based human models can be integrated into a
social robot's planning process and applied in HRI scenarios. Specifically, we
present one case study on a simulated trust-based table-clearing task and
replicate past results that relied on custom models. Next, we conduct a new
robot utensil-passing experiment (n = 65) where preliminary results show that
planning with a LLM-based human model can achieve gains over a basic myopic
plan. In summary, our results show that LLMs offer a promising (but incomplete)
approach to human modeling for HRI.",2023-03-06
"DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only
  Training",2023-03-06 11:02:47+00:00,http://arxiv.org/abs/2303.03032v1,"Wei Li, Linchao Zhu, Longyin Wen, Yi Yang","cs.CV, cs.AI, cs.CL",table2text,"Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong
zero-shot transfer capability in many discriminative tasks. Their adaptation to
zero-shot image-conditioned text generation tasks has drawn increasing
interest. Prior arts approach to zero-shot captioning by either utilizing the
existing large language models (e.g., GPT-2) or pre-training the
encoder-decoder network in an end-to-end manner. In this work, we propose a
simple framework, named DeCap, for zero-shot captioning. We introduce a
lightweight visual-aware language decoder. This decoder is both data-efficient
and computation-efficient: 1) it only requires the text data for training,
easing the burden on the collection of paired data. 2) it does not require
end-to-end training. When trained with text-only data, the decoder takes the
text embedding extracted from the off-the-shelf CLIP encoder as a prefix
embedding. The challenge is that the decoder is trained on the text corpus but
at the inference stage, it needs to generate captions based on visual inputs.
The modality gap issue is widely observed in multi-modal contrastive models
that prevents us from directly taking the visual embedding as the prefix
embedding. We propose a training-free mechanism to reduce the modality gap. We
project the visual embedding into the CLIP text embedding space, while the
projected embedding retains the information of the visual input. Taking the
projected embedding as the prefix embedding, the decoder generates high-quality
descriptions that match the visual input. The experiments show that DeCap
outperforms other zero-shot captioning methods and unpaired captioning methods
on the typical image captioning benchmarks, i.e., MSCOCO and NoCaps.",2023-03-06
"UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data
  Generation for Cross-Lingual Learning in Tweet Intimacy Prediction",2023-03-02 12:18:53+00:00,http://arxiv.org/abs/2303.01194v1,"Andrianos Michail, Stefanos Konstantinou, Simon Clematide","cs.CL, cs.AI, 68T50",table2text,"This paper describes the submission of UZH_CLyp for the SemEval 2023 Task 9
""Multilingual Tweet Intimacy Analysis"". We achieved second-best results in all
10 languages according to the official Pearson's correlation regression
evaluation measure. Our cross-lingual transfer learning approach explores the
benefits of using a Head-First Fine-Tuning method (HeFiT) that first updates
only the regression head parameters and then also updates the pre-trained
transformer encoder parameters at a reduced learning rate. Additionally, we
study the impact of using a small set of automatically generated examples (in
our case, from ChatGPT) for low-resource settings where no human-labeled data
is available. Our study shows that HeFiT stabilizes training and consistently
improves results for pre-trained models that lack domain adaptation to tweets.
Our study also shows a noticeable performance increase in cross-lingual
learning when synthetic data is used, confirming the usefulness of current text
generation systems to improve zero-shot baseline results. Finally, we examine
how possible inconsistencies in the annotated data contribute to cross-lingual
interference issues.",2023-03-02
A Universal Question-Answering Platform for Knowledge Graphs,2023-03-01 15:35:32+00:00,http://arxiv.org/abs/2303.00595v1,"Reham Omar, Ishika Dhall, Panos Kalnis, Essam Mansour","cs.AI, cs.CL, cs.DB",table2text,"Knowledge from diverse application domains is organized as knowledge graphs
(KGs) that are stored in RDF engines accessible in the web via SPARQL
endpoints. Expressing a well-formed SPARQL query requires information about the
graph structure and the exact URIs of its components, which is impractical for
the average user. Question answering (QA) systems assist by translating natural
language questions to SPARQL. Existing QA systems are typically based on
application-specific human-curated rules, or require prior information,
expensive pre-processing and model adaptation for each targeted KG. Therefore,
they are hard to generalize to a broad set of applications and KGs.
  In this paper, we propose KGQAn, a universal QA system that does not need to
be tailored to each target KG. Instead of curated rules, KGQAn introduces a
novel formalization of question understanding as a text generation problem to
convert a question into an intermediate abstract representation via a neural
sequence-to-sequence model. We also develop a just-in-time linker that maps at
query time the abstract representation to a SPARQL query for a specific KG,
using only the publicly accessible APIs and the existing indices of the RDF
store, without requiring any pre-processing. Our experiments with several real
KGs demonstrate that KGQAn is easily deployed and outperforms by a large margin
the state-of-the-art in terms of quality of answers and processing time,
especially for arbitrary KGs, unseen during the training.",2023-03-01
TabGenie: A Toolkit for Table-to-Text Generation,2023-02-27 22:05:46+00:00,http://arxiv.org/abs/2302.14169v1,"Zdenk Kasner, Ekaterina Garanina, Ondej Pltek, Ondej Duek",cs.CL,table2text,"Heterogenity of data-to-text generation datasets limits the research on
data-to-text generation systems. We present TabGenie - a toolkit which enables
researchers to explore, preprocess, and analyze a variety of data-to-text
generation datasets through the unified framework of table-to-text generation.
In TabGenie, all the inputs are represented as tables with associated metadata.
The tables can be explored through the web interface, which also provides an
interactive mode for debugging table-to-text generation, facilitates
side-by-side comparison of generated system outputs, and allows easy exports
for manual analysis. Furthermore, TabGenie is equipped with command line
processing tools and Python bindings for unified dataset loading and
processing. We release TabGenie as a PyPI package and provide its open-source
code and a live demo at https://github.com/kasnerz/tabgenie.",2023-02-27
Tailoring Language Generation Models under Total Variation Distance,2023-02-26 16:32:52+00:00,http://arxiv.org/abs/2302.13344v1,"Haozhe Ji, Pei Ke, Zhipeng Hu, Rongsheng Zhang, Minlie Huang",cs.CL,table2text,"The standard paradigm of neural language generation adopts maximum likelihood
estimation (MLE) as the optimizing method. From a distributional view, MLE in
fact minimizes the Kullback-Leibler divergence (KLD) between the distribution
of the real data and that of the model. However, this approach forces the model
to distribute non-zero (sometimes large) probability mass to all training
samples regardless of their quality. Moreover, in the attempt to cover the
low-probability regions in the data distribution, the model systematically
overestimates the probability of corrupted text sequences, which we conjecture
is one of the main reasons for text degeneration during autoregressive
decoding. To remedy this problem, we leverage the total variation distance
(TVD) with its robustness to outliers, and develop practical bounds to apply it
to language generation. Then, we introduce the TaiLr objective that balances
the tradeoff of estimating TVD. Intuitively, TaiLr downweights real data
samples that have low model probabilities with tunable penalization intensity.
Experimental results show that our method alleviates the overestimation of
degenerated sequences without sacrificing diversity and improves generation
quality on a wide range of text generation tasks.",2023-02-26
Few-Shot Table-to-Text Generation with Prompt-based Adapter,2023-02-24 05:48:53+00:00,http://arxiv.org/abs/2302.12468v1,"Zhixin Guo, Minyxuan Yan, Jiexing Qi, Jianping Zhou, Ziwei He, Zhouhan Lin, Guanjie Zheng, Xinbing Wang",cs.CL,table2text,"Pre-trained language models (PLMs) have made remarkable progress in
table-to-text generation tasks. However, the topological gap between tabular
data and text and the lack of domain-specific knowledge make it difficult for
PLMs to produce faithful text, especially in real-world applications with
limited resources. In this paper, we mitigate the above challenges by
introducing a novel augmentation method: Prompt-based Adapter (PA), which
targets table-to-text generation under few-shot conditions. The core insight
design of the PA is to inject prompt templates for augmenting domain-specific
knowledge and table-related representations into the model for bridging the
structural gap between tabular data and descriptions through adapters. Such
prompt-based knowledge augmentation method brings at least two benefits: (1)
enables us to fully use the large amounts of unlabelled domain-specific
knowledge, which can alleviate the PLMs' inherent shortcomings of lacking
domain knowledge; (2) allows us to design different types of tasks supporting
the generative challenge. Extensive experiments and analyses are conducted on
three open-domain few-shot NLG datasets: Humans, Books, and Songs. Compared to
previous state-of-the-art approaches, our model achieves superior performance
in terms of both fluency and accuracy as judged by human and automatic
evaluations.",2023-02-24
Improved Training of Mixture-of-Experts Language GANs,2023-02-23 09:25:46+00:00,http://arxiv.org/abs/2302.11875v1,"Yekun Chai, Qiyue Yin, Junge Zhang",cs.CL,table2text,"Despite the dramatic success in image generation, Generative Adversarial
Networks (GANs) still face great challenges in synthesizing sequences of
discrete elements, in particular human language. The difficulty in generator
training arises from the limited representation capacity and uninformative
learning signals obtained from the discriminator. In this work, we (1) first
empirically show that the mixture-of-experts approach is able to enhance the
representation capacity of the generator for language GANs and (2) harness the
Feature Statistics Alignment (FSA) paradigm to render fine-grained learning
signals to advance the generator training. Specifically, FSA forces the mean
statistics of the distribution of fake data to approach that of real samples as
close as possible in the finite-dimensional feature space. Empirical study on
synthetic and real benchmarks shows the superior performance in quantitative
evaluation and demonstrates the effectiveness of our approach to adversarial
text generation.",2023-02-23
Improving User Controlled Table-To-Text Generation Robustness,2023-02-20 07:51:15+00:00,http://arxiv.org/abs/2302.09820v1,"Hanxu Hu, Yunqing Liu, Zhongyi Yu, Laura Perez-Beltrachini",cs.CL,table2text,"In this work we study user controlled table-to-text generation where users
explore the content in a table by selecting cells and reading a natural
language description thereof automatically produce by a natural language
generator. Such generation models usually learn from carefully selected cell
combinations (clean cell selections); however, in practice users may select
unexpected, redundant, or incoherent cell combinations (noisy cell selections).
In experiments, we find that models perform well on test sets coming from the
same distribution as the train data but their performance drops when evaluated
on realistic noisy user inputs. We propose a fine-tuning regime with additional
user-simulated noisy cell selections. Models fine-tuned with the proposed
regime gain 4.85 BLEU points on user noisy test cases and 1.4 on clean test
cases; and achieve comparable state-of-the-art performance on the ToTTo
dataset.",2023-02-20
Do We Still Need Clinical Language Models?,2023-02-16 05:08:34+00:00,http://arxiv.org/abs/2302.08091v1,"Eric Lehman, Evan Hernandez, Diwakar Mahajan, Jonas Wulff, Micah J. Smith, Zachary Ziegler, Daniel Nadler, Peter Szolovits, Alistair Johnson, Emily Alsentzer",cs.CL,table2text,"Although recent advances in scaling large language models (LLMs) have
resulted in improvements on many NLP tasks, it remains unclear whether these
models trained primarily with general web text are the right tool in highly
specialized, safety critical domains such as clinical text. Recent results have
suggested that LLMs encode a surprising amount of medical knowledge. This
raises an important question regarding the utility of smaller domain-specific
language models. With the success of general-domain LLMs, is there still a need
for specialized clinical models? To investigate this question, we conduct an
extensive empirical analysis of 12 language models, ranging from 220M to 175B
parameters, measuring their performance on 3 different clinical tasks that test
their ability to parse and reason over electronic health records. As part of
our experiments, we train T5-Base and T5-Large models from scratch on clinical
notes from MIMIC III and IV to directly investigate the efficiency of clinical
tokens. We show that relatively small specialized clinical models substantially
outperform all in-context learning approaches, even when finetuned on limited
annotated data. Further, we find that pretraining on clinical tokens allows for
smaller, more parameter-efficient models that either match or outperform much
larger language models trained on general text. We release the code and the
models used under the PhysioNet Credentialed Health Data license and data use
agreement.",2023-02-16
"Tree-Based Representation and Generation of Natural and Mathematical
  Language",2023-02-15 22:38:34+00:00,http://arxiv.org/abs/2302.07974v1,"Alexander Scarlatos, Andrew Lan",cs.CL,table2text,"Mathematical language in scientific communications and educational scenarios
is important yet relatively understudied compared to natural languages. Recent
works on mathematical language focus either on representing stand-alone
mathematical expressions, especially in their natural tree format, or
mathematical reasoning in pre-trained natural language models. Existing works
on jointly modeling and generating natural and mathematical languages simply
treat mathematical expressions as text, without accounting for the rigid
structural properties of mathematical expressions. In this paper, we propose a
series of modifications to existing language models to jointly represent and
generate text and math: representing mathematical expressions as sequences of
node tokens in their operator tree format, using math symbol and tree position
embeddings to preserve the semantic and structural properties of mathematical
expressions, and using a constrained decoding method to generate mathematically
valid expressions. We ground our modifications in GPT-2, resulting in a model
MathGPT, and demonstrate that it outperforms baselines on mathematical
expression generation tasks.",2023-02-15
"AutoBiasTest: Controllable Sentence Generation for Automated and
  Open-Ended Social Bias Testing in Language Models",2023-02-14 22:07:57+00:00,http://arxiv.org/abs/2302.07371v1,"Rafal Kocielnik, Shrimai Prabhumoye, Vivian Zhang, R. Michael Alvarez, Anima Anandkumar","cs.CL, cs.CY, 68T50, I.2.7; J.5; K.4.1",table2text,"Social bias in Pretrained Language Models (PLMs) affects text generation and
other downstream NLP tasks. Existing bias testing methods rely predominantly on
manual templates or on expensive crowd-sourced data. We propose a novel
AutoBiasTest method that automatically generates sentences for testing bias in
PLMs, hence providing a flexible and low-cost alternative. Our approach uses
another PLM for generation and controls the generation of sentences by
conditioning on social group and attribute terms. We show that generated
sentences are natural and similar to human-produced content in terms of word
length and diversity. We illustrate that larger models used for generation
produce estimates of social bias with lower variance. We find that our bias
scores are well correlated with manual templates, but AutoBiasTest highlights
biases not captured by these templates due to more diverse and realistic test
sentences. By automating large-scale test sentence generation, we enable better
estimation of underlying bias distributions",2023-02-14
Large Scale Multi-Lingual Multi-Modal Summarization Dataset,2023-02-13 18:00:23+00:00,http://arxiv.org/abs/2302.06560v1,"Yash Verma, Anubhav Jangra, Raghvendra Kumar, Sriparna Saha","cs.CL, cs.MM",table2text,"Significant developments in techniques such as encoder-decoder models have
enabled us to represent information comprising multiple modalities. This
information can further enhance many downstream tasks in the field of
information retrieval and natural language processing; however, improvements in
multi-modal techniques and their performance evaluation require large-scale
multi-modal data which offers sufficient diversity. Multi-lingual modeling for
a variety of tasks like multi-modal summarization, text generation, and
translation leverages information derived from high-quality multi-lingual
annotated data. In this work, we present the current largest multi-lingual
multi-modal summarization dataset (M3LS), and it consists of over a million
instances of document-image pairs along with a professionally annotated
multi-modal summary for each pair. It is derived from news articles published
by British Broadcasting Corporation(BBC) over a decade and spans 20 languages,
targeting diversity across five language roots, it is also the largest
summarization dataset for 13 languages and consists of cross-lingual
summarization data for 2 languages. We formally define the multi-lingual
multi-modal summarization task utilizing our dataset and report baseline scores
from various state-of-the-art summarization techniques in a multi-lingual
setting. We also compare it with many similar datasets to analyze the
uniqueness and difficulty of M3LS.",2023-02-13
"Combined Location Online Weather Data: Easy-to-use Targeted Weather
  Analysis for Agriculture",2023-02-13 07:03:53+00:00,http://arxiv.org/abs/2302.06142v1,"Darren Yates, Christopher Blanchard, Allister Clarke, Sabih-Ur Rehman, Md Zahidul Islam, Russell Ford, Rob Walsh","cs.SI, J.2",table2text,"The continuing effects of climate change require farmers and growers to have
greater understanding of how these changes affect crop production. However,
while climatic data is generally available to help provide much of that
understanding, it can often be in a form not easy to digest. The proposed
Combined Location Online Weather Data (CLOWD) framework is an easy-to-use
online platform for analysing recent and historical weather data of any
location within Australia at the click of a map. CLOWD requires no programming
skills and operates in any HTML5 web browser on PC and mobile devices. It
enables comparison between current and previous growing seasons over a range of
environmental parameters, and can create a plain-English PDF report for offline
use, using natural language generation (NLG). This paper details the platform,
the design decisions taken and outlines how farmers and growers can use CLOWD
to better understand current growing conditions. Prototypes of CLOWD are now
online for PCs and smartphones.",2023-02-13
"Investigating the Effect of Relative Positional Embeddings on
  AMR-to-Text Generation with Structural Adapters",2023-02-12 12:43:36+00:00,http://arxiv.org/abs/2302.05900v1,"Sebastien Montella, Alexis Nasr, Johannes Heinecke, Frederic Bechet, Lina M. Rojas-Barahona",cs.CL,table2text,"Text generation from Abstract Meaning Representation (AMR) has substantially
benefited from the popularized Pretrained Language Models (PLMs). Myriad
approaches have linearized the input graph as a sequence of tokens to fit the
PLM tokenization requirements. Nevertheless, this transformation jeopardizes
the structural integrity of the graph and is therefore detrimental to its
resulting representation. To overcome this issue, Ribeiro et al. have recently
proposed StructAdapt, a structure-aware adapter which injects the input graph
connectivity within PLMs using Graph Neural Networks (GNNs). In this paper, we
investigate the influence of Relative Position Embeddings (RPE) on AMR-to-Text,
and, in parallel, we examine the robustness of StructAdapt. Through ablation
studies, graph attack and link prediction, we reveal that RPE might be
partially encoding input graphs. We suggest further research regarding the role
of RPE will provide valuable insights for Graph-to-Text generation.",2023-02-12
Plan-then-Seam: Towards Efficient Table-to-Text Generation,2023-02-10 09:43:15+00:00,http://arxiv.org/abs/2302.05138v1,"Liang Li, Ruiying Geng, Chengyang Fang, Bing Li, Can Ma, Binhua Li, Yongbin Li",cs.CL,table2text,"Table-to-text generation aims at automatically generating text to help people
conveniently obtain salient information in tables. Recent works explicitly
decompose the generation process into content planning and surface generation
stages, employing two autoregressive networks for them respectively. However,
they are computationally expensive due to the non-parallelizable nature of
autoregressive decoding and the redundant parameters of two networks. In this
paper, we propose the first totally non-autoregressive table-to-text model
(Plan-then-Seam, PTS) that produces its outputs in parallel with one single
network. PTS firstly writes and calibrates one plan of the content to be
generated with a novel rethinking pointer predictor, and then takes the plan as
the context for seaming to decode the description. These two steps share
parameters and perform iteratively to capture token inter-dependency while
keeping parallel decoding. Experiments on two public benchmarks show that PTS
achieves 3.0~5.6 times speedup for inference time, reducing 50% parameters,
while maintaining as least comparable performance against strong two-stage
table-to-text competitors.",2023-02-10
"Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot
  Image Captioning",2023-02-09 18:57:56+00:00,http://arxiv.org/abs/2302.04858v1,"Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhiding Yu, Shiyi Lan, Bo Li, Ming-Yu Liu, Yuke Zhu, Mohammad Shoeybi, Bryan Catanzaro, Chaowei Xiao, Anima Anandkumar","cs.CV, cs.AI, cs.CL, cs.IR, cs.LG",table2text,"Augmenting pretrained language models (LMs) with a vision encoder (e.g.,
Flamingo) has obtained state-of-the-art results in image-to-text generation.
However, these models store all the knowledge within their parameters, thus
often requiring enormous model parameters to model the abundant visual concepts
and very rich textual descriptions. Additionally, they are inefficient in
incorporating new data, requiring a computational-expensive fine-tuning
process. In this work, we introduce a Retrieval-augmented Visual Language
Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant
knowledge from the external database for zero and in-context few-shot
image-to-text generations. By storing certain knowledge explicitly in the
external database, our approach reduces the number of model parameters and can
easily accommodate new data during evaluation by simply updating the database.
We also construct an interleaved image and text data that facilitates
in-context few-shot learning capabilities. We demonstrate that Re-ViLM
significantly boosts performance for image-to-text generation tasks, especially
for zero-shot and few-shot generation in out-of-domain settings with 4 times
less parameters compared with baseline methods.",2023-02-09
Lightweight Transformers for Clinical Natural Language Processing,2023-02-09 16:07:31+00:00,http://arxiv.org/abs/2302.04725v1,"Omid Rohanian, Mohammadmahdi Nouriborji, Hannah Jauncey, Samaneh Kouchaki, ISARIC Clinical Characterisation Group, Lei Clifton, Laura Merson, David A. Clifton","cs.CL, cs.AI, cs.LG, 68T50, I.2.7",table2text,"Specialised pre-trained language models are becoming more frequent in NLP
since they can potentially outperform models trained on generic texts. BioBERT
and BioClinicalBERT are two examples of such models that have shown promise in
medical NLP tasks. Many of these models are overparametrised and
resource-intensive, but thanks to techniques like Knowledge Distillation (KD),
it is possible to create smaller versions that perform almost as well as their
larger counterparts. In this work, we specifically focus on development of
compact language models for processing clinical texts (i.e. progress notes,
discharge summaries etc). We developed a number of efficient lightweight
clinical transformers using knowledge distillation and continual learning, with
the number of parameters ranging from 15 million to 65 million. These models
performed comparably to larger models such as BioBERT and ClinicalBioBERT and
significantly outperformed other compact models trained on general or
biomedical data. Our extensive evaluation was done across several standard
datasets and covered a wide range of clinical text-mining tasks, including
Natural Language Inference, Relation Extraction, Named Entity Recognition, and
Sequence Classification. To our knowledge, this is the first comprehensive
study specifically focused on creating efficient and compact transformers for
clinical NLP tasks. The models and code used in this study can be found on our
Huggingface profile at https://huggingface.co/nlpie and Github page at
https://github.com/nlpie-research/Lightweight-Clinical-Transformers,
respectively, promoting reproducibility of our results.",2023-02-09
"Auto-Learning: An Adversarial Process of Two Pre-trained Models for
  Natural Language Generation",2023-02-08 06:09:55+00:00,http://arxiv.org/abs/2302.03896v1,"Zhengqing Yuan, Yuelin Lu, Chao Zhang, Huiwen Xue",cs.CL,table2text,"Pre-trained models have been used in many fields in recent years, ranging
from natural language understanding to computer vision and natural language
generation. However, the performance of these natural language generation
models is overly dependent on the scale of the model and the size of the
dataset. While the larger language model is excellent in some respects, it
cannot learn up-to-date knowledge and is relatively difficult to relearn. In
this paper, a new adversarial process learning method called Auto-Learning.
This can improve the performance of any natural language generation model
without the help of additional datasets. Auto-Learning includes two models: $G$
is a text generation model and $D$ can test whether the data generated by G is
legitimate. Firstly, the fine-tuned $D$ model is used as the brain's knowledge
base before the process. Then the text generated by the $G$ model is used as
the input of $D$ to determine whether the text is legitimate or not. Finally,
$G$ is fine-tuned according to the output of $D$. This adversarial process is
like a self-escalation of the brain through some a priori knowledge. When this
adversarial system wants to learn something new, simply fine-tune the $D$
model. Our approach applies to Autoregressive Language Modeling for all
Transformer classes. The results are good in existing experimental tasks,
including more grammatical text generation and better performance on some text
comprehension tasks.",2023-02-08
What Matters In The Structured Pruning of Generative Language Models?,2023-02-07 22:05:55+00:00,http://arxiv.org/abs/2302.03773v1,"Michael Santacroce, Zixin Wen, Yelong Shen, Yuanzhi Li","cs.CL, cs.LG",table2text,"Auto-regressive large language models such as GPT-3 require enormous
computational resources to use. Traditionally, structured pruning methods are
employed to reduce resource usage. However, their application to and efficacy
for generative language models is heavily under-explored. In this paper we
conduct an comprehensive evaluation of common structured pruning methods,
including magnitude, random, and movement pruning on the feed-forward layers in
GPT-type models. Unexpectedly, random pruning results in performance that is
comparable to the best established methods, across multiple natural language
generation tasks. To understand these results, we provide a framework for
measuring neuron-level redundancy of models pruned by different methods, and
discover that established structured pruning methods do not take into account
the distinctiveness of neurons, leaving behind excess redundancies. In view of
this, we introduce Globally Unique Movement (GUM) to improve the uniqueness of
neurons in pruned models. We then discuss the effects of our techniques on
different redundancy metrics to explain the improved performance.",2023-02-07
"Unleashing the True Potential of Sequence-to-Sequence Models for
  Sequence Tagging and Structure Parsing",2023-02-05 01:37:26+00:00,http://arxiv.org/abs/2302.02275v1,"Han He, Jinho D. Choi",cs.CL,table2text,"Sequence-to-Sequence (S2S) models have achieved remarkable success on various
text generation tasks. However, learning complex structures with S2S models
remains challenging as external neural modules and additional lexicons are
often supplemented to predict non-textual outputs. We present a systematic
study of S2S modeling using contained decoding on four core tasks:
part-of-speech tagging, named entity recognition, constituency and dependency
parsing, to develop efficient exploitation methods costing zero extra
parameters. In particular, 3 lexically diverse linearization schemas and
corresponding constrained decoding methods are designed and evaluated.
Experiments show that although more lexicalized schemas yield longer output
sequences that require heavier training, their sequences being closer to
natural language makes them easier to learn. Moreover, S2S models using our
constrained decoding outperform other S2S approaches using external resources.
Our best models perform better than or comparably to the state-of-the-art for
all 4 tasks, lighting a promise for S2S models to generate non-sequential
structures.",2023-02-05
Grounding Language Models to Images for Multimodal Generation,2023-01-31 18:33:44+00:00,http://arxiv.org/abs/2301.13823v1,"Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried","cs.CL, cs.AI, cs.CV, cs.LG",table2text,"We propose an efficient method to ground pretrained text-only language models
to the visual domain, enabling them to process and generate arbitrarily
interleaved image-and-text data. Our method leverages the abilities of language
models learnt from large scale text-only pretraining, such as in-context
learning and free-form text generation. We keep the language model frozen, and
finetune input and output linear layers to enable cross-modality interactions.
This allows our model to process arbitrarily interleaved image-and-text inputs,
and generate free-form text interleaved with retrieved images. We achieve
strong zero-shot performance on grounded tasks such as contextual image
retrieval and multimodal dialogue, and showcase compelling interactive
abilities. Our approach works with any off-the-shelf language model and paves
the way towards an effective, general solution for leveraging pretrained
language models in visually grounded settings.",2023-01-31
Semi-Parametric Video-Grounded Text Generation,2023-01-27 03:00:43+00:00,http://arxiv.org/abs/2301.11507v1,"Sungdong Kim, Jin-Hwa Kim, Jiyoung Lee, Minjoon Seo","cs.CV, cs.CL, cs.LG",table2text,"Efficient video-language modeling should consider the computational cost
because of a large, sometimes intractable, number of video frames. Parametric
approaches such as the attention mechanism may not be ideal since its
computational cost quadratically increases as the video length increases.
Rather, previous studies have relied on offline feature extraction or frame
sampling to represent the video efficiently, focusing on cross-modal modeling
in short video clips. In this paper, we propose a semi-parametric
video-grounded text generation model, SeViT, a novel perspective on scalable
video-language modeling toward long untrimmed videos. Treating a video as an
external data store, SeViT includes a non-parametric frame retriever to select
a few query-relevant frames from the data store for a given query and a
parametric generator to effectively aggregate the frames with the query via
late fusion methods. Experimental results demonstrate our method has a
significant advantage in longer videos and causal video understanding.
Moreover, our model achieves the new state of the art on four video-language
datasets, iVQA (+4.8), Next-QA (+6.9), and Activitynet-QA (+4.8) in accuracy,
and MSRVTT-Caption (+3.6) in CIDEr.",2023-01-27
"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability
  Curvature",2023-01-26 18:44:06+00:00,http://arxiv.org/abs/2301.11305v1,"Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, Chelsea Finn","cs.CL, cs.AI",table2text,"The fluency and factual knowledge of large language models (LLMs) heightens
the need for corresponding systems to detect whether a piece of text is
machine-written. For example, students may use LLMs to complete written
assignments, leaving instructors unable to accurately assess student learning.
In this paper, we first demonstrate that text sampled from an LLM tends to
occupy negative curvature regions of the model's log probability function.
Leveraging this observation, we then define a new curvature-based criterion for
judging if a passage is generated from a given LLM. This approach, which we
call DetectGPT, does not require training a separate classifier, collecting a
dataset of real or generated passages, or explicitly watermarking generated
text. It uses only log probabilities computed by the model of interest and
random perturbations of the passage from another generic pre-trained language
model (e.g, T5). We find DetectGPT is more discriminative than existing
zero-shot methods for model sample detection, notably improving detection of
fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the
strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See
https://ericmitchell.ai/detectgpt for code, data, and other project
information.",2023-01-26
Distilling Text into Circuits,2023-01-25 13:56:34+00:00,http://arxiv.org/abs/2301.10595v1,"Vincent Wang-Mascianica, Jonathon Liu, Bob Coecke","cs.CL, cs.AI, cs.LO, math.CT",table2text,"This paper concerns the structure of meanings within natural language.
Earlier, a framework named DisCoCirc was sketched that (1) is compositional and
distributional (a.k.a. vectorial); (2) applies to general text; (3) captures
linguistic `connections' between meanings (cf. grammar) (4) updates word
meanings as text progresses; (5) structures sentence types; (6) accommodates
ambiguity. Here, we realise DisCoCirc for a substantial fragment of English.
  When passing to DisCoCirc's text circuits, some `grammatical bureaucracy' is
eliminated, that is, DisCoCirc displays a significant degree of (7) inter- and
intra-language independence. That is, e.g., independence from word-order
conventions that differ across languages, and independence from choices like
many short sentences vs. few long sentences. This inter-language independence
means our text circuits should carry over to other languages, unlike the
language-specific typings of categorial grammars. Hence, text circuits are a
lean structure for the `actual substance of text', that is, the inner-workings
of meanings within text across several layers of expressiveness (cf. words,
sentences, text), and may capture that what is truly universal beneath grammar.
The elimination of grammatical bureaucracy also explains why DisCoCirc: (8)
applies beyond language, e.g. to spatial, visual and other cognitive modes.
While humans could not verbally communicate in terms of text circuits, machines
can.
  We first define a `hybrid grammar' for a fragment of English, i.e. a
purpose-built, minimal grammatical formalism needed to obtain text circuits. We
then detail a translation process such that all text generated by this grammar
yields a text circuit. Conversely, for any text circuit obtained by freely
composing the generators, there exists a text (with hybrid grammar) that gives
rise to it. Hence: (9) text circuits are generative for text.",2023-01-25
"One Model for All Domains: Collaborative Domain-Prefix Tuning for
  Cross-Domain NER",2023-01-25 05:16:43+00:00,http://arxiv.org/abs/2301.10410v1,"Xiang Chen, Lei Li, Qiaoshuo Fei, Ningyu Zhang, Chuanqi Tan, Yong Jiang, Fei Huang, Huajun Chen","cs.CL, cs.AI, cs.DB, cs.IR, cs.LG",table2text,"Cross-domain NER is a challenging task to address the low-resource problem in
practical scenarios. Previous typical solutions mainly obtain a NER model by
pre-trained language models (PLMs) with data from a rich-resource domain and
adapt it to the target domain. Owing to the mismatch issue among entity types
in different domains, previous approaches normally tune all parameters of PLMs,
ending up with an entirely new NER model for each domain. Moreover, current
models only focus on leveraging knowledge in one general source domain while
failing to successfully transfer knowledge from multiple sources to the target.
To address these issues, we introduce Collaborative Domain-Prefix Tuning for
cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,
we present text-to-text generation grounding domain-related instructors to
transfer knowledge to new domain NER tasks without structural modifications. We
utilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate
the potential of PLMs to handle NER tasks across various domains. Experimental
results on the Cross-NER benchmark show that the proposed approach has flexible
transfer ability and performs better on both one-source and multiple-source
cross-domain NER tasks. Codes will be available in
https://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.",2023-01-25
Audience-Centric Natural Language Generation via Style Infusion,2023-01-24 19:57:50+00:00,http://arxiv.org/abs/2301.10283v1,"Samraj Moorjani, Adit Krishnan, Hari Sundaram, Ewa Maslowska, Aravind Sankar","cs.CL, cs.LG",table2text,"Adopting contextually appropriate, audience-tailored linguistic styles is
critical to the success of user-centric language generation systems (e.g.,
chatbots, computer-aided writing, dialog systems). While existing approaches
demonstrate textual style transfer with large volumes of parallel or
non-parallel data, we argue that grounding style on audience-independent
external factors is innately limiting for two reasons. First, it is difficult
to collect large volumes of audience-specific stylistic data. Second, some
stylistic objectives (e.g., persuasiveness, memorability, empathy) are hard to
define without audience feedback.
  In this paper, we propose the novel task of style infusion - infusing the
stylistic preferences of audiences in pretrained language generation models.
Since humans are better at pairwise comparisons than direct scoring - i.e., is
Sample-A more persuasive/polite/empathic than Sample-B - we leverage limited
pairwise human judgments to bootstrap a style analysis model and augment our
seed set of judgments. We then infuse the learned textual style in a GPT-2
based text generator while balancing fluency and style adoption. With
quantitative and qualitative assessments, we show that our infusion approach
can generate compelling stylized examples with generic text prompts. The code
and data are accessible at https://github.com/CrowdDynamicsLab/StyleInfusion.",2023-01-24
ExClaim: Explainable Neural Claim Verification Using Rationalization,2023-01-21 08:26:27+00:00,http://arxiv.org/abs/2301.08914v1,"Sai Gurrapu, Lifu Huang, Feras A. Batarseh",cs.CL,table2text,"With the advent of deep learning, text generation language models have
improved dramatically, with text at a similar level as human-written text. This
can lead to rampant misinformation because content can now be created cheaply
and distributed quickly. Automated claim verification methods exist to validate
claims, but they lack foundational data and often use mainstream news as
evidence sources that are strongly biased towards a specific agenda. Current
claim verification methods use deep neural network models and complex
algorithms for a high classification accuracy but it is at the expense of model
explainability. The models are black-boxes and their decision-making process
and the steps it took to arrive at a final prediction are obfuscated from the
user. We introduce a novel claim verification approach, namely: ExClaim, that
attempts to provide an explainable claim verification system with foundational
evidence. Inspired by the legal system, ExClaim leverages rationalization to
provide a verdict for the claim and justifies the verdict through a natural
language explanation (rationale) to describe the model's decision-making
process. ExClaim treats the verdict classification task as a question-answer
problem and achieves a performance of 0.93 F1 score. It provides subtasks
explanations to also justify the intermediate outcomes. Statistical and
Explainable AI (XAI) evaluations are conducted to ensure valid and trustworthy
outcomes. Ensuring claim verification systems are assured, rational, and
explainable is an essential step toward improving Human-AI trust and the
accessibility of black-box systems.",2023-01-21
Regeneration Learning: A Learning Paradigm for Data Generation,2023-01-21 01:33:34+00:00,http://arxiv.org/abs/2301.08846v1,"Xu Tan, Tao Qin, Jiang Bian, Tie-Yan Liu, Yoshua Bengio","cs.LG, cs.AI, cs.CL, cs.CV, eess.AS",table2text,"Machine learning methods for conditional data generation usually build a
mapping from source conditional data X to target data Y. The target Y (e.g.,
text, speech, music, image, video) is usually high-dimensional and complex, and
contains information that does not exist in source data, which hinders
effective and efficient learning on the source-target mapping. In this paper,
we present a learning paradigm called regeneration learning for data
generation, which first generates Y' (an abstraction/representation of Y) from
X and then generates Y from Y'. During training, Y' is obtained from Y through
either handcrafted rules or self-supervised learning and is used to learn
X-->Y' and Y'-->Y. Regeneration learning extends the concept of representation
learning to data generation tasks, and can be regarded as a counterpart of
traditional representation learning, since 1) regeneration learning handles the
abstraction (Y') of the target data Y for data generation while traditional
representation learning handles the abstraction (X') of source data X for data
understanding; 2) both the processes of Y'-->Y in regeneration learning and
X-->X' in representation learning can be learned in a self-supervised way
(e.g., pre-training); 3) both the mappings from X to Y' in regeneration
learning and from X' to Y in representation learning are simpler than the
direct mapping from X to Y. We show that regeneration learning can be a
widely-used paradigm for data generation (e.g., text generation, speech
recognition, speech synthesis, music composition, image generation, and video
generation) and can provide valuable insights into developing data generation
methods.",2023-01-21
"UserSimCRS: A User Simulation Toolkit for Evaluating Conversational
  Recommender Systems",2023-01-13 13:41:20+00:00,http://arxiv.org/abs/2301.05544v2,"Jafar Afzali, Aleksander Mark Drzewiecki, Krisztian Balog, Shuo Zhang",cs.IR,table2text,"We present an extensible user simulation toolkit to facilitate automatic
evaluation of conversational recommender systems. It builds on an established
agenda-based approach and extends it with several novel elements, including
user satisfaction prediction, persona and context modeling, and conditional
natural language generation. We showcase the toolkit with a pre-existing movie
recommender system and demonstrate its ability to simulate dialogues that mimic
real conversations, while requiring only a handful of manually annotated
dialogues as training data.",2023-01-13
Universal Multimodal Representation for Language Understanding,2023-01-09 13:54:11+00:00,http://arxiv.org/abs/2301.03344v1,"Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao Li, Hai Zhao","cs.CL, cs.AI, cs.CV",table2text,"Representation learning is the foundation of natural language processing
(NLP). This work presents new methods to employ visual information as assistant
signals to general NLP tasks. For each sentence, we first retrieve a flexible
number of images either from a light topic-image lookup table extracted over
the existing sentence-image pairs or a shared cross-modal embedding space that
is pre-trained on out-of-shelf text-image pairs. Then, the text and images are
encoded by a Transformer encoder and convolutional neural network,
respectively. The two sequences of representations are further fused by an
attention layer for the interaction of the two modalities. In this study, the
retrieval process is controllable and flexible. The universal visual
representation overcomes the lack of large-scale bilingual sentence-image
pairs. Our method can be easily applied to text-only tasks without manually
annotated multimodal parallel corpora. We apply the proposed method to a wide
range of natural language generation and understanding tasks, including neural
machine translation, natural language inference, and semantic similarity.
Experimental results show that our method is generally effective for different
tasks and languages. Analysis indicates that the visual signals enrich textual
representations of content words, provide fine-grained grounding information
about the relationship between concepts and events, and potentially conduce to
disambiguation.",2023-01-09
Sequentially Controlled Text Generation,2023-01-05 21:23:51+00:00,http://arxiv.org/abs/2301.02299v1,"Alexander Spangher, Xinyu Hua, Yao Ming, Nanyun Peng","cs.CL, cs.AI, cs.LG",table2text,"While GPT-2 generates sentences that are remarkably human-like, longer
documents can ramble and do not follow human-like writing structure. We study
the problem of imposing structure on long-range text. We propose a novel
controlled text generation task, sequentially controlled text generation, and
identify a dataset, NewsDiscourse as a starting point for this task. We develop
a sequential controlled text generation pipeline with generation and editing.
We test different degrees of structural awareness and show that, in general,
more structural awareness results in higher control-accuracy, grammaticality,
coherency and topicality, approaching human-level writing performance.",2023-01-05
"Towards Table-to-Text Generation with Pretrained Language Model: A Table
  Structure Understanding and Text Deliberating Approach",2023-01-05 14:03:26+00:00,http://arxiv.org/abs/2301.02071v1,"Miao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Jingbo Zhou, Dejing Dou, Hui Xiong","cs.CL, cs.AI",table2text,"Although remarkable progress on the neural table-to-text methods has been
made, the generalization issues hinder the applicability of these models due to
the limited source tables. Large-scale pretrained language models sound like a
promising solution to tackle such issues. However, how to effectively bridge
the gap between the structured table and the text input by fully leveraging
table information to fuel the pretrained model is still not well explored.
Besides, another challenge of integrating the deliberation mechanism into the
text-to-text pretrained model for solving the table-to-text task remains seldom
studied. In this paper, to implement the table-to-text generation with
pretrained language model, we propose a table structure understanding and text
deliberating approach, namely TASD. Specifically, we devise a three-layered
multi-head attention network to realize the table-structure-aware text
generation model with the help of the pretrained language model. Furthermore, a
multi-pass decoder framework is adopted to enhance the capability of polishing
generated text for table descriptions. The empirical studies, as well as human
evaluation, on two public datasets, validate that our approach can generate
faithful and fluent descriptive texts for different types of tables.",2023-01-05
eVAE: Evolutionary Variational Autoencoder,2023-01-01 23:54:35+00:00,http://arxiv.org/abs/2301.00011v1,"Zhangkai Wu, Longbing Cao, Lei Qi","cs.NE, cs.LG",table2text,"The surrogate loss of variational autoencoders (VAEs) poses various
challenges to their training, inducing the imbalance between task fitting and
representation inference. To avert this, the existing strategies for VAEs focus
on adjusting the tradeoff by introducing hyperparameters, deriving a tighter
bound under some mild assumptions, or decomposing the loss components per
certain neural settings. VAEs still suffer from uncertain tradeoff learning.We
propose a novel evolutionary variational autoencoder (eVAE) building on the
variational information bottleneck (VIB) theory and integrative evolutionary
neural learning. eVAE integrates a variational genetic algorithm into VAE with
variational evolutionary operators including variational mutation, crossover,
and evolution. Its inner-outer-joint training mechanism synergistically and
dynamically generates and updates the uncertain tradeoff learning in the
evidence lower bound (ELBO) without additional constraints. Apart from learning
a lossy compression and representation of data under the VIB assumption, eVAE
presents an evolutionary paradigm to tune critical factors of VAEs and deep
neural networks and addresses the premature convergence and random search
problem by integrating evolutionary optimization into deep learning.
Experiments show that eVAE addresses the KL-vanishing problem for text
generation with low reconstruction loss, generates all disentangled factors
with sharp images, and improves the image generation quality,respectively. eVAE
achieves better reconstruction loss, disentanglement, and generation-inference
balance than its competitors.",2023-01-01
MAUVE Scores for Generative Models: Theory and Practice,2022-12-30 07:37:40+00:00,http://arxiv.org/abs/2212.14578v1,"Krishna Pillutla, Lang Liu, John Thickstun, Sean Welleck, Swabha Swayamdipta, Rowan Zellers, Sewoong Oh, Yejin Choi, Zaid Harchaoui","cs.LG, cs.AI, cs.CL",table2text,"Generative AI has matured to a point where large-scale models can generate
text that seems indistinguishable from human-written text and remarkably
photorealistic images. Automatically measuring how close the distribution of
generated data is to the target real data distribution is a key step in
diagnosing existing models and developing better models. We present MAUVE, a
family of comparison measures between pairs of distributions such as those
encountered in the generative modeling of text or images. These scores are
statistical summaries of divergence frontiers capturing two types of errors in
generative modeling. We explore four approaches to statistically estimate these
scores: vector quantization, non-parametric estimation, classifier-based
estimation, and parametric Gaussian approximations. We provide statistical
bounds for the vector quantization approach. Empirically, we find that the
proposed scores paired with a range of $f$-divergences and statistical
estimation methods can quantify the gaps between the distributions of
human-written text and those of modern neural language models by correlating
with human judgments and identifying known properties of the generated texts.
We conclude the paper by demonstrating its applications to other AI domains and
discussing practical recommendations.",2022-12-30
"TegFormer: Topic-to-Essay Generation with Good Topic Coverage and High
  Text Coherence",2022-12-27 11:50:14+00:00,http://arxiv.org/abs/2212.13456v1,"Wang Qi, Rui Liu, Yuan Zuo, Yong Chen, Dell Zhang",cs.CL,table2text,"Creating an essay based on a few given topics is a challenging NLP task.
Although several effective methods for this problem, topic-to-essay generation,
have appeared recently, there is still much room for improvement, especially in
terms of the coverage of the given topics and the coherence of the generated
text. In this paper, we propose a novel approach called TegFormer which
utilizes the Transformer architecture where the encoder is enriched with
domain-specific contexts while the decoder is enhanced by a large-scale
pre-trained language model. Specifically, a \emph{Topic-Extension} layer
capturing the interaction between the given topics and their domain-specific
contexts is plugged into the encoder. Since the given topics are usually
concise and sparse, such an additional layer can bring more topic-related
semantics in to facilitate the subsequent natural language generation.
Moreover, an \emph{Embedding-Fusion} module that combines the domain-specific
word embeddings learnt from the given corpus and the general-purpose word
embeddings provided by a GPT-2 model pre-trained on massive text data is
integrated into the decoder. Since GPT-2 is at a much larger scale, it contains
a lot more implicit linguistic knowledge which would help the decoder to
produce more grammatical and readable text. Extensive experiments have shown
that the pieces of text generated by TegFormer have better topic coverage and
higher text coherence than those from SOTA topic-to-essay techniques, according
to automatic and human evaluations. As revealed by ablation studies, both the
Topic-Extension layer and the Embedding-Fusion module contribute substantially
to TegFormer's performance advantage.",2022-12-27
TextBox 2.0: A Text Generation Library with Pre-trained Language Models,2022-12-26 03:50:36+00:00,http://arxiv.org/abs/2212.13005v1,"Tianyi Tang, Junyi Li, Zhipeng Chen, Yiwen Hu, Zhuohao Yu, Wenxun Dai, Zican Dong, Xiaoxue Cheng, Yuhao Wang, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen",cs.CL,table2text,"To facilitate research on text generation, this paper presents a
comprehensive and unified library, TextBox 2.0, focusing on the use of
pre-trained language models (PLMs). To be comprehensive, our library covers
$13$ common text generation tasks and their corresponding $83$ datasets and
further incorporates $45$ PLMs covering general, translation, Chinese,
dialogue, controllable, distilled, prompting, and lightweight PLMs. We also
implement $4$ efficient training strategies and provide $4$ generation
objectives for pre-training new PLMs from scratch. To be unified, we design the
interfaces to support the entire research pipeline (from data loading to
training and evaluation), ensuring that each step can be fulfilled in a unified
way. Despite the rich functionality, it is easy to use our library, either
through the friendly Python API or command line. To validate the effectiveness
of our library, we conduct extensive experiments and exemplify four types of
research scenarios. The project is released at the link:
https://github.com/RUCAIBox/TextBox.",2022-12-26
"CORRPUS: Detecting Story Inconsistencies via Codex-Bootstrapped
  Neurosymbolic Reasoning",2022-12-21 04:21:35+00:00,http://arxiv.org/abs/2212.10754v1,"Yijiang River Dong, Lara J. Martin, Chris Callison-Burch",cs.CL,table2text,"Story generation and understanding -- as with all NLG/NLU tasks -- has seen a
surge in neurosymbolic work. Researchers have recognized that, while large
language models (LLMs) have tremendous utility, they can be augmented with
symbolic means to be even better and to make up for any flaws that the neural
networks might have. However, symbolic methods are extremely costly in terms of
the amount of time and expertise needed to create them. In this work, we
capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use
of symbolic methods for tracking the state of stories and aiding in story
understanding. We show that our CoRRPUS system and abstracted prompting
procedures can beat current state-of-the-art structured LLM techniques on
pre-existing story understanding tasks (bAbI task 2 and Re^3) with minimal hand
engineering. We hope that this work can help highlight the importance of
symbolic representations and specialized prompting for LLMs as these models
require some guidance for performing reasoning tasks properly.",2022-12-21
Tracing and Removing Data Errors in Natural Language Generation Datasets,2022-12-21 02:28:07+00:00,http://arxiv.org/abs/2212.10722v1,"Faisal Ladhak, Esin Durmus, Tatsunori Hashimoto",cs.CL,table2text,"Recent work has identified noisy and misannotated data as a core cause of
hallucinations and unfaithful outputs in Natural Language Generation (NLG)
tasks. Consequently, identifying and removing these examples is a key open
challenge in creating reliable NLG systems. In this work, we introduce a
framework to identify and remove low-quality training instances that lead to
undesirable outputs, such as faithfulness errors in text summarization. We show
that existing approaches for error tracing, such as gradient-based influence
measures, do not perform reliably for detecting faithfulness errors in
summarization. We overcome the drawbacks of existing error tracing methods
through a new, contrast-based estimate that compares undesired generations to
human-corrected outputs. Our proposed method can achieve a mean average
precision of 0.91 across synthetic tasks with known ground truth and can
achieve a two-fold reduction in hallucinations on a real entity hallucination
evaluation on the NYT dataset.",2022-12-21
SimpleStyle: An Adaptable Style Transfer Approach,2022-12-20 18:12:49+00:00,http://arxiv.org/abs/2212.10498v1,"Elron Bandel, Yoav Katz, Noam Slonim, Liat Ein-Dor",cs.CL,table2text,"Attribute Controlled Text Rewriting, also known as text style transfer, has
received significant attention in the natural language generation community due
to its crucial role in controllable natural language generation systems. In
this work we present SimpleStyle a minimalist yet effective approach for
attribute controlled text rewriting based on a simple mechanism composed of two
ingredients. controlled denoising and output filtering. Despite the simplicity
of our approach, which can be succinctly explained with just a few lines of
code, it is competitive with previous state-of-the-art methods both in
automatic and in human evaluations. Additionally, we demonstrate the practical
effectiveness of our system, by applying it to real-world data from social
networks. Additionally, we introduce a soft masking sampling technique that
further improves the performance of the system. We also show that feeding the
output of our system into a text-to-text student model can produce high-quality
results without the need for additional filtering. Finally, we suggest that our
method can solve the fundamental missing baseline absence that holding progress
in the field by offering our protocol as a simple, adaptive and very strong
baseline for works wish to make incremental advancements in the field of
attribute controlled text rewriting.",2022-12-20
"CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data
  Limitation With Contrastive Learning",2022-12-20 15:26:19+00:00,http://arxiv.org/abs/2212.10341v1,"Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Yu Lan, Chao Shen",cs.CL,table2text,"Machine-Generated Text (MGT) detection, a task that discriminates MGT from
Human-Written Text (HWT), plays a crucial role in preventing misuse of text
generative models, which excel in mimicking human writing style recently.
Latest proposed detectors usually take coarse text sequence as input and output
some good results by fine-tune pretrained models with standard cross-entropy
loss. However, these methods fail to consider the linguistic aspect of text
(e.g., coherence) and sentence-level structures. Moreover, they lack the
ability to handle the low-resource problem which could often happen in practice
considering the enormous amount of textual data online. In this paper, we
present a coherence-based contrastive learning model named CoCo to detect the
possible MGT under low-resource scenario. Inspired by the distinctiveness and
permanence properties of linguistic feature, we represent text as a coherence
graph to capture its entity consistency, which is further encoded by the
pretrained model and graph neural network. To tackle the challenges of data
limitations, we employ a contrastive learning framework and propose an improved
contrastive loss for making full use of hard negative samples in training
stage. The experiment results on two public datasets prove our approach
outperforms the state-of-art methods significantly.",2022-12-20
"Toward Human-Like Evaluation for Natural Language Generation with Error
  Analysis",2022-12-20 11:36:22+00:00,http://arxiv.org/abs/2212.10179v1,"Qingyu Lu, Liang Ding, Liping Xie, Kanjian Zhang, Derek F. Wong, Dacheng Tao",cs.CL,table2text,"The state-of-the-art language model-based automatic metrics, e.g. BARTScore,
benefiting from large-scale contextualized pre-training, have been successfully
used in a wide range of natural language generation (NLG) tasks, including
machine translation, text summarization, and data-to-text. Recent studies show
that considering both major errors (e.g. mistranslated tokens) and minor errors
(e.g. imperfections in fluency) can produce high-quality human judgments. This
inspires us to approach the final goal of the evaluation metrics (human-like
evaluations) by automatic error analysis. To this end, we augment BARTScore by
incorporating the human-like error analysis strategies, namely BARTScore++,
where the final score consists of both the evaluations of major errors and
minor errors. Experimental results show that BARTScore++ can consistently
improve the performance of vanilla BARTScore and outperform existing
top-scoring metrics in 20 out of 25 test settings. We hope our technique can
also be extended to other pre-trained model-based metrics. We will release our
code and scripts to facilitate the community.",2022-12-20
"WeCheck: Strong Factual Consistency Checker via Weakly Supervised
  Learning",2022-12-20 08:04:36+00:00,http://arxiv.org/abs/2212.10057v1,"Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Sujian Li, Yajuan Lv",cs.CL,table2text,"A crucial issue of current text generation models is that they often
uncontrollably generate factually inconsistent text with respective of their
inputs. Limited by the lack of annotated data, existing works in evaluating
factual consistency directly transfer the reasoning ability of models trained
on other data-rich upstream tasks like question answering (QA) and natural
language inference (NLI) without any further adaptation. As a result, they
perform poorly on the real generated text and are biased heavily by their
single-source upstream tasks. To alleviate this problem, we propose a weakly
supervised framework that aggregates multiple resources to train a precise and
efficient factual metric, namely WeCheck. WeCheck first utilizes a generative
model to accurately label a real generated sample by aggregating its weak
labels, which are inferred from multiple resources. Then, we train the target
metric model with the weak supervision while taking noises into consideration.
Comprehensive experiments on a variety of tasks demonstrate the strong
performance of WeCheck, which achieves a 3.4\% absolute improvement over
previous state-of-the-art methods on TRUE benchmark on average.",2022-12-20
On the Blind Spots of Model-Based Evaluation Metrics for Text Generation,2022-12-20 06:24:25+00:00,http://arxiv.org/abs/2212.10020v1,"Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James Glass, Yulia Tsvetkov",cs.CL,table2text,"In this work, we explore a useful but often neglected methodology for
robustness analysis of text generation evaluation metrics: stress tests with
synthetic data. Basically, we design and synthesize a wide range of potential
errors and check whether they result in a commensurate drop in the metric
scores. We examine a range of recently proposed evaluation metrics based on
pretrained language models, for the tasks of open-ended generation,
translation, and summarization. Our experiments reveal interesting
insensitivities, biases, or even loopholes in existing metrics. For example, we
find that BERTScore ignores truncation errors in summarization, and MAUVE
(built on top of GPT-2) is insensitive to errors at the beginning of
generations. Further, we investigate the reasons behind these blind spots and
suggest practical workarounds for a more reliable evaluation of text
generation.",2022-12-20
"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",2022-12-19 18:57:05+00:00,http://arxiv.org/abs/2212.09741v2,"Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu",cs.CL,table2text,"We introduce INSTRUCTOR, a new method for computing text embeddings given
task instructions: every text input is embedded together with instructions
explaining the use case (e.g., task and domain descriptions). Unlike encoders
from prior work that are more specialized, INSTRUCTOR is a single embedder that
can generate text embeddings tailored to different downstream tasks and
domains, without any further training. We first annotate instructions for 330
diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive
loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are
unseen during training), ranging from classification and information retrieval
to semantic textual similarity and text generation evaluation. INSTRUCTOR,
while having an order of magnitude fewer parameters than the previous best
model, achieves state-of-the-art performance, with an average improvement of
3.4% compared to the previous best results on the 70 diverse datasets. Our
analysis suggests that INSTRUCTOR is robust to changes in instructions, and
that instruction finetuning mitigates the challenge of training a single model
on diverse datasets. Our model, code, and data are available at
https://instructor-embedding.github.io.",2022-12-19
"Difformer: Empowering Diffusion Model on Embedding Space for Text
  Generation",2022-12-19 12:44:25+00:00,http://arxiv.org/abs/2212.09412v1,"Zhujin Gao, Junliang Guo, Xu Tan, Yongxin Zhu, Fang Zhang, Jiang Bian, Linli Xu","cs.CL, cs.AI, cs.LG",table2text,"Diffusion models have achieved state-of-the-art synthesis quality on visual
and audio tasks, and recent works adapt them to textual data by diffusing on
the embedding space. But the difference between the continuous data space and
the embedding space raises challenges to the diffusion model, which have not
been carefully explored. In this paper, we conduct systematic studies and
analyze the challenges threefold. Firstly, the data distribution is learnable
for embeddings, which may lead to the collapse of the loss function. Secondly,
as the norm of embedding varies between popular and rare words, adding the same
noise scale will lead to sub-optimal results. In addition, we find that noises
sampled from a standard Gaussian distribution may distract the diffusion
process. To solve the above challenges, we propose Difformer, a denoising
diffusion probabilistic model based on Transformer, which consists of three
techniques including utilizing an anchor loss function, a layer normalization
module for embeddings, and a norm factor to the Gaussian noise. All techniques
are complementary to each other and critical to boosting the model performance
together. Experiments are conducted on benchmark datasets over two seminal text
generation tasks including machine translation and text summarization. The
results show that Difformer significantly outperforms the embedding diffusion
baselines, while achieving competitive results with strong autoregressive
baselines.",2022-12-19
SEScore2: Retrieval Augmented Pretraining for Text Generation Evaluation,2022-12-19 09:02:16+00:00,http://arxiv.org/abs/2212.09305v1,"Wenda Xu, Xian Qian, Mingxuan Wang, Lei Li, William Yang Wang",cs.CL,table2text,"Is it possible to leverage large scale raw and raw parallel corpora to build
a general learned metric? Existing learned metrics have gaps to human
judgements, are model-dependent or are limited to the domains or tasks where
human ratings are available. In this paper, we propose SEScore2, a model-based
metric pretrained over million-scale synthetic dataset constructed by our novel
retrieval augmented data synthesis pipeline. SEScore2 achieves high correlation
to human judgements without any human rating supervisions. Importantly, our
unsupervised SEScore2 can outperform supervised metrics, which are trained on
the News human ratings, at the TED domain. We evaluate SEScore2 over four text
generation tasks across three languages. SEScore2 outperforms all prior
unsupervised evaluation metrics in machine translation, speech translation,
data-to-text and dialogue generation, with average Kendall improvements 0.158.
SEScore2 even outperforms SOTA supervised BLEURT at data-to-text, dialogue
generation and overall correlation.",2022-12-19
"Synthesis and Evaluation of a Domain-specific Large Data Set for
  Dungeons & Dragons",2022-12-18 12:54:45+00:00,http://arxiv.org/abs/2212.09080v1,"Akila Peiris, Nisansa de Silva","cs.CL, cs.LG",table2text,"This paper introduces the Forgotten Realms Wiki (FRW) data set and domain
specific natural language generation using FRW along with related analyses.
Forgotten Realms is the de-facto default setting of the popular open ended
tabletop fantasy role playing game, Dungeons & Dragons. The data set was
extracted from the Forgotten Realms Fandom wiki consisting of more than over
45,200 articles. The FRW data set is constituted of 11 sub-data sets in a
number of formats: raw plain text, plain text annotated by article title,
directed link graphs, wiki info-boxes annotated by the wiki article title,
Poincar\'e embedding of first link graph, multiple Word2Vec and Doc2Vec models
of the corpus. This is the first data set of this size for the Dungeons &
Dragons domain. We then present a pairwise similarity comparison benchmark
which utilizes similarity measures. In addition, we perform D&D domain specific
natural language generation using the corpus and evaluate the named entity
classification with respect to the lore of Forgotten Realms.",2022-12-18
RISE: Leveraging Retrieval Techniques for Summarization Evaluation,2022-12-17 01:09:22+00:00,http://arxiv.org/abs/2212.08775v1,"David Uthus, Jianmo Ni",cs.CL,table2text,"Evaluating automatically-generated text summaries is a challenging task.
While there have been many interesting approaches, they still fall short of
human evaluations. We present RISE, a new approach for evaluating summaries by
leveraging techniques from information retrieval. RISE is first trained as a
retrieval task using a dual-encoder retrieval setup, and can then be
subsequently utilized for evaluating a generated summary given an input
document, without gold reference summaries. RISE is especially well suited when
working on new datasets where one may not have reference summaries available
for evaluation. We conduct comprehensive experiments on the SummEval benchmark
(Fabbri et al., 2021) and the results show that RISE has higher correlation
with human evaluations compared to many past approaches to summarization
evaluation. Furthermore, RISE also demonstrates data-efficiency and
generalizability across languages.",2022-12-17
"DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text
  Generation",2022-12-16 21:44:34+00:00,http://arxiv.org/abs/2212.08724v1,"Yuxi Feng, Xiaoyuan Yi, Xiting Wang, Laks V. S. Lakshmanan, Xing Xie",cs.CL,table2text,"Self-training (ST) has prospered again in language understanding by
augmenting the fine-tuning of pre-trained language models when labeled data is
insufficient. However, it remains challenging to incorporate ST into
attribute-controllable language generation. Augmented by only self-generated
pseudo text, generation models over-emphasize exploitation of the previously
learned space, suffering from a constrained generalization boundary. We revisit
ST and propose a novel method, DuNST to alleviate this problem. DuNST jointly
models text generation and classification with a shared Variational AutoEncoder
and corrupts the generated pseudo text by two kinds of flexible noise to
disturb the space. In this way, our model could construct and utilize both
pseudo text from given labels and pseudo labels from available unlabeled text,
which are gradually refined during the ST process. We theoretically demonstrate
that DuNST can be regarded as enhancing exploration towards the potential real
text space, providing a guarantee of improved performance. Experiments on three
controllable generation tasks show that DuNST could significantly boost control
accuracy while maintaining comparable generation fluency and diversity against
several strong baselines.",2022-12-16
"MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text
  Generation",2022-12-16 17:36:23+00:00,http://arxiv.org/abs/2212.08607v1,"Swarnadeep Saha, Xinyan Velocity Yu, Mohit Bansal, Ramakanth Pasunuru, Asli Celikyilmaz","cs.CL, cs.AI, cs.LG",table2text,"Prompting large language models has enabled significant recent progress in
multi-step reasoning over text. However, when applied to text generation from
semi-structured data (e.g., graphs or tables), these methods typically suffer
from low semantic coverage, hallucination, and logical inconsistency. We
propose MURMUR, a neuro-symbolic modular approach to text generation from
semi-structured data with multi-step reasoning. MURMUR is a best-first search
method that generates reasoning paths using: (1) neural and symbolic modules
with specific linguistic and logical skills, (2) a grammar whose production
rules define valid compositions of modules, and (3) value functions that assess
the quality of each reasoning step. We conduct experiments on two diverse
data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in
their data representations (graphs and tables) and span multiple linguistic and
logical skills. MURMUR obtains significant improvements over recent few-shot
baselines like direct prompting and chain-of-thought prompting, while also
achieving comparable performance to fine-tuned GPT-2 on out-of-domain data.
Moreover, human evaluation shows that MURMUR generates highly faithful and
correct reasoning paths that lead to 26% more logically consistent summaries on
LogicNLG, compared to direct prompting.",2022-12-16
"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for
  Programming Languages",2022-12-13 17:21:44+00:00,http://arxiv.org/abs/2212.06742v1,"Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu","cs.CL, cs.LG, cs.PL, cs.SE",table2text,"Software engineers working with the same programming language (PL) may speak
different natural languages (NLs) and vice versa, erecting huge barriers to
communication and working efficiency. Recent studies have demonstrated the
effectiveness of generative pre-training in computer programs, yet they are
always English-centric. In this work, we step towards bridging the gap between
multilingual NLs and multilingual PLs for large language models (LLMs). We
release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.
We employ two methods for universal cross-lingual pre-training: span-corruption
language modeling that learns patterns from monolingual NL or PL; and
pivot-based translation language modeling that relies on parallel data of many
NLs and PLs. Extensive results show that ERNIE-Code outperforms previous
multilingual LLMs for PL or NL across a wide range of end tasks of code
intelligence, including multilingual code-to-text, text-to-code, code-to-code,
and text-to-text generation. We further show its advantage of zero-shot
prompting on multilingual code summarization and text-to-text translation. We
will make our code and pre-trained models publicly available.",2022-12-13
"Collaborating Heterogeneous Natural Language Processing Tasks via
  Federated Learning",2022-12-12 09:27:50+00:00,http://arxiv.org/abs/2212.05789v1,"Chenhe Dong, Yuexiang Xie, Bolin Ding, Ying Shen, Yaliang Li",cs.CL,table2text,"The increasing privacy concerns on personal private text data promote the
development of federated learning (FL) in recent years. However, the existing
studies on applying FL in NLP are not suitable to coordinate participants with
heterogeneous or private learning objectives. In this study, we further broaden
the application scope of FL in NLP by proposing an Assign-Then-Contrast
(denoted as ATC) framework, which enables clients with heterogeneous NLP tasks
to construct an FL course and learn useful knowledge from each other.
Specifically, the clients are suggested to first perform local training with
the unified tasks assigned by the server rather than using their own learning
objectives, which is called the Assign training stage. After that, in the
Contrast training stage, clients train with different local learning objectives
and exchange knowledge with other clients who contribute consistent and useful
model updates. We conduct extensive experiments on six widely-used datasets
covering both Natural Language Understanding (NLU) and Natural Language
Generation (NLG) tasks, and the proposed ATC framework achieves significant
improvements compared with various baseline methods. The source code is
available at
\url{https://github.com/alibaba/FederatedScope/tree/master/federatedscope/nlp/hetero_tasks}.",2022-12-12
T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics,2022-12-12 06:29:04+00:00,http://arxiv.org/abs/2212.05726v1,"Yiwei Qin, Weizhe Yuan, Graham Neubig, Pengfei Liu",cs.CL,table2text,"Modern embedding-based metrics for evaluation of generated text generally
fall into one of two paradigms: discriminative metrics that are trained to
directly predict which outputs are of higher quality according to supervised
human annotations, and generative metrics that are trained to evaluate text
based on the probabilities of a generative model. Both have their advantages;
discriminative metrics are able to directly optimize for the problem of
distinguishing between good and bad outputs, while generative metrics can be
trained using abundant raw text. In this paper, we present a framework that
combines the best of both worlds, using both supervised and unsupervised
signals from whatever data we have available. We operationalize this idea by
training T5Score, a metric that uses these training signals with mT5 as the
backbone. We perform an extensive empirical comparison with other existing
metrics on 5 datasets, 19 languages and 280 systems, demonstrating the utility
of our method. Experimental results show that: T5Score achieves the best
performance on all datasets against existing top-scoring metrics at the segment
level. We release our code and models at https://github.com/qinyiwei/T5Score.",2022-12-12
"The Role of AI in Drug Discovery: Challenges, Opportunities, and
  Strategies",2022-12-08 23:23:39+00:00,http://arxiv.org/abs/2212.08104v1,"Alexandre Blanco-Gonzalez, Alfonso Cabezon, Alejandro Seco-Gonzalez, Daniel Conde-Torres, Paula Antelo-Riveiro, Angel Pineiro, Rebeca Garcia-Fandino","cs.CL, cs.AI, cs.CY",table2text,"Artificial intelligence (AI) has the potential to revolutionize the drug
discovery process, offering improved efficiency, accuracy, and speed. However,
the successful application of AI is dependent on the availability of
high-quality data, the addressing of ethical concerns, and the recognition of
the limitations of AI-based approaches. In this article, the benefits,
challenges and drawbacks of AI in this field are reviewed, and possible
strategies and approaches for overcoming the present obstacles are proposed.
The use of data augmentation, explainable AI, and the integration of AI with
traditional experimental methods, as well as the potential advantages of AI in
pharmaceutical research are also discussed. Overall, this review highlights the
potential of AI in drug discovery and provides insights into the challenges and
opportunities for realizing its potential in this field.
  Note from the human-authors: This article was created to test the ability of
ChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors
in writing review articles. The text generated by the AI following our
instructions (see Supporting Information) was used as a starting point, and its
ability to automatically generate content was evaluated. After conducting a
thorough review, human authors practically rewrote the manuscript, striving to
maintain a balance between the original proposal and scientific criteria. The
advantages and limitations of using AI for this purpose are discussed in the
last section.",2022-12-08
Controlled Language Generation for Language Learning Items,2022-11-28 19:28:12+00:00,http://arxiv.org/abs/2211.15731v1,"Kevin Stowe, Debanjan Ghosh, Mengxuan Zhao","cs.CL, I.2.7",table2text,"This work aims to employ natural language generation (NLG) to rapidly
generate items for English language learning applications: this requires both
language models capable of generating fluent, high-quality English, and to
control the output of the generation to match the requirements of the relevant
items. We experiment with deep pretrained models for this task, developing
novel methods for controlling items for factors relevant in language learning:
diverse sentences for different proficiency levels and argument structure to
test grammar. Human evaluation demonstrates high grammatically scores for all
models (3.4 and above out of 4), and higher length (24%) and complexity (9%)
over the baseline for the advanced proficiency model. Our results show that we
can achieve strong performance while adding additional control to ensure
diverse, tailored content for individual users.",2022-11-28
CodeExp: Explanatory Code Document Generation,2022-11-25 18:05:44+00:00,http://arxiv.org/abs/2211.15395v1,"Haotian Cui, Chenglong Wang, Junjie Huang, Jeevana Priya Inala, Todd Mytkowicz, Bo Wang, Jianfeng Gao, Nan Duan","cs.CL, cs.LG, I.2.2; I.2.7",table2text,"Developing models that can automatically generate detailed code explanation
can greatly benefit software maintenance and programming education. However,
existing code-to-text generation models often produce only high-level summaries
of code that do not capture implementation-level choices essential for these
scenarios. To fill in this gap, we propose the code explanation generation
task. We first conducted a human study to identify the criteria for
high-quality explanatory docstring for code. Based on that, we collected and
refined a large-scale code docstring corpus and formulated automatic evaluation
metrics that best match human assessments. Finally, we present a multi-stage
fine-tuning strategy and baseline models for the task. Our experiments show
that (1) our refined training dataset lets models achieve better performance in
the explanation generation tasks compared to larger unrefined data (15x
larger), and (2) fine-tuned models can generate well-structured long docstrings
comparable to human-written ones. We envision our training dataset,
human-evaluation protocol, recommended metrics, and fine-tuning strategy can
boost future code explanation research. The code and annotated data are
available at https://github.com/subercui/CodeExp.",2022-11-25
"MUSIED: A Benchmark for Event Detection from Multi-Source Heterogeneous
  Informal Texts",2022-11-25 05:05:29+00:00,http://arxiv.org/abs/2211.13896v1,"Xiangyu Xi, Jianwei Lv, Shuaipeng Liu, Wei Ye, Fan Yang, Guanglu Wan",cs.CL,table2text,"Event detection (ED) identifies and classifies event triggers from
unstructured texts, serving as a fundamental task for information extraction.
Despite the remarkable progress achieved in the past several years, most
research efforts focus on detecting events from formal texts (e.g., news
articles, Wikipedia documents, financial announcements). Moreover, the texts in
each dataset are either from a single source or multiple yet relatively
homogeneous sources. With massive amounts of user-generated text accumulating
on the Web and inside enterprises, identifying meaningful events in these
informal texts, usually from multiple heterogeneous sources, has become a
problem of significant practical value. As a pioneering exploration that
expands event detection to the scenarios involving informal and heterogeneous
texts, we propose a new large-scale Chinese event detection dataset based on
user reviews, text conversations, and phone conversations in a leading
e-commerce platform for food service. We carefully investigate the proposed
dataset's textual informality and multi-source heterogeneity characteristics by
inspecting data samples quantitatively and qualitatively. Extensive experiments
with state-of-the-art event detection methods verify the unique challenges
posed by these characteristics, indicating that multi-source informal event
detection remains an open problem and requires further efforts. Our benchmark
and code are released at \url{https://github.com/myeclipse/MUSIED}.",2022-11-25
Retrieval-Augmented Multimodal Language Modeling,2022-11-22 20:26:44+00:00,http://arxiv.org/abs/2211.12561v1,"Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, Wen-tau Yih","cs.CV, cs.CL, cs.LG",table2text,"Recent multimodal models such as DALL-E and CM3 have achieved remarkable
progress in text-to-image and image-to-text generation. However, these models
store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the
model parameters, requiring increasingly larger models and training data to
capture more knowledge. To integrate knowledge in a more scalable and modular
way, we propose a retrieval-augmented multimodal model, which enables a base
multimodal model (generator) to refer to relevant knowledge fetched by a
retriever from external memory (e.g., multimodal documents on the web).
Specifically, we implement a retriever using the pretrained CLIP model and a
generator using the CM3 Transformer architecture, and train this model using
the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3),
is the first multimodal model that can retrieve and generate mixtures of text
and images. We show that RA-CM3 significantly outperforms baseline multimodal
models such as DALL-E and CM3 on both image and caption generation tasks (12
FID and 17 CIDEr improvements on MS-COCO), while requiring much less compute
for training (<30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel
capabilities such as knowledge-intensive image generation and multimodal
in-context learning.",2022-11-22
"How to Describe Images in a More Funny Way? Towards a Modular Approach
  to Cross-Modal Sarcasm Generation",2022-11-20 14:38:24+00:00,http://arxiv.org/abs/2211.10992v1,"Jie Ruan, Yue Wu, Xiaojun Wan, Yuesheng Zhu","cs.CV, cs.CL",table2text,"Sarcasm generation has been investigated in previous studies by considering
it as a text-to-text generation problem, i.e., generating a sarcastic sentence
for an input sentence. In this paper, we study a new problem of cross-modal
sarcasm generation (CMSG), i.e., generating a sarcastic description for a given
image. CMSG is challenging as models need to satisfy the characteristics of
sarcasm, as well as the correlation between different modalities. In addition,
there should be some inconsistency between the two modalities, which requires
imagination. Moreover, high-quality training data is insufficient. To address
these problems, we take a step toward generating sarcastic descriptions from
images without paired training data and propose an
Extraction-Generation-Ranking based Modular method (EGRM) for cross-model
sarcasm generation. Specifically, EGRM first extracts diverse information from
an image at different levels and uses the obtained image tags, sentimental
descriptive caption, and commonsense-based consequence to generate candidate
sarcastic texts. Then, a comprehensive ranking algorithm, which considers
image-text relation, sarcasticness, and grammaticality, is proposed to select a
final text from the candidate texts. Human evaluation at five criteria on a
total of 1200 generated image-text pairs from eight systems and auxiliary
automatic evaluation show the superiority of our method.",2022-11-20
"GENIUS: Sketch-based Language Model Pre-training via Extreme and
  Selective Masking for Text Generation and Augmentation",2022-11-18 16:39:45+00:00,http://arxiv.org/abs/2211.10330v1,"Biyang Guo, Yeyun Gong, Yelong Shen, Songqiao Han, Hailiang Huang, Nan Duan, Weizhu Chen",cs.CL,table2text,"We introduce GENIUS: a conditional text generation model using sketches as
input, which can fill in the missing contexts for a given sketch (key
information consisting of textual spans, phrases, or words, concatenated by
mask tokens). GENIUS is pre-trained on a large-scale textual corpus with a
novel reconstruction from sketch objective using an extreme and selective
masking strategy, enabling it to generate diverse and high-quality texts given
sketches. Comparison with other competitive conditional language models (CLMs)
reveals the superiority of GENIUS's text generation quality. We further show
that GENIUS can be used as a strong and ready-to-use data augmentation tool for
various natural language processing (NLP) tasks. Most existing textual data
augmentation methods are either too conservative, by making small changes to
the original text, or too aggressive, by creating entirely new samples. With
GENIUS, we propose GeniusAug, which first extracts the target-aware sketches
from the original training set and then generates new samples based on the
sketches. Empirical experiments on 6 text classification datasets show that
GeniusAug significantly improves the models' performance in both
in-distribution (ID) and out-of-distribution (OOD) settings. We also
demonstrate the effectiveness of GeniusAug on named entity recognition (NER)
and machine reading comprehension (MRC) tasks. (Code and models are publicly
available at https://github.com/microsoft/SCGLab and
https://github.com/beyondguo/genius)",2022-11-18
"Towards Computationally Verifiable Semantic Grounding for Language
  Models",2022-11-16 17:35:52+00:00,http://arxiv.org/abs/2211.09070v1,"Chris Alberti, Kuzman Ganchev, Michael Collins, Sebastian Gehrmann, Ciprian Chelba",cs.CL,table2text,"The paper presents an approach to semantic grounding of language models (LMs)
that conceptualizes the LM as a conditional model generating text given a
desired semantic message formalized as a set of entity-relationship triples. It
embeds the LM in an auto-encoder by feeding its output to a semantic parser
whose output is in the same representation domain as the input message.
Compared to a baseline that generates text using greedy search, we demonstrate
two techniques that improve the fluency and semantic accuracy of the generated
text: The first technique samples multiple candidate text sequences from which
the semantic parser chooses. The second trains the language model while keeping
the semantic parser frozen to improve the semantic accuracy of the
auto-encoder. We carry out experiments on the English WebNLG 3.0 data set,
using BLEU to measure the fluency of generated text and standard parsing
metrics to measure semantic accuracy. We show that our proposed approaches
significantly improve on the greedy search baseline. Human evaluation
corroborates the results of the automatic evaluation experiments.",2022-11-16
Reward Gaming in Conditional Text Generation,2022-11-16 07:10:02+00:00,http://arxiv.org/abs/2211.08714v1,"Richard Yuanzhe Pang, Vishakh Padmakumar, Thibault Sellam, Ankur P. Parikh, He He","cs.CL, cs.AI, cs.LG",table2text,"To align conditional text generation model outputs with desired behaviors,
there has been an increasing focus on training the model using reinforcement
learning (RL) with reward functions learned from human annotations. Under this
framework, we identify three common cases where high rewards are incorrectly
assigned to undesirable patterns: noise-induced spurious correlation, naturally
occurring spurious correlation, and covariate shift. We show that even though
learned metrics achieve high performance on the distribution of the data used
to train the reward function, the undesirable patterns may be amplified during
RL training of the text generation model. While there has been discussion about
reward gaming in the RL or safety community, in this short discussion piece, we
would like to highlight reward gaming in the NLG community using concrete
conditional text generation examples and discuss potential fixes and areas for
future work.",2022-11-16
"Follow the Wisdom of the Crowd: Effective Text Generation via Minimum
  Bayes Risk Decoding",2022-11-14 18:57:37+00:00,http://arxiv.org/abs/2211.07634v1,"Mirac Suzgun, Luke Melas-Kyriazi, Dan Jurafsky","cs.CL, cs.LG",table2text,"In open-ended natural-language generation, existing text decoding methods
typically struggle to produce text which is both diverse and high-quality.
Greedy and beam search are known to suffer from text degeneration and
linguistic diversity issues, while temperature, top-k, and nucleus sampling
often yield diverse but low-quality outputs. In this work, we present crowd
sampling, a family of decoding methods based on Bayesian risk minimization, to
address this diversity-quality trade-off. Inspired by the principle of ""the
wisdom of the crowd,"" crowd sampling seeks to select a candidate from a pool of
candidates that has the least expected risk (i.e., highest expected reward)
under a generative model according to a given utility function. Crowd sampling
can be seen as a generalization of numerous existing methods, including
majority voting, and in practice, it can be used as a drop-in replacement for
existing sampling methods. Extensive experiments show that crowd sampling
delivers improvements of 3-7 ROUGE and BLEU points across a wide range of
tasks, including summarization, data-to-text, translation, and textual style
transfer, while achieving new state-of-the-art results on WebNLG and WMT'16.",2022-11-14
"Evade the Trap of Mediocrity: Promoting Diversity and Novelty in Text
  Generation via Concentrating Attention",2022-11-14 07:53:16+00:00,http://arxiv.org/abs/2211.07164v1,"Wenhao Li, Xiaoyuan Yi, Jinyi Hu, Maosong Sun, Xing Xie",cs.CL,table2text,"Recently, powerful Transformer architectures have proven superior in
generating high-quality sentences. Nevertheless, these models tend to produce
dull high-frequency phrases, severely hurting the diversity and novelty of
generated text. In this work, we dig into the intrinsic mechanism of this
problem and found that sparser attention values in Transformer could improve
diversity. To understand such a phenomenon, we first conduct both empirical and
theoretical analysis and then attribute it to representation degeneration
caused by the attentive mixture of the hidden states during training. We term
this process the Trap of Mediocrity. To escape from such a trap, we introduce a
novel attention regularization loss to control the sharpness of the attention
distribution, which is transparent to model structures and can be easily
implemented within 20 lines of python code. We prove that this method could be
mathematically regarded as learning a Bayesian approximation of posterior
attention. Experiments show that our method improved the diversity and novelty
of the generated text while maintaining comparable quality on a variety of
conditional and unconditional generation tasks.",2022-11-14
Controllable Citation Text Generation,2022-11-14 01:54:08+00:00,http://arxiv.org/abs/2211.07066v1,"Nianlong Gu, Richard H. R. Hahnloser",cs.CL,table2text,"The aim of citation generation is usually to automatically generate a
citation sentence that refers to a chosen paper in the context of a manuscript.
However, a rigid citation generation process is at odds with an author's desire
to control the generated text based on certain attributes, such as 1) the
citation intent of e.g. either introducing background information or comparing
results; 2) keywords that should appear in the citation text; or 3) specific
sentences in the cited paper that characterize the citation content. To provide
these degrees of freedom, we present a controllable citation generation system.
In data from a large corpus, we first parse the attributes of each citation
sentence and use these as additional input sources during training of the
BART-based abstractive summarizer. We further develop an attribute suggestion
module that infers the citation intent and suggests relevant keywords and
sentences that users can select to tune the generation. Our framework gives
users more control over generated citations, outperforming citation generation
models without attribute awareness in both ROUGE and human evaluations.",2022-11-14
Self-conditioned Embedding Diffusion for Text Generation,2022-11-08 13:30:27+00:00,http://arxiv.org/abs/2211.04236v1,"Robin Strudel, Corentin Tallec, Florent Altch, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, Rmi Leblond","cs.CL, cs.LG",table2text,"Can continuous diffusion models bring the same performance breakthrough on
natural language they did for image generation? To circumvent the discrete
nature of text data, we can simply project tokens in a continuous space of
embeddings, as is standard in language modeling. We propose Self-conditioned
Embedding Diffusion, a continuous diffusion mechanism that operates on token
embeddings and allows to learn flexible and scalable diffusion models for both
conditional and unconditional text generation. Through qualitative and
quantitative evaluation, we show that our text diffusion models generate
samples comparable with those produced by standard autoregressive language
models - while being in theory more efficient on accelerator hardware at
inference time. Our work paves the way for scaling up diffusion models for
text, similarly to autoregressive models, and for improving performance with
recent refinements to continuous diffusion.",2022-11-08
Generative Transformers for Design Concept Generation,2022-11-07 11:29:10+00:00,http://arxiv.org/abs/2211.03468v1,"Qihao Zhu, Jianxi Luo",cs.CL,table2text,"Generating novel and useful concepts is essential during the early design
stage to explore a large variety of design opportunities, which usually
requires advanced design thinking ability and a wide range of knowledge from
designers. Growing works on computer-aided tools have explored the retrieval of
knowledge and heuristics from design data. However, they only provide stimuli
to inspire designers from limited aspects. This study explores the recent
advance of the natural language generation (NLG) technique in the artificial
intelligence (AI) field to automate the early-stage design concept generation.
Specifically, a novel approach utilizing the generative pre-trained transformer
(GPT) is proposed to leverage the knowledge and reasoning from textual data and
transform them into new concepts in understandable language. Three concept
generation tasks are defined to leverage different knowledge and reasoning:
domain knowledge synthesis, problem-driven synthesis, and analogy-driven
synthesis. The experiments with both human and data-driven evaluation show good
performance in generating novel and useful concepts.",2022-11-07
"Human-Machine Collaboration Approaches to Build a Dialogue Dataset for
  Hate Speech Countering",2022-11-07 10:37:13+00:00,http://arxiv.org/abs/2211.03433v1,"Helena Bonaldi, Sara Dellantonio, Serra Sinem Tekiroglu, Marco Guerini","cs.CL, cs.CY",table2text,"Fighting online hate speech is a challenge that is usually addressed using
Natural Language Processing via automatic detection and removal of hate
content. Besides this approach, counter narratives have emerged as an effective
tool employed by NGOs to respond to online hate on social media platforms. For
this reason, Natural Language Generation is currently being studied as a way to
automatize counter narrative writing. However, the existing resources necessary
to train NLG models are limited to 2-turn interactions (a hate speech and a
counter narrative as response), while in real life, interactions can consist of
multiple turns. In this paper, we present a hybrid approach for dialogical data
collection, which combines the intervention of human expert annotators over
machine generated dialogues obtained using 19 different configurations. The
result of this work is DIALOCONAN, the first dataset comprising over 3000
fictitious multi-turn dialogues between a hater and an NGO operator, covering 6
targets of hate.",2022-11-07
Time-aware Prompting for Text Generation,2022-11-03 22:10:25+00:00,http://arxiv.org/abs/2211.02162v1,"Shuyang Cao, Lu Wang",cs.CL,table2text,"In this paper, we study the effects of incorporating timestamps, such as
document creation dates, into generation systems. Two types of time-aware
prompts are investigated: (1) textual prompts that encode document timestamps
in natural language sentences; and (2) linear prompts that convert timestamps
into continuous vectors. To explore extrapolation to future data points, we
further introduce a new data-to-text generation dataset, TempWikiBio,
containing more than 4 millions of chronologically ordered revisions of
biographical articles from English Wikipedia, each paired with structured
personal profiles. Through data-to-text generation on TempWikiBio, text-to-text
generation on the content transfer dataset, and summarization on XSum, we show
that linear prompts on encoder and textual prompts improve the generation
quality on all datasets. Despite having less performance drop when testing on
data drawn from a later time, linear prompts focus more on non-temporal
information and are less sensitive to the given timestamps, according to human
evaluations and sensitivity analyses. Meanwhile, textual prompts establish the
association between the given timestamps and the output dates, yielding more
factual temporal information in the output.",2022-11-03
TaTa: A Multilingual Table-to-Text Dataset for African Languages,2022-10-31 21:05:42+00:00,http://arxiv.org/abs/2211.00142v1,"Sebastian Gehrmann, Sebastian Ruder, Vitaly Nikolaev, Jan A. Botha, Michael Chavinda, Ankur Parikh, Clara Rivera","cs.CL, cs.LG",table2text,"Existing data-to-text generation datasets are mostly limited to English. To
address this lack of data, we create Table-to-Text in African languages (TaTa),
the first large multilingual table-to-text dataset with a focus on African
languages. We created TaTa by transcribing figures and accompanying text in
bilingual reports by the Demographic and Health Surveys Program, followed by
professional translation to make the dataset fully parallel. TaTa includes
8,700 examples in nine languages including four African languages (Hausa, Igbo,
Swahili, and Yor\`ub\'a) and a zero-shot test language (Russian). We
additionally release screenshots of the original figures for future research on
multilingual multi-modal approaches. Through an in-depth human evaluation, we
show that TaTa is challenging for current models and that less than half the
outputs from an mT5-XXL-based model are understandable and attributable to the
source data. We further demonstrate that existing metrics perform poorly for
TaTa and introduce learned metrics that achieve a high correlation with human
judgments. We release all data and annotations at
https://github.com/google-research/url-nlp.",2022-10-31
DiffusER: Discrete Diffusion via Edit-based Reconstruction,2022-10-30 16:55:23+00:00,http://arxiv.org/abs/2210.16886v1,"Machel Reid, Vincent J. Hellendoorn, Graham Neubig","cs.CL, cs.LG",table2text,"In text generation, models that generate text from scratch one token at a
time are currently the dominant paradigm. Despite being performant, these
models lack the ability to revise existing text, which limits their usability
in many practical scenarios. We look to address this, with DiffusER (Diffusion
via Edit-based Reconstruction), a new edit-based generative model for text
based on denoising diffusion models -- a class of models that use a Markov
chain of denoising steps to incrementally generate data. DiffusER is not only a
strong generative model in general, rivalling autoregressive models on several
tasks spanning machine translation, summarization, and style transfer; it can
also perform other varieties of generation that standard autoregressive models
are not well-suited for. For instance, we demonstrate that DiffusER makes it
possible for a user to condition generation on a prototype, or an incomplete
sequence, and continue revising based on previous edit steps.",2022-10-30
"Diverse Parallel Data Synthesis for Cross-Database Adaptation of
  Text-to-SQL Parsers",2022-10-29 14:30:53+00:00,http://arxiv.org/abs/2210.16613v1,"Abhijeet Awasthi, Ashutosh Sathe, Sunita Sarawagi","cs.CL, cs.AI, cs.LG",table2text,"Text-to-SQL parsers typically struggle with databases unseen during the train
time. Adapting parsers to new databases is a challenging problem due to the
lack of natural language queries in the new schemas. We present ReFill, a
framework for synthesizing high-quality and textually diverse parallel datasets
for adapting a Text-to-SQL parser to a target schema. ReFill learns to
retrieve-and-edit text queries from the existing schemas and transfers them to
the target schema. We show that retrieving diverse existing text, masking their
schema-specific tokens, and refilling with tokens relevant to the target
schema, leads to significantly more diverse text queries than achievable by
standard SQL-to-Text generation methods. Through experiments spanning multiple
databases, we demonstrate that fine-tuning parsers on datasets synthesized
using ReFill consistently outperforms the prior data-augmentation methods.",2022-10-29
Nearest Neighbor Language Models for Stylistic Controllable Generation,2022-10-27 20:46:12+00:00,http://arxiv.org/abs/2210.15762v1,"Severino Trotta, Lucie Flek, Charles Welch",cs.CL,table2text,"Recent language modeling performance has been greatly improved by the use of
external memory. This memory encodes the context so that similar contexts can
be recalled during decoding. This similarity depends on how the model learns to
encode context, which can be altered to include other attributes, such as
style. We construct and evaluate an architecture for this purpose, using
corpora annotated for politeness, formality, and toxicity. Through extensive
experiments and human evaluation we demonstrate the potential of our method to
generate text while controlling style. We find that style-specific datastores
improve generation performance, though results vary greatly across styles, and
the effect of pretraining data and specific styles should be explored in future
work.",2022-10-27
Categorical SDEs with Simplex Diffusion,2022-10-26 15:27:43+00:00,http://arxiv.org/abs/2210.14784v1,"Pierre H. Richemond, Sander Dieleman, Arnaud Doucet",cs.LG,table2text,"Diffusion models typically operate in the standard framework of generative
modelling by producing continuously-valued datapoints. To this end, they rely
on a progressive Gaussian smoothing of the original data distribution, which
admits an SDE interpretation involving increments of a standard Brownian
motion. However, some applications such as text generation or reinforcement
learning might naturally be better served by diffusing categorical-valued data,
i.e., lifting the diffusion to a space of probability distributions. To this
end, this short theoretical note proposes Simplex Diffusion, a means to
directly diffuse datapoints located on an n-dimensional probability simplex. We
show how this relates to the Dirichlet distribution on the simplex and how the
analogous SDE is realized thanks to a multi-dimensional Cox-Ingersoll-Ross
process (abbreviated as CIR), previously used in economics and mathematical
finance. Finally, we make remarks as to the numerical implementation of
trajectories of the CIR process, and discuss some limitations of our approach.",2022-10-26
SentBS: Sentence-level Beam Search for Controllable Summarization,2022-10-26 06:21:01+00:00,http://arxiv.org/abs/2210.14502v1,"Chenhui Shen, Liying Cheng, Lidong Bing, Yang You, Luo Si",cs.CL,table2text,"A wide range of control perspectives have been explored in controllable text
generation. Structure-controlled summarization is recently proposed as a useful
and interesting research direction. However, current structure-controlling
methods have limited effectiveness in enforcing the desired structure. To
address this limitation, we propose a sentence-level beam search generation
method (SentBS), where evaluation is conducted throughout the generation
process to select suitable sentences for subsequent generations. We experiment
with different combinations of decoding methods to be used as subcomponents by
SentBS and evaluate results on the structure-controlled dataset MReD.
Experiments show that all explored combinations for SentBS can improve the
agreement between the generated text and the desired structure, with the best
method significantly reducing the structural discrepancies suffered by the
existing model, by approximately 68%.",2022-10-26
On the Effectiveness of Automated Metrics for Text Generation Systems,2022-10-24 08:15:28+00:00,http://arxiv.org/abs/2210.13025v1,"Pius von Dniken, Jan Deriu, Don Tuggener, Mark Cieliebak","cs.CL, cs.AI",table2text,"A major challenge in the field of Text Generation is evaluation because we
lack a sound theory that can be leveraged to extract guidelines for evaluation
campaigns. In this work, we propose a first step towards such a theory that
incorporates different sources of uncertainty, such as imperfect automated
metrics and insufficiently sized test sets. The theory has practical
applications, such as determining the number of samples needed to reliably
distinguish the performance of a set of Text Generation systems in a given
setting. We showcase the application of the theory on the WMT 21 and
Spot-The-Bot evaluation data and outline how it can be leveraged to improve the
evaluation protocol regarding the reliability, robustness, and significance of
the evaluation outcome.",2022-10-24
"Finding Memo: Extractive Memorization in Constrained Sequence Generation
  Tasks",2022-10-24 03:01:52+00:00,http://arxiv.org/abs/2210.12929v1,"Vikas Raunak, Arul Menezes","cs.CL, cs.AI, cs.LG",table2text,"Memorization presents a challenge for several constrained Natural Language
Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the
proclivity of neural models to memorize noisy and atypical samples reacts
adversely with the noisy (web crawled) datasets. However, previous studies of
memorization in constrained NLG tasks have only focused on counterfactual
memorization, linking it to the problem of hallucinations. In this work, we
propose a new, inexpensive algorithm for extractive memorization (exact
training data generation under insufficient context) in constrained sequence
generation tasks and use it to study extractive memorization and its effects in
NMT. We demonstrate that extractive memorization poses a serious threat to NMT
reliability by qualitatively and quantitatively characterizing the memorized
samples as well as the model behavior in their vicinity. Based on empirical
observations, we develop a simple algorithm which elicits non-memorized
translations of memorized samples from the same model, for a large fraction of
such samples. Finally, we show that the proposed algorithm could also be
leveraged to mitigate memorization in the model through finetuning. We have
released the code to reproduce our results at
https://github.com/vyraun/Finding-Memo.",2022-10-24
"Mapping Process for the Task: Wikidata Statements to Text as Wikipedia
  Sentences",2022-10-23 08:34:33+00:00,http://arxiv.org/abs/2210.12659v1,"Hoang Thang Ta, Alexander Gelbukha, Grigori Sidorov","cs.CL, cs.AI",table2text,"Acknowledged as one of the most successful online cooperative projects in
human society, Wikipedia has obtained rapid growth in recent years and desires
continuously to expand content and disseminate knowledge values for everyone
globally. The shortage of volunteers brings to Wikipedia many issues, including
developing content for over 300 languages at the present. Therefore, the
benefit that machines can automatically generate content to reduce human
efforts on Wikipedia language projects could be considerable. In this paper, we
propose our mapping process for the task of converting Wikidata statements to
natural language text (WS2T) for Wikipedia projects at the sentence level. The
main step is to organize statements, represented as a group of quadruples and
triples, and then to map them to corresponding sentences in English Wikipedia.
We evaluate the output corpus in various aspects: sentence structure analysis,
noise filtering, and relationships between sentence components based on word
embedding models. The results are helpful not only for the data-to-text
generation task but also for other relevant works in the field.",2022-10-23
"Hard Gate Knowledge Distillation -- Leverage Calibration for Robust and
  Reliable Language Model",2022-10-22 11:57:10+00:00,http://arxiv.org/abs/2210.12427v1,"Dongkyu Lee, Zhiliang Tian, Yingxiu Zhao, Ka Chun Cheung, Nevin L. Zhang","cs.CL, cs.AI",table2text,"In knowledge distillation, a student model is trained with supervisions from
both knowledge from a teacher and observations drawn from a training data
distribution. Knowledge of a teacher is considered a subject that holds
inter-class relations which send a meaningful supervision to a student; hence,
much effort has been put to find such knowledge to be distilled. In this paper,
we explore a question that has been given little attention: ""when to distill
such knowledge."" The question is answered in our work with the concept of model
calibration; we view a teacher model not only as a source of knowledge but also
as a gauge to detect miscalibration of a student. This simple and yet novel
view leads to a hard gate knowledge distillation scheme that switches between
learning from a teacher model and training data. We verify the gating mechanism
in the context of natural language generation at both the token-level and the
sentence-level. Empirical comparisons with strong baselines show that hard gate
knowledge distillation not only improves model generalization, but also
significantly lowers model calibration error.",2022-10-22
"Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in
  Transformer-Based Variational AutoEncoder for Diverse Text Generation",2022-10-22 10:25:35+00:00,http://arxiv.org/abs/2210.12409v2,"Jinyi Hu, Xiaoyuan Yi, Wenhao Li, Maosong Sun, Xing Xie",cs.CL,table2text,"Variational Auto-Encoder (VAE) has been widely adopted in text generation.
Among many variants, recurrent VAE learns token-wise latent variables with each
conditioned on the preceding ones, which captures sequential variability better
in the era of RNN. However, it is unclear how to incorporate such recurrent
dynamics into the recently dominant Transformer due to its parallelism. In this
work, we propose TRACE, a Transformer-based recurrent VAE structure. TRACE
imposes recurrence on segment-wise latent variables with arbitrarily separated
text segments and constructs the posterior distribution with residual
parameterization. Besides, we design an acceleration method by approximating
idempotent matrices, which allows parallelism while maintaining the conditional
dependence of latent variables. We demonstrate that TRACE could enhance the
entanglement of each segment and preceding latent variables and deduce a
non-zero lower bound of the KL term, providing a theoretical guarantee of
generation diversity. Experiments on two unconditional and one conditional
generation tasks show that TRACE achieves significantly improved diversity
while maintaining satisfactory generation quality.",2022-10-22
"ReasTAP: Injecting Table Reasoning Skills During Pre-training via
  Synthetic Reasoning Examples",2022-10-22 07:04:02+00:00,http://arxiv.org/abs/2210.12374v1,"Yilun Zhao, Linyong Nan, Zhenting Qi, Rui Zhang, Dragomir Radev",cs.CL,table2text,"Reasoning over tabular data requires both table structure understanding and a
broad set of table reasoning skills. Current models with table-specific
architectures and pre-training methods perform well on understanding table
structures, but they still struggle with tasks that require various table
reasoning skills. In this work, we develop ReasTAP to show that high-level
table reasoning skills can be injected into models during pre-training without
a complex table-specific architecture design. We define 7 table reasoning
skills, such as numerical operation, temporal comparison, and conjunction. Each
reasoning skill is associated with one example generator, which synthesizes
questions over semi-structured tables according to the sampled templates. We
model the table pre-training task as a sequence generation task and pre-train
ReasTAP to generate precise answers to the synthetic examples. ReasTAP is
evaluated on four benchmarks covering three downstream tasks including: 1)
WikiSQL and WTQ for Table Question Answering; 2) TabFact for Table Fact
Verification; and 3) LogicNLG for Faithful Table-to-Text Generation.
Experimental results demonstrate that ReasTAP achieves new state-of-the-art
performance on all benchmarks and delivers a significant improvement on
low-resource setting. Our code is publicly available at
https://github.com/Yale-LILY/ReasTAP.",2022-10-22
"The University of Edinburgh's Submission to the WMT22 Code-Mixing Shared
  Task (MixMT)",2022-10-20 14:40:10+00:00,http://arxiv.org/abs/2210.11309v1,"Faheem Kirefu, Vivek Iyer, Pinzhen Chen, Laurie Burchell",cs.CL,table2text,"The University of Edinburgh participated in the WMT22 shared task on
code-mixed translation. This consists of two subtasks: i) generating code-mixed
Hindi/English (Hinglish) text generation from parallel Hindi and English
sentences and ii) machine translation from Hinglish to English. As both
subtasks are considered low-resource, we focused our efforts on careful data
generation and curation, especially the use of backtranslation from monolingual
resources. For subtask 1 we explored the effects of constrained decoding on
English and transliterated subwords in order to produce Hinglish. For subtask
2, we investigated different pretraining techniques, namely comparing simple
initialisation from existing machine translation models and aligned
augmentation. For both subtasks, we found that our baseline systems worked
best. Our systems for both subtasks were one of the overall top-performing
submissions.",2022-10-20
Image Semantic Relation Generation,2022-10-19 16:15:19+00:00,http://arxiv.org/abs/2210.11253v1,Mingzhe Du,"cs.CV, cs.CL",table2text,"Scene graphs provide structured semantic understanding beyond images. For
downstream tasks, such as image retrieval, visual question answering, visual
relationship detection, and even autonomous vehicle technology, scene graphs
can not only distil complex image information but also correct the bias of
visual models using semantic-level relations, which has broad application
prospects. However, the heavy labour cost of constructing graph annotations may
hinder the application of PSG in practical scenarios. Inspired by the
observation that people usually identify the subject and object first and then
determine the relationship between them, we proposed to decouple the scene
graphs generation task into two sub-tasks: 1) an image segmentation task to
pick up the qualified objects. 2) a restricted auto-regressive text generation
task to generate the relation between given objects. Therefore, in this work,
we introduce image semantic relation generation (ISRG), a simple but effective
image-to-text model, which achieved 31 points on the OpenPSG dataset and
outperforms strong baselines respectively by 16 points (ResNet-50) and 5 points
(CLIP).",2022-10-19
NGEP: A Graph-based Event Planning Framework for Story Generation,2022-10-19 14:49:27+00:00,http://arxiv.org/abs/2210.10602v1,"Chen Tang, Zhihao Zhang, Tyler Loakman, Chenghua Lin, Frank Guerin","cs.CL, cs.AI",table2text,"To improve the performance of long text generation, recent studies have
leveraged automatically planned event structures (i.e. storylines) to guide
story generation. Such prior works mostly employ end-to-end neural generation
models to predict event sequences for a story. However, such generation models
struggle to guarantee the narrative coherence of separate events due to the
hallucination problem, and additionally the generated event sequences are often
hard to control due to the end-to-end nature of the models. To address these
challenges, we propose NGEP, an novel event planning framework which generates
an event sequence by performing inference on an automatically constructed event
graph and enhances generalisation ability through a neural event advisor. We
conduct a range of experiments on multiple criteria, and the results
demonstrate that our graph-based neural framework outperforms the
state-of-the-art (SOTA) event planning approaches, considering both the
performance of event sequence generation and the effectiveness on the
downstream task of story generation.",2022-10-19
"Attribution and Obfuscation of Neural Text Authorship: A Data Mining
  Perspective",2022-10-19 11:53:13+00:00,http://arxiv.org/abs/2210.10488v2,"Adaku Uchendu, Thai Le, Dongwon Lee","cs.CL, cs.LG",table2text,"Two interlocking research questions of growing interest and importance in
privacy research are Authorship Attribution (AA) and Authorship Obfuscation
(AO). Given an artifact, especially a text t in question, an AA solution aims
to accurately attribute t to its true author out of many candidate authors
while an AO solution aims to modify t to hide its true authorship.
Traditionally, the notion of authorship and its accompanying privacy concern is
only toward human authors. However, in recent years, due to the explosive
advancements in Neural Text Generation (NTG) techniques in NLP, capable of
synthesizing human-quality open-ended texts (so-called ""neural texts""), one has
to now consider authorships by humans, machines, or their combination. Due to
the implications and potential threats of neural texts when used maliciously,
it has become critical to understand the limitations of traditional AA/AO
solutions and develop novel AA/AO solutions in dealing with neural texts. In
this survey, therefore, we make a comprehensive review of recent literature on
the attribution and obfuscation of neural text authorship from a Data Mining
perspective, and share our view on their limitations and promising research
directions.",2022-10-19
"Team Flow at DRC2022: Pipeline System for Travel Destination
  Recommendation Task in Spoken Dialogue",2022-10-18 01:11:16+00:00,http://arxiv.org/abs/2210.09518v1,"Ryu Hirai, Atsumoto Ohashi, Ao Guo, Hideki Shiroma, Xulin Zhou, Yukihiko Tone, Shinya Iizuka, Ryuichiro Higashinaka","cs.CL, cs.AI, cs.RO",table2text,"To improve the interactive capabilities of a dialogue system, e.g., to adapt
to different customers, the Dialogue Robot Competition (DRC2022) was held. As
one of the teams, we built a dialogue system with a pipeline structure
containing four modules. The natural language understanding (NLU) and natural
language generation (NLG) modules were GPT-2 based models, and the dialogue
state tracking (DST) and policy modules were designed on the basis of
hand-crafted rules. After the preliminary round of the competition, we found
that the low variation in training examples for the NLU and failed
recommendation due to the policy used were probably the main reasons for the
limited performance of the system.",2022-10-18
Table-To-Text generation and pre-training with TabT5,2022-10-17 15:05:53+00:00,http://arxiv.org/abs/2210.09162v1,"Ewa Andrejczuk, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Yasemin Altun","cs.CL, cs.LG",table2text,"Encoder-only transformer models have been successfully applied to different
table understanding tasks, as in TAPAS (Herzig et al., 2020). A major
limitation of these architectures is that they are constrained to
classification-like tasks such as cell selection or entailment detection. We
present TABT5, an encoder-decoder model that generates natural language text
based on tables and textual inputs. TABT5 overcomes the encoder-only limitation
by incorporating a decoder component and leverages the input structure with
table specific embeddings and pre-training. TABT5 achieves new state-of-the-art
results on several domains, including spreadsheet formula prediction with a 15%
increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and
data-to-text generation with a 2.5% increase in BLEU.",2022-10-17
Model Criticism for Long-Form Text Generation,2022-10-16 04:35:58+00:00,http://arxiv.org/abs/2210.08444v1,"Yuntian Deng, Volodymyr Kuleshov, Alexander M. Rush","cs.CL, cs.LG, stat.ML",table2text,"Language models have demonstrated the ability to generate highly fluent text;
however, it remains unclear whether their output retains coherent high-level
structure (e.g., story progression). Here, we propose to apply a statistical
tool, model criticism in latent space, to evaluate the high-level structure of
the generated text. Model criticism compares the distributions between real and
generated data in a latent space obtained according to an assumptive generative
process. Different generative processes identify specific failure modes of the
underlying model. We perform experiments on three representative aspects of
high-level discourse -- coherence, coreference, and topicality -- and find that
transformer-based language models are able to capture topical structures but
have a harder time maintaining structural coherence or modeling coreference.",2022-10-16
"LEATHER: A Framework for Learning to Generate Human-like Text in
  Dialogue",2022-10-14 13:05:11+00:00,http://arxiv.org/abs/2210.07777v1,"Anthony Sicilia, Malihe Alikhani","cs.CL, cs.LG",table2text,"Algorithms for text-generation in dialogue can be misguided. For example, in
task-oriented settings, reinforcement learning that optimizes only task-success
can lead to abysmal lexical diversity. We hypothesize this is due to poor
theoretical understanding of the objectives in text-generation and their
relation to the learning process (i.e., model training). To this end, we
propose a new theoretical framework for learning to generate text in dialogue.
Compared to existing theories of learning, our framework allows for analysis of
the multi-faceted goals inherent to text-generation. We use our framework to
develop theoretical guarantees for learners that adapt to unseen data. As an
example, we apply our theory to study data-shift within a cooperative learning
algorithm proposed for the GuessWhat?! visual dialogue game. From this insight,
we propose a new algorithm, and empirically, we demonstrate our proposal
improves both task-success and human-likeness of the generated text. Finally,
we show statistics from our theory are empirically predictive of multiple
qualities of the generated dialogue, suggesting our theory is useful for
model-selection when human evaluations are not available.",2022-10-14
Towards a Unified Multi-Dimensional Evaluator for Text Generation,2022-10-13 17:17:03+00:00,http://arxiv.org/abs/2210.07197v1,"Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, Jiawei Han",cs.CL,table2text,"Multi-dimensional evaluation is the dominant paradigm for human evaluation in
Natural Language Generation (NLG), i.e., evaluating the generated text from
multiple explainable dimensions, such as coherence and fluency. However,
automatic evaluation in NLG is still dominated by similarity-based metrics, and
we lack a reliable framework for a more comprehensive evaluation of advanced
models. In this paper, we propose a unified multi-dimensional evaluator UniEval
for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task,
and by guiding the model with different questions, we can use one evaluator to
evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean
QA format, we are able to introduce an intermediate learning phase that enables
UniEval to incorporate external knowledge from multiple related tasks and gain
further improvement. Experiments on three typical NLG tasks show that UniEval
correlates substantially better with human judgments than existing metrics.
Specifically, compared to the top-performing unified evaluators, UniEval
achieves a 23% higher correlation on text summarization, and over 43% on
dialogue response generation. Also, UniEval demonstrates a strong zero-shot
learning ability for unseen evaluation dimensions and tasks. Source code, data
and all pre-trained evaluators are available on our GitHub repository
(https://github.com/maszhongming/UniEval).",2022-10-13
"Scaling Back-Translation with Domain Text Generation for Sign Language
  Gloss Translation",2022-10-13 14:25:08+00:00,http://arxiv.org/abs/2210.07054v1,"Jinhui Ye, Wenxiang Jiao, Xing Wang, Zhaopeng Tu","cs.CL, cs.AI",table2text,"Sign language gloss translation aims to translate the sign glosses into
spoken language texts, which is challenging due to the scarcity of labeled
gloss-text parallel data. Back translation (BT), which generates
pseudo-parallel data by translating in-domain spoken language texts into sign
glosses, has been applied to alleviate the data scarcity problem. However, the
lack of large-scale high-quality domain spoken language text data limits the
effect of BT. In this paper, to overcome the limitation, we propose a Prompt
based domain text Generation (PGEN) approach to produce the large-scale
in-domain spoken language text data. Specifically, PGEN randomly concatenates
sentences from the original in-domain spoken language text data as prompts to
induce a pre-trained language model (i.e., GPT-2) to generate spoken language
texts in a similar style. Experimental results on three benchmarks of sign
language gloss translation in varied languages demonstrate that BT with spoken
language texts generated by PGEN significantly outperforms the compared
methods. In addition, as the scale of spoken language texts generated by PGEN
increases, the BT technique can achieve further improvements, demonstrating the
effectiveness of our approach. We release the code and data for facilitating
future research in this field.",2022-10-13
Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis,2022-10-12 23:38:57+00:00,http://arxiv.org/abs/2210.06629v1,"Siddharth Varia, Shuai Wang, Kishaloy Halder, Robert Vacareanu, Miguel Ballesteros, Yassine Benajiba, Neha Anna John, Rishita Anubhai, Smaranda Muresan, Dan Roth",cs.CL,table2text,"Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis
task which involves four elements from user-generated texts: aspect term,
aspect category, opinion term, and sentiment polarity. Most computational
approaches focus on some of the ABSA sub-tasks such as tuple (aspect term,
sentiment polarity) or triplet (aspect term, opinion term, sentiment polarity)
extraction using either pipeline or joint modeling approaches. Recently,
generative approaches have been proposed to extract all four elements as (one
or more) quadruplets from text as a single task. In this work, we take a step
further and propose a unified framework for solving ABSA, and the associated
sub-tasks to improve the performance in few-shot scenarios. To this end, we
fine-tune a T5 model with instructional prompts in a multi-task learning
fashion covering all the sub-tasks, as well as the entire quadruple prediction
task. In experiments with multiple benchmark data sets, we show that the
proposed multi-task prompting approach brings performance boost (by absolute
$6.75$ F1) in the few-shot learning setting.",2022-10-12
DATScore: Evaluating Translation with Data Augmented Translations,2022-10-12 20:31:42+00:00,http://arxiv.org/abs/2210.06576v1,"Moussa Kamal Eddine, Guokan Shang, Michalis Vazirgiannis",cs.CL,table2text,"The rapid development of large pretrained language models has revolutionized
not only the field of Natural Language Generation (NLG) but also its
evaluation. Inspired by the recent work of BARTScore: a metric leveraging the
BART language model to evaluate the quality of generated text from various
aspects, we introduce DATScore. DATScore uses data augmentation techniques to
improve the evaluation of machine translation. Our main finding is that
introducing data augmented translations of the source and reference texts is
greatly helpful in evaluating the quality of the generated translation. We also
propose two novel score averaging and term weighting strategies to improve the
original score computing process of BARTScore. Experimental results on WMT show
that DATScore correlates better with human meta-evaluations than the other
recent state-of-the-art metrics, especially for low-resource languages.
Ablation studies demonstrate the value added by our new scoring strategies.
Moreover, we report in our extended experiments the performance of DATScore on
3 NLG tasks other than translation.",2022-10-12
ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models,2022-10-09 19:17:43+00:00,http://arxiv.org/abs/2210.04325v2,"Jiannan Xiang, Zhengzhong Liu, Yucheng Zhou, Eric P. Xing, Zhiting Hu",cs.CL,table2text,"Data-to-text generation is challenging due to the great variety of the input
data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse
predicates). Recent end-to-end neural methods thus require substantial training
examples to learn to disambiguate and describe the data. Yet, real-world
data-to-text problems often suffer from various data-scarce issues: one may
have access to only a handful of or no training examples, and/or have to rely
on examples in a different domain or schema. To fill this gap, we propose
Any-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse
settings by making efficient use of any given (or no) examples. ASDOT consists
of two steps, data disambiguation and sentence fusion, both of which are
amenable to be solved with off-the-shelf pretrained language models (LMs) with
optional finetuning. In the data disambiguation stage, we employ the prompted
GPT-3 model to understand possibly ambiguous triples from the input data and
convert each into a short sentence with reduced ambiguity. The sentence fusion
stage then uses an LM like T5 to fuse all the resulting sentences into a
coherent paragraph as the final description. We evaluate extensively on various
datasets in different scenarios, including the zero-/few-/full-shot settings,
and generalization to unseen predicates and out-of-domain data. Experimental
results show that ASDOT consistently achieves significant improvement over
baselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot
setting.",2022-10-09
"FAST: Improving Controllability for Text Generation with Feedback Aware
  Self-Training",2022-10-06 19:00:51+00:00,http://arxiv.org/abs/2210.03167v1,"Junyi Chai, Reid Pryzant, Victor Ye Dong, Konstantin Golobokov, Chenguang Zhu, Yi Liu",cs.CL,table2text,"Controllable text generation systems often leverage control codes to direct
various properties of the output like style and length. Inspired by recent work
on causal inference for NLP, this paper reveals a previously overlooked flaw in
these control code-based conditional text generation algorithms. Spurious
correlations in the training data can lead models to incorrectly rely on parts
of the input other than the control code for attribute selection, significantly
undermining downstream generation quality and controllability. We demonstrate
the severity of this issue with a series of case studies and then propose two
simple techniques to reduce these correlations in training sets. The first
technique is based on resampling the data according to an example's propensity
towards each linguistic attribute (IPS). The second produces multiple
counterfactual versions of each example and then uses an additional feedback
mechanism to remove noisy examples (feedback aware self-training, FAST). We
evaluate on 3 tasks -- news headline, meta review, and search ads generation --
and demonstrate that FAST can significantly improve the controllability and
language quality of generated outputs when compared to state-of-the-art
controllable text generation approaches.",2022-10-06
A Distributional Lens for Multi-Aspect Controllable Text Generation,2022-10-06 13:08:04+00:00,http://arxiv.org/abs/2210.02889v1,"Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Lingyuan Zhang, Heng Gong, Bing Qin",cs.CL,table2text,"Multi-aspect controllable text generation is a more challenging and practical
task than single-aspect control. Existing methods achieve complex multi-aspect
control by fusing multiple controllers learned from single-aspect, but suffer
from attribute degeneration caused by the mutual interference of these
controllers. To address this, we provide observations on attribute fusion from
a distributional perspective and propose to directly search for the
intersection areas of multiple attribute distributions as their combination for
generation. Our method first estimates the attribute space with an autoencoder
structure. Afterward, we iteratively approach the intersections by jointly
minimizing distances to points representing different attributes. Finally, we
map them to attribute-relevant sentences with a prefix-tuning-based decoder.
Experiments on the three-aspect control task, including sentiment, topic, and
detoxification aspects, reveal that our method outperforms several strong
baselines on attribute relevance and text quality and achieves the SOTA.
Further analysis also supplies some explanatory support for the effectiveness
of our approach.",2022-10-06
"Unsupervised Sentence Textual Similarity with Compositional Phrase
  Semantics",2022-10-05 14:14:04+00:00,http://arxiv.org/abs/2210.02284v1,"Zihao Wang, Jiaheng Dou, Yong Zhang",cs.CL,table2text,"Measuring Sentence Textual Similarity (STS) is a classic task that can be
applied to many downstream NLP applications such as text generation and
retrieval. In this paper, we focus on unsupervised STS that works on various
domains but only requires minimal data and computational resources.
Theoretically, we propose a light-weighted Expectation-Correction (EC)
formulation for STS computation. EC formulation unifies unsupervised STS
approaches including the cosine similarity of Additively Composed (AC) sentence
embeddings, Optimal Transport (OT), and Tree Kernels (TK). Moreover, we propose
the Recursive Optimal Transport Similarity (ROTS) algorithm to capture the
compositional phrase semantics by composing multiple recursive EC formulations.
ROTS finishes in linear time and is faster than its predecessors. ROTS is
empirically more effective and scalable than previous approaches. Extensive
experiments on 29 STS tasks under various settings show the clear advantage of
ROTS over existing approaches. Detailed ablation studies demonstrate the
effectiveness of our approaches.",2022-10-05
CodeDSI: Differentiable Code Search,2022-10-01 17:39:57+00:00,http://arxiv.org/abs/2210.00328v1,"Usama Nadeem, Noah Ziems, Shaoen Wu","cs.SE, cs.IR",table2text,"Reimplementing solutions to previously solved software engineering problems
is not only inefficient but also introduces inadequate and error-prone code.
Many existing methods achieve impressive performance on this issue by using
autoregressive text-generation models trained on code. However, these methods
are not without their flaws. The generated code from these models can be buggy,
lack documentation, and introduce vulnerabilities that may go unnoticed by
developers. An alternative to code generation -- neural code search -- is a
field of machine learning where a model takes natural language queries as input
and, in turn, relevant code samples from a database are returned. Due to the
nature of this pre-existing database, code samples can be documented, tested,
licensed, and checked for vulnerabilities before being used by developers in
production. In this work, we present CodeDSI, an end-to-end unified approach to
code search. CodeDSI is trained to directly map natural language queries to
their respective code samples, which can be retrieved later. In an effort to
improve the performance of code search, we have investigated docid
representation strategies, impact of tokenization on docid structure, and
dataset sizes on overall code search performance. Our results demonstrate
CodeDSI strong performance, exceeding conventional robust baselines by 2-6%
across varying dataset sizes.",2022-10-01
Calibrating Sequence likelihood Improves Conditional Language Generation,2022-09-30 19:16:16+00:00,http://arxiv.org/abs/2210.00045v1,"Yao Zhao, Misha Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, Peter J. Liu",cs.CL,table2text,"Conditional language models are predominantly trained with maximum likelihood
estimation (MLE), giving probability mass to sparsely observed target
sequences. While MLE trained models assign high probability to plausible
sequences given the context, the model probabilities often do not accurately
rank-order generated sequences by quality. This has been empirically observed
in beam search decoding as output quality degrading with large beam sizes, and
decoding strategies benefiting from heuristics such as length normalization and
repetition-blocking. In this work, we introduce sequence likelihood calibration
(SLiC) where the likelihood of model generated sequences are calibrated to
better align with reference sequences in the model's latent space. With SLiC,
decoding heuristics become unnecessary and decoding candidates' quality
significantly improves regardless of the decoding method. Furthermore, SLiC
shows no sign of diminishing returns with model scale, and presents alternative
ways to improve quality with limited training and inference budgets. With SLiC,
we exceed or match SOTA results on a wide range of generation tasks spanning
abstractive summarization, question generation, abstractive question answering
and data-to-text generation, even with modest-sized models.",2022-09-30
"Co-Writing Screenplays and Theatre Scripts with Language Models: An
  Evaluation by Industry Professionals",2022-09-29 17:26:22+00:00,http://arxiv.org/abs/2209.14958v1,"Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, Richard Evans","cs.HC, cs.CL",table2text,"Language models are increasingly attracting interest from writers. However,
such models lack long-range semantic coherence, limiting their usefulness for
longform creative writing. We address this limitation by applying language
models hierarchically, in a system we call Dramatron. By building structural
context via prompt chaining, Dramatron can generate coherent scripts and
screenplays complete with title, characters, story beats, location
descriptions, and dialogue. We illustrate Dramatron's usefulness as an
interactive co-creative system with a user study of 15 theatre and film
industry professionals. Participants co-wrote theatre scripts and screenplays
with Dramatron and engaged in open-ended interviews. We report critical
reflections both from our interviewees and from independent reviewers who
watched stagings of the works to illustrate how both Dramatron and hierarchical
text generation could be useful for human-machine co-creativity. Finally, we
discuss the suitability of Dramatron for co-creativity, ethical considerations
-- including plagiarism and bias -- and participatory models for the design and
deployment of such tools.",2022-09-29
"DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language
  Processing",2022-09-29 16:05:53+00:00,http://arxiv.org/abs/2209.14901v1,"Yanjun Gao, Dmitriy Dligach, Timothy Miller, John Caskey, Brihat Sharma, Matthew M Churpek, Majid Afshar","cs.CL, cs.AI",table2text,"The meaningful use of electronic health records (EHR) continues to progress
in the digital era with clinical decision support systems augmented by
artificial intelligence. A priority in improving provider experience is to
overcome information overload and reduce the cognitive burden so fewer medical
errors and cognitive biases are introduced during patient care. One major type
of medical error is diagnostic error due to systematic or predictable errors in
judgment that rely on heuristics. The potential for clinical natural language
processing (cNLP) to model diagnostic reasoning in humans with forward
reasoning from data to diagnosis and potentially reduce the cognitive burden
and medical error has not been investigated. Existing tasks to advance the
science in cNLP have largely focused on information extraction and named entity
recognition through classification tasks. We introduce a novel suite of tasks
coined as Diagnostic Reasoning Benchmarks, DR.BENCH, as a new benchmark for
developing and evaluating cNLP models with clinical diagnostic reasoning
ability. The suite includes six tasks from ten publicly available datasets
addressing clinical text understanding, medical knowledge reasoning, and
diagnosis generation. DR.BENCH is the first clinical suite of tasks designed to
be a natural language generation framework to evaluate pre-trained language
models. Experiments with state-of-the-art pre-trained generative language
models using large general domain models and models that were continually
trained on a medical corpus demonstrate opportunities for improvement when
evaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab
repository with a systematic approach to load and evaluate models for the cNLP
community.",2022-09-29
Informative Text Generation from Knowledge Triples,2022-09-26 14:35:57+00:00,http://arxiv.org/abs/2209.12733v1,"Zihao Fu, Yijiang River Dong, Lidong Bing, Wai Lam",cs.CL,table2text,"As the development of the encoder-decoder architecture, researchers are able
to study the text generation tasks with broader types of data. Among them,
KB-to-text aims at converting a set of knowledge triples into human readable
sentences. In the original setting, the task assumes that the input triples and
the text are exactly aligned in the perspective of the embodied
knowledge/information. In this paper, we extend this setting and explore how to
facilitate the trained model to generate more informative text, namely,
containing more information about the triple entities but not conveyed by the
input triples. To solve this problem, we propose a novel memory augmented
generator that employs a memory network to memorize the useful knowledge
learned during the training and utilizes such information together with the
input triples to generate text in the operational or testing phase. We derive a
dataset from WebNLG for our new setting and conduct extensive experiments to
investigate the effectiveness of our model as well as uncover the intrinsic
characteristics of the setting.",2022-09-26
Controllable Text Generation for Open-Domain Creativity and Fairness,2022-09-24 22:40:01+00:00,http://arxiv.org/abs/2209.12099v1,Nanyun Peng,"cs.CL, cs.AI",table2text,"Recent advances in large pre-trained language models have demonstrated strong
results in generating natural languages and significantly improved performances
for many natural language generation (NLG) applications such as machine
translation and text summarization. However, when the generation tasks are more
open-ended and the content is under-specified, existing techniques struggle to
generate long-term coherent and creative content. Moreover, the models exhibit
and even amplify social biases that are learned from the training corpora. This
happens because the generation models are trained to capture the surface
patterns (i.e. sequences of words), instead of capturing underlying semantics
and discourse structures, as well as background knowledge including social
norms. In this paper, I introduce our recent works on controllable text
generation to enhance the creativity and fairness of language generation
models. We explore hierarchical generation and constrained decoding, with
applications to creative language generation including story, poetry, and
figurative languages, and bias mitigation for generation models.",2022-09-24
XF2T: Cross-lingual Fact-to-Text Generation for Low-Resource Languages,2022-09-22 18:01:27+00:00,http://arxiv.org/abs/2209.11252v1,"Shivprasad Sagare, Tushar Abhishek, Bhavyajeet Singh, Anubhav Sharma, Manish Gupta, Vasudeva Varma",cs.CL,table2text,"Multiple business scenarios require an automated generation of descriptive
human-readable text from structured input data. Hence, fact-to-text generation
systems have been developed for various downstream tasks like generating soccer
reports, weather and financial reports, medical reports, person biographies,
etc. Unfortunately, previous work on fact-to-text (F2T) generation has focused
primarily on English mainly due to the high availability of relevant datasets.
Only recently, the problem of cross-lingual fact-to-text (XF2T) was proposed
for generation across multiple languages alongwith a dataset, XALIGN for eight
languages. However, there has been no rigorous work on the actual XF2T
generation problem. We extend XALIGN dataset with annotated data for four more
languages: Punjabi, Malayalam, Assamese and Oriya. We conduct an extensive
study using popular Transformer-based text generation models on our extended
multi-lingual dataset, which we call XALIGNV2. Further, we investigate the
performance of different text generation strategies: multiple variations of
pretraining, fact-aware embeddings and structure-aware input encoding. Our
extensive experiments show that a multi-lingual mT5 model which uses fact-aware
embeddings with structure-aware input encoding leads to best results on average
across the twelve languages. We make our code, dataset and model publicly
available, and hope that this will help advance further research in this
critical area.",2022-09-22
Selective Token Generation for Few-shot Natural Language Generation,2022-09-17 00:48:52+00:00,http://arxiv.org/abs/2209.08206v1,"Daejin Jo, Taehwan Kwon, Eun-Sol Kim, Sungwoong Kim","cs.CL, cs.LG",table2text,"Natural language modeling with limited training data is a challenging
problem, and many algorithms make use of large-scale pretrained language models
(PLMs) for this due to its great generalization ability. Among them, additive
learning that incorporates a task-specific adapter on top of the fixed
large-scale PLM has been popularly used in the few-shot setting. However, this
added adapter is still easy to disregard the knowledge of the PLM especially
for few-shot natural language generation (NLG) since an entire sequence is
usually generated by only the newly trained adapter. Therefore, in this work,
we develop a novel additive learning algorithm based on reinforcement learning
(RL) that selectively outputs language tokens between the task-general PLM and
the task-specific adapter during both training and inference. This output token
selection over the two generators allows the adapter to take into account
solely the task-relevant parts in sequence generation, and therefore makes it
more robust to overfitting as well as more stable in RL training. In addition,
to obtain the complementary adapter from the PLM for each few-shot task, we
exploit a separate selecting module that is also simultaneously trained using
RL. Experimental results on various few-shot NLG tasks including question
answering, data-to-text generation and text summarization demonstrate that the
proposed selective token generation significantly outperforms the previous
additive learning algorithms based on the PLMs.",2022-09-17
"Adaptive Natural Language Generation for Task-oriented Dialogue via
  Reinforcement Learning",2022-09-16 12:08:57+00:00,http://arxiv.org/abs/2209.07873v1,"Atsumoto Ohashi, Ryuichiro Higashinaka","cs.CL, cs.AI",table2text,"When a natural language generation (NLG) component is implemented in a
real-world task-oriented dialogue system, it is necessary to generate not only
natural utterances as learned on training data but also utterances adapted to
the dialogue environment (e.g., noise from environmental sounds) and the user
(e.g., users with low levels of understanding ability). Inspired by recent
advances in reinforcement learning (RL) for language generation tasks, we
propose ANTOR, a method for Adaptive Natural language generation for
Task-Oriented dialogue via Reinforcement learning. In ANTOR, a natural language
understanding (NLU) module, which corresponds to the user's understanding of
system utterances, is incorporated into the objective function of RL. If the
NLG's intentions are correctly conveyed to the NLU, which understands a
system's utterances, the NLG is given a positive reward. We conducted
experiments on the MultiWOZ dataset, and we confirmed that ANTOR could generate
adaptive utterances against speech recognition errors and the different
vocabulary levels of users.",2022-09-16
"TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for
  Multilingual Tweet Representations",2022-09-15 19:01:21+00:00,http://arxiv.org/abs/2209.07562v1,"Xinyang Zhang, Yury Malkov, Omar Florez, Serim Park, Brian McWilliams, Jiawei Han, Ahmed El-Kishky",cs.CL,table2text,"We present TwHIN-BERT, a multilingual language model trained on in-domain
data from the popular social network Twitter. TwHIN-BERT differs from prior
pre-trained language models as it is trained with not only text-based
self-supervision, but also with a social objective based on the rich social
engagements within a Twitter heterogeneous information network (TwHIN). Our
model is trained on 7 billion tweets covering over 100 distinct languages
providing a valuable representation to model short, noisy, user-generated text.
We evaluate our model on a variety of multilingual social recommendation and
semantic understanding tasks and demonstrate significant metric improvement
over established pre-trained language models. We will freely open-source
TwHIN-BERT and our curated hashtag prediction and social engagement benchmark
datasets to the research community.",2022-09-15
Distribution Aware Metrics for Conditional Natural Language Generation,2022-09-15 17:58:13+00:00,http://arxiv.org/abs/2209.07518v1,"David M Chan, Yiming Ni, Austin Myers, Sudheendra Vijayanarasimhan, David A Ross, John Canny","cs.CL, cs.AI, cs.CV, cs.LG",table2text,"Traditional automated metrics for evaluating conditional natural language
generation use pairwise comparisons between a single generated text and the
best-matching gold-standard ground truth text. When multiple ground truths are
available, scores are aggregated using an average or max operation across
references. While this approach works well when diversity in the ground truth
data (i.e. dispersion of the distribution of conditional texts) can be ascribed
to noise, such as in automated speech recognition, it does not allow for robust
evaluation in the case where diversity in the ground truths represents signal
for the model. In this work we argue that existing metrics are not appropriate
for domains such as visual description or summarization where ground truths are
semantically diverse, and where the diversity in those captions captures useful
additional information about the context. We propose a novel paradigm for
multi-candidate evaluation of conditional language generation models, and a new
family of metrics that compare the distributions of reference and
model-generated caption sets using small sample sets of each. We demonstrate
the utility of our approach with a case study in visual description: where we
show that existing models optimize for single-description quality over
diversity, and gain some insights into how sampling methods and temperature
impact description quality and diversity.",2022-09-15
vec2text with Round-Trip Translations,2022-09-14 17:20:18+00:00,http://arxiv.org/abs/2209.06792v1,"Geoffrey Cideron, Sertan Girgin, Anton Raichuk, Olivier Pietquin, Olivier Bachem, Lonard Hussenot","cs.CL, cs.LG",table2text,"We investigate models that can generate arbitrary natural language text (e.g.
all English sentences) from a bounded, convex and well-behaved control space.
We call them universal vec2text models. Such models would allow making semantic
decisions in the vector space (e.g. via reinforcement learning) while the
natural language generation is handled by the vec2text model. We propose four
desired properties: universality, diversity, fluency, and semantic structure,
that such vec2text models should possess and we provide quantitative and
qualitative methods to assess them. We implement a vec2text model by adding a
bottleneck to a 250M parameters Transformer model and training it with an
auto-encoding objective on 400M sentences (10B tokens) extracted from a massive
web corpus. We propose a simple data augmentation technique based on round-trip
translations and show in extensive experiments that the resulting vec2text
model surprisingly leads to vector spaces that fulfill our four desired
properties and that this model strongly outperforms both standard and denoising
auto-encoders.",2022-09-14
LibertyMFD: A Lexicon to Assess the Moral Foundation of Liberty,2022-09-14 16:14:54+00:00,http://arxiv.org/abs/2209.06750v1,"Oscar Araque, Lorenzo Gatti, Kyriaki Kalimeri",cs.CL,table2text,"Quantifying the moral narratives expressed in the user-generated text, news,
or public discourses is fundamental for understanding individuals' concerns and
viewpoints and preventing violent protests and social polarisation. The Moral
Foundation Theory (MFT) was developed to operationalise morality in a
five-dimensional scale system. Recent developments of the theory urged for the
introduction of a new foundation, the Liberty Foundation. Being only recently
added to the theory, there are no available linguistic resources to assess
whether liberty is present in text corpora. Given its importance to current
social issues such as the vaccination debate, we propose two data-driven
approaches, deriving two candidate lexicons generated based on aligned
documents from online news sources with different worldviews. After extensive
experimentation, we contribute to the research community a novel lexicon that
assesses the liberty moral foundation in the way individuals with contrasting
viewpoints express themselves through written text. The LibertyMFD dictionary
can be a valuable tool for policymakers to understand diverse viewpoints on
controversial social issues such as vaccination, abortion, or even uprisings,
as they happen and on a large scale.",2022-09-14
"OPAL: Ontology-Aware Pretrained Language Model for End-to-End
  Task-Oriented Dialogue",2022-09-10 04:38:27+00:00,http://arxiv.org/abs/2209.04595v1,"Zhi Chen, Yuncong Liu, Lu Chen, Su Zhu, Mengyue Wu, Kai Yu",cs.CL,table2text,"This paper presents an ontology-aware pretrained language model (OPAL) for
end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models,
task-oriented dialogue models fulfill at least two task-specific modules:
dialogue state tracker (DST) and response generator (RG). The dialogue state
consists of the domain-slot-value triples, which are regarded as the user's
constraints to search the domain-related databases. The large-scale
task-oriented dialogue data with the annotated structured dialogue state
usually are inaccessible. It prevents the development of the pretrained
language model for the task-oriented dialogue. We propose a simple yet
effective pretraining method to alleviate this problem, which consists of two
pretraining phases. The first phase is to pretrain on large-scale contextual
text data, where the structured information of the text is extracted by the
information extracting tool. To bridge the gap between the pretraining method
and downstream tasks, we design two pretraining tasks: ontology-like triple
recovery and next-text generation, which simulates the DST and RG,
respectively. The second phase is to fine-tune the pretrained model on the TOD
data. The experimental results show that our proposed method achieves an
exciting boost and get competitive performance even without any TOD data on
CamRest676 and MultiWOZ benchmarks.",2022-09-10
"Layer or Representation Space: What makes BERT-based Evaluation Metrics
  Robust?",2022-09-06 09:10:54+00:00,http://arxiv.org/abs/2209.02317v2,"Doan Nam Long Vu, Nafise Sadat Moosavi, Steffen Eger",cs.CL,table2text,"The evaluation of recent embedding-based evaluation metrics for text
generation is primarily based on measuring their correlation with human
evaluations on standard benchmarks. However, these benchmarks are mostly from
similar domains to those used for pretraining word embeddings. This raises
concerns about the (lack of) generalization of embedding-based metrics to new
and noisy domains that contain a different vocabulary than the pretraining
data. In this paper, we examine the robustness of BERTScore, one of the most
popular embedding-based metrics for text generation. We show that (a) an
embedding-based metric that has the highest correlation with human evaluations
on a standard benchmark can have the lowest correlation if the amount of input
noise or unknown tokens increases, (b) taking embeddings from the first layer
of pretrained models improves the robustness of all metrics, and (c) the
highest robustness is achieved when using character-level embeddings, instead
of token-based embeddings, from the first layer of the pretrained model.",2022-09-06
"Every picture tells a story: Image-grounded controllable stylistic story
  generation",2022-09-04 15:07:53+00:00,http://arxiv.org/abs/2209.01638v1,"Holy Lovenia, Bryan Wilie, Romain Barraud, Samuel Cahyawijaya, Willy Chung, Pascale Fung",cs.CL,table2text,"Generating a short story out of an image is arduous. Unlike image captioning,
story generation from an image poses multiple challenges: preserving the story
coherence, appropriately assessing the quality of the story, steering the
generated story into a certain style, and addressing the scarcity of
image-story pair reference datasets limiting supervision during training. In
this work, we introduce Plug-and-Play Story Teller (PPST) and improve
image-to-story generation by: 1) alleviating the data scarcity problem by
incorporating large pre-trained models, namely CLIP and GPT-2, to facilitate a
fluent image-to-text generation with minimal supervision, and 2) enabling a
more style-relevant generation by incorporating stylistic adapters to control
the story generation. We conduct image-to-story generation experiments with
non-styled, romance-styled, and action-styled PPST approaches and compare our
generated stories with those of previous work over three aspects, i.e., story
coherence, image-story relevance, and style fitness, using both automatic and
human evaluation. The results show that PPST improves story coherence and has
better image-story relevance, but has yet to be adequately stylistic.",2022-09-04
Multi-Modal Experience Inspired AI Creation,2022-09-02 11:50:41+00:00,http://arxiv.org/abs/2209.02427v1,"Qian Cao, Xu Chen, Ruihua Song, Hao Jiang, Guang Yang, Zhao Cao",cs.AI,table2text,"AI creation, such as poem or lyrics generation, has attracted increasing
attention from both industry and academic communities, with many promising
models proposed in the past few years. Existing methods usually estimate the
outputs based on single and independent visual or textual information. However,
in reality, humans usually make creations according to their experiences, which
may involve different modalities and be sequentially correlated. To model such
human capabilities, in this paper, we define and solve a novel AI creation
problem based on human experiences. More specifically, we study how to generate
texts based on sequential multi-modal information. Compared with the previous
works, this task is much more difficult because the designed model has to well
understand and adapt the semantics among different modalities and effectively
convert them into the output in a sequential manner. To alleviate these
difficulties, we firstly design a multi-channel sequence-to-sequence
architecture equipped with a multi-modal attention network. For more effective
optimization, we then propose a curriculum negative sampling strategy tailored
for the sequential inputs. To benchmark this problem and demonstrate the
effectiveness of our model, we manually labeled a new multi-modal experience
dataset. With this dataset, we conduct extensive experiments by comparing our
model with a series of representative baselines, where we can demonstrate
significant improvements in our model based on both automatic and
human-centered metrics. The code and data are available at:
\url{https://github.com/Aman-4-Real/MMTG}.",2022-09-02
A Spanish dataset for Targeted Sentiment Analysis of political headlines,2022-08-30 01:30:30+00:00,http://arxiv.org/abs/2208.13947v1,"Toms Alves Salgueiro, Emilio Recart Zapata, Damin Furman, Juan Manuel Prez, Pablo Nicols Fernndez Larrosa",cs.CL,table2text,"Subjective texts have been studied by several works as they can induce
certain behaviours in their users. Most work focuses on user-generated texts in
social networks, but some other texts also comprise opinions on certain topics
and could influence judgement criteria during political decisions. In this
work, we address the task of Targeted Sentiment Analysis for the domain of news
headlines, published by the main outlets during the 2019 Argentinean
Presidential Elections. For this purpose, we present a polarity dataset of
1,976 headlines mentioning candidates in the 2019 elections at the target
level. Preliminary experiments with state-of-the-art classification algorithms
based on pre-trained linguistic models suggest that target information is
helpful for this task. We make our data and pre-trained models publicly
available.",2022-08-30
"StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse
  Representations and Content Enhancing",2022-08-29 08:47:49+00:00,http://arxiv.org/abs/2208.13423v1,"Xuekai Zhu, Jian Guan, Minlie Huang, Juan Liu",cs.CL,table2text,"Non-parallel text style transfer is an important task in natural language
generation. However, previous studies concentrate on the token or sentence
level, such as sentence sentiment and formality transfer, but neglect long
style transfer at the discourse level. Long texts usually involve more
complicated author linguistic preferences such as discourse structures than
sentences. In this paper, we formulate the task of non-parallel story
author-style transfer, which requires transferring an input story into a
specified author style while maintaining source semantics. To tackle this
problem, we propose a generation model, named StoryTrans, which leverages
discourse representations to capture source content information and transfer
them to target styles with learnable style embeddings. We use an additional
training objective to disentangle stylistic features from the learned discourse
representation to prevent the model from degenerating to an auto-encoder.
Moreover, to enhance content preservation, we design a mask-and-fill framework
to explicitly fuse style-specific keywords of source texts into generation.
Furthermore, we constructed new datasets for this task in Chinese and English,
respectively. Extensive experiments show that our model outperforms strong
baselines in overall performance of style transfer and content preservation.",2022-08-29
Nearest Neighbor Non-autoregressive Text Generation,2022-08-26 08:21:21+00:00,http://arxiv.org/abs/2208.12496v1,"Ayana Niwa, Sho Takase, Naoaki Okazaki",cs.CL,table2text,"Non-autoregressive (NAR) models can generate sentences with less computation
than autoregressive models but sacrifice generation quality. Previous studies
addressed this issue through iterative decoding. This study proposes using
nearest neighbors as the initial state of an NAR decoder and editing them
iteratively. We present a novel training strategy to learn the edit operations
on neighbors to improve NAR text generation. Experimental results show that the
proposed method (NeighborEdit) achieves higher translation quality (1.69 points
higher than the vanilla Transformer) with fewer decoding iterations
(one-eighteenth fewer iterations) on the JRC-Acquis En-De dataset, the common
benchmark dataset for machine translation using nearest neighbors. We also
confirm the effectiveness of the proposed method on a data-to-text task
(WikiBio). In addition, the proposed method outperforms an NAR baseline on the
WMT'14 En-De dataset. We also report analysis on neighbor examples used in the
proposed method.",2022-08-26
"GenTUS: Simulating User Behaviour and Language in Task-oriented
  Dialogues with Generative Transformers",2022-08-23 09:01:17+00:00,http://arxiv.org/abs/2208.10817v1,"Hsien-Chin Lin, Christian Geishauser, Shutong Feng, Nurul Lubis, Carel van Niekerk, Michael Heck, Milica Gai",cs.CL,table2text,"User simulators (USs) are commonly used to train task-oriented dialogue
systems (DSs) via reinforcement learning. The interactions often take place on
semantic level for efficiency, but there is still a gap from semantic actions
to natural language, which causes a mismatch between training and deployment
environment. Incorporating a natural language generation (NLG) module with USs
during training can partly deal with this problem. However, since the policy
and NLG of USs are optimised separately, these simulated user utterances may
not be natural enough in a given context. In this work, we propose a generative
transformer-based user simulator (GenTUS). GenTUS consists of an
encoder-decoder structure, which means it can optimise both the user policy and
natural language generation jointly. GenTUS generates both semantic actions and
natural language utterances, preserving interpretability and enhancing language
variation. In addition, by representing the inputs and outputs as word
sequences and by using a large pre-trained language model we can achieve
generalisability in feature representation. We evaluate GenTUS with automatic
metrics and human evaluation. Our results show that GenTUS generates more
natural language and is able to transfer to an unseen ontology in a zero-shot
fashion. In addition, its behaviour can be further shaped with reinforcement
learning opening the door to training specialised user simulators.",2022-08-23
Few-Shot Table-to-Text Generation with Prefix-Controlled Generator,2022-08-23 03:23:26+00:00,http://arxiv.org/abs/2208.10709v1,"Yutao Luo, Menghua Lu, Gongshen Liu, Shilin Wang",cs.CL,table2text,"Neural table-to-text generation approaches are data-hungry, limiting their
adaptation for low-resource real-world applications. Previous works mostly
resort to Pre-trained Language Models (PLMs) to generate fluent summaries of a
table. However, they often contain hallucinated contents due to the
uncontrolled nature of PLMs. Moreover, the topological differences between
tables and sequences are rarely studied. Last but not least, fine-tuning on
PLMs with a handful of instances may lead to over-fitting and catastrophic
forgetting. To alleviate these problems, we propose a prompt-based approach,
Prefix-Controlled Generator (i.e., PCG), for few-shot table-to-text generation.
We prepend a task-specific prefix for a PLM to make the table structure better
fit the pre-trained input. In addition, we generate an input-specific prefix to
control the factual contents and word order of the generated text. Both
automatic and human evaluations on different domains (humans, books and songs)
of the Wikibio dataset show substantial improvements over baseline approaches.",2022-08-23
Automatic tagging of knowledge points for K12 math problems,2022-08-21 11:11:30+00:00,http://arxiv.org/abs/2208.09867v1,"Xiaolu Wang, Ziqi Ding, Liangyu Chen",cs.CL,table2text,"Automatic tagging of knowledge points for practice problems is the basis for
managing question bases and improving the automation and intelligence of
education. Therefore, it is of great practical significance to study the
automatic tagging technology for practice problems. However, there are few
studies on the automatic tagging of knowledge points for math problems. Math
texts have more complex structures and semantics compared with general texts
because they contain unique elements such as symbols and formulas. Therefore,
it is difficult to meet the accuracy requirement of knowledge point prediction
by directly applying the text classification techniques in general domains. In
this paper, K12 math problems taken as the research object, the LABS model
based on label-semantic attention and multi-label smoothing combining textual
features is proposed to improve the automatic tagging of knowledge points for
math problems. The model combines the text classification techniques in general
domains and the unique features of math texts. The results show that the models
using label-semantic attention or multi-label smoothing perform better on
precision, recall, and F1-score metrics than the traditional BiLSTM model,
while the LABS model using both performs best. It can be seen that label
information can guide the neural networks to extract meaningful information
from the problem text, which improves the text classification performance of
the model. Moreover, multi-label smoothing combining textual features can fully
explore the relationship between text and labels, improve the model's
prediction ability for new data and improve the model's classification
accuracy.",2022-08-21
"Beyond Text Generation: Supporting Writers with Continuous Automatic
  Text Summaries",2022-08-19 13:09:56+00:00,http://arxiv.org/abs/2208.09323v1,"Hai Dang, Karim Benharrak, Florian Lehmann, Daniel Buschek","cs.HC, cs.CL, H.5.2; I.2.7",table2text,"We propose a text editor to help users plan, structure and reflect on their
writing process. It provides continuously updated paragraph-wise summaries as
margin annotations, using automatic text summarization. Summary levels range
from full text, to selected (central) sentences, down to a collection of
keywords. To understand how users interact with this system during writing, we
conducted two user studies (N=4 and N=8) in which people wrote analytic essays
about a given topic and article. As a key finding, the summaries gave users an
external perspective on their writing and helped them to revise the content and
scope of their drafted paragraphs. People further used the tool to quickly gain
an overview of the text and developed strategies to integrate insights from the
automated summaries. More broadly, this work explores and highlights the value
of designing AI tools for writers, with Natural Language Processing (NLP)
capabilities that go beyond direct text generation and correction.",2022-08-19
"Coarse-to-Fine: Hierarchical Multi-task Learning for Natural Language
  Understanding",2022-08-19 02:46:20+00:00,http://arxiv.org/abs/2208.09129v1,"Zhaoye Fei, Yu Tian, Yongkang Wu, Xinyu Zhang, Yutao Zhu, Zheng Liu, Jiawen Wu, Dejiang Kong, Ruofei Lai, Zhao Cao, Zhicheng Dou, Xipeng Qiu",cs.CL,table2text,"Generalized text representations are the foundation of many natural language
understanding tasks. To fully utilize the different corpus, it is inevitable
that models need to understand the relevance among them. However, many methods
ignore the relevance and adopt a single-channel model (a coarse paradigm)
directly for all tasks, which lacks enough rationality and interpretation. In
addition, some existing works learn downstream tasks by stitches skill block(a
fine paradigm), which might cause irrationalresults due to its redundancy and
noise. Inthis work, we first analyze the task correlation through three
different perspectives, i.e., data property, manual design, and model-based
relevance, based on which the similar tasks are grouped together. Then, we
propose a hierarchical framework with a coarse-to-fine paradigm, with the
bottom level shared to all the tasks, the mid-level divided to different
groups, and the top-level assigned to each of the tasks. This allows our model
to learn basic language properties from all tasks, boost performance on
relevant tasks, and reduce the negative impact from irrelevant tasks. Our
experiments on 13 benchmark datasets across five natural language understanding
tasks demonstrate the superiority of our method.",2022-08-19
"Performance Optimization for Semantic Communications: An Attention-based
  Reinforcement Learning Approach",2022-08-17 11:39:16+00:00,http://arxiv.org/abs/2208.08239v1,"Yining Wang, Mingzhe Chen, Tao Luo, Walid Saad, Dusit Niyato, H. Vincent Poor, Shuguang Cui","cs.IT, cs.AI, math.IT",table2text,"In this paper, a semantic communication framework is proposed for textual
data transmission. In the studied model, a base station (BS) extracts the
semantic information from textual data, and transmits it to each user. The
semantic information is modeled by a knowledge graph (KG) that consists of a
set of semantic triples. After receiving the semantic information, each user
recovers the original text using a graph-to-text generation model. To measure
the performance of the considered semantic communication framework, a metric of
semantic similarity (MSS) that jointly captures the semantic accuracy and
completeness of the recovered text is proposed. Due to wireless resource
limitations, the BS may not be able to transmit the entire semantic information
to each user and satisfy the transmission delay constraint. Hence, the BS must
select an appropriate resource block for each user as well as determine and
transmit part of the semantic information to the users. As such, we formulate
an optimization problem whose goal is to maximize the total MSS by jointly
optimizing the resource allocation policy and determining the partial semantic
information to be transmitted. To solve this problem, a
proximal-policy-optimization-based reinforcement learning (RL) algorithm
integrated with an attention network is proposed. The proposed algorithm can
evaluate the importance of each triple in the semantic information using an
attention network and then, build a relationship between the importance
distribution of the triples in the semantic information and the total MSS.
Compared to traditional RL algorithms, the proposed algorithm can dynamically
adjust its learning rate thus ensuring convergence to a locally optimal
solution.",2022-08-17
High Recall Data-to-text Generation with Progressive Edit,2022-08-09 06:22:05+00:00,http://arxiv.org/abs/2208.04558v1,"Choonghan Kim, Gary Geunbae Lee","cs.CL, cs.AI",table2text,"Data-to-text (D2T) generation is the task of generating texts from structured
inputs. We observed that when the same target sentence was repeated twice,
Transformer (T5) based model generates an output made up of asymmetric
sentences from structured inputs. In other words, these sentences were
different in length and quality. We call this phenomenon ""Asymmetric
Generation"" and we exploit this in D2T generation. Once asymmetric sentences
are generated, we add the first part of the output with a no-repeated-target.
As this goes through progressive edit (ProEdit), the recall increases. Hence,
this method better covers structured inputs than before editing. ProEdit is a
simple but effective way to improve performance in D2T generation and it
achieves the new stateof-the-art result on the ToTTo dataset",2022-08-09
"Suggestion Lists vs. Continuous Generation: Interaction Design for
  Writing with Generative Models on Mobile Devices Affect Text Length, Wording
  and Perceived Authorship",2022-08-01 13:57:11+00:00,http://arxiv.org/abs/2208.00870v1,"Florian Lehmann, Niklas Markert, Hai Dang, Daniel Buschek","cs.HC, cs.AI",table2text,"Neural language models have the potential to support human writing. However,
questions remain on their integration and influence on writing and output. To
address this, we designed and compared two user interfaces for writing with AI
on mobile devices, which manipulate levels of initiative and control: 1)
Writing with continuously generated text, the AI adds text word-by-word and
user steers. 2) Writing with suggestions, the AI suggests phrases and user
selects from a list. In a supervised online study (N=18), participants used
these prototypes and a baseline without AI. We collected touch interactions,
ratings on inspiration and authorship, and interview data. With AI suggestions,
people wrote less actively, yet felt they were the author. Continuously
generated text reduced this perceived authorship, yet increased editing
behavior. In both designs, AI increased text length and was perceived to
influence wording. Our findings add new empirical evidence on the impact of UI
design decisions on user experience and output with co-creative systems.",2022-08-01
"LaKo: Knowledge-driven Visual Question Answering via Late
  Knowledge-to-Text Injection",2022-07-26 13:29:51+00:00,http://arxiv.org/abs/2207.12888v1,"Zhuo Chen, Yufeng Huang, Jiaoyan Chen, Yuxia Geng, Yin Fang, Jeff Pan, Ningyu Zhang, Wen Zhang","cs.CV, cs.AI",table2text,"Visual question answering (VQA) often requires an understanding of visual
concepts and language semantics, which relies on external knowledge. Most
existing methods exploit pre-trained language models or/and unstructured text,
but the knowledge in these resources are often incomplete and noisy. Some
methods prefer to use knowledge graphs (KGs) which often have intensive
structured knowledge, but the research is still quite preliminary. In this
paper, we propose LaKo, a knowledge-driven VQA method via Late
Knowledge-to-text Injection. To effectively incorporate an external KG, we
transfer triples into text and propose a late injection mechanism. Finally we
address VQA as a text generation task with an effective encoder-decoder
paradigm. In the evaluation with OKVQA datasets, our method achieves
state-of-the-art results.",2022-07-26
Innovations in Neural Data-to-text Generation,2022-07-25 23:21:48+00:00,http://arxiv.org/abs/2207.12571v1,"Mandar Sharma, Ajay Gogineni, Naren Ramakrishnan",cs.CL,table2text,"The neural boom that has sparked natural language processing (NLP) research
through the last decade has similarly led to significant innovations in
data-to-text generation (DTG). This survey offers a consolidated view into the
neural DTG paradigm with a structured examination of the approaches, benchmark
datasets, and evaluation protocols. This survey draws boundaries separating DTG
from the rest of the natural language generation (NLG) landscape, encompassing
an up-to-date synthesis of the literature, and highlighting the stages of
technological adoption from within and outside the greater NLG umbrella. With
this holistic view, we highlight promising avenues for DTG research that not
only focus on the design of linguistically capable systems but also systems
that exhibit fairness and accountability.",2022-07-25
"Leveraging Natural Supervision for Language Representation Learning and
  Generation",2022-07-21 17:26:03+00:00,http://arxiv.org/abs/2207.10617v1,Mingda Chen,cs.CL,table2text,"Recent breakthroughs in Natural Language Processing (NLP) have been driven by
language models trained on a massive amount of plain text. While powerful,
deriving supervision from textual resources is still an open question. For
example, language model pretraining often neglects the rich, freely-available
structures in textual data. In this thesis, we describe three lines of work
that seek to improve the training and evaluation of neural models using
naturally-occurring supervision.
  We first investigate self-supervised training losses to help enhance the
performance of pretrained language models for various NLP tasks. Specifically,
we alter the sentence prediction loss to make it better suited to other
pretraining losses and more challenging to solve. We design an intermediate
finetuning step that uses self-supervised training to promote models' ability
in cross-task generalization.
  Then we describe methods to leverage the structures in Wikipedia and
paraphrases. In particular, we propose training losses to exploit hyperlinks,
article structures, and article category graphs for entity-, discourse-,
entailment-related knowledge. We propose a framework that uses paraphrase pairs
to disentangle semantics and syntax in sentence representations. We extend the
framework for a novel generation task that controls the syntax of output text
with a sentential exemplar.
  Lastly, we discuss our work on tailoring textual resources for establishing
challenging evaluation tasks. We introduce three datasets by defining novel
tasks using various fan-contributed websites, including a long-form
data-to-text generation dataset, a screenplay summarization dataset, and a
long-form story generation dataset. These datasets have unique characteristics
offering challenges to future work in their respective task settings.",2022-07-21
"Neural Data-to-Text Generation Based on Small Datasets: Comparing the
  Added Value of Two Semi-Supervised Learning Approaches on Top of a Large
  Language Model",2022-07-14 11:53:04+00:00,http://arxiv.org/abs/2207.06839v1,"Chris van der Lee, Thiago Castro Ferreira, Chris Emmery, Travis Wiltshire, Emiel Krahmer",cs.CL,table2text,"This study discusses the effect of semi-supervised learning in combination
with pretrained language models for data-to-text generation. It is not known
whether semi-supervised learning is still helpful when a large-scale language
model is also supplemented. This study aims to answer this question by
comparing a data-to-text system only supplemented with a language model, to two
data-to-text systems that are additionally enriched by a data augmentation or a
pseudo-labeling semi-supervised learning approach.
  Results show that semi-supervised learning results in higher scores on
diversity metrics. In terms of output quality, extending the training set of a
data-to-text system with a language model using the pseudo-labeling approach
did increase text quality scores, but the data augmentation approach yielded
similar scores to the system without training set extension. These results
indicate that semi-supervised learning approaches can bolster output quality
and diversity, even when a language model is also present.",2022-07-14
"Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent
  Variable Inference for Text Generation",2022-07-13 11:27:46+00:00,http://arxiv.org/abs/2207.06130v1,"Jinyi Hu, Xiaoyuan Yi, Wenhao Li, Maosong Sun, Xing Xie",cs.CL,table2text,"The past several years have witnessed Variational Auto-Encoder's superiority
in various text generation tasks. However, due to the sequential nature of the
text, auto-regressive decoders tend to ignore latent variables and then reduce
to simple language models, known as the KL vanishing problem, which would
further deteriorate when VAE is combined with Transformer-based structures. To
ameliorate this problem, we propose DELLA, a novel variational Transformer
framework. DELLA learns a series of layer-wise latent variables with each
inferred from those of lower layers and tightly coupled with the hidden states
by low-rank tensor product. In this way, DELLA forces these posterior latent
variables to be fused deeply with the whole computation path and hence
incorporate more information. We theoretically demonstrate that our method can
be regarded as entangling latent variables to avoid posterior information
decrease through layers, enabling DELLA to get higher non-zero KL values even
without any annealing or thresholding tricks. Experiments on four unconditional
and three conditional generation tasks show that DELLA could better alleviate
KL vanishing and improve both quality and diversity compared to several strong
baselines.",2022-07-13
Towards Multimodal Vision-Language Models Generating Non-Generic Text,2022-07-09 01:56:35+00:00,http://arxiv.org/abs/2207.04174v1,"Wes Robbins, Zanyar Zohourianshahzadi, Jugal Kalita","cs.CV, cs.AI",table2text,"Vision-language models can assess visual context in an image and generate
descriptive text. While the generated text may be accurate and syntactically
correct, it is often overly general. To address this, recent work has used
optical character recognition to supplement visual information with text
extracted from an image. In this work, we contend that vision-language models
can benefit from additional information that can be extracted from an image,
but are not used by current models. We modify previous multimodal frameworks to
accept relevant information from any number of auxiliary classifiers. In
particular, we focus on person names as an additional set of tokens and create
a novel image-caption dataset to facilitate captioning with person names. The
dataset, Politicians and Athletes in Captions (PAC), consists of captioned
images of well-known people in context. By fine-tuning pretrained models with
this dataset, we demonstrate a model that can naturally integrate facial
recognition tokens into generated text by training on limited data. For the PAC
dataset, we provide a discussion on collection and baseline benchmark scores.",2022-07-09
"TalkToModel: Understanding Machine Learning Models With Open Ended
  Dialogues",2022-07-08 23:42:56+00:00,http://arxiv.org/abs/2207.04154v1,"Dylan Slack, Satyapriya Krishna, Himabindu Lakkaraju, Sameer Singh","cs.LG, cs.AI, cs.CL",table2text,"Machine Learning (ML) models are increasingly used to make critical decisions
in real-world applications, yet they have also become more complex, making them
harder to understand. To this end, several techniques to explain model
predictions have been proposed. However, practitioners struggle to leverage
explanations because they often do not know which to use, how to interpret the
results, and may have insufficient data science experience to obtain
explanations. In addition, most current works focus on generating one-shot
explanations and do not allow users to follow up and ask fine-grained questions
about the explanations, which can be frustrating. In this work, we address
these challenges by introducing TalkToModel: an open-ended dialogue system for
understanding machine learning models. Specifically, TalkToModel comprises
three key components: 1) a natural language interface for engaging in
dialogues, making understanding ML models highly accessible, 2) a dialogue
engine that adapts to any tabular model and dataset, interprets natural
language, maps it to appropriate operations (e.g., feature importance
explanations, counterfactual explanations, showing model errors), and generates
text responses, and 3) an execution component that run the operations and
ensures explanations are accurate. We carried out quantitative and human
subject evaluations of TalkToModel. We found the system understands user
questions on novel datasets and models with high accuracy, demonstrating the
system's capacity to generalize to new situations. In human evaluations, 73% of
healthcare workers (e.g., doctors and nurses) agreed they would use TalkToModel
over baseline point-and-click systems, and 84.6% of ML graduate students agreed
TalkToModel was easier to use.",2022-07-08
Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk,2022-07-02 04:30:07+00:00,http://arxiv.org/abs/2207.00735v1,"Benyou Wang, Xiangbo Wu, Xiaokang Liu, Jianquan Li, Prayag Tiwari, Qianqian Xie",cs.CL,table2text,"Language is the principal tool for human communication, in which humor is one
of the most attractive parts. Producing natural language like humans using
computers, a.k.a, Natural Language Generation (NLG), has been widely used for
dialogue systems, chatbots, machine translation, as well as computer-aid
creation e.g., idea generations, scriptwriting. However, the humor aspect of
natural language is relatively under-investigated, especially in the age of
pre-trained language models. In this work, we aim to preliminarily test whether
NLG can generate humor as humans do. We build a new dataset consisting of
numerous digitized Chinese Comical Crosstalk scripts (called C$^3$ in short),
which is for a popular Chinese performing art called `Xiangsheng' since 1800s.
(For convenience for non-Chinese speakers, we called `crosstalk' for
`Xiangsheng' in this paper.) We benchmark various generation approaches
including training-from-scratch Seq2seq, fine-tuned middle-scale PLMs, and
large-scale PLMs (with and without fine-tuning). Moreover, we also conduct a
human assessment, showing that 1) large-scale pretraining largely improves
crosstalk generation quality; and 2) even the scripts generated from the best
PLM is far from what we expect, with only 65% quality of human-created
crosstalk. We conclude, humor generation could be largely improved using
large-scaled PLMs, but it is still in its infancy.
  The data and benchmarking code is publicly available in
\url{https://github.com/anonNo2/crosstalk-generation}.",2022-07-02
"Syntax Controlled Knowledge Graph-to-Text Generation with Order and
  Semantic Consistency",2022-07-02 02:42:14+00:00,http://arxiv.org/abs/2207.00719v1,"Jin Liu, Chongfeng Fan, Fengyu Zhou, Huijuan Xu",cs.AI,table2text,"The knowledge graph (KG) stores a large amount of structural knowledge, while
it is not easy for direct human understanding. Knowledge graph-to-text
(KG-to-text) generation aims to generate easy-to-understand sentences from the
KG, and at the same time, maintains semantic consistency between generated
sentences and the KG. Existing KG-to-text generation methods phrase this task
as a sequence-to-sequence generation task with linearized KG as input and
consider the consistency issue of the generated texts and KG through a simple
selection between decoded sentence word and KG node word at each time step.
However, the linearized KG order is commonly obtained through a heuristic
search without data-driven optimization. In this paper, we optimize the
knowledge description order prediction under the order supervision extracted
from the caption and further enhance the consistency of the generated sentences
and KG through syntactic and semantic regularization. We incorporate the
Part-of-Speech (POS) syntactic tags to constrain the positions to copy words
from the KG and employ a semantic context scoring function to evaluate the
semantic fitness for each word in its local context when decoding each word in
the generated sentence. Extensive experiments are conducted on two datasets,
WebNLG and DART, and achieve state-of-the-art performances.",2022-07-02
Mapping the Design Space of Human-AI Interaction in Text Summarization,2022-06-29 19:03:25+00:00,http://arxiv.org/abs/2206.14863v1,"Ruijia Cheng, Alison Smith-Renner, Ke Zhang, Joel R. Tetreault, Alejandro Jaimes",cs.HC,table2text,"Automatic text summarization systems commonly involve humans for preparing
data or evaluating model performance, yet, there lacks a systematic
understanding of humans' roles, experience, and needs when interacting with or
being assisted by AI. From a human-centered perspective, we map the design
opportunities and considerations for human-AI interaction in text summarization
and broader text generation tasks. We first conducted a systematic literature
review of 70 papers, developing a taxonomy of five interactions in AI-assisted
text generation and relevant design dimensions. We designed text summarization
prototypes for each interaction. We then interviewed 16 users, aided by the
prototypes, to understand their expectations, experience, and needs regarding
efficiency, control, and trust with AI in text summarization and propose design
considerations accordingly.",2022-06-29
Joint Generator-Ranker Learning for Natural Language Generation,2022-06-28 12:58:30+00:00,http://arxiv.org/abs/2206.13974v1,"Weizhou Shen, Yeyun Gong, Yelong Shen, Song Wang, Xiaojun Quan, Nan Duan, Weizhu Chen",cs.CL,table2text,"Due to exposure bias, most existing natural language generation (NLG) models
trained by maximizing the likelihood objective predict poor text results during
the inference stage. In this paper, to tackle this problem, we revisit the
generate-then-rank framework and propose a joint generator-ranker (JGR)
training algorithm for text generation tasks. In JGR, the generator model is
trained by maximizing two objectives: the likelihood of the training corpus and
the expected reward given by the ranker model. Meanwhile, the ranker model
takes input samples from the generator model and learns to distinguish good
samples from the generation pool. The generator and ranker models are
alternately optimized till convergence. In the empirical study, the proposed
JGR model achieves new state-of-the-art performance on five public benchmarks
covering three popular generation tasks: summarization, question generation,
and response generation. We will make code, data, and models available at
https://github.com/microsoft/AdvNLG.",2022-06-28
Megapixel Image Generation with Step-Unrolled Denoising Autoencoders,2022-06-24 15:47:42+00:00,http://arxiv.org/abs/2206.12351v1,"Alex F. McKinney, Chris G. Willcocks","cs.CV, cs.LG",table2text,"An ongoing trend in generative modelling research has been to push sample
resolutions higher whilst simultaneously reducing computational requirements
for training and sampling. We aim to push this trend further via the
combination of techniques - each component representing the current pinnacle of
efficiency in their respective areas. These include vector-quantized GAN
(VQ-GAN), a vector-quantization (VQ) model capable of high levels of lossy -
but perceptually insignificant - compression; hourglass transformers, a highly
scaleable self-attention model; and step-unrolled denoising autoencoders
(SUNDAE), a non-autoregressive (NAR) text generative model. Unexpectedly, our
method highlights weaknesses in the original formulation of hourglass
transformers when applied to multidimensional data. In light of this, we
propose modifications to the resampling mechanism, applicable in any task
applying hierarchical transformers to multidimensional data. Additionally, we
demonstrate the scalability of SUNDAE to long sequence lengths - four times
longer than prior work. Our proposed framework scales to high-resolutions
($1024 \times 1024$) and trains quickly (2-4 days). Crucially, the trained
model produces diverse and realistic megapixel samples in approximately 2
seconds on a consumer-grade GPU (GTX 1080Ti). In general, the framework is
flexible: supporting an arbitrary number of sampling steps, sample-wise
self-stopping, self-correction capabilities, conditional generation, and a NAR
formulation that allows for arbitrary inpainting masks. We obtain FID scores of
10.56 on FFHQ256 - close to the original VQ-GAN in less than half the sampling
steps - and 21.85 on FFHQ1024 in only 100 sampling steps.",2022-06-24
MVP: Multi-task Supervised Pre-training for Natural Language Generation,2022-06-24 07:49:47+00:00,http://arxiv.org/abs/2206.12131v1,"Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen",cs.CL,table2text,"Pre-trained language models (PLMs) have achieved notable success in natural
language generation (NLG) tasks. Up to now, most of the PLMs are pre-trained in
an unsupervised manner using large-scale general corpus. In the meanwhile, an
increasing number of models pre-trained with less labeled data showcase
superior performance compared to unsupervised models. Motivated by the success
of supervised pre-training, we propose Multi-task superVised Pre-training (MVP)
for natural language generation. For pre-training the text generation model
MVP, we collect a labeled pre-training corpus from 45 datasets over seven
generation tasks. For each task, we further pre-train specific soft prompts to
stimulate the model capacity in performing a specific task. Extensive
experiments have demonstrated the effectiveness of our supervised pre-training
in a number of NLG tasks, and our general methods achieve state-of-the-art
performance on 12 of 17 datasets.",2022-06-24
"Comparing informativeness of an NLG chatbot vs graphical app in
  diet-information domain",2022-06-23 07:15:58+00:00,http://arxiv.org/abs/2206.13435v1,"Simone Balloccu, Ehud Reiter","cs.CL, cs.AI",table2text,"Visual representation of data like charts and tables can be challenging to
understand for readers. Previous work showed that combining visualisations with
text can improve the communication of insights in static contexts, but little
is known about interactive ones. In this work we present an NLG chatbot that
processes natural language queries and provides insights through a combination
of charts and text. We apply it to nutrition, a domain communication quality is
critical. Through crowd-sourced evaluation we compare the informativeness of
our chatbot against traditional, static diet-apps. We find that the
conversational context significantly improved users' understanding of dietary
data in various tasks, and that users considered the chatbot as more useful and
quick to use than traditional apps.",2022-06-23
GEMv2: Multilingual NLG Benchmarking in a Single Line of Code,2022-06-22 17:52:30+00:00,http://arxiv.org/abs/2206.11249v3,"Sebastian Gehrmann, Abhik Bhattacharjee, Abinaya Mahendiran, Alex Wang, Alexandros Papangelis, Aman Madaan, Angelina McMillan-Major, Anna Shvets, Ashish Upadhyay, Bingsheng Yao, Bryan Wilie, Chandra Bhagavatula, Chaobin You, Craig Thomson, Cristina Garbacea, Dakuo Wang, Daniel Deutsch, Deyi Xiong, Di Jin, Dimitra Gkatzia, Dragomir Radev, Elizabeth Clark, Esin Durmus, Faisal Ladhak, Filip Ginter, Genta Indra Winata, Hendrik Strobelt, Hiroaki Hayashi, Jekaterina Novikova, Jenna Kanerva, Jenny Chim, Jiawei Zhou, Jordan Clive, Joshua Maynez, Joo Sedoc, Juraj Juraska, Kaustubh Dhole, Khyathi Raghavi Chandu, Laura Perez-Beltrachini, Leonardo F. R. Ribeiro, Lewis Tunstall, Li Zhang, Mahima Pushkarna, Mathias Creutz, Michael White, Mihir Sanjay Kale, Moussa Kamal Eddine, Nico Daheim, Nishant Subramani, Ondrej Dusek, Paul Pu Liang, Pawan Sasanka Ammanamanchi, Qi Zhu, Ratish Puduppully, Reno Kriz, Rifat Shahriyar, Ronald Cardenas, Saad Mahamood, Salomey Osei, Samuel Cahyawijaya, Sanja tajner, Sebastien Montella, Shailza, Shailza Jolly, Simon Mille, Tahmid Hasan, Tianhao Shen, Tosin Adewumi, Vikas Raunak, Vipul Raheja, Vitaly Nikolaev, Vivian Tsai, Yacine Jernite, Ying Xu, Yisi Sang, Yixin Liu, Yufang Hou","cs.CL, cs.AI, cs.LG",table2text,"Evaluation in machine learning is usually informed by past choices, for
example which datasets or metrics to use. This standardization enables the
comparison on equal footing using leaderboards, but the evaluation choices
become sub-optimal as better alternatives arise. This problem is especially
pertinent in natural language generation which requires ever-improving suites
of datasets, metrics, and human evaluation to make definitive claims. To make
following best model evaluation practices easier, we introduce GEMv2. The new
version of the Generation, Evaluation, and Metrics Benchmark introduces a
modular infrastructure for dataset, model, and metric developers to benefit
from each others work. GEMv2 supports 40 documented datasets in 51 languages.
Models for all datasets can be evaluated online and our interactive data card
creation and rendering tools make it easier to add new datasets to the living
benchmark.",2022-06-22
"BenchCLAMP: A Benchmark for Evaluating Language Models on Semantic
  Parsing",2022-06-21 18:34:11+00:00,http://arxiv.org/abs/2206.10668v1,"Subhro Roy, Sam Thomson, Tongfei Chen, Richard Shin, Adam Pauls, Jason Eisner, Benjamin Van Durme",cs.CL,table2text,"We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model
Parsing, which produces semantic outputs based on the analysis of input text
through constrained decoding of a prompted or fine-tuned language model.
Developers of pretrained language models currently benchmark on classification,
span extraction and free-text generation tasks. Semantic parsing is neglected
in language model evaluation because of the complexity of handling
task-specific architectures and representations. Recent work has shown that
generation from a prompted or fine-tuned language model can perform well at
semantic parsing when the output is constrained to be a valid semantic
representation. BenchCLAMP includes context-free grammars for six semantic
parsing datasets with varied output meaning representations, as well as a
constrained decoding interface to generate outputs covered by these grammars.
We provide low, medium, and high resource splits for each dataset, allowing
accurate comparison of various language models under different data regimes.
Our benchmark supports both prompt-based learning as well as fine-tuning, and
provides an easy-to-use toolkit for language model developers to evaluate on
semantic parsing.",2022-06-21
Prefix Language Models are Unified Modal Learners,2022-06-15 17:49:38+00:00,http://arxiv.org/abs/2206.07699v1,"Shizhe Diao, Wangchunshu Zhou, Xinsong Zhang, Jiawei Wang","cs.CV, cs.CL, cs.LG",table2text,"With the success of vision-language pre-training, we have witnessed the
state-of-the-art has been pushed on multi-modal understanding and generation.
However, the current pre-training paradigm is either incapable of targeting all
modalities at once (e.g., text generation and image generation), or requires
multi-fold well-designed tasks which significantly limits the scalability. We
demonstrate that a unified modal model could be learned with a prefix language
modeling objective upon text and image sequences. Thanks to the simple but
powerful pre-training paradigm, our proposed model, DaVinci, is simple to
train, scalable to huge data, and adaptable to a variety of downstream tasks
across modalities (language / vision / vision+language), types (understanding /
generation) and settings (e.g., zero-shot, fine-tuning, linear evaluation) with
a single unified architecture. DaVinci achieves the competitive performance on
a wide range of 26 understanding / generation tasks, and outperforms previous
unified vision-language models on most tasks, including ImageNet classification
(+1.6%), VQAv2 (+1.4%), COCO caption generation (BLEU@4 +1.1%, CIDEr +1.5%) and
COCO image generation (IS +0.9%, FID -1.0%), at the comparable model and data
scale. Furthermore, we offer a well-defined benchmark for future research by
reporting the performance on different scales of the pre-training dataset on a
heterogeneous and wide distribution coverage. Our results establish new,
stronger baselines for future comparisons at different data scales and shed
light on the difficulties of comparing VLP models more generally.",2022-06-15
A Benchmark for Federated Hetero-Task Learning,2022-06-07 16:43:09+00:00,http://arxiv.org/abs/2206.03436v2,"Liuyi Yao, Dawei Gao, Zhen Wang, Yuexiang Xie, Weirui Kuang, Daoyuan Chen, Haohui Wang, Chenhe Dong, Bolin Ding, Yaliang Li",cs.LG,table2text,"To investigate the heterogeneity in federated learning in real-world
scenarios, we generalize the classic federated learning to federated
hetero-task learning, which emphasizes the inconsistency across the
participants in federated learning in terms of both data distribution and
learning tasks. We also present B-FHTL, a federated hetero-task learning
benchmark consisting of simulation dataset, FL protocols and a unified
evaluation mechanism. B-FHTL dataset contains three well-designed federated
learning tasks with increasing heterogeneity. Each task simulates the clients
with different non-IID data and learning tasks. To ensure fair comparison among
different FL algorithms, B-FHTL builds in a full suite of FL protocols by
providing high-level APIs to avoid privacy leakage, and presets most common
evaluation metrics spanning across different learning tasks, such as
regression, classification, text generation and etc. Furthermore, we compare
the FL algorithms in fields of federated multi-task learning, federated
personalization and federated meta learning within B-FHTL, and highlight the
influence of heterogeneity and difficulties of federated hetero-task learning.
Our benchmark, including the federated dataset, protocols, the evaluation
mechanism and the preliminary experiment, is open-sourced at
https://github.com/alibaba/FederatedScope/tree/master/benchmark/B-FHTL",2022-06-07
DeepCAVE: An Interactive Analysis Tool for Automated Machine Learning,2022-06-07 12:59:39+00:00,http://arxiv.org/abs/2206.03493v1,"Ren Sass, Eddie Bergman, Andr Biedenkapp, Frank Hutter, Marius Lindauer",cs.LG,table2text,"Automated Machine Learning (AutoML) is used more than ever before to support
users in determining efficient hyperparameters, neural architectures, or even
full machine learning pipelines. However, users tend to mistrust the
optimization process and its results due to a lack of transparency, making
manual tuning still widespread. We introduce DeepCAVE, an interactive framework
to analyze and monitor state-of-the-art optimization procedures for AutoML
easily and ad hoc. By aiming for full and accessible transparency, DeepCAVE
builds a bridge between users and AutoML and contributes to establishing trust.
Our framework's modular and easy-to-extend nature provides users with
automatically generated text, tables, and graphic visualizations. We show the
value of DeepCAVE in an exemplary use-case of outlier detection, in which our
framework makes it easy to identify problems, compare multiple runs and
interpret optimization processes. The package is freely available on GitHub
https://github.com/automl/DeepCAVE.",2022-06-07
Plot Writing From Pre-Trained Language Models,2022-06-07 05:30:46+00:00,http://arxiv.org/abs/2206.03021v1,"Yiping Jin, Vishakha Kadam, Dittaya Wanvarie",cs.CL,table2text,"Pre-trained language models (PLMs) fail to generate long-form narrative text
because they do not consider global structure. As a result, the generated texts
are often incohesive, repetitive, or lack content. Recent work in story
generation reintroduced explicit content planning in the form of prompts,
keywords, or semantic frames. Trained on large parallel corpora, these models
can generate more logical event sequences and thus more contentful stories.
However, these intermediate representations are often not in natural language
and cannot be utilized by PLMs without fine-tuning. We propose generating story
plots using off-the-shelf PLMs while maintaining the benefit of content
planning to generate cohesive and contentful stories. Our proposed method,
ScratchPlot, first prompts a PLM to compose a content plan. Then, we generate
the story's body and ending conditioned on the content plan. Furthermore, we
take a generate-and-rank approach by using additional PLMs to rank the
generated (story, ending) pairs. We benchmark our method with various baselines
and achieved superior results in both human and automatic evaluation.",2022-06-07
"Curriculum-Based Self-Training Makes Better Few-Shot Learners for
  Data-to-Text Generation",2022-06-06 16:11:58+00:00,http://arxiv.org/abs/2206.02712v1,"Pei Ke, Haozhe Ji, Zhenyu Yang, Yi Huang, Junlan Feng, Xiaoyan Zhu, Minlie Huang",cs.CL,table2text,"Despite the success of text-to-text pre-trained models in various natural
language generation (NLG) tasks, the generation performance is largely
restricted by the number of labeled data in downstream tasks, particularly in
data-to-text generation tasks. Existing works mostly utilize abundant unlabeled
structured data to conduct unsupervised pre-training for task adaption, which
fail to model the complex relationship between source structured data and
target texts. Thus, we introduce self-training as a better few-shot learner
than task-adaptive pre-training, which explicitly captures this relationship
via pseudo-labeled data generated by the pre-trained model. To alleviate the
side-effect of low-quality pseudo-labeled data during self-training, we propose
a novel method called Curriculum-Based Self-Training (CBST) to effectively
leverage unlabeled data in a rearranged order determined by the difficulty of
text generation. Experimental results show that our method can outperform
fine-tuning and task-adaptive pre-training methods, and achieve
state-of-the-art performance in the few-shot setting of data-to-text
generation.",2022-06-06
"Learning to Break the Loop: Analyzing and Mitigating Repetitions for
  Neural Text Generation",2022-06-06 05:51:12+00:00,http://arxiv.org/abs/2206.02369v1,"Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, Jian Li",cs.CL,table2text,"While large-scale neural language models, such as GPT2 and BART, have
achieved impressive results on various text generation tasks, they tend to get
stuck in undesirable sentence-level loops with maximization-based decoding
algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive
since there are few consecutive sentence-level repetitions in human corpora
(e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for
generating consecutive sentence-level repetitions, we study the relationship
between the probabilities of the repetitive tokens and their previous
repetitions in the context. Through our quantitative experiments, we find that
1) Language models have a preference to repeat the previous sentence; 2) The
sentence-level repetitions have a \textit{self-reinforcement effect}: the more
times a sentence is repeated in the context, the higher the probability of
continuing to generate that sentence; 3) The sentences with higher initial
probabilities usually have a stronger self-reinforcement effect. Motivated by
our findings, we propose a simple and effective training method \textbf{DITTO}
(Pseu\underline{D}o-Repet\underline{IT}ion
Penaliza\underline{T}i\underline{O}n), where the model learns to penalize
probabilities of sentence-level repetitions from pseudo repetitive data.
Although our method is motivated by mitigating repetitions, experiments show
that DITTO not only mitigates the repetition issue without sacrificing
perplexity, but also achieves better generation quality. Extensive experiments
on open-ended text generation (Wikitext-103) and text summarization
(CNN/DailyMail) demonstrate the generality and effectiveness of our method.",2022-06-06
"Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for
  Text-to-Speech",2022-06-05 10:50:34+00:00,http://arxiv.org/abs/2206.02147v1,"Ziyue Jiang, Su Zhe, Zhou Zhao, Qian Yang, Yi Ren, Jinglin Liu, Zhenhui Ye","eess.AS, cs.CL, cs.SD",table2text,"Polyphone disambiguation aims to capture accurate pronunciation knowledge
from natural text sequences for reliable Text-to-speech (TTS) systems. However,
previous approaches require substantial annotated training data and additional
efforts from language experts, making it difficult to extend high-quality
neural TTS systems to out-of-domain daily conversations and countless languages
worldwide. This paper tackles the polyphone disambiguation problem from a
concise and novel perspective: we propose Dict-TTS, a semantic-aware generative
text-to-speech model with an online website dictionary (the existing prior
information in the natural language). Specifically, we design a
semantics-to-pronunciation attention (S2PA) module to match the semantic
patterns between the input text sequence and the prior semantics in the
dictionary and obtain the corresponding pronunciations; The S2PA module can be
easily trained with the end-to-end TTS model without any annotated phoneme
labels. Experimental results in three languages show that our model outperforms
several strong baseline models in terms of pronunciation accuracy and improves
the prosody modeling of TTS systems. Further extensive analyses with different
linguistic encoders demonstrate that each design in Dict-TTS is effective.
Audio samples are available at \url{https://dicttts.github.io/DictTTS-Demo/}.",2022-06-05
CoNT: Contrastive Neural Text Generation,2022-05-29 15:18:37+00:00,http://arxiv.org/abs/2205.14690v1,"Chenxin An, Jiangtao Feng, Kai Lv, Lingpeng Kong, Xipeng Qiu, Xuanjing Huang",cs.CL,table2text,"Recently, contrastive learning attracts increasing interests in neural text
generation as a new solution to alleviate the exposure bias problem. It
introduces a sequence-level training signal which is crucial to generation
tasks that always rely on auto-regressive decoding. However, previous methods
using contrastive learning in neural text generation usually lead to inferior
performance. In this paper, we analyse the underlying reasons and propose a new
Contrastive Neural Text generation framework, CoNT. CoNT addresses bottlenecks
that prevent contrastive learning from being widely adopted in generation tasks
from three aspects -- the construction of contrastive examples, the choice of
the contrastive loss, and the strategy in decoding. We validate CoNT on five
generation tasks with ten benchmarks, including machine translation,
summarization, code comment generation, data-to-text generation and commonsense
generation. Experimental results show that CoNT clearly outperforms the
conventional training framework on all the ten benchmarks with a convincing
margin. Especially, CoNT surpasses previous the most competitive contrastive
learning method for text generation, by 1.50 BLEU on machine translation and
1.77 ROUGE-1 on summarization, respectively. It achieves new state-of-the-art
on summarization, code comment generation (without external data) and
data-to-text generation.",2022-05-29
Controllable Text Generation with Neurally-Decomposed Oracle,2022-05-27 20:17:53+00:00,http://arxiv.org/abs/2205.14219v1,"Tao Meng, Sidi Lu, Nanyun Peng, Kai-Wei Chang",cs.CL,table2text,"We propose a general and efficient framework to control auto-regressive
generation models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained
base language model and a sequence-level boolean oracle function, we propose to
decompose the oracle function into token-level guidance to steer the base model
in text generation. Specifically, the token-level guidance is approximated by a
neural model trained with examples sampled from the base model, demanding no
additional auxiliary labeled data. We present the closed-form optimal solution
to incorporate the token-level guidance into the base model for controllable
generation. We further provide a theoretical analysis of how the approximation
quality of NADO affects the controllable generation results. Experiments
conducted on two applications: (1) text generation with lexical constraints and
(2) machine translation with formality control demonstrate that our framework
efficiently guides the base model towards the given oracle while maintaining
high generation quality.",2022-05-27
Diffusion-LM Improves Controllable Text Generation,2022-05-27 20:12:09+00:00,http://arxiv.org/abs/2205.14217v1,"Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, Tatsunori B. Hashimoto","cs.CL, cs.AI, cs.LG",table2text,"Controlling the behavior of language models (LMs) without re-training is a
major open problem in natural language generation. While recent works have
demonstrated successes on controlling simple sentence attributes (e.g.,
sentiment), there has been little progress on complex, fine-grained controls
(e.g., syntactic structure). To address this challenge, we develop a new
non-autoregressive language model based on continuous diffusions that we call
Diffusion-LM. Building upon the recent successes of diffusion models in
continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian
vectors into word vectors, yielding a sequence of intermediate latent
variables. The continuous, hierarchical nature of these intermediate variables
enables a simple gradient-based algorithm to perform complex, controllable
generation tasks. We demonstrate successful control of Diffusion-LM for six
challenging fine-grained control tasks, significantly outperforming prior work.",2022-05-27
Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach,2022-05-26 06:36:53+00:00,http://arxiv.org/abs/2205.13183v1,"Chao Zhao, Faeze Brahman, Tenghao Huang, Snigdha Chaturvedi",cs.CL,table2text,"Pre-trained models (PTMs) have lead to great improvements in natural language
generation (NLG). However, it is still unclear how much commonsense knowledge
they possess. With the goal of evaluating commonsense knowledge of NLG models,
recent work has proposed the problem of generative commonsense reasoning, e.g.,
to compose a logical sentence given a set of unordered concepts. Existing
approaches to this problem hypothesize that PTMs lack sufficient parametric
knowledge for this task, which can be overcome by introducing external
knowledge or task-specific pre-training objectives. Different from this trend,
we argue that PTM's inherent ability for generative commonsense reasoning is
underestimated due to the order-agnostic property of its input. In particular,
we hypothesize that the order of the input concepts can affect the PTM's
ability to utilize its commonsense knowledge. To this end, we propose a
pre-ordering approach to elaborately manipulate the order of the given concepts
before generation. Experiments show that our approach can outperform the more
sophisticated models that have access to a lot of external data and resources.",2022-05-26
"Automatic question generation based on sentence structure analysis using
  machine learning approach",2022-05-25 14:35:29+00:00,http://arxiv.org/abs/2205.12811v1,"Miroslav Bltk, Viera Rozinajov","cs.CL, cs.AI",table2text,"Automatic question generation is one of the most challenging tasks of Natural
Language Processing. It requires ""bidirectional"" language processing: firstly,
the system has to understand the input text (Natural Language Understanding)
and it then has to generate questions also in the form of text (Natural
Language Generation). In this article, we introduce our framework for
generating the factual questions from unstructured text in the English
language. It uses a combination of traditional linguistic approaches based on
sentence patterns with several machine learning methods. We firstly obtain
lexical, syntactic and semantic information from an input text and we then
construct a hierarchical set of patterns for each sentence. The set of features
is extracted from the patterns and it is then used for automated learning of
new transformation rules. Our learning process is totally data-driven because
the transformation rules are obtained from a set of initial sentence-question
pairs. The advantages of this approach lie in a simple expansion of new
transformation rules which allows us to generate various types of questions and
also in the continuous improvement of the system by reinforcement learning. The
framework also includes a question evaluation module which estimates the
quality of generated questions. It serves as a filter for selecting the best
questions and eliminating incorrect ones or duplicates. We have performed
several experiments to evaluate the correctness of generated questions and we
have also compared our system with several state-of-the-art systems. Our
results indicate that the quality of generated questions outperforms the
state-of-the-art systems and our questions are also comparable to questions
created by humans. We have also created and published an interface with all
created datasets and evaluated questions, so it is possible to follow up on our
work.",2022-05-25
PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation,2022-05-25 11:55:54+00:00,http://arxiv.org/abs/2205.12697v1,"Ao Liu, Haoyu Dong, Naoaki Okazaki, Shi Han, Dongmei Zhang",cs.CL,table2text,"Logical table-to-text generation is a task that involves generating logically
faithful sentences from tables, which requires models to derive logical level
facts from table records via logical inference. It raises a new challenge on
the logical-level content planning of table-to-text models. However, directly
learning the logical inference knowledge from table-text pairs is very
difficult for neural models because of the ambiguity of natural language and
the scarcity of parallel data. Hence even large-scale pre-trained language
models present low logical fidelity on logical table-to-text. In this work, we
propose a PLOG (Pretrained Logical Form Generator) framework to improve the
generation fidelity. Specifically, PLOG is first pretrained on a
table-to-logic-form generation (table-to-logic) task, then finetuned on
downstream table-to-text tasks. The formal definition of logical forms enables
us to collect large amount of accurate logical forms from tables without human
annotation. In addition, PLOG can learn logical inference from table-logic
pairs much more definitely than from table-text pairs. To evaluate our model,
we further collect a controlled logical table-to-text dataset CONTLOG based on
an existing dataset. On two benchmarks, LOGICNLG and CONTLOG, PLOG outperforms
strong baselines by a large margin on the logical fidelity, demonstrating the
effectiveness of table-to-logic pretraining.",2022-05-25
