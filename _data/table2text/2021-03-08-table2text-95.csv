title,pubdate,id,authors,categories,search,abstract,displaydate
Text Simplification by Tagging,2021-03-08 20:57:55+00:00,http://arxiv.org/abs/2103.05070v1,"Kostiantyn Omelianchuk, Vipul Raheja, Oleksandr Skurzhanskyi",cs.CL,table2text,"Edit-based approaches have recently shown promising results on multiple
monolingual sequence transduction tasks. In contrast to conventional
sequence-to-sequence (Seq2Seq) models, which learn to generate text from
scratch as they are trained on parallel corpora, these methods have proven to
be much more effective since they are able to learn to make fast and accurate
transformations while leveraging powerful pre-trained language models. Inspired
by these ideas, we present TST, a simple and efficient Text Simplification
system based on sequence Tagging, leveraging pre-trained Transformer-based
encoders. Our system makes simplistic data augmentations and tweaks in training
and inference on a pre-existing system, which makes it less reliant on large
amounts of parallel training data, provides more control over the outputs and
enables faster inference speeds. Our best model achieves near state-of-the-art
performance on benchmark test datasets for the task. Since it is fully
non-autoregressive, it achieves faster inference speeds by over 11 times than
the current state-of-the-art text simplification system.",2021-03-08
InFillmore: Neural Frame Lexicalization for Narrative Text Infilling,2021-03-08 17:59:41+00:00,http://arxiv.org/abs/2103.04941v1,"Jiefu Ou, Nathaniel Weir, Anton Belyy, Felix Yu, Benjamin Van Durme",cs.CL,table2text,"We propose a structured extension to bidirectional-context conditional
language generation, or ""infilling,"" inspired by Frame Semantic theory
(Fillmore, 1976). Guidance is provided through two approaches: (1) model
fine-tuning, conditioning directly on observed symbolic frames, and (2) a novel
extension to disjunctive lexically constrained decoding that leverages frame
semantic lexical units. Automatic and human evaluations confirm that
frame-guided generation allows for explicit manipulation of intended infill
semantics, with minimal loss of indistinguishability from the human-generated
text. Our methods flexibly apply to a variety of use scenarios, and we provide
an interactive web demo available at https://nlp.jhu.edu/demos.",2021-03-08
"Towards Faithfulness in Open Domain Table-to-text Generation from an
  Entity-centric View",2021-02-17 05:41:06+00:00,http://arxiv.org/abs/2102.08585v1,"Tianyu Liu, Xin Zheng, Baobao Chang, Zhifang Sui","cs.CL, cs.AI",table2text,"In open domain table-to-text generation, we notice that the unfaithful
generation usually contains hallucinated content which can not be aligned to
any input table record. We thus try to evaluate the generation faithfulness
with two entity-centric metrics: table record coverage and the ratio of
hallucinated entities in text, both of which are shown to have strong agreement
with human judgements. Then based on these metrics, we quantitatively analyze
the correlation between training data quality and generation fidelity which
indicates the potential usage of entity information in faithful generation.
Motivated by these findings, we propose two methods for faithful generation: 1)
augmented training by incorporating the auxiliary entity information, including
both an augmented plan-based model and an unsupervised model and 2) training
instance selection based on faithfulness ranking. We show these approaches
improve generation fidelity in both full dataset setting and few shot learning
settings by both automatic and human evaluations.",2021-02-17
Structural Information Preserving for Graph-to-Text Generation,2021-02-12 20:09:01+00:00,http://arxiv.org/abs/2102.06749v1,"Linfeng Song, Ante Wang, Jinsong Su, Yue Zhang, Kun Xu, Yubin Ge, Dong Yu",cs.CL,table2text,"The task of graph-to-text generation aims at producing sentences that
preserve the meaning of input graphs. As a crucial defect, the current
state-of-the-art models may mess up or even drop the core structural
information of input graphs when generating outputs. We propose to tackle this
problem by leveraging richer training signals that can guide our model for
preserving input information. In particular, we introduce two types of
autoencoding losses, each individually focusing on different aspects (a.k.a.
views) of input graphs. The losses are then back-propagated to better calibrate
our model via multi-task training. Experiments on two benchmarks for
graph-to-text generation show the effectiveness of our approach over a
state-of-the-art baseline. Our code is available at
\url{http://github.com/Soistesimmer/AMR-multiview}.",2021-02-12
Generating Synthetic Text Data to Evaluate Causal Inference Methods,2021-02-10 18:53:11+00:00,http://arxiv.org/abs/2102.05638v1,"Zach Wood-Doughty, Ilya Shpitser, Mark Dredze",cs.CL,table2text,"Drawing causal conclusions from observational data requires making
assumptions about the true data-generating process. Causal inference research
typically considers low-dimensional data, such as categorical or numerical
fields in structured medical records. High-dimensional and unstructured data
such as natural language complicates the evaluation of causal inference
methods; such evaluations rely on synthetic datasets with known causal effects.
Models for natural language generation have been widely studied and perform
well empirically. However, existing methods not immediately applicable to
producing synthetic datasets for causal evaluations, as they do not allow for
quantifying a causal effect on the text itself. In this work, we develop a
framework for adapting existing generation models to produce synthetic text
datasets with known causal effects. We use this framework to perform an
empirical comparison of four recently-proposed methods for estimating causal
effects from text data. We release our code and synthetic datasets.",2021-02-10
Neural Data-to-Text Generation with LM-based Text Augmentation,2021-02-06 10:21:48+00:00,http://arxiv.org/abs/2102.03556v1,"Ernie Chang, Xiaoyu Shen, Dawei Zhu, Vera Demberg, Hui Su",cs.CL,table2text,"For many new application domains for data-to-text generation, the main
obstacle in training neural models consists of a lack of training data. While
usually large numbers of instances are available on the data side, often only
very few text samples are available. To address this problem, we here propose a
novel few-shot approach for this setting. Our approach automatically augments
the data available for training by (i) generating new text samples based on
replacing specific values by alternative ones from the same category, (ii)
generating new text samples based on GPT-2, and (iii) proposing an automatic
method for pairing the new text samples with data samples. As the text
augmentation can introduce noise to the training data, we use cycle consistency
as an objective, in order to make sure that a given data sample can be
correctly reconstructed after having been formulated as text (and that text
samples can be reconstructed from data). On both the E2E and WebNLG benchmarks,
we show that this weakly supervised training paradigm is able to outperform
fully supervised seq2seq models with less than 10% annotations. By utilizing
all annotated data, our model can boost the performance of a standard seq2seq
model by over 5 BLEU points, establishing a new state-of-the-art on both
datasets.",2021-02-06
"Does the Order of Training Samples Matter? Improving Neural Data-to-Text
  Generation with Curriculum Learning",2021-02-06 10:14:18+00:00,http://arxiv.org/abs/2102.03554v1,"Ernie Chang, Hui-Syuan Yeh, Vera Demberg",cs.CL,table2text,"Recent advancements in data-to-text generation largely take on the form of
neural end-to-end systems. Efforts have been dedicated to improving text
generation systems by changing the order of training samples in a process known
as curriculum learning. Past research on sequence-to-sequence learning showed
that curriculum learning helps to improve both the performance and convergence
speed. In this work, we delve into the same idea surrounding the training
samples consisting of structured data and text pairs, where at each update, the
curriculum framework selects training samples based on the model's competence.
Specifically, we experiment with various difficulty metrics and put forward a
soft edit distance metric for ranking training samples. Our benchmarks show
faster convergence speed where training time is reduced by 38.7% and
performance is boosted by 4.84 BLEU.",2021-02-06
"Jointly Improving Language Understanding and Generation with
  Quality-Weighted Weak Supervision of Automatic Labeling",2021-02-06 10:06:15+00:00,http://arxiv.org/abs/2102.03551v1,"Ernie Chang, Vera Demberg, Alex Marin","cs.CL, cs.AI",table2text,"Neural natural language generation (NLG) and understanding (NLU) models are
data-hungry and require massive amounts of annotated data to be competitive.
Recent frameworks address this bottleneck with generative models that
synthesize weak labels at scale, where a small amount of training labels are
expert-curated and the rest of the data is automatically annotated. We follow
that approach, by automatically constructing a large-scale weakly-labeled data
with a fine-tuned GPT-2, and employ a semi-supervised framework to jointly
train the NLG and NLU models. The proposed framework adapts the parameter
updates to the models according to the estimated label-quality. On both the E2E
and Weather benchmarks, we show that this weakly supervised training paradigm
is an effective approach under low resource scenarios and outperforming
benchmark systems on both datasets when 100% of training data is used.",2021-02-06
Controlling Hallucinations at Word Level in Data-to-Text Generation,2021-02-04 18:58:28+00:00,http://arxiv.org/abs/2102.02810v1,"Cl√©ment Rebuffel, Marco Roberti, Laure Soulier, Geoffrey Scoutheeten, Rossella Cancelliere, Patrick Gallinari","cs.CL, cs.AI, cs.LG, cs.NE, 68T50 (Primary), 68T07 (Secondary), 68T05, I.2.6; I.2.7",table2text,"Data-to-Text Generation (DTG) is a subfield of Natural Language Generation
aiming at transcribing structured data in natural language descriptions. The
field has been recently boosted by the use of neural-based generators which
exhibit on one side great syntactic skills without the need of hand-crafted
pipelines; on the other side, the quality of the generated text reflects the
quality of the training data, which in realistic settings only offer
imperfectly aligned structure-text pairs. Consequently, state-of-art neural
models include misleading statements - usually called hallucinations - in their
outputs. The control of this phenomenon is today a major challenge for DTG, and
is the problem addressed in the paper.
  Previous work deal with this issue at the instance level: using an alignment
score for each table-reference pair. In contrast, we propose a finer-grained
approach, arguing that hallucinations should rather be treated at the word
level. Specifically, we propose a Multi-Branch Decoder which is able to
leverage word-level labels to learn the relevant parts of each training
instance. These labels are obtained following a simple and efficient scoring
procedure based on co-occurrence analysis and dependency parsing. Extensive
evaluations, via automated metrics and human judgment on the standard WikiBio
benchmark, show the accuracy of our alignment labels and the effectiveness of
the proposed Multi-Branch Decoder. Our model is able to reduce and control
hallucinations, while keeping fluency and coherence in generated texts. Further
experiments on a degraded version of ToTTo show that our model could be
successfully used on very noisy settings.",2021-02-04
Data-to-text Generation with Macro Planning,2021-02-04 16:32:57+00:00,http://arxiv.org/abs/2102.02723v1,"Ratish Puduppully, Mirella Lapata",cs.CL,table2text,"Recent approaches to data-to-text generation have adopted the very successful
encoder-decoder architecture or variants thereof. These models generate text
which is fluent (but often imprecise) and perform quite poorly at selecting
appropriate content and ordering it coherently. To overcome some of these
issues, we propose a neural model with a macro planning stage followed by a
generation stage reminiscent of traditional methods which embrace separate
modules for planning and surface realization. Macro plans represent high level
organization of important content such as entities, events and their
interactions; they are learnt from data and given as input to the generator.
Extensive experiments on two data-to-text benchmarks (RotoWire and MLB) show
that our approach outperforms competitive baselines in terms of automatic and
human evaluation.",2021-02-04
"GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial
  Networks",2021-01-25 09:42:58+00:00,http://arxiv.org/abs/2101.09978v2,"Tianming Zhao, Chunyang Chen, Yuanning Liu, Xiaodong Zhu","cs.HC, cs.CV, cs.LG, cs.SE",table2text,"Graphical User Interface (GUI) is ubiquitous in almost all modern desktop
software, mobile applications, and online websites. A good GUI design is
crucial to the success of the software in the market, but designing a good GUI
which requires much innovation and creativity is difficult even to well-trained
designers. Besides, the requirement of the rapid development of GUI design also
aggravates designers' working load. So, the availability of various automated
generated GUIs can help enhance the design personalization and specialization
as they can cater to the taste of different designers. To assist designers, we
develop a model GUIGAN to automatically generate GUI designs. Different from
conventional image generation models based on image pixels, our GUIGAN is to
reuse GUI components collected from existing mobile app GUIs for composing a
new design that is similar to natural-language generation. Our GUIGAN is based
on SeqGAN by modeling the GUI component style compatibility and GUI structure.
The evaluation demonstrates that our model significantly outperforms the best
of the baseline methods by 30.77% in Frechet Inception distance (FID) and
12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we
provide initial evidence of the usefulness of our approach for generating
acceptable brand new GUI designs.",2021-01-25
Generating (Formulaic) Text by Splicing Together Nearest Neighbors,2021-01-20 18:43:11+00:00,http://arxiv.org/abs/2101.08248v1,"Sam Wiseman, Arturs Backurs, Karl Stratos","cs.CL, cs.LG",table2text,"We propose to tackle conditional text generation tasks, especially those
which require generating formulaic text, by splicing together segments of text
from retrieved ""neighbor"" source-target pairs. Unlike recent work that
conditions on retrieved neighbors in an encoder-decoder setting but generates
text token-by-token, left-to-right, we learn a policy that directly manipulates
segments of neighbor text (i.e., by inserting or replacing them) to form an
output. Standard techniques for training such a policy require an oracle
derivation for each generation, and we prove that finding the shortest such
derivation can be reduced to parsing under a particular weighted context-free
grammar. We find that policies learned in this way allow for interpretable
table-to-text and headline generation that is competitive with or better than
state-of-the-art autoregressive token-level policies in terms of automatic
metrics, and moreover allows for faster decoding.",2021-01-20
What Makes Good In-Context Examples for GPT-$3$?,2021-01-17 23:38:40+00:00,http://arxiv.org/abs/2101.06804v1,"Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen",cs.CL,table2text,"GPT-$3$ has attracted lots of attention due to its superior performance
across a wide range of NLP tasks, especially with its powerful and versatile
in-context few-shot learning ability. Despite its success, we found that the
empirical results of GPT-$3$ depend heavily on the choice of in-context
examples. In this work, we investigate whether there are more effective
strategies for judiciously selecting in-context examples (relative to random
sampling) that better leverage GPT-$3$'s few-shot capabilities. Inspired by the
recent success of leveraging a retrieval module to augment large-scale neural
network models, we propose to retrieve examples that are semantically-similar
to a test sample to formulate its corresponding prompt. Intuitively, the
in-context examples selected with such a strategy may serve as more informative
inputs to unleash GPT-$3$'s extensive knowledge. We evaluate the proposed
approach on several natural language understanding and generation benchmarks,
where the retrieval-based prompt selection approach consistently outperforms
the random baseline. Moreover, it is observed that the sentence encoders
fine-tuned on task-related datasets yield even more helpful retrieval results.
Notably, significant gains are observed on tasks such as table-to-text
generation (41.9% on the ToTTo dataset) and open-domain question answering
(45.5% on the NQ dataset). We hope our investigation could help understand the
behaviors of GPT-$3$ and large-scale pre-trained LMs in general and enhance
their few-shot capabilities.",2021-01-17
Narration Generation for Cartoon Videos,2021-01-17 23:23:09+00:00,http://arxiv.org/abs/2101.06803v1,"Nikos Papasarantopoulos, Shay B. Cohen",cs.CL,table2text,"Research on text generation from multimodal inputs has largely focused on
static images, and less on video data. In this paper, we propose a new task,
narration generation, that is complementing videos with narration texts that
are to be interjected in several places. The narrations are part of the video
and contribute to the storyline unfolding in it. Moreover, they are
context-informed, since they include information appropriate for the timeframe
of video they cover, and also, do not need to include every detail shown in
input scenes, as a caption would. We collect a new dataset from the animated
television series Peppa Pig. Furthermore, we formalize the task of narration
generation as including two separate tasks, timing and content generation, and
present a set of models on the new task.",2021-01-17
Transforming Multi-Conditioned Generation from Meaning Representation,2021-01-12 01:45:06+00:00,http://arxiv.org/abs/2101.04257v1,Joosung Lee,"cs.CL, cs.AI",table2text,"In task-oriented conversation systems, natural language generation systems
that generate sentences with specific information related to conversation flow
are useful. Our study focuses on language generation by considering various
information representing the meaning of utterances as multiple conditions of
generation. NLG from meaning representations, the conditions for sentence
meaning, generally goes through two steps: sentence planning and surface
realization. However, we propose a simple one-stage framework to generate
utterances directly from MR (Meaning Representation). Our model is based on
GPT2 and generates utterances with flat conditions on slot and value pairs,
which does not need to determine the structure of the sentence. We evaluate
several systems in the E2E dataset with 6 automatic metrics. Our system is a
simple method, but it demonstrates comparable performance to previous systems
in automated metrics. In addition, using only 10\% of the data set without any
other techniques, our model achieves comparable performance, and shows the
possibility of performing zero-shot generation and expanding to other datasets.",2021-01-12
"Political Depolarization of News Articles Using Attribute-aware Word
  Embeddings",2021-01-05 07:39:12+00:00,http://arxiv.org/abs/2101.01391v1,"Ruibo Liu, Lili Wang, Chenyan Jia, Soroush Vosoughi","cs.CL, cs.AI",table2text,"Political polarization in the US is on the rise. This polarization negatively
affects the public sphere by contributing to the creation of ideological echo
chambers. In this paper, we focus on addressing one of the factors that
contributes to this polarity, polarized media. We introduce a framework for
depolarizing news articles. Given an article on a certain topic with a
particular ideological slant (eg., liberal or conservative), the framework
first detects polar language in the article and then generates a new article
with the polar language replaced with neutral expressions. To detect polar
words, we train a multi-attribute-aware word embedding model that is aware of
ideology and topics on 360k full-length media articles. Then, for text
generation, we propose a new algorithm called Text Annealing Depolarization
Algorithm (TADA). TADA retrieves neutral expressions from the word embedding
model that not only decrease ideological polarity but also preserve the
original argument of the text, while maintaining grammatical correctness. We
evaluate our framework by comparing the depolarized output of our model in two
modes, fully-automatic and semi-automatic, on 99 stories spanning 11 topics.
Based on feedback from 161 human testers, our framework successfully
depolarized 90.1% of paragraphs in semi-automatic mode and 78.3% of paragraphs
in fully-automatic mode. Furthermore, 81.2% of the testers agree that the
non-polar content information is well-preserved and 79% agree that
depolarization does not harm semantic correctness when they compare the
original text and the depolarized text. Our work shows that data-driven methods
can help to locate political polarity and aid in the depolarization of
articles.",2021-01-05
Prefix-Tuning: Optimizing Continuous Prompts for Generation,2021-01-01 08:00:36+00:00,http://arxiv.org/abs/2101.00190v1,"Xiang Lisa Li, Percy Liang",cs.CL,table2text,"Fine-tuning is the de facto way to leverage large pretrained language models
to perform downstream tasks. However, it modifies all the language model
parameters and therefore necessitates storing a full copy for each task. In
this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning
for natural language generation tasks, which keeps language model parameters
frozen, but optimizes a small continuous task-specific vector (called the
prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent
tokens to attend to this prefix as if it were ""virtual tokens"". We apply
prefix-tuning to GPT-2 for table-to-text generation and to BART for
summarization. We find that by learning only 0.1\% of the parameters,
prefix-tuning obtains comparable performance in the full data setting,
outperforms fine-tuning in low-data settings, and extrapolates better to
examples with topics unseen during training.",2021-01-01
"DISCOS: Bridging the Gap between Discourse Knowledge and Commonsense
  Knowledge",2021-01-01 03:30:38+00:00,http://arxiv.org/abs/2101.00154v1,"Tianqing Fang, Hongming Zhang, Weiqi Wang, Yangqiu Song, Bin He","cs.CL, cs.AI",table2text,"Commonsense knowledge is crucial for artificial intelligence systems to
understand natural language. Previous commonsense knowledge acquisition
approaches typically rely on human annotations (e.g., ATOMIC) or text
generation models (e.g., COMET). Human annotation could provide high-quality
commonsense knowledge, yet its high cost often results in relatively small
scale and low coverage. On the other hand, generation models have the potential
to automatically generate more knowledge. Nonetheless, machine learning models
often fit the training data too well to generate novel knowledge in high
quality, thus still suffering from coverage problems. To address the
limitations of previous approaches, in this paper, we propose an alternative
commonsense knowledge acquisition framework DISCOS (from DIScourse to
COmmonSense), which automatically mines expensive complex commonsense knowledge
from more affordable linguistic knowledge resources. Experiments demonstrate
that we can successfully convert discourse knowledge over eventualities from
ASER, a large-scale discourse knowledge graph, into inferential if-then
commonsense knowledge defined in ATOMIC without any additional annotation
effort. Further study suggests that DISCOS significantly outperforms previous
supervised approaches in terms of novelty and diversity with comparable
quality. In total, we can acquire 3.4M ATOMIC-like inferential commonsense
knowledge by populating ATOMIC on the core part of ASER. Codes and data are
available at https://github.com/HKUST-KnowComp/DISCOS-commonsense.",2021-01-01
Continual Learning in Task-Oriented Dialogue Systems,2020-12-31 08:44:25+00:00,http://arxiv.org/abs/2012.15504v1,"Andrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Seungwhan Moon, Paul Crook, Bing Liu, Zhou Yu, Eunjoon Cho, Zhiguang Wang","cs.CL, cs.AI",table2text,"Continual learning in task-oriented dialogue systems can allow us to add new
domains and functionalities through time without incurring the high cost of a
whole system retraining. In this paper, we propose a continual learning
benchmark for task-oriented dialogue systems with 37 domains to be learned
continuously in four settings, such as intent recognition, state tracking,
natural language generation, and end-to-end. Moreover, we implement and compare
multiple existing continual learning baselines, and we propose a simple yet
effective architectural method based on residual adapters. Our experiments
demonstrate that the proposed architectural method and a simple replay-based
strategy perform comparably well but they both achieve inferior performance to
the multi-task learning baseline, in where all the data are shown at once,
showing that continual learning in task-oriented dialogue systems is a
challenging task. Furthermore, we reveal several trade-offs between different
continual learning methods in term of parameter usage and memory size, which
are important in the design of a task-oriented dialogue system. The proposed
benchmark is released together with several baselines to promote more research
in this direction.",2020-12-31
Generating Wikipedia Article Sections from Diverse Data Sources,2020-12-29 19:35:34+00:00,http://arxiv.org/abs/2012.14919v1,"Mingda Chen, Sam Wiseman, Kevin Gimpel",cs.CL,table2text,"Datasets for data-to-text generation typically focus either on multi-domain,
single-sentence generation or on single-domain, long-form generation. In this
work, we create a large-scale dataset, WikiTableT, that pairs Wikipedia
sections with their corresponding tabular data and various metadata. WikiTableT
contains millions of instances, covering a broad range of topics, as well as a
variety of flavors of generation tasks with different levels of flexibility. We
benchmark several training and decoding strategies on WikiTableT. Our
qualitative analysis shows that the best approaches can generate fluent and
high quality texts but they sometimes struggle with coherence.",2020-12-29
"Interpretable NLG for Task-oriented Dialogue Systems with Heterogeneous
  Rendering Machines",2020-12-29 07:41:48+00:00,http://arxiv.org/abs/2012.14645v1,"Yangming Li, Kaisheng Yao",cs.CL,table2text,"End-to-end neural networks have achieved promising performances in natural
language generation (NLG). However, they are treated as black boxes and lack
interpretability. To address this problem, we propose a novel framework,
heterogeneous rendering machines (HRM), that interprets how neural generators
render an input dialogue act (DA) into an utterance. HRM consists of a renderer
set and a mode switcher. The renderer set contains multiple decoders that vary
in both structure and functionality. For every generation step, the mode
switcher selects an appropriate decoder from the renderer set to generate an
item (a word or a phrase). To verify the effectiveness of our method, we have
conducted extensive experiments on 5 benchmark datasets. In terms of automatic
metrics (e.g., BLEU), our model is competitive with the current
state-of-the-art method. The qualitative analysis shows that our model can
interpret the rendering process of neural generators well. Human evaluation
also confirms the interpretability of our proposed approach.",2020-12-29
Towards Neural Programming Interfaces,2020-12-10 21:17:04+00:00,http://arxiv.org/abs/2012.05983v1,"Zachary C. Brown, Nathaniel Robinson, David Wingate, Nancy Fulda","cs.CL, cs.AI",table2text,"It is notoriously difficult to control the behavior of artificial neural
networks such as generative neural language models. We recast the problem of
controlling natural language generation as that of learning to interface with a
pretrained language model, just as Application Programming Interfaces (APIs)
control the behavior of programs by altering hyperparameters. In this new
paradigm, a specialized neural network (called a Neural Programming Interface
or NPI) learns to interface with a pretrained language model by manipulating
the hidden activations of the pretrained model to produce desired outputs.
Importantly, no permanent changes are made to the weights of the original
model, allowing us to re-purpose pretrained models for new tasks without
overwriting any aspect of the language model. We also contribute a new data set
construction algorithm and GAN-inspired loss function that allows us to train
NPI models to control outputs of autoregressive transformers. In experiments
against other state-of-the-art approaches, we demonstrate the efficacy of our
methods using OpenAI's GPT-2 model, successfully controlling noun selection,
topic aversion, offensive speech filtering, and other aspects of language while
largely maintaining the controlled model's fluency under deterministic
settings.",2020-12-10
"Denoising Pre-Training and Data Augmentation Strategies for Enhanced RDF
  Verbalization with Transformers",2020-12-01 15:25:47+00:00,http://arxiv.org/abs/2012.00571v1,"Sebastien Montella, Betty Fabre, Tanguy Urvoy, Johannes Heinecke, Lina Rojas-Barahona",cs.CL,table2text,"The task of verbalization of RDF triples has known a growth in popularity due
to the rising ubiquity of Knowledge Bases (KBs). The formalism of RDF triples
is a simple and efficient way to store facts at a large scale. However, its
abstract representation makes it difficult for humans to interpret. For this
purpose, the WebNLG challenge aims at promoting automated RDF-to-text
generation. We propose to leverage pre-trainings from augmented data with the
Transformer model using a data augmentation strategy. Our experiment results
show a minimum relative increases of 3.73%, 126.05% and 88.16% in BLEU score
for seen categories, unseen entities and unseen categories respectively over
the standard training.",2020-12-01
Latent Template Induction with Gumbel-CRFs,2020-11-29 01:00:57+00:00,http://arxiv.org/abs/2011.14244v1,"Yao Fu, Chuanqi Tan, Bin Bi, Mosha Chen, Yansong Feng, Alexander M. Rush","cs.CL, cs.AI, cs.LG",table2text,"Learning to control the structure of sentences is a challenging problem in
text generation. Existing work either relies on simple deterministic approaches
or RL-based hard structures. We explore the use of structured variational
autoencoders to infer latent templates for sentence generation using a soft,
continuous relaxation in order to utilize reparameterization for training.
Specifically, we propose a Gumbel-CRF, a continuous relaxation of the CRF
sampling algorithm using a relaxed Forward-Filtering Backward-Sampling (FFBS)
approach. As a reparameterized gradient estimator, the Gumbel-CRF gives more
stable gradients than score-function based estimators. As a structured
inference network, we show that it learns interpretable templates during
training, which allows us to control the decoder during testing. We demonstrate
the effectiveness of our methods with experiments on data-to-text generation
and unsupervised paraphrase generation.",2020-11-29
"TaylorGAN: Neighbor-Augmented Policy Update for Sample-Efficient Natural
  Language Generation",2020-11-27 02:26:15+00:00,http://arxiv.org/abs/2011.13527v1,"Chun-Hsing Lin, Siang-Ruei Wu, Hung-Yi Lee, Yun-Nung Chen","cs.CL, cs.LG",table2text,"Score function-based natural language generation (NLG) approaches such as
REINFORCE, in general, suffer from low sample efficiency and training
instability problems. This is mainly due to the non-differentiable nature of
the discrete space sampling and thus these methods have to treat the
discriminator as a black box and ignore the gradient information. To improve
the sample efficiency and reduce the variance of REINFORCE, we propose a novel
approach, TaylorGAN, which augments the gradient estimation by off-policy
update and the first-order Taylor expansion. This approach enables us to train
NLG models from scratch with smaller batch size -- without maximum likelihood
pre-training, and outperforms existing GAN-based methods on multiple metrics of
quality and diversity. The source code and data are available at
https://github.com/MiuLab/TaylorGAN",2020-11-27
DORB: Dynamically Optimizing Multiple Rewards with Bandits,2020-11-15 21:57:47+00:00,http://arxiv.org/abs/2011.07635v1,"Ramakanth Pasunuru, Han Guo, Mohit Bansal","cs.CL, cs.AI, cs.LG",table2text,"Policy gradients-based reinforcement learning has proven to be a promising
approach for directly optimizing non-differentiable evaluation metrics for
language generation tasks. However, optimizing for a specific metric reward
leads to improvements in mostly that metric only, suggesting that the model is
gaming the formulation of that metric in a particular way without often
achieving real qualitative improvements. Hence, it is more beneficial to make
the model optimize multiple diverse metric rewards jointly. While appealing,
this is challenging because one needs to manually decide the importance and
scaling weights of these metric rewards. Further, it is important to consider
using a dynamic combination and curriculum of metric rewards that flexibly
changes over time. Considering the above aspects, in our work, we automate the
optimization of multiple metric rewards simultaneously via a multi-armed bandit
approach (DORB), where at each round, the bandit chooses which metric reward to
optimize next, based on expected arm gains. We use the Exp3 algorithm for
bandits and formulate two approaches for bandit rewards: (1) Single
Multi-reward Bandit (SM-Bandit); (2) Hierarchical Multi-reward Bandit
(HM-Bandit). We empirically show the effectiveness of our approaches via
various automatic metrics and human evaluation on two important NLG tasks:
question generation and data-to-text generation, including on an unseen-test
transfer setup. Finally, we present interpretable analyses of the learned
bandit curriculum over the optimized rewards.",2020-11-15
"A Gold Standard Methodology for Evaluating Accuracy in Data-To-Text
  Systems",2020-11-08 14:49:18+00:00,http://arxiv.org/abs/2011.03992v1,"Craig Thomson, Ehud Reiter",cs.CL,table2text,"Most Natural Language Generation systems need to produce accurate texts. We
propose a methodology for high-quality human evaluation of the accuracy of
generated texts, which is intended to serve as a gold-standard for accuracy
evaluations of data-to-text systems. We use our methodology to evaluate the
accuracy of computer generated basketball summaries. We then show how our gold
standard evaluation can be used to validate automated metrics",2020-11-08
"Best Practices for Data-Efficient Modeling in NLG:How to Train
  Production-Ready Neural Models with Less Data",2020-11-08 00:38:08+00:00,http://arxiv.org/abs/2011.03877v1,"Ankit Arun, Soumya Batra, Vikas Bhardwaj, Ashwini Challa, Pinar Donmez, Peyman Heidari, Hakan Inan, Shashank Jain, Anuj Kumar, Shawn Mei, Karthik Mohan, Michael White",cs.CL,table2text,"Natural language generation (NLG) is a critical component in conversational
systems, owing to its role of formulating a correct and natural text response.
Traditionally, NLG components have been deployed using template-based
solutions. Although neural network solutions recently developed in the research
community have been shown to provide several benefits, deployment of such
model-based solutions has been challenging due to high latency, correctness
issues, and high data needs. In this paper, we present approaches that have
helped us deploy data-efficient neural solutions for NLG in conversational
systems to production. We describe a family of sampling and modeling techniques
to attain production quality with light-weight neural network models using only
a fraction of the data that would be necessary otherwise, and show a thorough
comparison between each. Our results show that domain complexity dictates the
appropriate approach to achieve high data efficiency. Finally, we distill the
lessons from our experimental findings into a list of best practices for
production-level NLG model development, and present them in a brief runbook.
Importantly, the end products of all of the techniques are small
sequence-to-sequence models (2Mb) that we can reliably deploy in production.",2020-11-08
"SeqGenSQL -- A Robust Sequence Generation Model for Structured Query
  Language",2020-11-07 19:22:59+00:00,http://arxiv.org/abs/2011.03836v1,"Ning Li, Bethany Keller, Mark Butler, Daniel Cer",cs.AI,table2text,"We explore using T5 (Raffel et al. (2019)) to directly translate natural
language questions into SQL statements. General purpose natural language that
interfaces to information stored within databases requires flexibly translating
natural language questions into database queries. The best performing
text-to-SQL systems approach this task by first converting questions into an
intermediate logical form (LF) (Lyu et al. (2020)). While LFs provide a
convenient intermediate representation and simplify query generation, they
introduce an additional layer of complexity and annotation requirements.
However, weakly supervised modeling that directly converts questions to SQL
statements has proven more difficult without the scaffolding provided by LFs
(Min et al. (2019)). We approach direct conversion of questions to SQL
statements using T5 (Raffel et al. (2019)), a pre-trained textto-text
generation model, modified to support pointer-generator style decoding (See et
al. (2017)). We explore using question augmentation with table schema
information and the use of automatically generated silver training data. The
resulting model achieves 90.5% execution accuracy on the WikiSQL (Zhong et al.
(2017)) test data set, a new state-of-the-art on weakly supervised SQL
generation. The performance improvement is 6.6% absolute over the prior
state-of-the-art (Min et al. (2019)) and approaches the performance of
state-ofthe-art systems making use of LFs.",2020-11-07
Machine Generation and Detection of Arabic Manipulated and Fake News,2020-11-05 20:50:22+00:00,http://arxiv.org/abs/2011.03092v1,"El Moatez Billah Nagoudi, AbdelRahim Elmadany, Muhammad Abdul-Mageed, Tariq Alhindi, Hasan Cavusoglu","cs.CL, cs.LG",table2text,"Fake news and deceptive machine-generated text are serious problems
threatening modern societies, including in the Arab world. This motivates work
on detecting false and manipulated stories online. However, a bottleneck for
this research is lack of sufficient data to train detection models. We present
a novel method for automatically generating Arabic manipulated (and potentially
fake) news stories. Our method is simple and only depends on availability of
true stories, which are abundant online, and a part of speech tagger (POS). To
facilitate future work, we dispense with both of these requirements altogether
by providing AraNews, a novel and large POS-tagged news dataset that can be
used off-the-shelf. Using stories generated based on AraNews, we carry out a
human annotation study that casts light on the effects of machine manipulation
on text veracity. The study also measures human ability to detect Arabic
machine manipulated text generated by our method. Finally, we develop the first
models for detecting manipulated Arabic news and achieve state-of-the-art
results on Arabic fake news detection (macro F1=70.06). Our models and data are
publicly available.",2020-11-05
Video Generative Adversarial Networks: A Review,2020-11-04 12:16:05+00:00,http://arxiv.org/abs/2011.02250v1,"Nuha Aldausari, Arcot Sowmya, Nadine Marcus, Gelareh Mohammadi","cs.CV, cs.LG, eess.IV",table2text,"With the increasing interest in the content creation field in multiple
sectors such as media, education, and entertainment, there is an increasing
trend in the papers that uses AI algorithms to generate content such as images,
videos, audio, and text. Generative Adversarial Networks (GANs) in one of the
promising models that synthesizes data samples that are similar to real data
samples. While the variations of GANs models, in general, have been covered to
some extent in several survey papers, to the best of our knowledge, this is
among the first survey papers that reviews the state-of-the-art video GANs
models. This paper first categorized GANs review papers into general GANs
review papers, image GANs review papers, and special field GANs review papers
such as anomaly detection, medical imaging, or cybersecurity. The paper then
summarizes the main improvements in GANs frameworks that are not initially
developed for the video domain but have been adopted in multiple video GANs
variations. Then, a comprehensive review of video GANs models is provided under
two main divisions according to the presence or non-presence of a condition.
The conditional models then further grouped according to the type of condition
into audio, text, video, and image. The paper is concluded by highlighting the
main challenges and limitations of the current video GANs models. A
comprehensive list of datasets, applied loss functions, and evaluation metrics
is provided in the supplementary material.",2020-11-04
Data Augmentation for End-to-end Code-switching Speech Recognition,2020-11-04 07:12:44+00:00,http://arxiv.org/abs/2011.02160v1,"Chenpeng Du, Hao Li, Yizhou Lu, Lan Wang, Yanmin Qian","cs.CL, eess.AS",table2text,"Training a code-switching end-to-end automatic speech recognition (ASR) model
normally requires a large amount of data, while code-switching data is often
limited. In this paper, three novel approaches are proposed for code-switching
data augmentation. Specifically, they are audio splicing with the existing
code-switching data, and TTS with new code-switching texts generated by word
translation or word insertion. Our experiments on 200 hours Mandarin-English
code-switching dataset show that all the three proposed approaches yield
significant improvements on code-switching ASR individually. Moreover, all the
proposed approaches can be combined with recent popular SpecAugment, and an
addition gain can be obtained. WER is significantly reduced by relative 24.0%
compared to the system without any data augmentation, and still relative 13.0%
gain compared to the system with only SpecAugment",2020-11-04
"Conditioned Text Generation with Transfer for Closed-Domain Dialogue
  Systems",2020-11-03 14:06:10+00:00,http://arxiv.org/abs/2011.02143v1,"St√©phane d'Ascoli, Alice Coucke, Francesco Caltagirone, Alexandre Caulier, Marc Lelarge","cs.CL, cs.AI, cs.LG",table2text,"Scarcity of training data for task-oriented dialogue systems is a well known
problem that is usually tackled with costly and time-consuming manual data
annotation. An alternative solution is to rely on automatic text generation
which, although less accurate than human supervision, has the advantage of
being cheap and fast. Our contribution is twofold. First we show how to
optimally train and control the generation of intent-specific sentences using a
conditional variational autoencoder. Then we introduce a new protocol called
query transfer that allows to leverage a large unlabelled dataset, possibly
containing irrelevant queries, to extract relevant information. Comparison with
two different baselines shows that this method, in the appropriate regime,
consistently improves the diversity of the generated queries without
compromising their quality. We also demonstrate the effectiveness of our
generation method as a data augmentation technique for language modelling
tasks.",2020-11-03
Data-to-Text Generation with Iterative Text Editing,2020-11-03 13:32:38+00:00,http://arxiv.org/abs/2011.01694v1,"Zdenƒõk Kasner, Ond≈ôej Du≈°ek","cs.CL, I.2.7",table2text,"We present a novel approach to data-to-text generation based on iterative
text editing. Our approach maximizes the completeness and semantic accuracy of
the output text while leveraging the abilities of recent pre-trained models for
text editing (LaserTagger) and language modeling (GPT-2) to improve the text
fluency. To this end, we first transform data items to text using trivial
templates, and then we iteratively improve the resulting text by a neural model
trained for the sentence fusion task. The output of the model is filtered by a
simple heuristic and reranked with an off-the-shelf pre-trained language model.
We evaluate our approach on two major data-to-text datasets (WebNLG, Cleaned
E2E) and analyze its caveats and benefits. Furthermore, we show that our
formulation of data-to-text generation opens up the possibility for zero-shot
domain adaptation using a general-domain dataset for sentence fusion.",2020-11-03
"Topic-Centric Unsupervised Multi-Document Summarization of Scientific
  and News Articles",2020-11-03 04:04:21+00:00,http://arxiv.org/abs/2011.08072v1,"Amanuel Alambo, Cori Lohstroh, Erik Madaus, Swati Padhee, Brandy Foster, Tanvi Banerjee, Krishnaprasad Thirunarayan, Michael Raymer","cs.CL, cs.IR, cs.LG",table2text,"Recent advances in natural language processing have enabled automation of a
wide range of tasks, including machine translation, named entity recognition,
and sentiment analysis. Automated summarization of documents, or groups of
documents, however, has remained elusive, with many efforts limited to
extraction of keywords, key phrases, or key sentences. Accurate abstractive
summarization has yet to be achieved due to the inherent difficulty of the
problem, and limited availability of training data. In this paper, we propose a
topic-centric unsupervised multi-document summarization framework to generate
extractive and abstractive summaries for groups of scientific articles across
20 Fields of Study (FoS) in Microsoft Academic Graph (MAG) and news articles
from DUC-2004 Task 2. The proposed algorithm generates an abstractive summary
by developing salient language unit selection and text generation techniques.
Our approach matches the state-of-the-art when evaluated on automated
extractive evaluation metrics and performs better for abstractive summarization
on five human evaluation metrics (entailment, coherence, conciseness,
readability, and grammar). We achieve a kappa score of 0.68 between two
co-author linguists who evaluated our results. We plan to publicly share
MAG-20, a human-validated gold standard dataset of topic-clustered research
articles and their summaries to promote research in abstractive summarization.",2020-11-03
Deep Learning for Text Style Transfer: A Survey,2020-11-01 04:04:43+00:00,http://arxiv.org/abs/2011.00416v2,"Di Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova, Rada Mihalcea","cs.CL, cs.AI, cs.LG",table2text,"Text style transfer (TST) is an important task in natural language generation
(NLG), which aims to control certain attributes in the generated text, such as
politeness, emotion, humor, and many others. It has a long history in the field
of natural language processing (NLP), but recently it has gained significant
attention thanks to the promising performance brought by deep learning models.
In this paper, we present a systematic survey of the research done on neural
text style transfer. We have collected, summarized, and discussed nearly 70
representative articles since the first neural text style transfer work in
2017. Overall, we have covered the task formulation, existing datasets and
subtasks, evaluation metrics, and methods on parallel and non-parallel data. We
also provide discussions a variety of important topics regarding TST, which can
shed light on new development in this field. Our curated paper list is at
https://github.com/zhijing-jin/Text_Style_Transfer_Survey",2020-11-01
Personalized Multimodal Feedback Generation in Education,2020-10-31 05:26:49+00:00,http://arxiv.org/abs/2011.00192v1,"Haochen Liu, Zitao Liu, Zhongqin Wu, Jiliang Tang","cs.CL, cs.AI",table2text,"The automatic evaluation for school assignments is an important application
of AI in the education field. In this work, we focus on the task of
personalized multimodal feedback generation, which aims to generate
personalized feedback for various teachers to evaluate students' assignments
involving multimodal inputs such as images, audios, and texts. This task
involves the representation and fusion of multimodal information and natural
language generation, which presents the challenges from three aspects: 1) how
to encode and integrate multimodal inputs; 2) how to generate feedback specific
to each modality; and 3) how to realize personalized feedback generation. In
this paper, we propose a novel Personalized Multimodal Feedback Generation
Network (PMFGN) armed with a modality gate mechanism and a personalized bias
mechanism to address these challenges. The extensive experiments on real-world
K-12 education data show that our model significantly outperforms several
baselines by generating more accurate and diverse feedback. In addition,
detailed ablation experiments are conducted to deepen our understanding of the
proposed framework.",2020-10-31
Fusion Models for Improved Visual Captioning,2020-10-28 21:55:25+00:00,http://arxiv.org/abs/2010.15251v2,"Marimuthu Kalimuthu, Aditya Mogadala, Marius Mosbach, Dietrich Klakow","cs.CV, cs.AI, cs.CL, cs.LG",table2text,"Visual captioning aims to generate textual descriptions given images or
videos. Traditionally, image captioning models are trained on human annotated
datasets such as Flickr30k and MS-COCO, which are limited in size and
diversity. This limitation hinders the generalization capabilities of these
models while also rendering them liable to making mistakes. Language models
can, however, be trained on vast amounts of freely available unlabelled data
and have recently emerged as successful language encoders and coherent text
generators. Meanwhile, several unimodal and multimodal fusion techniques have
been proven to work well for natural language generation and automatic speech
recognition. Building on these recent developments, and with the aim of
improving the quality of generated captions, the contribution of our work in
this paper is two-fold: First, we propose a generic multimodal model fusion
framework for caption generation as well as emendation where we utilize
different fusion strategies to integrate a pretrained Auxiliary Language Model
(AuxLM) within the traditional encoder-decoder visual captioning frameworks.
Next, we employ the same fusion strategies to integrate a pretrained Masked
Language Model (MLM), namely BERT, with a visual captioning model, viz. Show,
Attend, and Tell, for emending both syntactic and semantic errors in captions.
Our caption emendation experiments on three benchmark image captioning
datasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the
baseline, indicating the usefulness of our proposed multimodal fusion
strategies. Further, we perform a preliminary qualitative analysis on the
emended captions and identify error categories based on the type of
corrections.",2020-10-28
"NeuroLogic Decoding: (Un)supervised Neural Text Generation with
  Predicate Logic Constraints",2020-10-24 11:55:22+00:00,http://arxiv.org/abs/2010.12884v1,"Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi",cs.CL,table2text,"Conditional text generation often requires lexical constraints, i.e., which
words should or shouldn't be included in the output text. While the dominant
recipe for conditional text generation has been large-scale pretrained language
models that are finetuned on the task-specific training data, such models do
not learn to follow the underlying constraints reliably, even when supervised
with large amounts of task-specific examples.
  We propose NeuroLogic Decoding, a simple yet effective algorithm that enables
neural language models -- supervised or not -- to generate fluent text while
satisfying complex lexical constraints. Our approach is powerful yet efficient.
It handles any set of lexical constraints that is expressible under predicate
logic, while its asymptotic runtime is equivalent to conventional beam search.
  Empirical results on four benchmarks show that NeuroLogic Decoding
outperforms previous approaches, including algorithms that handle a subset of
our constraints. Moreover, we find that unsupervised models with NeuroLogic
Decoding often outperform supervised models with conventional decoding, even
when the latter is based on considerably larger networks. Our results suggest
the limit of large-scale neural networks for fine-grained controllable
generation and the promise of inference-time algorithms.",2020-10-24
Go Figure! A Meta Evaluation of Factuality in Summarization,2020-10-24 08:30:20+00:00,http://arxiv.org/abs/2010.12834v1,"Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, Jianfeng Gao",cs.CL,table2text,"Text generation models can generate factually inconsistent text containing
distorted or fabricated facts about the source text. Recent work has focused on
building evaluation models to verify the factual correctness of semantically
constrained text generation tasks such as document summarization. While the
field of factuality evaluation is growing fast, we don't have well-defined
criteria for measuring the effectiveness, generalizability, reliability, or
sensitivity of the factuality metrics. Focusing on these aspects, in this
paper, we introduce a meta-evaluation framework for evaluating factual
consistency metrics. We introduce five necessary, common-sense conditions for
effective factuality metrics and experiment with nine recent factuality metrics
using synthetic and human-labeled factuality data from short news, long news
and dialogue summarization domains. Our framework enables assessing the
efficiency of any new factual consistency metric on a variety of dimensions
over multiple summarization domains and can be easily extended with new
meta-evaluation criteria. We also present our conclusions towards standardizing
the factuality evaluation metrics.",2020-10-24
"Multilingual Speech Translation with Efficient Finetuning of Pretrained
  Models",2020-10-24 08:15:08+00:00,http://arxiv.org/abs/2010.12829v4,"Xian Li, Changhan Wang, Yun Tang, Chau Tran, Yuqing Tang, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli",cs.CL,table2text,"We present a simple yet effective approach to build multilingual
speech-to-text (ST) translation by efficient transfer learning from pretrained
speech encoder and text decoder. Our key finding is that a minimalistic LNA
(LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and
cross-modality transfer ability by only finetuning less than 10% of the
pretrained parameters. This enables effectively leveraging large pretrained
models with low training cost. Using wav2vec 2.0 for acoustic modeling, and
mBART for multilingual text generation, our approach advanced the new
state-of-the-art for 34 translation directions (and surpassing cascaded ST for
23 of them) on large-scale multilingual ST benchmark CoVoST 2 (+6.4 BLEU on
average across 15 En-X directions and +5.1 BLEU on average across 19 X-En
directions). Our approach demonstrates strong zero-shot performance in a
many-to-many multilingual model (+5.7 BLEU on average across 18 non-English
directions), making it an appealing approach for attaining high-quality speech
translation with improved parameter and data efficiency.",2020-10-24
CaM-Gen:Causally-aware Metric-guided Text Generation,2020-10-24 06:17:35+00:00,http://arxiv.org/abs/2010.12795v1,"Navita Goyal, Roodram Paneri, Ayush Agarwal, Udit Kalani, Abhilasha Sancheti, Niyati Chhaya",cs.CL,table2text,"Content is created for a well-defined purpose, often described by a metric or
a signal represented in the form of structured information. The relationship
between the metrics or the goal of a target content and the content itself are
non-trivial. While large scale language models show promising text generation
capabilities, guiding and informing the generated text with external metrics is
challenging. These metrics and the content tend to have inherent relationships
and not all of them may directly impact the content. We introduce a CaM-Gen:
Causally-aware Generative Networks guided by user-defined input metrics
incorporating the causal relationships between the metric and the content
features. We leverage causal inference techniques to identify the causally
significant aspects of text that leads to the target metric and then explicitly
guide the generative model towards these by a feedback mechanism. We propose
this mechanism for variational autoencoder-based and transformer-based
generative models. The proposed models beat baselines in terms of the target
metric accuracy while maintaining the fluency and the language quality of the
generated text. To the best of our knowledge, this is one of the early attempts
at incorporating a metric-guide using causal inference towards controlled
generation.",2020-10-24
"Large Scale Knowledge Graph Based Synthetic Corpus Generation for
  Knowledge-Enhanced Language Model Pre-training",2020-10-23 22:14:50+00:00,http://arxiv.org/abs/2010.12688v1,"Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami Al-Rfou",cs.CL,table2text,"Generating natural sentences from Knowledge Graph (KG) triples, known as
Data-To-Text Generation, is a task with many datasets for which numerous
complex systems have been developed. However, no prior work has attempted to
perform this generation at scale by converting an entire KG into natural text.
In this paper, we verbalize the entire Wikidata KG, and create a KG-Text
aligned corpus in the training process. We discuss the challenges in
verbalizing an entire KG versus verbalizing smaller datasets. We further show
that verbalizing an entire KG can be used to integrate structured and natural
language data. In contrast to the many architectures that have been developed
to integrate the structural differences between these two sources, our approach
converts the KG into the same format as natural text allowing it to be
seamlessly plugged into existing natural language systems. We evaluate this
approach by augmenting the retrieval corpus in REALM and showing improvements,
both on the LAMA knowledge probe and open domain QA.",2020-10-23
"Detecting and Exorcising Statistical Demons from Language Models with
  Anti-Models of Negative Data",2020-10-22 16:45:32+00:00,http://arxiv.org/abs/2010.11855v1,"Michael L. Wick, Kate Silverstein, Jean-Baptiste Tristan, Adam Pocock, Mark Johnson","cs.CL, cs.AI, cs.LG",table2text,"It's been said that ""Language Models are Unsupervised Multitask Learners.""
Indeed, self-supervised language models trained on ""positive"" examples of
English text generalize in desirable ways to many natural language tasks. But
if such models can stray so far from an initial self-supervision objective, a
wayward model might generalize in undesirable ways too, say to nonsensical
""negative"" examples of unnatural language. A key question in this work is: do
language models trained on (positive) training data also generalize to
(negative) test data? We use this question as a contrivance to assess the
extent to which language models learn undesirable properties of text, such as
n-grams, that might interfere with the learning of more desirable properties of
text, such as syntax. We find that within a model family, as the number of
parameters, training epochs, and data set size increase, so does a model's
ability to generalize to negative n-gram data, indicating standard
self-supervision generalizes too far. We propose a form of inductive bias that
attenuates such undesirable signals with negative data distributions
automatically learned from positive data. We apply the method to remove n-gram
signals from LSTMs and find that doing so causes them to favor syntactic
signals, as demonstrated by large error reductions (up to 46% on the hardest
cases) on a syntactic subject-verb agreement task.",2020-10-22
"Multi-dimensional Style Transfer for Partially Annotated Data using
  Language Models as Discriminators",2020-10-22 10:16:29+00:00,http://arxiv.org/abs/2010.11578v1,"Navita Goyal, Balaji Vasan Srinivasan, Anandhavelu N, Abhilasha Sancheti",cs.CL,table2text,"Style transfer has been widely explored in natural language generation with
non-parallel corpus by directly or indirectly extracting a notion of style from
source and target domain corpus. A common aspect among the existing approaches
is the prerequisite of joint annotations across all the stylistic dimensions
under consideration. Availability of such dataset across a combination of
styles is a limiting factor in extending state-of-the art style transfer setups
to multiple style dimensions. While cascading single-dimensional models across
multiple styles is a possibility, it suffers from content loss, especially when
the style dimensions are not completely independent of each other. In our work,
we attempt to relax this restriction on requirement of jointly annotated data
across multiple styles being inspected and make use of independently acquired
data across different style dimensions without any additional annotations. We
initialize an encoder-decoder setup with large transformer-based language
models pre-trained on a generic corpus and enhance its re-writing capability to
multiple styles by employing multiple language models as discriminators.
Through quantitative and qualitative evaluation, we show the ability of our
model to control for styles across multiple style-dimensions while preserving
content of the input text and compare it against baselines which involve
cascaded state-of-the-art uni-dimensional style transfer models.",2020-10-22
"PARENTing via Model-Agnostic Reinforcement Learning to Correct
  Pathological Behaviors in Data-to-Text Generation",2020-10-21 09:49:47+00:00,http://arxiv.org/abs/2010.10866v2,"Cl√©ment Rebuffel, Laure Soulier, Geoffrey Scoutheeten, Patrick Gallinari",cs.CL,table2text,"In language generation models conditioned by structured data, the classical
training via maximum likelihood almost always leads models to pick up on
dataset divergence (i.e., hallucinations or omissions), and to incorporate them
erroneously in their own generations at inference. In this work, we build ontop
of previous Reinforcement Learning based approaches and show that a
model-agnostic framework relying on the recently introduced PARENT metric is
efficient at reducing both hallucinations and omissions. Evaluations on the
widely used WikiBIO and WebNLG benchmarks demonstrate the effectiveness of this
framework compared to state-of-the-art models.",2020-10-21
"Chart-to-Text: Generating Natural Language Descriptions for Charts by
  Adapting the Transformer Model",2020-10-18 23:57:33+00:00,http://arxiv.org/abs/2010.09142v2,"Jason Obeid, Enamul Hoque","cs.CL, cs.AI",table2text,"Information visualizations such as bar charts and line charts are very
popular for exploring data and communicating insights. Interpreting and making
sense of such visualizations can be challenging for some people, such as those
who are visually impaired or have low visualization literacy. In this work, we
introduce a new dataset and present a neural model for automatically generating
natural language summaries for charts. The generated summaries provide an
interpretation of the chart and convey the key insights found within that
chart. Our neural model is developed by extending the state-of-the-art model
for the data-to-text generation task, which utilizes a transformer-based
encoder-decoder architecture. We found that our approach outperforms the base
model on a content selection metric by a wide margin (55.42% vs. 8.49%) and
generates more informative, concise, and coherent summaries.",2020-10-18
"RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich
  Semantic Annotations for Task-Oriented Dialogue Modeling",2020-10-17 08:18:59+00:00,http://arxiv.org/abs/2010.08738v1,"Jun Quan, Shian Zhang, Qian Cao, Zizhong Li, Deyi Xiong",cs.CL,table2text,"In order to alleviate the shortage of multi-domain data and to capture
discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a
large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic
Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn
semantically annotated dialogues, with more than 150K utterances spanning over
12 domains, which is larger than all previous annotated H2H conversational
datasets. Both single- and multi-domain dialogues are constructed, accounting
for 65% and 35%, respectively. Each dialogue is labeled with comprehensive
dialogue annotations, including dialogue goal in the form of natural language
description, domain, dialogue states and acts at both the user and system side.
In addition to traditional dialogue annotations, we especially provide
linguistic annotations on discourse phenomena, e.g., ellipsis and coreference,
in dialogues, which are useful for dialogue coreference and ellipsis resolution
tasks. Apart from the fully annotated dataset, we also present a detailed
description of the data collection procedure, statistics and analysis of the
dataset. A series of benchmark models and results are reported, including
natural language understanding (intent detection & slot filling), dialogue
state tracking and dialogue context-to-text generation, as well as coreference
and ellipsis resolution, which facilitate the baseline comparison for future
research on this corpus.",2020-10-17
Learning Better Representation for Tables by Self-Supervised Tasks,2020-10-15 09:03:38+00:00,http://arxiv.org/abs/2010.07606v1,"Liang Li, Can Ma, Yinliang Yue, Linjun Shou, Dayong Hu",cs.CL,table2text,"Table-to-text generation aims at automatically generating natural text to
help people to conveniently obtain the important information in tables.
Although neural models for table-to-text have achieved remarkable progress,
some problems still overlooked. The first is that the values recorded in many
tables are mostly numbers in practice. The existing approaches do not do
special treatment for these, and still regard these as words in natural
language text. Secondly, the target texts in training dataset may contain
redundant information or facts do not exist in the input tables. These may give
wrong supervision signals to some methods based on content selection and
planning and auxiliary supervision. To solve these problems, we propose two
self-supervised tasks, Number Ordering and Significance Ordering, to help to
learn better table representation. The former works on the column dimension to
help to incorporate the size property of numbers into table representation. The
latter acts on row dimension and help to learn a significance-aware table
representation. We test our methods on the widely used dataset ROTOWIRE which
consists of NBA game statistic and related news. The experimental results
demonstrate that the model trained together with these two self-supervised
tasks can generate text that contains more salient and well-organized facts,
even without modeling context selection and planning. And we achieve the
state-of-the-art performance on automatic metrics.",2020-10-15
"Grammatical Error Correction in Low Error Density Domains: A New
  Benchmark and Analyses",2020-10-15 07:52:01+00:00,http://arxiv.org/abs/2010.07574v1,"Simon Flachs, Oph√©lie Lacroix, Helen Yannakoudakis, Marek Rei, Anders S√∏gaard",cs.CL,table2text,"Evaluation of grammatical error correction (GEC) systems has primarily
focused on essays written by non-native learners of English, which however is
only part of the full spectrum of GEC applications. We aim to broaden the
target domain of GEC and release CWEB, a new benchmark for GEC consisting of
website text generated by English speakers of varying levels of proficiency.
Website data is a common and important domain that contains far fewer
grammatical errors than learner essays, which we show presents a challenge to
state-of-the-art GEC systems. We demonstrate that a factor behind this is the
inability of systems to rely on a strong internal language model in low error
density domains. We hope this work shall facilitate the development of
open-domain GEC models that generalize to different topics and genres.",2020-10-15
Neural Deepfake Detection with Factual Structure of Text,2020-10-15 02:35:31+00:00,http://arxiv.org/abs/2010.07475v1,"Wanjun Zhong, Duyu Tang, Zenan Xu, Ruize Wang, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin",cs.CL,table2text,"Deepfake detection, the task of automatically discriminating
machine-generated text, is increasingly critical with recent advances in
natural language generative models. Existing approaches to deepfake detection
typically represent documents with coarse-grained representations. However,
they struggle to capture factual structures of documents, which is a
discriminative factor between machine-generated and human-written text
according to our statistical analysis. To address this, we propose a
graph-based model that utilizes the factual structure of a document for
deepfake detection of text. Our approach represents the factual structure of a
given document as an entity graph, which is further utilized to learn sentence
representations with a graph neural network. Sentence representations are then
composed to a document representation for making predictions, where consistent
relations between neighboring sentences are sequentially modeled. Results of
experiments on two public deepfake datasets show that our approach
significantly improves strong base models built with RoBERTa. Model analysis
further indicates that our model can distinguish the difference in the factual
structure between machine-generated text and human-written text.",2020-10-15
"ReviewRobot: Explainable Paper Review Generation based on Knowledge
  Synthesis",2020-10-13 02:17:58+00:00,http://arxiv.org/abs/2010.06119v3,"Qingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight, Heng Ji, Nazneen Fatema Rajani","cs.CL, cs.AI",table2text,"To assist human review process, we build a novel ReviewRobot to automatically
assign a review score and write comments for multiple categories such as
novelty and meaningful comparison. A good review needs to be knowledgeable,
namely that the comments should be constructive and informative to help improve
the paper; and explainable by providing detailed evidence. ReviewRobot achieves
these goals via three steps: (1) We perform domain-specific Information
Extraction to construct a knowledge graph (KG) from the target paper under
review, a related work KG from the papers cited by the target paper, and a
background KG from a large collection of previous papers in the domain. (2) By
comparing these three KGs, we predict a review score and detailed structured
knowledge as evidence for each review category. (3) We carefully select and
generalize human review sentences into templates, and apply these templates to
transform the review scores and evidence into natural language comments.
Experimental results show that our review score predictor reaches 71.4%-100%
accuracy. Human assessment by domain experts shows that 41.7%-70.5% of the
comments generated by ReviewRobot are valid and constructive, and better than
human-written ones for 20% of the time. Thus, ReviewRobot can serve as an
assistant for paper reviewers, program chairs and authors.",2020-10-13
Improving Text Generation with Student-Forcing Optimal Transport,2020-10-12 19:42:25+00:00,http://arxiv.org/abs/2010.05994v1,"Guoyin Wang, Chunyuan Li, Jianqiao Li, Hao Fu, Yuh-Chen Lin, Liqun Chen, Yizhe Zhang, Chenyang Tao, Ruiyi Zhang, Wenlin Wang, Dinghan Shen, Qian Yang, Lawrence Carin","cs.CL, cs.LG",table2text,"Neural language models are often trained with maximum likelihood estimation
(MLE), where the next word is generated conditioned on the ground-truth word
tokens. During testing, however, the model is instead conditioned on previously
generated tokens, resulting in what is termed exposure bias. To reduce this gap
between training and testing, we propose using optimal transport (OT) to match
the sequences generated in these two modes. An extension is further proposed to
improve the OT learning, based on the structural and contextual information of
the text sequences. The effectiveness of the proposed method is validated on
machine translation, text summarization, and text generation tasks.",2020-10-12
"Controlled Hallucinations: Learning to Generate Faithfully from Noisy
  Data",2020-10-12 17:25:02+00:00,http://arxiv.org/abs/2010.05873v1,Katja Filippova,cs.CL,table2text,"Neural text generation (data- or text-to-text) demonstrates remarkable
performance when training data is abundant which for many applications is not
the case. To collect a large corpus of parallel data, heuristic rules are often
used but they inevitably let noise into the data, such as phrases in the output
which cannot be explained by the input. Consequently, models pick up on the
noise and may hallucinate--generate fluent but unsupported text. Our
contribution is a simple but powerful technique to treat such hallucinations as
a controllable aspect of the generated text, without dismissing any input and
without modifying the model architecture. On the WikiBio corpus (Lebret et al.,
2016), a particularly noisy dataset, we demonstrate the efficacy of the
technique both in an automatic and in a human evaluation.",2020-10-12
"A Sentiment-Controllable Topic-to-Essay Generator with Topic Knowledge
  Graph",2020-10-12 08:06:12+00:00,http://arxiv.org/abs/2010.05511v1,"Lin Qiao, Jianhao Yan, Fandong Meng, Zhendong Yang, Jie Zhou",cs.CL,table2text,"Generating a vivid, novel, and diverse essay with only several given topic
words is a challenging task of natural language generation. In previous work,
there are two problems left unsolved: neglect of sentiment beneath the text and
insufficient utilization of topic-related knowledge. Therefore, we propose a
novel Sentiment-Controllable topic-to-essay generator with a Topic Knowledge
Graph enhanced decoder, named SCTKG, which is based on the conditional
variational autoencoder (CVAE) framework. We firstly inject the sentiment
information into the generator for controlling sentiment for each sentence,
which leads to various generated essays. Then we design a Topic Knowledge Graph
enhanced decoder. Unlike existing models that use knowledge entities
separately, our model treats the knowledge graph as a whole and encodes more
structured, connected semantic information in the graph to generate a more
relevant essay. Experimental results show that our SCTKG can generate sentiment
controllable essays and outperform the state-of-the-art approach in terms of
topic relevance, fluency, and diversity on both automatic and human evaluation.",2020-10-12
Evaluating Factuality in Generation with Dependency-level Entailment,2020-10-12 06:43:10+00:00,http://arxiv.org/abs/2010.05478v2,"Tanya Goyal, Greg Durrett",cs.CL,table2text,"Despite significant progress in text generation models, a serious limitation
is their tendency to produce text that is factually inconsistent with
information in the input. Recent work has studied whether textual entailment
systems can be used to identify factual errors; however, these sentence-level
entailment models are trained to solve a different problem than generation
filtering and they do not localize which part of a generation is non-factual.
In this paper, we propose a new formulation of entailment that decomposes it at
the level of dependency arcs. Rather than focusing on aggregate decisions, we
instead ask whether the semantic relationship manifested by individual
dependency arcs in the generated output is supported by the input. Human
judgments on this task are difficult to obtain; we therefore propose a method
to automatically create data based on existing entailment or paraphrase
corpora. Experiments show that our dependency arc entailment model trained on
this data can identify factual inconsistencies in paraphrasing and
summarization better than sentence-level methods or those based on question
generation, while additionally localizing the erroneous parts of the
generation.",2020-10-12
On Long-Tailed Phenomena in Neural Machine Translation,2020-10-10 07:00:57+00:00,http://arxiv.org/abs/2010.04924v1,"Vikas Raunak, Siddharth Dalmia, Vivek Gupta, Florian Metze","cs.CL, cs.AI, cs.LG",table2text,"State-of-the-art Neural Machine Translation (NMT) models struggle with
generating low-frequency tokens, tackling which remains a major challenge. The
analysis of long-tailed phenomena in the context of structured prediction tasks
is further hindered by the added complexities of search during inference. In
this work, we quantitatively characterize such long-tailed phenomena at two
levels of abstraction, namely, token classification and sequence generation. We
propose a new loss function, the Anti-Focal loss, to better adapt model
training to the structural dependencies of conditional text generation by
incorporating the inductive biases of beam search in the training process. We
show the efficacy of the proposed technique on a number of Machine Translation
(MT) datasets, demonstrating that it leads to significant gains over
cross-entropy across different language pairs, especially on the generation of
low-frequency words. We have released the code to reproduce our results.",2020-10-10
"Adversarial Self-Supervised Data-Free Distillation for Text
  Classification",2020-10-10 02:46:06+00:00,http://arxiv.org/abs/2010.04883v1,"Xinyin Ma, Yongliang Shen, Gongfan Fang, Chen Chen, Chenghao Jia, Weiming Lu","cs.CL, cs.AI",table2text,"Large pre-trained transformer-based language models have achieved impressive
results on a wide range of NLP tasks. In the past few years, Knowledge
Distillation(KD) has become a popular paradigm to compress a computationally
expensive model to a resource-efficient lightweight model. However, most KD
algorithms, especially in NLP, rely on the accessibility of the original
training dataset, which may be unavailable due to privacy issues. To tackle
this problem, we propose a novel two-stage data-free distillation method, named
Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed
for compressing large-scale transformer-based models (e.g., BERT). To avoid
text generation in discrete space, we introduce a Plug & Play Embedding
Guessing method to craft pseudo embeddings from the teacher's hidden knowledge.
Meanwhile, with a self-supervised module to quantify the student's ability, we
adapt the difficulty of pseudo embeddings in an adversarial training manner. To
the best of our knowledge, our framework is the first data-free distillation
framework designed for NLP tasks. We verify the effectiveness of our method on
several text classification datasets.",2020-10-10
A Survey of Knowledge-Enhanced Text Generation,2020-10-09 06:46:46+00:00,http://arxiv.org/abs/2010.04389v1,"Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, Meng Jiang","cs.CL, cs.AI, cs.LG",table2text,"The goal of text generation is to make machines express in human language. It
is one of the most important yet challenging tasks in natural language
processing (NLP). Since 2014, various neural encoder-decoder models pioneered
by Seq2Seq have been proposed to achieve the goal by learning to map input text
to output text. However, the input text alone often provides limited knowledge
to generate the desired output, so the performance of text generation is still
far from satisfaction in many real-world scenarios. To address this issue,
researchers have considered incorporating various forms of knowledge beyond the
input text into the generation models. This research direction is known as
knowledge-enhanced text generation. In this survey, we present a comprehensive
review of the research on knowledge enhanced text generation over the past five
years. The main content includes two parts: (i) general methods and
architectures for integrating knowledge into text generation; (ii) specific
techniques and applications according to different forms of knowledge data.
This survey can have broad audiences, researchers and practitioners, in
academia and industry.",2020-10-09
"Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text
  Generation",2020-10-09 06:03:46+00:00,http://arxiv.org/abs/2010.04383v1,"Yan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, Shay B. Cohen, Zuozhu Liu, Lidong Bing",cs.CL,table2text,"AMR-to-text generation is used to transduce Abstract Meaning Representation
structures (AMR) into text. A key challenge in this task is to efficiently
learn effective graph representations. Previously, Graph Convolution Networks
(GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to
capture non-local information and additionally, they follow a local
(first-order) information aggregation scheme. To account for these issues,
larger and deeper GCN models are required to capture more complex interactions.
In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight
Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local
interactions by synthesizing higher order information from the input graphs. We
further develop two novel parameter saving strategies based on the group graph
convolutions and weight tied convolutions to reduce memory usage and model
complexity. With the help of these strategies, we are able to train a model
with fewer parameters while maintaining the model capacity. Experiments
demonstrate that LDGCNs outperform state-of-the-art models on two benchmark
datasets for AMR-to-text generation with significantly fewer parameters.",2020-10-09
"A Cascade Approach to Neural Abstractive Summarization with Content
  Selection and Fusion",2020-10-08 01:49:16+00:00,http://arxiv.org/abs/2010.03722v1,"Logan Lebanoff, Franck Dernoncourt, Doo Soon Kim, Walter Chang, Fei Liu",cs.CL,table2text,"We present an empirical study in favor of a cascade architecture to neural
text summarization. Summarization practices vary widely but few other than news
summarization can provide a sufficient amount of training data enough to meet
the requirement of end-to-end neural abstractive systems which perform content
selection and surface realization jointly to generate abstracts. Such systems
also pose a challenge to summarization evaluation, as they force content
selection to be evaluated along with text generation, yet evaluation of the
latter remains an unsolved problem. In this paper, we present empirical results
showing that the performance of a cascaded pipeline that separately identifies
important content pieces and stitches them together into a coherent text is
comparable to or outranks that of end-to-end systems, whereas a pipeline
architecture allows for flexible content selection. We finally discuss how we
can take advantage of a cascaded pipeline in neural text summarization and shed
light on important directions for future research.",2020-10-08
"Stepwise Extractive Summarization and Planning with Structured
  Transformers",2020-10-06 14:12:58+00:00,http://arxiv.org/abs/2010.02744v1,"Shashi Narayan, Joshua Maynez, Jakub Adamek, Daniele Pighin, Bla≈æ Brataniƒç, Ryan McDonald",cs.CL,table2text,"We propose encoder-centric stepwise models for extractive summarization using
structured transformers -- HiBERT and Extended Transformers. We enable stepwise
summarization by injecting the previously generated summary into the structured
transformer as an auxiliary sub-structure. Our models are not only efficient in
modeling the structure of long inputs, but they also do not rely on
task-specific redundancy-aware modeling, making them a general purpose
extractive content planner for different tasks. When evaluated on CNN/DailyMail
extractive summarization, stepwise models achieve state-of-the-art performance
in terms of Rouge without any redundancy aware modeling or sentence filtering.
This also holds true for Rotowire table-to-text generation, where our models
surpass previously reported metrics for content selection, planning and
ordering, highlighting the strength of stepwise modeling. Amongst the two
structured transformers we test, stepwise Extended Transformers provides the
best performance across both datasets and sets a new standard for these
challenges.",2020-10-06
"Investigating African-American Vernacular English in Transformer-Based
  Text Generation",2020-10-06 06:27:02+00:00,http://arxiv.org/abs/2010.02510v2,"Sophie Groenwold, Lily Ou, Aesha Parekh, Samhita Honnavalli, Sharon Levy, Diba Mirza, William Yang Wang","cs.CL, cs.AI",table2text,"The growth of social media has encouraged the written use of African American
Vernacular English (AAVE), which has traditionally been used only in oral
contexts. However, NLP models have historically been developed using dominant
English varieties, such as Standard American English (SAE), due to text corpora
availability. We investigate the performance of GPT-2 on AAVE text by creating
a dataset of intent-equivalent parallel AAVE/SAE tweet pairs, thereby isolating
syntactic structure and AAVE- or SAE-specific language for each pair. We
evaluate each sample and its GPT-2 generated text with pretrained sentiment
classifiers and find that while AAVE text results in more classifications of
negative sentiment than SAE, the use of GPT-2 generally increases occurrences
of positive sentiment for both. Additionally, we conduct human evaluation of
AAVE and SAE text generated with GPT-2 to compare contextual rigor and overall
quality.",2020-10-06
"Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on
  Chest X-rays",2020-10-06 04:18:18+00:00,http://arxiv.org/abs/2010.02467v1,"Jianmo Ni, Chun-Nan Hsu, Amilcare Gentili, Julian McAuley","cs.CV, cs.CL",table2text,"Automatic medical image report generation has drawn growing attention due to
its potential to alleviate radiologists' workload. Existing work on report
generation often trains encoder-decoder networks to generate complete reports.
However, such models are affected by data bias (e.g.~label imbalance) and face
common issues inherent in text generation models (e.g.~repetition). In this
work, we focus on reporting abnormal findings on radiology images; instead of
training on complete radiology reports, we propose a method to identify
abnormal findings from the reports in addition to grouping them with
unsupervised clustering and minimal rules. We formulate the task as cross-modal
retrieval and propose Conditional Visual-Semantic Embeddings to align images
and fine-grained abnormal findings in a joint embedding space. We demonstrate
that our method is able to retrieve abnormal findings and outperforms existing
generation models on both clinical correctness and text generation metrics.",2020-10-06
KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation,2020-10-05 19:59:05+00:00,http://arxiv.org/abs/2010.02307v2,"Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang","cs.CL, cs.AI",table2text,"Data-to-text generation has recently attracted substantial interests due to
its wide applications. Existing methods have shown impressive performance on an
array of tasks. However, they rely on a significant amount of labeled data for
each task, which is costly to acquire and thus limits their application to new
tasks and domains. In this paper, we propose to leverage pre-training and
transfer learning to address this issue. We propose a knowledge-grounded
pre-training (KGPT), which consists of two parts, 1) a general
knowledge-grounded generation model to generate knowledge-enriched text. 2) a
pre-training paradigm on a massive knowledge-grounded text corpus crawled from
the web. The pre-trained model can be fine-tuned on various data-to-text
generation tasks to generate task-specific text. We adopt three settings,
namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness.
Under the fully-supervised setting, our model can achieve remarkable gains over
the known baselines. Under zero-shot setting, our model without seeing any
examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail.
Under the few-shot setting, our model only needs about one-fifteenth as many
labeled examples to achieve the same level of performance as baseline models.
These experiments consistently prove the strong generalization ability of our
proposed framework https://github.com/wenhuchen/KGPT.",2020-10-05
"PAIR: Planning and Iterative Refinement in Pre-trained Transformers for
  Long Text Generation",2020-10-05 19:45:03+00:00,http://arxiv.org/abs/2010.02301v1,"Xinyu Hua, Lu Wang",cs.CL,table2text,"Pre-trained Transformers have enabled impressive breakthroughs in generating
long and fluent text, yet their outputs are often ""rambling"" without coherently
arranged content. In this work, we present a novel content-controlled text
generation framework, PAIR, with planning and iterative refinement, which is
built upon a large model, BART. We first adapt the BERT model to automatically
construct the content plans, consisting of keyphrase assignments and their
corresponding sentence-level positions. The BART model is employed for
generation without modifying its structure. We then propose a refinement
algorithm to gradually enhance the generation quality within the
sequence-to-sequence framework. Evaluation with automatic metrics shows that
adding planning consistently improves the generation quality on three distinct
domains, with an average of 20 BLEU points and 12 METEOR points improvements.
In addition, human judges rate our system outputs to be more relevant and
coherent than comparisons without planning.",2020-10-05
GenAug: Data Augmentation for Finetuning Text Generators,2020-10-05 05:46:39+00:00,http://arxiv.org/abs/2010.01794v2,"Steven Y. Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura, Eduard Hovy","cs.CL, cs.AI, cs.LG",table2text,"In this paper, we investigate data augmentation for text generation, which we
call GenAug. Text generation and language modeling are important tasks within
natural language processing, and are especially challenging for low-data
regimes. We propose and evaluate various augmentation methods, including some
that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp
Reviews. We also examine the relationship between the amount of augmentation
and the quality of the generated text. We utilize several metrics that evaluate
important aspects of the generated text including its diversity and fluency.
Our experiments demonstrate that insertion of character-level synthetic noise
and keyword replacement with hypernyms are effective augmentation methods, and
that the quality of generations improves to a peak at approximately three times
the amount of original data.",2020-10-05
Transformer-Based Neural Text Generation with Syntactic Guidance,2020-10-05 01:33:58+00:00,http://arxiv.org/abs/2010.01737v1,"Yinghao Li, Rui Feng, Isaac Rehg, Chao Zhang",cs.CL,table2text,"We study the problem of using (partial) constituency parse trees as syntactic
guidance for controlled text generation. Existing approaches to this problem
use recurrent structures, which not only suffer from the long-term dependency
problem but also falls short in modeling the tree structure of the syntactic
guidance. We propose to leverage the parallelism of Transformer to better
incorporate parse trees. Our method first expands a partial template
constituency parse tree to a full-fledged parse tree tailored for the input
source text, and then uses the expanded tree to guide text generation. The
effectiveness of our model in this process hinges upon two new attention
mechanisms: 1) a path attention mechanism that forces one node to attend to
only other nodes located in its path in the syntax tree to better incorporate
syntax guidance; 2) a multi-encoder attention mechanism that allows the decoder
to dynamically attend to information from multiple encoders. Our experiments in
the controlled paraphrasing task show that our method outperforms SOTA models
both semantically and syntactically, improving the best baseline's BLEU score
from 11.83 to 26.27.",2020-10-05
Partially-Aligned Data-to-Text Generation with Distant Supervision,2020-10-03 03:18:52+00:00,http://arxiv.org/abs/2010.01268v1,"Zihao Fu, Bei Shi, Wai Lam, Lidong Bing, Zhiyuan Liu",cs.CL,table2text,"The Data-to-Text task aims to generate human-readable text for describing
some given structured data enabling more interpretability. However, the typical
generation task is confined to a few particular domains since it requires
well-aligned data which is difficult and expensive to obtain. Using
partially-aligned data is an alternative way of solving the dataset scarcity
problem. This kind of data is much easier to obtain since it can be produced
automatically. However, using this kind of data induces the over-generation
problem posing difficulties for existing models, which tends to add unrelated
excerpts during the generation procedure. In order to effectively utilize
automatically annotated partially-aligned datasets, we extend the traditional
generation task to a refined task called Partially-Aligned Data-to-Text
Generation (PADTG) which is more practical since it utilizes automatically
annotated data for training and thus considerably expands the application
domains. To tackle this new task, we propose a novel distant supervision
generation framework. It firstly estimates the input data's supportiveness for
each target word with an estimator and then applies a supportiveness adaptor
and a rebalanced beam search to harness the over-generation problem in the
training and generation phases respectively. We also contribute a
partially-aligned dataset (The data and source code of this paper can be
obtained from https://github.com/fuzihaofzh/distant_supervision_nlg by sampling
sentences from Wikipedia and automatically extracting corresponding KB triples
for each sentence from Wikidata. The experimental results show that our
framework outperforms all baseline models as well as verify the feasibility of
utilizing partially-aligned data.",2020-10-03
"Continual Learning for Natural Language Generation in Task-oriented
  Dialog Systems",2020-10-02 10:32:29+00:00,http://arxiv.org/abs/2010.00910v1,"Fei Mi, Liangwei Chen, Mengjie Zhao, Minlie Huang, Boi Faltings","cs.CL, cs.LG",table2text,"Natural language generation (NLG) is an essential component of task-oriented
dialog systems. Despite the recent success of neural approaches for NLG, they
are typically developed in an offline manner for particular domains. To better
fit real-life applications where new data come in a stream, we study NLG in a
""continual learning"" setting to expand its knowledge to new domains or
functionalities incrementally. The major challenge towards this goal is
catastrophic forgetting, meaning that a continually trained model tends to
forget the knowledge it has learned before. To this end, we propose a method
called ARPER (Adaptively Regularized Prioritized Exemplar Replay) by replaying
prioritized historical exemplars, together with an adaptive regularization
technique based on ElasticWeight Consolidation. Extensive experiments to
continually learn new domains and intents are conducted on MultiWoZ-2.0 to
benchmark ARPER with a wide range of techniques. Empirical results demonstrate
that ARPER significantly outperforms other methods by effectively mitigating
the detrimental catastrophic forgetting issue.",2020-10-02
"Learning from Mistakes: Combining Ontologies via Self-Training for
  Dialogue Generation",2020-09-30 23:54:38+00:00,http://arxiv.org/abs/2010.00150v1,"Lena Reed, Vrindavan Harrison, Shereen Oraby, Dilek Hakkani-Tur, Marilyn Walker",cs.CL,table2text,"Natural language generators (NLGs) for task-oriented dialogue typically take
a meaning representation (MR) as input. They are trained end-to-end with a
corpus of MR/utterance pairs, where the MRs cover a specific set of dialogue
acts and domain attributes. Creation of such datasets is labor-intensive and
time-consuming. Therefore, dialogue systems for new domain ontologies would
benefit from using data for pre-existing ontologies. Here we explore, for the
first time, whether it is possible to train an NLG for a new larger ontology
using existing training sets for the restaurant domain, where each set is based
on a different ontology. We create a new, larger combined ontology, and then
train an NLG to produce utterances covering it. For example, if one dataset has
attributes for family-friendly and rating information, and the other has
attributes for decor and service, our aim is an NLG for the combined ontology
that can produce utterances that realize values for family-friendly, rating,
decor and service. Initial experiments with a baseline neural
sequence-to-sequence model show that this task is surprisingly challenging. We
then develop a novel self-training method that identifies (errorful) model
outputs, automatically constructs a corrected MR input to form a new (MR,
utterance) training pair, and then repeatedly adds these new instances back
into the training data. We then test the resulting model on a new test set. The
result is a self-trained model whose performance is an absolute 75.4%
improvement over the baseline model. We also report a human qualitative
evaluation of the final model showing that it achieves high naturalness,
semantic coherence and grammaticality",2020-09-30
Utterance-level Dialogue Understanding: An Empirical Study,2020-09-29 09:50:21+00:00,http://arxiv.org/abs/2009.13902v5,"Deepanway Ghosal, Navonil Majumder, Rada Mihalcea, Soujanya Poria",cs.CL,table2text,"The recent abundance of conversational data on the Web and elsewhere calls
for effective NLP systems for dialog understanding. Complete utterance-level
understanding often requires context understanding, defined by nearby
utterances. In recent years, a number of approaches have been proposed for
various utterance-level dialogue understanding tasks. Most of these approaches
account for the context for effective understanding. In this paper, we explore
and quantify the role of context for different aspects of a dialogue, namely
emotion, intent, and dialogue act identification, using state-of-the-art dialog
understanding methods as baselines. Specifically, we employ various
perturbations to distort the context of a given utterance and study its impact
on the different tasks and baselines. This provides us with insights into the
fundamental contextual controlling factors of different aspects of a dialogue.
Such insights can inspire more effective dialogue understanding models, and
provide support for future text generation approaches. The implementation
pertaining to this work is available at
https://github.com/declare-lab/dialogue-understanding.",2020-09-29
iNLTK: Natural Language Toolkit for Indic Languages,2020-09-26 08:21:32+00:00,http://arxiv.org/abs/2009.12534v2,Gaurav Arora,cs.CL,table2text,"We present iNLTK, an open-source NLP library consisting of pre-trained
language models and out-of-the-box support for Data Augmentation, Textual
Similarity, Sentence Embeddings, Word Embeddings, Tokenization and Text
Generation in 13 Indic Languages. By using pre-trained models from iNLTK for
text classification on publicly available datasets, we significantly outperform
previously reported results. On these datasets, we also show that by using
pre-trained models and data augmentation from iNLTK, we can achieve more than
95% of the previous best performance by using less than 10% of the training
data. iNLTK is already being widely used by the community and has 40,000+
downloads, 600+ stars and 100+ forks on GitHub. The library is available at
https://github.com/goru001/inltk.",2020-09-26
"Language Generation with Multi-Hop Reasoning on Commonsense Knowledge
  Graph",2020-09-24 13:55:32+00:00,http://arxiv.org/abs/2009.11692v1,"Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, Minlie Huang",cs.CL,table2text,"Despite the success of generative pre-trained language models on a series of
text generation tasks, they still suffer in cases where reasoning over
underlying commonsense knowledge is required during generation. Existing
approaches that integrate commonsense knowledge into generative pre-trained
language models simply transfer relational knowledge by post-training on
individual knowledge triples while ignoring rich connections within the
knowledge graph. We argue that exploiting both the structural and semantic
information of the knowledge graph facilitates commonsense-aware text
generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow
(GRF) that enables pre-trained models with dynamic multi-hop reasoning on
multi-relational paths extracted from the external commonsense knowledge graph.
We empirically show that our model outperforms existing baselines on three text
generation tasks that require reasoning over commonsense knowledge. We also
demonstrate the effectiveness of the dynamic multi-hop reasoning module with
reasoning paths inferred by the model that provide rationale to the generation.",2020-09-24
"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language
  Models",2020-09-24 03:17:19+00:00,http://arxiv.org/abs/2009.11462v2,"Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, Noah A. Smith",cs.CL,table2text,"Pretrained neural language models (LMs) are prone to generating racist,
sexist, or otherwise toxic language which hinders their safe deployment. We
investigate the extent to which pretrained LMs can be prompted to generate
toxic language, and the effectiveness of controllable text generation
algorithms at preventing such toxic degeneration. We create and release
RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level
prompts derived from a large corpus of English web text, paired with toxicity
scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we
find that pretrained LMs can degenerate into toxic text even from seemingly
innocuous prompts. We empirically assess several controllable generation
methods, and find that while data- or compute-intensive methods (e.g., adaptive
pretraining on non-toxic data) are more effective at steering away from
toxicity than simpler solutions (e.g., banning ""bad"" words), no current method
is failsafe against neural toxic degeneration. To pinpoint the potential cause
of such persistent toxic degeneration, we analyze two web text corpora used to
pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a
significant amount of offensive, factually unreliable, and otherwise toxic
content. Our work provides a test bed for evaluating toxic generations by LMs
and stresses the need for better data selection processes for pretraining.",2020-09-24
Content Planning for Neural Story Generation with Aristotelian Rescoring,2020-09-21 13:41:32+00:00,http://arxiv.org/abs/2009.09870v2,"Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph Weischedel, Nanyun Peng","cs.CL, cs.AI",table2text,"Long-form narrative text generated from large language models manages a
fluent impersonation of human writing, but only at the local sentence level,
and lacks structure or global cohesion. We posit that many of the problems of
story generation can be addressed via high-quality content planning, and
present a system that focuses on how to learn good plot structures to guide
story generation. We utilize a plot-generation language model along with an
ensemble of rescoring models that each implement an aspect of good
story-writing as detailed in Aristotle's Poetics. We find that stories written
with our more principled plot-structure are both more relevant to a given
prompt and higher quality than baselines that do not content plan, or that plan
in an unprincipled way.",2020-09-21
Prior Art Search and Reranking for Generated Patent Text,2020-09-19 01:16:18+00:00,http://arxiv.org/abs/2009.09132v1,"Jieh-Sheng Lee, Jieh Hsiang",cs.CL,table2text,"Generative models, such as GPT-2, have demonstrated impressive results
recently. A fundamental question we'd like to address is: where did the
generated text come from? This work is our initial effort toward answering the
question by using prior art search. The purpose of the prior art search is to
find the most similar prior text in the training data of GPT-2. We take a
reranking approach and apply it to the patent domain. Specifically, we
pre-train GPT-2 models from scratch by using the patent data from the USPTO.
The input for the prior art search is the patent text generated by the GPT-2
model. We also pre-trained BERT models from scratch for converting patent text
to embeddings. The steps of reranking are: (1) search the most similar text in
the training data of GPT-2 by taking a bag-of-word ranking approach (BM25), (2)
convert the search results in text format to BERT embeddings, and (3) provide
the final result by ranking the BERT embeddings based on their similarities
with the patent text generated by GPT-2. The experiments in this work show that
such reranking is better than ranking with embeddings alone. However, our mixed
results also indicate that calculating the semantic similarities among long
text spans is still challenging. To our knowledge, this work is the first to
implement a reranking system to identify retrospectively the most similar
inputs to a GPT model based on its output.",2020-09-19
Text Generation by Learning from Off-Policy Demonstrations,2020-09-16 17:58:37+00:00,http://arxiv.org/abs/2009.07839v1,"Richard Yuanzhe Pang, He He","cs.CL, cs.LG",table2text,"Current approaches to text generation largely rely on autoregressive models
and maximum likelihood estimation. This paradigm leads to (i) diverse but
low-quality samples due to mismatched learning objective and evaluation metric
(likelihood vs. quality) and (ii) exposure bias due to mismatched history
distributions (gold vs. model-generated). To alleviate these problems, we frame
text generation as a reinforcement learning (RL) problem with expert
demonstrations (i.e., the training data), where the goal is to maximize quality
given model-generated histories. Prior RL approaches to generation often face
optimization issues due to the large action space and sparse reward. We propose
GOLD (generation by off-policy learning from demonstrations): an algorithm that
learns from the off-policy demonstrations by importance weighting and does not
suffer from degenerative solutions. We find that GOLD outperforms the baselines
according to automatic and human evaluation on summarization, question
generation, and machine translation, including attaining state-of-the-art
results for CNN/DailyMail summarization. Further, we show that models trained
by GOLD are less sensitive to decoding algorithms and the generation quality
does not degrade much as the length increases.",2020-09-16
Knowledge Graphs for Multilingual Language Translation and Generation,2020-09-16 14:36:41+00:00,http://arxiv.org/abs/2009.07715v1,Diego Moussallem,cs.CL,table2text,"The Natural Language Processing (NLP) community has recently seen outstanding
progress, catalysed by the release of different Neural Network (NN)
architectures. Neural-based approaches have proven effective by significantly
increasing the output quality of a large number of automated solutions for NLP
tasks (Belinkov and Glass, 2019). Despite these notable advancements, dealing
with entities still poses a difficult challenge as they are rarely seen in
training data. Entities can be classified into two groups, i.e., proper nouns
and common nouns. Proper nouns are also known as Named Entities (NE) and
correspond to the name of people, organizations, or locations, e.g., John, WHO,
or Canada. Common nouns describe classes of objects, e.g., spoon or cancer.
Both types of entities can be found in a Knowledge Graph (KG). Recent work has
successfully exploited the contribution of KGs in NLP tasks, such as Natural
Language Inference (NLI) (KM et al.,2018) and Question Answering (QA) (Sorokin
and Gurevych, 2018). Only a few works had exploited the benefits of KGs in
Neural Machine Translation (NMT) when the work presented herein began.
Additionally, few works had studied the contribution of KGs to Natural Language
Generation (NLG) tasks. Moreover, the multilinguality also remained an open
research area in these respective tasks (Young et al., 2018). In this thesis,
we focus on the use of KGs for machine translation and the generation of texts
to deal with the problems caused by entities and consequently enhance the
quality of automatically generated texts.",2020-09-16
A Comparison of LSTM and BERT for Small Corpus,2020-09-11 14:01:14+00:00,http://arxiv.org/abs/2009.05451v1,Aysu Ezen-Can,"cs.CL, cs.LG",table2text,"Recent advancements in the NLP field showed that transfer learning helps with
achieving state-of-the-art results for new tasks by tuning pre-trained models
instead of starting from scratch. Transformers have made a significant
improvement in creating new state-of-the-art results for many NLP tasks
including but not limited to text classification, text generation, and sequence
labeling. Most of these success stories were based on large datasets. In this
paper we focus on a real-life scenario that scientists in academia and industry
face frequently: given a small dataset, can we use a large pre-trained model
like BERT and get better results than simple models? To answer this question,
we use a small dataset for intent classification collected for building
chatbots and compare the performance of a simple bidirectional LSTM model with
a pre-trained BERT model. Our experimental results show that bidirectional LSTM
models can achieve significantly higher results than a BERT model for a small
dataset and these simple models get trained in much less time than tuning the
pre-trained counterparts. We conclude that the performance of a model is
dependent on the task and the data, and therefore before making a model choice,
these factors should be taken into consideration instead of directly choosing
the most popular model.",2020-09-11
Modern Methods for Text Generation,2020-09-10 16:17:10+00:00,http://arxiv.org/abs/2009.04968v1,Dimas Munoz Montesinos,"cs.CL, cs.LG",table2text,"Synthetic text generation is challenging and has limited success. Recently, a
new architecture, called Transformers, allow machine learning models to
understand better sequential data, such as translation or summarization. BERT
and GPT-2, using Transformers in their cores, have shown a great performance in
tasks such as text classification, translation and NLI tasks. In this article,
we analyse both algorithms and compare their output quality in text generation
tasks.",2020-09-10
Leam: An Interactive System for In-situ Visual Text Analysis,2020-09-08 05:18:29+00:00,http://arxiv.org/abs/2009.03520v1,"Sajjadur Rahman, Peter Griggs, √áaƒüatay Demiralp","cs.DB, cs.CL, cs.HC",table2text,"With the increase in scale and availability of digital text generated on the
web, enterprises such as online retailers and aggregators often use text
analytics to mine and analyze the data to improve their services and products
alike. Text data analysis is an iterative, non-linear process with diverse
workflows spanning multiple stages, from data cleaning to visualization.
Existing text analytics systems usually accommodate a subset of these stages
and often fail to address challenges related to data heterogeneity, provenance,
workflow reusability and reproducibility, and compatibility with established
practices. Based on a set of design considerations we derive from these
challenges, we propose Leam, a system that treats the text analysis process as
a single continuum by combining advantages of computational notebooks,
spreadsheets, and visualization tools. Leam features an interactive user
interface for running text analysis workflows, a new data model for managing
multiple atomic and composite data types, and an expressive algebra that
captures diverse sets of operations representing various stages of text
analysis and enables coordination among different components of the system,
including data, code, and visualizations. We report our current progress in
Leam development while demonstrating its usefulness with usage examples.
Finally, we outline a number of enhancements to Leam and identify several
research directions for developing an interactive visual text analysis system.",2020-09-08
Robust Conversational AI with Grounded Text Generation,2020-09-07 23:49:28+00:00,http://arxiv.org/abs/2009.03457v1,"Jianfeng Gao, Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh, Lars Liden, Heung-Yeung Shum","cs.AI, cs.CL",table2text,"This article presents a hybrid approach based on a Grounded Text Generation
(GTG) model to building robust task bots at scale. GTG is a hybrid model which
uses a large-scale Transformer neural network as its backbone, combined with
symbol-manipulation modules for knowledge base inference and prior knowledge
encoding, to generate responses grounded in dialog belief state and real-world
knowledge for task completion. GTG is pre-trained on large amounts of raw text
and human conversational data, and can be fine-tuned to complete a wide range
of tasks.
  The hybrid approach and its variants are being developed simultaneously by
multiple research teams. The primary results reported on task-oriented dialog
benchmarks are very promising, demonstrating the big potential of this
approach. This article provides an overview of this progress and discusses
related methods and technologies that can be incorporated for building robust
conversational AI systems.",2020-09-07
"Black Box to White Box: Discover Model Characteristics Based on
  Strategic Probing",2020-09-07 14:44:28+00:00,http://arxiv.org/abs/2009.03136v1,"Josh Kalin, Matthew Ciolino, David Noever, Gerry Dozier","cs.LG, stat.ML",table2text,"In Machine Learning, White Box Adversarial Attacks rely on knowing underlying
knowledge about the model attributes. This works focuses on discovering to
distrinct pieces of model information: the underlying architecture and primary
training dataset. With the process in this paper, a structured set of input
probes and the output of the model become the training data for a deep
classifier. Two subdomains in Machine Learning are explored: image based
classifiers and text transformers with GPT-2. With image classification, the
focus is on exploring commonly deployed architectures and datasets available in
popular public libraries. Using a single transformer architecture with multiple
levels of parameters, text generation is explored by fine tuning off different
datasets. Each dataset explored in image and text are distinguishable from one
another. Diversity in text transformer outputs implies further research is
needed to successfully classify architecture attribution in text domain.",2020-09-07
"Adversarial Watermarking Transformer: Towards Tracing Text Provenance
  with Data Hiding",2020-09-07 11:01:24+00:00,http://arxiv.org/abs/2009.03015v1,"Sahar Abdelnabi, Mario Fritz","cs.CR, cs.CL, cs.CY, cs.LG, I.2.7",table2text,"Recent advances in natural language generation have introduced powerful
language models with high-quality output text. However, this raises concerns
about the potential misuse of such models for malicious purposes. In this
paper, we study natural language watermarking as a defense to help better mark
and trace the provenance of text. We introduce the Adversarial Watermarking
Transformer (AWT) with a jointly trained encoder-decoder and adversarial
training that, given an input text and a binary message, generates an output
text that is unobtrusively encoded with the given message. We further study
different training and inference strategies to achieve minimal changes to the
semantics and correctness of the input text. AWT is the first end-to-end model
to hide data in text by automatically learning -- without ground truth -- word
substitutions along with their locations in order to encode the message. We
show that our model is effective in largely preserving text utility and
decoding the watermark while hiding its presence against adversaries.
Additionally, we demonstrate that our method is robust against a range of local
changes and denoising attacks.",2020-09-07
"Tweet to News Conversion: An Investigation into Unsupervised
  Controllable Text Generation",2020-08-21 06:56:57+00:00,http://arxiv.org/abs/2008.09333v1,"Zishan Ahmad, Mukuntha N S, Asif Ekbal, Pushpak Bhattacharyya","cs.CL, cs.LG",table2text,"Text generator systems have become extremely popular with the advent of
recent deep learning models such as encoder-decoder. Controlling the
information and style of the generated output without supervision is an
important and challenging Natural Language Processing (NLP) task. In this
paper, we define the task of constructing a coherent paragraph from a set of
disaster domain tweets, without any parallel data. We tackle the problem by
building two systems in pipeline. The first system focuses on unsupervised
style transfer and converts the individual tweets into news sentences. The
second system stitches together the outputs from the first system to form a
coherent news paragraph. We also propose a novel training mechanism, by
splitting the sentences into propositions and training the second system to
merge the sentences. We create a validation and test set consisting of
tweet-sets and their equivalent news paragraphs to perform empirical
evaluation. In a completely unsupervised setting, our model was able to achieve
a BLEU score of 19.32, while successfully transferring styles and joining
tweets to form a meaningful news paragraph.",2020-08-21
"Learning to Create Better Ads: Generation and Ranking Approaches for Ad
  Creative Refinement",2020-08-17 16:46:28+00:00,http://arxiv.org/abs/2008.07467v2,"Shaunak Mishra, Manisha Verma, Yichao Zhou, Kapil Thadani, Wei Wang","cs.CL, cs.IR, cs.LG",table2text,"In the online advertising industry, the process of designing an ad creative
(i.e., ad text and image) requires manual labor. Typically, each advertiser
launches multiple creatives via online A/B tests to infer effective creatives
for the target audience, that are then refined further in an iterative fashion.
Due to the manual nature of this process, it is time-consuming to learn,
refine, and deploy the modified creatives. Since major ad platforms typically
run A/B tests for multiple advertisers in parallel, we explore the possibility
of collaboratively learning ad creative refinement via A/B tests of multiple
advertisers. In particular, given an input ad creative, we study approaches to
refine the given ad text and image by: (i) generating new ad text, (ii)
recommending keyphrases for new ad text, and (iii) recommending image tags
(objects in image) to select new ad image. Based on A/B tests conducted by
multiple advertisers, we form pairwise examples of inferior and superior ad
creatives, and use such pairs to train models for the above tasks. For
generating new ad text, we demonstrate the efficacy of an encoder-decoder
architecture with copy mechanism, which allows some words from the (inferior)
input text to be copied to the output while incorporating new words associated
with higher click-through-rate. For the keyphrase and image tag recommendation
task, we demonstrate the efficacy of a deep relevance matching model, as well
as the relative robustness of ranking approaches compared to ad text generation
in cold-start scenarios with unseen advertisers. We also share broadly
applicable insights from our experiments using data from the Yahoo Gemini ad
platform.",2020-08-17
Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems,2020-08-14 08:23:21+00:00,http://arxiv.org/abs/2008.06239v2,"Andrea Madotto, Zihan Liu, Zhaojiang Lin, Pascale Fung","cs.CL, cs.LG",table2text,"Task-oriented dialogue systems use four connected modules, namely, Natural
Language Understanding (NLU), a Dialogue State Tracking (DST), Dialogue Policy
(DP) and Natural Language Generation (NLG). A research challenge is to learn
each module with the least amount of samples (i.e., few-shots) given the high
cost related to the data collection. The most common and effective technique to
solve this problem is transfer learning, where large language models, either
pre-trained on text or task-specific data, are fine-tuned on the few samples.
These methods require fine-tuning steps and a set of parameters for each task.
Differently, language models, such as GPT-2 (Radford et al., 2019) and GPT-3
(Brown et al., 2020), allow few-shot learning by priming the model with few
examples. In this paper, we evaluate the priming few-shot ability of language
models in the NLU, DST, DP and NLG tasks. Importantly, we highlight the current
limitations of this approach, and we discuss the possible implication for
future work.",2020-08-14
"The Language Interpretability Tool: Extensible, Interactive
  Visualizations and Analysis for NLP Models",2020-08-12 06:07:44+00:00,http://arxiv.org/abs/2008.05122v1,"Ian Tenney, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy Coenen, Sebastian Gehrmann, Ellen Jiang, Mahima Pushkarna, Carey Radebaugh, Emily Reif, Ann Yuan",cs.CL,table2text,"We present the Language Interpretability Tool (LIT), an open-source platform
for visualization and understanding of NLP models. We focus on core questions
about model behavior: Why did my model make this prediction? When does it
perform poorly? What happens under a controlled change in the input? LIT
integrates local explanations, aggregate analysis, and counterfactual
generation into a streamlined, browser-based interface to enable rapid
exploration and error analysis. We include case studies for a diverse set of
workflows, including exploring counterfactuals for sentiment analysis,
measuring gender bias in coreference systems, and exploring local behavior in
text generation. LIT supports a wide range of models--including classification,
seq2seq, and structured prediction--and is highly extensible through a
declarative, framework-agnostic API. LIT is under active development, with code
and full documentation available at https://github.com/pair-code/lit.",2020-08-12
Navigating Human Language Models with Synthetic Agents,2020-08-10 14:39:53+00:00,http://arxiv.org/abs/2008.04162v7,"Philip Feldman, Antonio Bucchiarone","cs.AI, cs.CL, cs.MA, I.2; I.6; J.4",table2text,"Modern natural language models such as the GPT-2/GPT-3 contain tremendous
amounts of information about human belief in a consistently testable form. If
these models could be shown to accurately reflect the underlying beliefs of the
human beings that produced the data used to train these models, then such
models become a powerful sociological tool in ways that are distinct from
traditional methods, such as interviews and surveys. In this study, We train a
version of the GPT-2 on a corpora of historical chess games, and then ""launch""
clusters of synthetic agents into the model, using text strings to create
context and orientation. We compare the trajectories contained in the text
generated by the agents/model and compare that to the known ground truth of the
chess board, move legality, and historical patterns of play. We find that the
percentages of moves by piece using the model are substantially similar from
human patterns. We further find that the model creates an accurate latent
representation of the chessboard, and that it is possible to plot trajectories
of legal moves across the board using this knowledge.",2020-08-10
"Neural Language Generation: Formulation, Methods, and Evaluation",2020-07-31 00:08:28+00:00,http://arxiv.org/abs/2007.15780v1,"Cristina Garbacea, Qiaozhu Mei","cs.CL, cs.AI, cs.LG",table2text,"Recent advances in neural network-based generative modeling have reignited
the hopes in having computer systems capable of seamlessly conversing with
humans and able to understand natural language. Neural architectures have been
employed to generate text excerpts to various degrees of success, in a
multitude of contexts and tasks that fulfil various user needs. Notably, high
capacity deep learning models trained on large scale datasets demonstrate
unparalleled abilities to learn patterns in the data even in the lack of
explicit supervision signals, opening up a plethora of new possibilities
regarding producing realistic and coherent texts. While the field of natural
language generation is evolving rapidly, there are still many open challenges
to address. In this survey we formally define and categorize the problem of
natural language generation. We review particular application tasks that are
instantiations of these general formulations, in which generating natural
language is of practical importance. Next we include a comprehensive outline of
methods and neural architectures employed for generating diverse texts.
Nevertheless, there is no standard way to assess the quality of text produced
by these generative models, which constitutes a serious bottleneck towards the
progress of the field. To this end, we also review current approaches to
evaluating natural language generation systems. We hope this survey will
provide an informative overview of formulations, methods, and assessments of
neural natural language generation.",2020-07-31
Multimodal Dialogue State Tracking By QA Approach with Data Augmentation,2020-07-20 06:23:18+00:00,http://arxiv.org/abs/2007.09903v1,"Xiangyang Mou, Brandyn Sigouin, Ian Steenstra, Hui Su",cs.CL,table2text,"Recently, a more challenging state tracking task, Audio-Video Scene-Aware
Dialogue (AVSD), is catching an increasing amount of attention among
researchers. Different from purely text-based dialogue state tracking, the
dialogue in AVSD contains a sequence of question-answer pairs about a video and
the final answer to the given question requires additional understanding of the
video. This paper interprets the AVSD task from an open-domain Question
Answering (QA) point of view and proposes a multimodal open-domain QA system to
deal with the problem. The proposed QA system uses common encoder-decoder
framework with multimodal fusion and attention. Teacher forcing is applied to
train a natural language generator. We also propose a new data augmentation
approach specifically under QA assumption. Our experiments show that our model
and techniques bring significant improvements over the baseline model on the
DSTC7-AVSD dataset and demonstrate the potentials of our data augmentation
techniques.",2020-07-20
Investigating Pretrained Language Models for Graph-to-Text Generation,2020-07-16 16:05:34+00:00,http://arxiv.org/abs/2007.08426v2,"Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Sch√ºtze, Iryna Gurevych",cs.CL,table2text,"Graph-to-text generation aims to generate fluent texts from graph-based data.
In this paper, we investigate two recently proposed pretrained language models
(PLMs) and analyze the impact of different task-adaptive pretraining strategies
for PLMs in graph-to-text generation. We present a study across three graph
domains: meaning representations, Wikipedia knowledge graphs (KGs) and
scientific KGs. We show that the PLMs BART and T5 achieve new state-of-the-art
results and that task-adaptive pretraining strategies improve their performance
even further. In particular, we report new state-of-the-art BLEU scores of
49.72 on LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative
improvement of 31.8%, 4.5%, and 42.4%, respectively. In an extensive analysis,
we identify possible reasons for the PLMs' success on graph-to-text tasks. We
find evidence that their knowledge about true facts helps them perform well
even when the input graph representation is reduced to a simple bag of node and
edge labels.",2020-07-16
"Modeling Coherency in Generated Emails by Leveraging Deep Neural
  Learners",2020-07-14 23:47:08+00:00,http://arxiv.org/abs/2007.07403v1,"Avisha Das, Rakesh M. Verma",cs.CL,table2text,"Advanced machine learning and natural language techniques enable attackers to
launch sophisticated and targeted social engineering-based attacks. To counter
the active attacker issue, researchers have since resorted to proactive methods
of detection. Email masquerading using targeted emails to fool the victim is an
advanced attack method. However automatic text generation requires controlling
the context and coherency of the generated content, which has been identified
as an increasingly difficult problem. The method used leverages a hierarchical
deep neural model which uses a learned representation of the sentences in the
input document to generate structured written emails. We demonstrate the
generation of short and targeted text messages using the deep model. The global
coherency of the synthesized text is evaluated using a qualitative study as
well as multiple quantitative measures.",2020-07-14
"Deep Transformer based Data Augmentation with Subword Units for
  Morphologically Rich Online ASR",2020-07-14 10:22:05+00:00,http://arxiv.org/abs/2007.06949v3,"Bal√°zs Tarj√°n, Gy√∂rgy Szasz√°k, Tibor Fegy√≥, P√©ter Mihajlik","eess.AS, cs.CL",table2text,"Recently Deep Transformer models have proven to be particularly powerful in
language modeling tasks for ASR. Their high complexity, however, makes them
very difficult to apply in the first (single) pass of an online system. Recent
studies showed that a considerable part of the knowledge of neural network
Language Models (LM) can be transferred to traditional n-grams by using neural
text generation based data augmentation. In our paper, we pre-train a GPT-2
Transformer LM on a general text corpus and fine-tune it on our Hungarian
conversational call center ASR task. We show that although data augmentation
with Transformer-generated text works well for isolating languages, it causes a
vocabulary explosion in a morphologically rich language. Therefore, we propose
a new method called subword-based neural text augmentation, where we retokenize
the generated text into statistically derived subwords. We compare Morfessor
and BPE statistical subword tokenizers and show that both methods can
significantly improve the WER while greatly reducing vocabulary size and memory
requirements. Finally, we also demonstrate that subword-based neural text
augmentation outperforms the word-based approach not only in terms of overall
WER but also in recognition of OOV words.",2020-07-14
